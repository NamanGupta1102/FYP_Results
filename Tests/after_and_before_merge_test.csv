after_merge,before_merge
"def cmdb_get_mainline_object_topo(request, bk_biz_id, bk_supplier_account=''):
    """"""
    @summary: 获取配置平台业务拓扑模型
    @param request:
    @param bk_biz_id:
    @param bk_supplier_account:
    @return:
    """"""
    kwargs = {
        'bk_biz_id': bk_biz_id,
        'bk_supplier_account': bk_supplier_account,
    }
    client = get_client_by_user(request.user.username)
    cc_result = client.cc.get_mainline_object_topo(kwargs)
    if not cc_result['result']:
        message = handle_api_error(_(u""配置平台(CMDB)""),
                                   'cc.get_mainline_object_topo',
                                   kwargs,
                                   cc_result['message'])
        return {'result': cc_result['result'], 'code': cc_result['code'], 'message': message}
    data = cc_result['data']
    for bk_obj in data:
        if bk_obj['bk_obj_id'] == 'host':
            bk_obj['bk_obj_name'] = 'IP'
    result = {'result': cc_result['result'], 'code': cc_result['code'], 'data': cc_result['data']}
    return JsonResponse(result)","def cmdb_get_mainline_object_topo(request, bk_biz_id, bk_supplier_account=''):
    """"""
    @summary: 获取配置平台业务拓扑模型
    @param request:
    @param bk_biz_id:
    @param bk_supplier_account:
    @return:
    """"""
    kwargs = {
        'bk_biz_id': bk_biz_id,
        'bk_supplier_account': bk_supplier_account,
    }
    client = get_client_by_request(request)
    cc_result = client.cc.get_mainline_object_topo(kwargs)
    if not cc_result['result']:
        message = handle_api_error(_(u""配置平台(CMDB)""),
                                   'cc.get_mainline_object_topo',
                                   kwargs,
                                   cc_result['message'])
        return {'result': cc_result['result'], 'code': cc_result['code'], 'message': message}
    data = cc_result['data']
    for bk_obj in data:
        if bk_obj['bk_obj_id'] == 'host':
            bk_obj['bk_obj_name'] = 'IP'
    result = {'result': cc_result['result'], 'code': cc_result['code'], 'data': cc_result['data']}
    return JsonResponse(result)"
"def cc_search_object_attribute(request, obj_id, biz_cc_id, supplier_account):
    """"""
    @summary: 获取对象自定义属性
    @param request:
    @param biz_cc_id:
    @return:
    """"""
    client = get_client_by_user(request.user.username)
    kwargs = {
        'bk_obj_id': obj_id,
        'bk_supplier_account': supplier_account
    }
    cc_result = client.cc.search_object_attribute(kwargs)
    if not cc_result['result']:
        message = handle_api_error('cc', 'cc.search_object_attribute', kwargs, cc_result['message'])
        logger.error(message)
        result = {
            'result': False,
            'data': [],
            'message': message
        }
        return JsonResponse(result)

    obj_property = []
    for item in cc_result['data']:
        if item['editable']:
            obj_property.append({
                'value': item['bk_property_id'],
                'text': item['bk_property_name']
            })

    return JsonResponse({'result': True, 'data': obj_property})","def cc_search_object_attribute(request, obj_id, biz_cc_id, supplier_account):
    """"""
    @summary: 获取对象自定义属性
    @param request:
    @param biz_cc_id:
    @return:
    """"""
    client = get_client_by_request(request)
    kwargs = {
        'bk_obj_id': obj_id,
        'bk_supplier_account': supplier_account
    }
    cc_result = client.cc.search_object_attribute(kwargs)
    if not cc_result['result']:
        message = handle_api_error('cc', 'cc.search_object_attribute', kwargs, cc_result['message'])
        logger.error(message)
        result = {
            'result': False,
            'data': [],
            'message': message
        }
        return JsonResponse(result)

    obj_property = []
    for item in cc_result['data']:
        if item['editable']:
            obj_property.append({
                'value': item['bk_property_id'],
                'text': item['bk_property_name']
            })

    return JsonResponse({'result': True, 'data': obj_property})"
"def cc_search_create_object_attribute(request, obj_id, biz_cc_id, supplier_account):
    client = get_client_by_user(request.user.username)
    kwargs = {
        'bk_obj_id': obj_id,
        'bk_supplier_account': supplier_account
    }
    cc_result = client.cc.search_object_attribute(kwargs)
    if not cc_result['result']:
        message = handle_api_error('cc', 'cc.search_object_attribute', kwargs, cc_result['message'])
        logger.error(message)
        result = {
            'result': False,
            'data': [],
            'message': message
        }
        return JsonResponse(result)

    obj_property = []
    for item in cc_result['data']:
        if item['editable']:
            prop_dict = {
                'tag_code': item['bk_property_id'],
                'type': ""input"",
                'attrs': {
                    'name': item['bk_property_name'],
                    'editable': 'true',
                },
            }
            if item['bk_property_id'] in ['bk_set_name']:
                prop_dict[""attrs""][""validation""] = [
                    {
                        ""type"": ""required""
                    }
                ]
            obj_property.append(prop_dict)

    return JsonResponse({'result': True, 'data': obj_property})","def cc_search_create_object_attribute(request, obj_id, biz_cc_id, supplier_account):
    client = get_client_by_request(request)
    kwargs = {
        'bk_obj_id': obj_id,
        'bk_supplier_account': supplier_account
    }
    cc_result = client.cc.search_object_attribute(kwargs)
    if not cc_result['result']:
        message = handle_api_error('cc', 'cc.search_object_attribute', kwargs, cc_result['message'])
        logger.error(message)
        result = {
            'result': False,
            'data': [],
            'message': message
        }
        return JsonResponse(result)

    obj_property = []
    for item in cc_result['data']:
        if item['editable']:
            prop_dict = {
                'tag_code': item['bk_property_id'],
                'type': ""input"",
                'attrs': {
                    'name': item['bk_property_name'],
                    'editable': 'true',
                },
            }
            if item['bk_property_id'] in ['bk_set_name']:
                prop_dict[""attrs""][""validation""] = [
                    {
                        ""type"": ""required""
                    }
                ]
            obj_property.append(prop_dict)

    return JsonResponse({'result': True, 'data': obj_property})"
"def cc_search_topo(request, obj_id, category, biz_cc_id, supplier_account):
    """"""
    @summary: 查询对象拓扑
    @param request:
    @param biz_cc_id:
    @return:
    """"""
    client = get_client_by_user(request.user.username)
    kwargs = {
        'bk_biz_id': biz_cc_id,
        'bk_supplier_account': supplier_account
    }
    cc_result = client.cc.search_biz_inst_topo(kwargs)
    if not cc_result['result']:
        message = handle_api_error('cc', 'cc.search_biz_inst_topo', kwargs, cc_result['message'])
        logger.error(message)
        result = {
            'result': False,
            'data': [],
            'message': message
        }
        return JsonResponse(result)

    if category in [""normal"", ""prev"", ""picker""]:
        cc_topo = cc_format_topo_data(cc_result['data'], obj_id, category)
    else:
        cc_topo = []

    return JsonResponse({'result': True, 'data': cc_topo})","def cc_search_topo(request, obj_id, category, biz_cc_id, supplier_account):
    """"""
    @summary: 查询对象拓扑
    @param request:
    @param biz_cc_id:
    @return:
    """"""
    client = get_client_by_request(request)
    kwargs = {
        'bk_biz_id': biz_cc_id,
        'bk_supplier_account': supplier_account
    }
    cc_result = client.cc.search_biz_inst_topo(kwargs)
    if not cc_result['result']:
        message = handle_api_error('cc', 'cc.search_biz_inst_topo', kwargs, cc_result['message'])
        logger.error(message)
        result = {
            'result': False,
            'data': [],
            'message': message
        }
        return JsonResponse(result)

    if category in [""normal"", ""prev"", ""picker""]:
        cc_topo = cc_format_topo_data(cc_result['data'], obj_id, category)
    else:
        cc_topo = []

    return JsonResponse({'result': True, 'data': cc_topo})"
"def job_get_script_list(request, biz_cc_id):
    """"""
    查询业务脚本列表
    :param request:
    :param biz_cc_id:
    :return:
    """"""
    # 查询脚本列表
    client = get_client_by_user(request.user.username)
    script_type = request.GET.get('type')
    kwargs = {
        'bk_biz_id': biz_cc_id,
        'is_public': True if script_type == 'public' else False
    }
    script_result = client.job.get_script_list(kwargs)

    if not script_result['result']:
        message = handle_api_error('cc', 'job.get_script_list', kwargs, script_result['message'])
        logger.error(message)
        result = {
            'result': False,
            'message': message
        }
        return JsonResponse(result)

    script_dict = {}
    for script in script_result['data']['data']:
        script_dict.setdefault(script['name'], []).append(script['id'])

    version_data = []
    for name, version in script_dict.items():
        version_data.append({
            ""text"": name,
            ""value"": max(version)
        })

    return JsonResponse({'result': True, 'data': version_data})","def job_get_script_list(request, biz_cc_id):
    """"""
    查询业务脚本列表
    :param request:
    :param biz_cc_id:
    :return:
    """"""
    # 查询脚本列表
    client = get_client_by_request(request)
    script_type = request.GET.get('type')
    kwargs = {
        'bk_biz_id': biz_cc_id,
        'is_public': True if script_type == 'public' else False
    }
    script_result = client.job.get_script_list(kwargs)

    if not script_result['result']:
        message = handle_api_error('cc', 'job.get_script_list', kwargs, script_result['message'])
        logger.error(message)
        result = {
            'result': False,
            'message': message
        }
        return JsonResponse(result)

    script_dict = {}
    for script in script_result['data']['data']:
        script_dict.setdefault(script['name'], []).append(script['id'])

    version_data = []
    for name, version in script_dict.items():
        version_data.append({
            ""text"": name,
            ""value"": max(version)
        })

    return JsonResponse({'result': True, 'data': version_data})"
"def job_get_job_tasks_by_biz(request, biz_cc_id):
    client = get_client_by_user(request.user.username)
    job_result = client.job.get_job_list({'bk_biz_id': biz_cc_id})
    if not job_result['result']:
        message = _(u""查询作业平台(JOB)的作业模板[app_id=%s]接口job.get_task返回失败: %s"") % (
            biz_cc_id, job_result['message'])
        logger.error(message)
        result = {
            'result': False,
            'data': [],
            'message': message
        }
        return JsonResponse(result)
    task_list = []
    for task in job_result['data']:
        task_list.append({
            'value': task['bk_job_id'],
            'text': task['name'],
        })
    return JsonResponse({'result': True, 'data': task_list})","def job_get_job_tasks_by_biz(request, biz_cc_id):
    client = get_client_by_request(request)
    job_result = client.job.get_job_list({'bk_biz_id': biz_cc_id})
    if not job_result['result']:
        message = _(u""查询作业平台(JOB)的作业模板[app_id=%s]接口job.get_task返回失败: %s"") % (
            biz_cc_id, job_result['message'])
        logger.error(message)
        result = {
            'result': False,
            'data': [],
            'message': message
        }
        return JsonResponse(result)
    task_list = []
    for task in job_result['data']:
        task_list.append({
            'value': task['bk_job_id'],
            'text': task['name'],
        })
    return JsonResponse({'result': True, 'data': task_list})"
"def job_get_job_task_detail(request, biz_cc_id, task_id):
    client = get_client_by_user(request.user.username)
    job_result = client.job.get_job_detail({'bk_biz_id': biz_cc_id,
                                            'bk_job_id': task_id})
    if not job_result['result']:
        message = _(u""查询作业平台(JOB)的作业模板详情[app_id=%s]接口job.get_task_detail返回失败: %s"") % (
            biz_cc_id, job_result['message'])
        logger.error(message)
        result = {
            'result': False,
            'data': [],
            'message': message
        }
        return JsonResponse(result)

    job_step_type_name = {
        1: _(u""脚本""),
        2: _(u""文件""),
        4: u""SQL""
    }
    task_detail = job_result['data']
    global_var = []
    steps = []
    for var in task_detail.get('global_vars', []):
        # 1-字符串, 2-IP, 3-索引数组, 4-关联数组
        if var['type'] in [JOB_VAR_TYPE_STR, JOB_VAR_TYPE_IP, JOB_VAR_TYPE_ARRAY]:
            value = var.get('value', '')
        else:
            value = ['{plat_id}:{ip}'.format(plat_id=ip_item['bk_cloud_id'], ip=ip_item['ip'])
                     for ip_item in var.get('ip_list', [])]
        global_var.append({
            'id': var['id'],
            # 全局变量类型：1:云参, 2:上下文参数，3:IP
            'category': var.get('category', 1),
            'name': var['name'],
            'type': var['type'],
            'value': value,
            'description': var['description']
        })
    for info in task_detail.get('steps', []):
        # 1-执行脚本, 2-传文件, 4-传SQL
        steps.append({
            'stepId': info['step_id'],
            'name': info['name'],
            'scriptParams': info.get('script_param', ''),
            'account': info.get('account', ''),
            'ipList': '',
            'type': info['type'],
            'type_name': job_step_type_name.get(info['type'], info['type'])
        })
    return JsonResponse({'result': True, 'data': {'global_var': global_var, 'steps': steps}})","def job_get_job_task_detail(request, biz_cc_id, task_id):
    client = get_client_by_request(request)
    job_result = client.job.get_job_detail({'bk_biz_id': biz_cc_id,
                                            'bk_job_id': task_id})
    if not job_result['result']:
        message = _(u""查询作业平台(JOB)的作业模板详情[app_id=%s]接口job.get_task_detail返回失败: %s"") % (
            biz_cc_id, job_result['message'])
        logger.error(message)
        result = {
            'result': False,
            'data': [],
            'message': message
        }
        return JsonResponse(result)

    job_step_type_name = {
        1: _(u""脚本""),
        2: _(u""文件""),
        4: u""SQL""
    }
    task_detail = job_result['data']
    global_var = []
    steps = []
    for var in task_detail.get('global_vars', []):
        # 1-字符串, 2-IP, 3-索引数组, 4-关联数组
        if var['type'] in [JOB_VAR_TYPE_STR, JOB_VAR_TYPE_IP, JOB_VAR_TYPE_ARRAY]:
            value = var.get('value', '')
        else:
            value = ['{plat_id}:{ip}'.format(plat_id=ip_item['bk_cloud_id'], ip=ip_item['ip'])
                     for ip_item in var.get('ip_list', [])]
        global_var.append({
            'id': var['id'],
            # 全局变量类型：1:云参, 2:上下文参数，3:IP
            'category': var.get('category', 1),
            'name': var['name'],
            'type': var['type'],
            'value': value,
            'description': var['description']
        })
    for info in task_detail.get('steps', []):
        # 1-执行脚本, 2-传文件, 4-传SQL
        steps.append({
            'stepId': info['step_id'],
            'name': info['name'],
            'scriptParams': info.get('script_param', ''),
            'account': info.get('account', ''),
            'ipList': '',
            'type': info['type'],
            'type_name': job_step_type_name.get(info['type'], info['type'])
        })
    return JsonResponse({'result': True, 'data': {'global_var': global_var, 'steps': steps}})"
"def get_bk_user(request):
    bkuser = None
    if request.weixin_user and not isinstance(request.weixin_user, AnonymousUser):
        user_model = get_user_model()
        try:
            user_property = UserProperty.objects.get(key='wx_userid', value=request.weixin_user.userid)
        except UserProperty.DoesNotExist:
            logger.warning('user[wx_userid=%s] not in UserProperty' % request.weixin_user.userid)
        else:
            bkuser = user_model.objects.get(username=user_property.user.username)
    return bkuser or AnonymousUser()","def get_bk_user(request):
    bkuser = None
    if request.weixin_user and not isinstance(request.weixin_user, AnonymousUser):
        try:
            user_property = UserProperty.objects.get(key='wx_userid', value=request.weixin_user.userid)
            bkuser = user_property.user
        except UserProperty.DoesNotExist:
            bkuser = None
    return bkuser or AnonymousUser()"
"    def fit(self, dataset: Dataset):
        """"""Calculates statistics for this workflow on the input dataset

        Parameters
        -----------
        dataset: Dataset
            The input dataset to calculate statistics for. If there is a train/test split this
            data should be the training dataset only.
        """"""
        self._clear_worker_cache()
        ddf = dataset.to_ddf(columns=self._input_columns())

        # Get a dictionary mapping all StatOperators we need to fit to a set of any dependant
        # StatOperators (having StatOperators that depend on the output of other StatOperators
        # means that will have multiple phases in the fit cycle here)
        stat_ops = {op: _get_stat_ops(op.parents) for op in _get_stat_ops([self.column_group])}

        while stat_ops:
            # get all the StatOperators that we can currently call fit on (no outstanding
            # dependencies)
            current_phase = [op for op, dependencies in stat_ops.items() if not dependencies]
            if not current_phase:
                # this shouldn't happen, but lets not infinite loop just in case
                raise RuntimeError(""failed to find dependency-free StatOperator to fit"")

            stats, ops = [], []
            for column_group in current_phase:
                # apply transforms necessary for the inputs to the current column group, ignoring
                # the transforms from the statop itself
                transformed_ddf = _transform_ddf(ddf, column_group.parents)

                op = column_group.op
                try:
                    stats.append(op.fit(column_group.input_column_names, transformed_ddf))
                    ops.append(op)
                except Exception:
                    LOG.exception(""Failed to fit operator %s"", column_group.op)
                    raise

            if self.client:
                results = [r.result() for r in self.client.compute(stats)]
            else:
                results = dask.compute(stats, scheduler=""synchronous"")[0]

            for computed_stats, op in zip(results, ops):
                op.fit_finalize(computed_stats)

            # Remove all the operators we processed in this phase, and remove
            # from the dependencies of other ops too
            for stat_op in current_phase:
                stat_ops.pop(stat_op)
            for dependencies in stat_ops.values():
                dependencies.difference_update(current_phase)

        # hack: store input/output dtypes here. We should have complete dtype
        # information for each operator (like we do for column names), but as
        # an interim solution this gets us what we need.
        input_dtypes = dataset.to_ddf()[self._input_columns()].dtypes
        self.input_dtypes = dict(zip(input_dtypes.index, input_dtypes))
        output_dtypes = self.transform(dataset).to_ddf().head(1).dtypes
        self.output_dtypes = dict(zip(output_dtypes.index, output_dtypes))","    def fit(self, dataset: Dataset):
        """"""Calculates statistics for this workflow on the input dataset

        Parameters
        -----------
        dataset: Dataset
            The input dataset to calculate statistics for. If there is a train/test split this
            data should be the training dataset only.
        """"""
        self._clear_worker_cache()
        ddf = dataset.to_ddf(columns=self._input_columns())

        # Get a dictionary mapping all StatOperators we need to fit to a set of any dependant
        # StatOperators (having StatOperators that depend on the output of other StatOperators
        # means that will have multiple phases in the fit cycle here)
        stat_ops = {op: _get_stat_ops(op.parents) for op in _get_stat_ops([self.column_group])}

        while stat_ops:
            # get all the StatOperators that we can currently call fit on (no outstanding
            # dependencies)
            current_phase = [op for op, dependencies in stat_ops.items() if not dependencies]
            if not current_phase:
                # this shouldn't happen, but lets not infinite loop just in case
                raise RuntimeError(""failed to find dependency-free StatOperator to fit"")

            stats, ops = [], []
            for column_group in current_phase:
                # apply transforms necessary for the inputs to the current column group, ignoring
                # the transforms from the statop itself
                transformed_ddf = _transform_ddf(ddf, column_group.parents)

                op = column_group.op
                try:
                    stats.append(op.fit(column_group.input_column_names, transformed_ddf))
                    ops.append(op)
                except Exception:
                    LOG.exception(""Failed to fit operator %s"", column_group.op)
                    raise

            if self.client:
                results = [r.result() for r in self.client.compute(stats)]
            else:
                results = dask.compute(stats, scheduler=""synchronous"")[0]

            for computed_stats, op in zip(results, ops):
                op.fit_finalize(computed_stats)

            # Remove all the operators we processed in this phase, and remove
            # from the dependencies of other ops too
            for stat_op in current_phase:
                stat_ops.pop(stat_op)
            for dependencies in stat_ops.values():
                dependencies.difference_update(current_phase)

        # hack: store input/output dtypes here. We should have complete dtype
        # information for each operator (like we do for column names), but as
        # an interim solution this gets us what we need.
        input_dtypes = dataset.to_ddf().dtypes
        self.input_dtypes = dict(zip(input_dtypes.index, input_dtypes))
        output_dtypes = self.transform(dataset).to_ddf().head(1).dtypes
        self.output_dtypes = dict(zip(output_dtypes.index, output_dtypes))"
"def main(args):
    """"""Multi-GPU Criteo/DLRM Preprocessing Benchmark

    This benchmark is designed to measure the time required to preprocess
    the Criteo (1TB) dataset for Facebook’s DLRM model.  The user must specify
    the path of the raw dataset (using the `--data-path` flag), as well as the
    output directory for all temporary/final data (using the `--out-path` flag)

    Example Usage
    -------------

    python dask-nvtabular-criteo-benchmark.py
                        --data-path /path/to/criteo_parquet --out-path /out/dir/`


    Dataset Requirements (Parquet)
    ------------------------------

    This benchmark is designed with a parquet-formatted dataset in mind.
    While a CSV-formatted dataset can be processed by NVTabular, converting
    to parquet will yield significantly better performance.  To convert your
    dataset, try using the `optimize_criteo.ipynb` notebook (also located
    in `NVTabular/examples/`)

    For a detailed parameter overview see `NVTabular/examples/MultiGPUBench.md`
    """"""

    # Input
    data_path = args.data_path[:-1] if args.data_path[-1] == ""/"" else args.data_path
    freq_limit = args.freq_limit
    out_files_per_proc = args.out_files_per_proc
    high_card_columns = args.high_cards.split("","")
    dashboard_port = args.dashboard_port
    if args.protocol == ""ucx"":
        UCX_TLS = os.environ.get(""UCX_TLS"", ""tcp,cuda_copy,cuda_ipc,sockcm"")
        os.environ[""UCX_TLS""] = UCX_TLS

    # Cleanup output directory
    base_dir = args.out_path[:-1] if args.out_path[-1] == ""/"" else args.out_path
    dask_workdir = os.path.join(base_dir, ""workdir"")
    output_path = os.path.join(base_dir, ""output"")
    stats_path = os.path.join(base_dir, ""stats"")
    setup_dirs(base_dir, dask_workdir, output_path, stats_path)

    # Use Criteo dataset by default (for now)
    cont_names = (
        args.cont_names.split("","") if args.cont_names else [""I"" + str(x) for x in range(1, 14)]
    )
    cat_names = (
        args.cat_names.split("","") if args.cat_names else [""C"" + str(x) for x in range(1, 27)]
    )
    label_name = [""label""]

    # Specify Categorify/GroupbyStatistics options
    tree_width = {}
    cat_cache = {}
    for col in cat_names:
        if col in high_card_columns:
            tree_width[col] = args.tree_width
            cat_cache[col] = args.cat_cache_high
        else:
            tree_width[col] = 1
            cat_cache[col] = args.cat_cache_low

    # Use total device size to calculate args.device_limit_frac
    device_size = device_mem_size(kind=""total"")
    device_limit = int(args.device_limit_frac * device_size)
    device_pool_size = int(args.device_pool_frac * device_size)
    part_size = int(args.part_mem_frac * device_size)

    # Parse shuffle option
    shuffle = None
    if args.shuffle == ""PER_WORKER"":
        shuffle = nvt_io.Shuffle.PER_WORKER
    elif args.shuffle == ""PER_PARTITION"":
        shuffle = nvt_io.Shuffle.PER_PARTITION

    # Check if any device memory is already occupied
    for dev in args.devices.split("",""):
        fmem = _pynvml_mem_size(kind=""free"", index=int(dev))
        used = (device_size - fmem) / 1e9
        if used > 1.0:
            warnings.warn(f""BEWARE - {used} GB is already occupied on device {int(dev)}!"")

    # Setup LocalCUDACluster
    if args.protocol == ""tcp"":
        cluster = LocalCUDACluster(
            protocol=args.protocol,
            n_workers=args.n_workers,
            CUDA_VISIBLE_DEVICES=args.devices,
            device_memory_limit=device_limit,
            local_directory=dask_workdir,
            dashboard_address="":"" + dashboard_port,
        )
    else:
        cluster = LocalCUDACluster(
            protocol=args.protocol,
            n_workers=args.n_workers,
            CUDA_VISIBLE_DEVICES=args.devices,
            enable_nvlink=True,
            device_memory_limit=device_limit,
            local_directory=dask_workdir,
            dashboard_address="":"" + dashboard_port,
        )
    client = Client(cluster)

    # Setup RMM pool
    if args.device_pool_frac > 0.01:
        setup_rmm_pool(client, device_pool_size)

    # Define Dask NVTabular ""Workflow""
    if args.normalize:
        cont_features = cont_names >> ops.FillMissing() >> ops.Normalize()
    else:
        cont_features = cont_names >> ops.FillMissing() >> ops.Clip(min_value=0) >> ops.LogOp()

    cat_features = cat_names >> ops.Categorify(
        out_path=stats_path,
        tree_width=tree_width,
        cat_cache=cat_cache,
        freq_threshold=freq_limit,
        search_sorted=not freq_limit,
        on_host=not args.cats_on_device,
    )
    processor = Workflow(cat_features + cont_features + label_name, client=client)

    dataset = Dataset(data_path, ""parquet"", part_size=part_size)

    # Execute the dask graph
    runtime = time.time()

    processor.fit(dataset)

    if args.profile is not None:
        with performance_report(filename=args.profile):
            processor.transform(dataset).to_parquet(
                output_path=output_path,
                num_threads=args.num_io_threads,
                shuffle=shuffle,
                out_files_per_proc=out_files_per_proc,
            )
    else:
        processor.transform(dataset).to_parquet(
            output_path=output_path,
            num_threads=args.num_io_threads,
            shuffle=shuffle,
            out_files_per_proc=out_files_per_proc,
        )
    runtime = time.time() - runtime

    print(""\\nDask-NVTabular DLRM/Criteo benchmark"")
    print(""--------------------------------------"")
    print(f""partition size     | {part_size}"")
    print(f""protocol           | {args.protocol}"")
    print(f""device(s)          | {args.devices}"")
    print(f""rmm-pool-frac      | {(args.device_pool_frac)}"")
    print(f""out-files-per-proc | {args.out_files_per_proc}"")
    print(f""num_io_threads     | {args.num_io_threads}"")
    print(f""shuffle            | {args.shuffle}"")
    print(f""cats-on-device     | {args.cats_on_device}"")
    print(""======================================"")
    print(f""Runtime[s]         | {runtime}"")
    print(""======================================\\n"")

    client.close()","def main(args):
    """"""Multi-GPU Criteo/DLRM Preprocessing Benchmark

    This benchmark is designed to measure the time required to preprocess
    the Criteo (1TB) dataset for Facebook’s DLRM model.  The user must specify
    the path of the raw dataset (using the `--data-path` flag), as well as the
    output directory for all temporary/final data (using the `--out-path` flag)

    Example Usage
    -------------

    python dask-nvtabular-criteo-benchmark.py
                        --data-path /path/to/criteo_parquet --out-path /out/dir/`


    Dataset Requirements (Parquet)
    ------------------------------

    This benchmark is designed with a parquet-formatted dataset in mind.
    While a CSV-formatted dataset can be processed by NVTabular, converting
    to parquet will yield significantly better performance.  To convert your
    dataset, try using the `optimize_criteo.ipynb` notebook (also located
    in `NVTabular/examples/`)

    For a detailed parameter overview see `NVTabular/examples/MultiGPUBench.md`
    """"""

    # Input
    data_path = args.data_path
    freq_limit = args.freq_limit
    out_files_per_proc = args.out_files_per_proc
    high_card_columns = args.high_cards.split("","")
    dashboard_port = args.dashboard_port
    if args.protocol == ""ucx"":
        UCX_TLS = os.environ.get(""UCX_TLS"", ""tcp,cuda_copy,cuda_ipc,sockcm"")
        os.environ[""UCX_TLS""] = UCX_TLS

    # Cleanup output directory
    BASE_DIR = args.out_path
    dask_workdir = os.path.join(BASE_DIR, ""workdir"")
    output_path = os.path.join(BASE_DIR, ""output"")
    stats_path = os.path.join(BASE_DIR, ""stats"")
    if not os.path.isdir(BASE_DIR):
        os.mkdir(BASE_DIR)
    for dir_path in (dask_workdir, output_path, stats_path):
        if os.path.isdir(dir_path):
            shutil.rmtree(dir_path)
        os.mkdir(dir_path)

    # Use Criteo dataset by default (for now)
    cont_names = (
        args.cont_names.split("","") if args.cont_names else [""I"" + str(x) for x in range(1, 14)]
    )
    cat_names = (
        args.cat_names.split("","") if args.cat_names else [""C"" + str(x) for x in range(1, 27)]
    )
    label_name = [""label""]

    # Specify Categorify/GroupbyStatistics options
    tree_width = {}
    cat_cache = {}
    for col in cat_names:
        if col in high_card_columns:
            tree_width[col] = args.tree_width
            cat_cache[col] = args.cat_cache_high
        else:
            tree_width[col] = 1
            cat_cache[col] = args.cat_cache_low

    # Use total device size to calculate args.device_limit_frac
    device_size = device_mem_size(kind=""total"")
    device_limit = int(args.device_limit_frac * device_size)
    device_pool_size = int(args.device_pool_frac * device_size)
    part_size = int(args.part_mem_frac * device_size)

    # Parse shuffle option
    shuffle = None
    if args.shuffle == ""PER_WORKER"":
        shuffle = nvt_io.Shuffle.PER_WORKER
    elif args.shuffle == ""PER_PARTITION"":
        shuffle = nvt_io.Shuffle.PER_PARTITION

    # Check if any device memory is already occupied
    for dev in args.devices.split("",""):
        fmem = _pynvml_mem_size(kind=""free"", index=int(dev))
        used = (device_size - fmem) / 1e9
        if used > 1.0:
            warnings.warn(f""BEWARE - {used} GB is already occupied on device {int(dev)}!"")

    # Setup LocalCUDACluster
    if args.protocol == ""tcp"":
        cluster = LocalCUDACluster(
            protocol=args.protocol,
            n_workers=args.n_workers,
            CUDA_VISIBLE_DEVICES=args.devices,
            device_memory_limit=device_limit,
            local_directory=dask_workdir,
            dashboard_address="":"" + dashboard_port,
        )
    else:
        cluster = LocalCUDACluster(
            protocol=args.protocol,
            n_workers=args.n_workers,
            CUDA_VISIBLE_DEVICES=args.devices,
            enable_nvlink=True,
            device_memory_limit=device_limit,
            local_directory=dask_workdir,
            dashboard_address="":"" + dashboard_port,
        )
    client = Client(cluster)

    # Setup RMM pool
    if args.device_pool_frac > 0.01:
        setup_rmm_pool(client, device_pool_size)

    # Define Dask NVTabular ""Workflow""
    if args.normalize:
        cont_features = cont_names >> ops.FillMissing() >> ops.Normalize()
    else:
        cont_features = cont_names >> ops.FillMissing() >> ops.Clip(min_value=0) >> ops.LogOp()

    cat_features = cat_names >> ops.Categorify(
        out_path=stats_path,
        tree_width=tree_width,
        cat_cache=cat_cache,
        freq_threshold=freq_limit,
        search_sorted=not freq_limit,
        on_host=not args.cats_on_device,
    )
    processor = Workflow(cat_features + cont_features + label_name, client=client)

    dataset = Dataset(data_path, ""parquet"", part_size=part_size)

    # Execute the dask graph
    runtime = time.time()

    processor.fit(dataset)

    if args.profile is not None:
        with performance_report(filename=args.profile):
            processor.transform(dataset).to_parquet(
                output_path=output_path,
                num_threads=args.num_io_threads,
                shuffle=shuffle,
                out_files_per_proc=out_files_per_proc,
            )
    else:
        processor.transform(dataset).to_parquet(
            output_path=output_path,
            num_threads=args.num_io_threads,
            shuffle=shuffle,
            out_files_per_proc=out_files_per_proc,
        )
    runtime = time.time() - runtime

    print(""\\nDask-NVTabular DLRM/Criteo benchmark"")
    print(""--------------------------------------"")
    print(f""partition size     | {part_size}"")
    print(f""protocol           | {args.protocol}"")
    print(f""device(s)          | {args.devices}"")
    print(f""rmm-pool-frac      | {(args.device_pool_frac)}"")
    print(f""out-files-per-proc | {args.out_files_per_proc}"")
    print(f""num_io_threads     | {args.num_io_threads}"")
    print(f""shuffle            | {args.shuffle}"")
    print(f""cats-on-device     | {args.cats_on_device}"")
    print(""======================================"")
    print(f""Runtime[s]         | {runtime}"")
    print(""======================================\\n"")

    client.close()"
"    def __init__(self, out_dir, **kwargs):
        super().__init__(out_dir, **kwargs)
        self.data_paths = []
        self.data_files = []
        self.data_writers = []
        self.data_bios = []
        self._lock = threading.RLock()
        self.pwriter = self._pwriter
        self.pwriter_kwargs = {}","    def __init__(self, out_dir, **kwargs):
        super().__init__(out_dir, **kwargs)
        self.data_paths = []
        self.data_writers = []
        self.data_bios = []
        self._lock = threading.RLock()
        self.pwriter = self._pwriter
        self.pwriter_kwargs = {}"
"    def _append_writer(self, path, schema=None, add_args=None, add_kwargs=None):
        # Add additional args and kwargs
        _args = add_args or []
        _kwargs = tlz.merge(self.pwriter_kwargs, add_kwargs or {})

        if self.bytes_io:
            bio = BytesIO()
            self.data_bios.append(bio)
            self.data_writers.append(self.pwriter(bio, *_args, **_kwargs))
        else:
            f = fsspec.open(path, mode=""wb"").open()
            self.data_files.append(f)
            self.data_writers.append(self.pwriter(f, *_args, **_kwargs))","    def _append_writer(self, path, schema=None, add_args=None, add_kwargs=None):
        # Add additional args and kwargs
        _args = add_args or []
        _kwargs = tlz.merge(self.pwriter_kwargs, add_kwargs or {})

        if self.bytes_io:
            bio = BytesIO()
            self.data_bios.append(bio)
            self.data_writers.append(self.pwriter(bio, *_args, **_kwargs))
        else:
            self.data_writers.append(self.pwriter(path, *_args, **_kwargs))"
"    def _close_writers(self):
        md_dict = {}
        for writer, path in zip(self.data_writers, self.data_paths):
            fn = path.split(self.fs.sep)[-1]
            md_dict[fn] = writer.close(metadata_file_path=fn)
        for f in self.data_files:
            f.close()
        return md_dict","    def _close_writers(self):
        md_dict = {}
        for writer, path in zip(self.data_writers, self.data_paths):
            fn = path.split(self.fs.sep)[-1]
            md_dict[fn] = writer.close(metadata_file_path=fn)
        return md_dict"
"def fetch_table_data(
    table_cache, path, cache=""disk"", cats_only=False, reader=None, columns=None, **kwargs
):
    """"""Utility to retrieve a cudf DataFrame from a cache (and add the
    DataFrame to a cache if the element is missing).  Note that `cats_only=True`
    results in optimized logic for the `Categorify` transformation.
    """"""
    table = table_cache.get(path, None)
    if table and not isinstance(table, cudf.DataFrame):
        if not cats_only:
            return cudf.io.read_parquet(table, index=False)
        df = cudf.io.read_parquet(table, index=False, columns=columns)
        df.index.name = ""labels""
        df.reset_index(drop=False, inplace=True)
        return df

    reader = reader or cudf.io.read_parquet
    if table is None:
        if cache in (""device"", ""disk""):
            table = reader(path, index=False, columns=columns, **kwargs)
        elif cache == ""host"":
            if reader == cudf.io.read_parquet:
                # If the file is already in parquet format,
                # we can just move the same bytes to host memory
                with fsspec.open(path, ""rb"") as f:
                    table_cache[path] = BytesIO(f.read())
                table = reader(table_cache[path], index=False, columns=columns, **kwargs)
            else:
                # Otherwise, we should convert the format to parquet
                table = reader(path, index=False, columns=columns, **kwargs)
                table_cache[path] = BytesIO()
                table.to_parquet(table_cache[path])
        if cats_only:
            table.index.name = ""labels""
            table.reset_index(drop=False, inplace=True)
        if cache == ""device"":
            table_cache[path] = table.copy(deep=False)
    return table","def fetch_table_data(
    table_cache, path, cache=""disk"", cats_only=False, reader=None, columns=None, **kwargs
):
    """"""Utility to retrieve a cudf DataFrame from a cache (and add the
    DataFrame to a cache if the element is missing).  Note that `cats_only=True`
    results in optimized logic for the `Categorify` transformation.
    """"""
    table = table_cache.get(path, None)
    if table and not isinstance(table, cudf.DataFrame):
        if not cats_only:
            return cudf.io.read_parquet(table, index=False)
        df = cudf.io.read_parquet(table, index=False, columns=columns)
        df.index.name = ""labels""
        df.reset_index(drop=False, inplace=True)
        return df

    reader = reader or cudf.io.read_parquet
    if table is None:
        if cache in (""device"", ""disk""):
            table = reader(path, index=False, columns=columns, **kwargs)
        elif cache == ""host"":
            if reader == cudf.io.read_parquet:
                # If the file is already in parquet format,
                # we can just move the same bytes to host memory
                with open(path, ""rb"") as f:
                    table_cache[path] = BytesIO(f.read())
                table = reader(table_cache[path], index=False, columns=columns, **kwargs)
            else:
                # Otherwise, we should convert the format to parquet
                table = reader(path, index=False, columns=columns, **kwargs)
                table_cache[path] = BytesIO()
                table.to_parquet(table_cache[path])
        if cats_only:
            table.index.name = ""labels""
            table.reset_index(drop=False, inplace=True)
        if cache == ""device"":
            table_cache[path] = table.copy(deep=False)
    return table"
"def _chunkwise_moments(df):
    df2 = cudf.DataFrame()
    for col in df.columns:
        df2[col] = df[col].astype(""float64"").pow(2)
    vals = {
        ""df-count"": df.count().to_frame().transpose(),
        ""df-sum"": df.sum().astype(""float64"").to_frame().transpose(),
        ""df2-sum"": df2.sum().to_frame().transpose(),
    }
    # NOTE: Perhaps we should convert to pandas here
    # (since we know the results should be small)?
    del df2
    return vals","def _chunkwise_moments(df):
    df2 = cudf.DataFrame()
    for col in df.columns:
        df2[col] = df[col].astype(""float64"").pow(2)
    vals = {
        ""df-count"": df.count().to_frame().transpose(),
        ""df-sum"": df.sum().to_frame().transpose(),
        ""df2-sum"": df2.sum().to_frame().transpose(),
    }
    # NOTE: Perhaps we should convert to pandas here
    # (since we know the results should be small)?
    del df2
    return vals"
"    def to_ddf(self, columns=None):
        return dask_cudf.read_parquet(
            self.paths,
            columns=columns,
            # can't omit reading the index in if we aren't being passed columns
            index=None if columns is None else False,
            gather_statistics=False,
            split_row_groups=self.row_groups_per_part,
            storage_options=self.storage_options,
        )","    def to_ddf(self, columns=None):
        return dask_cudf.read_parquet(
            self.paths,
            columns=columns,
            index=False,
            gather_statistics=False,
            split_row_groups=self.row_groups_per_part,
            storage_options=self.storage_options,
        )"
"    def get_ddf(self):
        if self.ddf is None:
            raise ValueError(""No dask_cudf frame available."")
        elif isinstance(self.ddf, Dataset):
            # Right now we can't distinguish between input columns and generated columns
            # in the dataset, we don't limit the columm set right now in the to_ddf call
            # (https://github.com/NVIDIA/NVTabular/issues/409 )
            return self.ddf.to_ddf(shuffle=self._shuffle_parts)
        return self.ddf","    def get_ddf(self):
        if self.ddf is None:
            raise ValueError(""No dask_cudf frame available."")
        elif isinstance(self.ddf, Dataset):
            columns = self.columns_ctx[""all""][""base""]
            return self.ddf.to_ddf(columns=columns, shuffle=self._shuffle_parts)
        return self.ddf"
"    def add_data(self, gdf):
        # Populate columns idxs
        if not self.col_idx:
            for i, x in enumerate(gdf.columns.values):
                self.col_idx[str(x)] = i

        # list columns in cudf don't currently support chunked writing in parquet.
        # hack around this by just writing a single file with this partition
        # this restriction can be removed once cudf supports chunked writing
        # in parquet
        if any(is_list_dtype(gdf[col].dtype) for col in gdf.columns):
            self._write_table(0, gdf, True)
            return

        # Generate `ind` array to map each row to an output file.
        # This approach is certainly more optimized for shuffling
        # than it is for non-shuffling, but using a single code
        # path is probably worth the (possible) minor overhead.
        nrows = gdf.shape[0]
        typ = np.min_scalar_type(nrows * 2)
        if self.shuffle:
            ind = cp.random.choice(cp.arange(self.num_out_files, dtype=typ), nrows)
        else:
            ind = cp.arange(nrows, dtype=typ)
            cp.floor_divide(ind, math.ceil(nrows / self.num_out_files), out=ind)
        for x, group in enumerate(
            gdf.scatter_by_map(ind, map_size=self.num_out_files, keep_index=False)
        ):
            self.num_samples[x] += len(group)
            if self.num_threads > 1:
                self.queue.put((x, group))
            else:
                self._write_table(x, group)

        # wait for all writes to finish before exiting
        # (so that we aren't using memory)
        if self.num_threads > 1:
            self.queue.join()","    def add_data(self, gdf):
        # Populate columns idxs
        if not self.col_idx:
            for i, x in enumerate(gdf.columns.values):
                self.col_idx[str(x)] = i

        # list columns in cudf don't currently support chunked writing in parquet.
        # hack around this by just writing a single file with this partition
        # this restriction can be removed once cudf supports chunked writing
        # in parquet
        if any(is_list_dtype(gdf[col].dtype) for col in gdf.columns):
            self._write_table(gdf, 0, True)
            return

        # Generate `ind` array to map each row to an output file.
        # This approach is certainly more optimized for shuffling
        # than it is for non-shuffling, but using a single code
        # path is probably worth the (possible) minor overhead.
        nrows = gdf.shape[0]
        typ = np.min_scalar_type(nrows * 2)
        if self.shuffle:
            ind = cp.random.choice(cp.arange(self.num_out_files, dtype=typ), nrows)
        else:
            ind = cp.arange(nrows, dtype=typ)
            cp.floor_divide(ind, math.ceil(nrows / self.num_out_files), out=ind)
        for x, group in enumerate(
            gdf.scatter_by_map(ind, map_size=self.num_out_files, keep_index=False)
        ):
            self.num_samples[x] += len(group)
            if self.num_threads > 1:
                self.queue.put((x, group))
            else:
                self._write_table(x, group)

        # wait for all writes to finish before exiting
        # (so that we aren't using memory)
        if self.num_threads > 1:
            self.queue.join()"
"    def __init__(
        self,
        paths,
        part_size,
        storage_options,
        row_groups_per_part=None,
        legacy=False,
        batch_size=None,
    ):
        # TODO: Improve dask_cudf.read_parquet performance so that
        # this class can be slimmed down.
        super().__init__(paths, part_size, storage_options)
        self.batch_size = batch_size
        self._metadata, self._base = self.metadata
        self._pieces = None
        if row_groups_per_part is None:
            file_path = self._metadata.row_group(0).column(0).file_path
            path0 = (
                self.fs.sep.join([self._base, file_path])
                if file_path != """"
                else self._base  # This is a single file
            )

        if row_groups_per_part is None:
            rg_byte_size_0 = _memory_usage(cudf.io.read_parquet(path0, row_groups=0, row_group=0))
            row_groups_per_part = self.part_size / rg_byte_size_0
            if row_groups_per_part < 1.0:
                warnings.warn(
                    f""Row group size {rg_byte_size_0} is bigger than requested part_size ""
                    f""{self.part_size}""
                )
                row_groups_per_part = 1.0

        self.row_groups_per_part = int(row_groups_per_part)

        assert self.row_groups_per_part > 0","    def __init__(
        self,
        paths,
        part_size,
        storage_options,
        row_groups_per_part=None,
        legacy=False,
        batch_size=None,
    ):
        # TODO: Improve dask_cudf.read_parquet performance so that
        # this class can be slimmed down.
        super().__init__(paths, part_size, storage_options)
        self.batch_size = batch_size
        self._metadata, self._base = self.metadata
        self._pieces = None
        if row_groups_per_part is None:
            file_path = self._metadata.row_group(0).column(0).file_path
            path0 = (
                self.fs.sep.join([self._base, file_path])
                if file_path != """"
                else self._base  # This is a single file
            )

        if row_groups_per_part is None:
            rg_byte_size_0 = (
                cudf.io.read_parquet(path0, row_groups=0, row_group=0)
                .memory_usage(deep=True, index=True)
                .sum()
            )
            row_groups_per_part = self.part_size / rg_byte_size_0
            if row_groups_per_part < 1.0:
                warnings.warn(
                    f""Row group size {rg_byte_size_0} is bigger than requested part_size ""
                    f""{self.part_size}""
                )
                row_groups_per_part = 1.0

        self.row_groups_per_part = int(row_groups_per_part)

        assert self.row_groups_per_part > 0"
"    def __init__(self, *args, **kwargs):
        super().__init__(*args)
        self._meta = {}
        self.csv_kwargs = kwargs
        self.names = self.csv_kwargs.get(""names"", None)
        # CSV reader needs a list of files
        # (Assume flat directory structure if this is a dir)
        if len(self.paths) == 1 and self.fs.isdir(self.paths[0]):
            self.paths = self.fs.glob(self.fs.sep.join([self.paths[0], ""*""]))","    def __init__(self, *args, **kwargs):
        super().__init__(*args)
        self._meta = {}
        self.names = kwargs.pop(""names"", None)
        self.csv_kwargs = kwargs
        # CSV reader needs a list of files
        # (Assume flat directory structure if this is a dir)
        if len(self.paths) == 1 and self.fs.isdir(self.paths[0]):
            self.paths = self.fs.glob(self.fs.sep.join([self.paths[0], ""*""]))"
"    def to_ddf(self, columns=None):
        return dask_cudf.read_csv(self.paths, chunksize=self.part_size, **self.csv_kwargs)[columns]","    def to_ddf(self, columns=None):
        return dask_cudf.read_csv(
            self.paths, names=self.names, chunksize=self.part_size, **self.csv_kwargs
        )[columns]"
"    def _predict(self, X):
        """"""Collect results from clf.predict calls.""""""

        if self.refit:
            return np.asarray([clf.predict(X) for clf in self.clfs_]).T
        else:
            return np.asarray([self.le_.transform(clf.predict(X))
                               for clf in self.clfs_]).T","    def _predict(self, X):
        """"""Collect results from clf.predict calls.""""""
        return np.asarray([clf.predict(X) for clf in self.clfs_]).T"
"    def transform(
        self,
        xx: Any,
        yy: Any,
        zz: Any = None,
        tt: Any = None,
        radians: bool = False,
        errcheck: bool = False,
        direction: Union[TransformDirection, str] = TransformDirection.FORWARD,
    ) -> Any:
        """"""
        Transform points between two coordinate systems.

        .. versionadded:: 2.1.1 errcheck
        .. versionadded:: 2.2.0 direction

        Parameters
        ----------
        xx: scalar or array (numpy or python)
            Input x coordinate(s).
        yy: scalar or array (numpy or python)
            Input y coordinate(s).
        zz: scalar or array (numpy or python), optional
            Input z coordinate(s).
        tt: scalar or array (numpy or python), optional
            Input time coordinate(s).
        radians: boolean, optional
            If True, will expect input data to be in radians and will return radians
            if the projection is geographic. Default is False (degrees). Ignored for
            pipeline transformations.
        errcheck: boolean, optional (default False)
            If True an exception is raised if the transformation is invalid.
            By default errcheck=False and an invalid transformation
            returns ``inf`` and no exception is raised.
        direction: pyproj.enums.TransformDirection, optional
            The direction of the transform.
            Default is :attr:`pyproj.enums.TransformDirection.FORWARD`.


        Example:

        >>> from pyproj import Transformer
        >>> transformer = Transformer.from_crs(""epsg:4326"", ""epsg:3857"")
        >>> x3, y3 = transformer.transform(33, 98)
        >>> ""%.3f  %.3f"" % (x3, y3)
        '10909310.098  3895303.963'
        >>> pipeline_str = (
        ...     ""+proj=pipeline +step +proj=longlat +ellps=WGS84 ""
        ...     ""+step +proj=unitconvert +xy_in=rad +xy_out=deg""
        ... )
        >>> pipe_trans = Transformer.from_pipeline(pipeline_str)
        >>> xt, yt = pipe_trans.transform(2.1, 0.001)
        >>> ""%.3f  %.3f"" % (xt, yt)
        '2.100  0.001'
        >>> transproj = Transformer.from_crs(
        ...     {""proj"":'geocent', ""ellps"":'WGS84', ""datum"":'WGS84'},
        ...     ""EPSG:4326"",
        ...     always_xy=True,
        ... )
        >>> xpj, ypj, zpj = transproj.transform(
        ...     -2704026.010,
        ...     -4253051.810,
        ...     3895878.820,
        ...     radians=True,
        ... )
        >>> ""%.3f %.3f %.3f"" % (xpj, ypj, zpj)
        '-2.137 0.661 -20.531'
        >>> transprojr = Transformer.from_crs(
        ...     ""EPSG:4326"",
        ...     {""proj"":'geocent', ""ellps"":'WGS84', ""datum"":'WGS84'},
        ...     always_xy=True,
        ... )
        >>> xpjr, ypjr, zpjr = transprojr.transform(xpj, ypj, zpj, radians=True)
        >>> ""%.3f %.3f %.3f"" % (xpjr, ypjr, zpjr)
        '-2704026.010 -4253051.810 3895878.820'
        >>> transformer = Transformer.from_proj(""epsg:4326"", 4326, skip_equivalent=True)
        >>> xeq, yeq = transformer.transform(33, 98)
        >>> ""%.0f  %.0f"" % (xeq, yeq)
        '33  98'

        """"""
        # process inputs, making copies that support buffer API.
        inx, xisfloat, xislist, xistuple = _copytobuffer(xx)
        iny, yisfloat, yislist, yistuple = _copytobuffer(yy)
        if zz is not None:
            inz, zisfloat, zislist, zistuple = _copytobuffer(zz)
        else:
            inz = None
        if tt is not None:
            intime, tisfloat, tislist, tistuple = _copytobuffer(tt)
        else:
            intime = None
        # call pj_transform.  inx,iny,inz buffers modified in place.
        self._transformer._transform(
            inx,
            iny,
            inz=inz,
            intime=intime,
            direction=direction,
            radians=radians,
            errcheck=errcheck,
        )
        # if inputs were lists, tuples or floats, convert back.
        outx = _convertback(xisfloat, xislist, xistuple, inx)
        outy = _convertback(yisfloat, yislist, xistuple, iny)
        return_data = (outx, outy)
        if inz is not None:
            return_data += (  # type: ignore
                _convertback(zisfloat, zislist, zistuple, inz),
            )
        if intime is not None:
            return_data += (  # type: ignore
                _convertback(tisfloat, tislist, tistuple, intime),
            )
        return return_data","    def transform(
        self,
        xx: Any,
        yy: Any,
        zz: Any = None,
        tt: Any = None,
        radians: bool = False,
        errcheck: bool = False,
        direction: Union[TransformDirection, str] = TransformDirection.FORWARD,
    ) -> Any:
        """"""
        Transform points between two coordinate systems.

        .. versionadded:: 2.1.1 errcheck
        .. versionadded:: 2.2.0 direction

        Parameters
        ----------
        xx: scalar or array (numpy or python)
            Input x coordinate(s).
        yy: scalar or array (numpy or python)
            Input y coordinate(s).
        zz: scalar or array (numpy or python), optional
            Input z coordinate(s).
        tt: scalar or array (numpy or python), optional
            Input time coordinate(s).
        radians: boolean, optional
            If True, will expect input data to be in radians and will return radians
            if the projection is geographic. Default is False (degrees). Ignored for
            pipeline transformations.
        errcheck: boolean, optional (default False)
            If True an exception is raised if the transformation is invalid.
            By default errcheck=False and an invalid transformation
            returns ``inf`` and no exception is raised.
        direction: pyproj.enums.TransformDirection, optional
            The direction of the transform.
            Default is :attr:`pyproj.enums.TransformDirection.FORWARD`.


        Example:

        >>> from pyproj import Transformer
        >>> transformer = Transformer.from_crs(""epsg:4326"", ""epsg:3857"")
        >>> x3, y3 = transformer.transform(33, 98)
        >>> ""%.3f  %.3f"" % (x3, y3)
        '10909310.098  3895303.963'
        >>> pipeline_str = (
        ...     ""+proj=pipeline +step +proj=longlat +ellps=WGS84 ""
        ...     ""+step +proj=unitconvert +xy_in=rad +xy_out=deg""
        ... )
        >>> pipe_trans = Transformer.from_pipeline(pipeline_str)
        >>> xt, yt = pipe_trans.transform(2.1, 0.001)
        >>> ""%.3f  %.3f"" % (xt, yt)
        '120.321  0.057'
        >>> transproj = Transformer.from_crs(
        ...     {""proj"":'geocent', ""ellps"":'WGS84', ""datum"":'WGS84'},
        ...     ""EPSG:4326"",
        ...     always_xy=True,
        ... )
        >>> xpj, ypj, zpj = transproj.transform(
        ...     -2704026.010,
        ...     -4253051.810,
        ...     3895878.820,
        ...     radians=True,
        ... )
        >>> ""%.3f %.3f %.3f"" % (xpj, ypj, zpj)
        '-2.137 0.661 -20.531'
        >>> transprojr = Transformer.from_crs(
        ...     ""EPSG:4326"",
        ...     {""proj"":'geocent', ""ellps"":'WGS84', ""datum"":'WGS84'},
        ...     always_xy=True,
        ... )
        >>> xpjr, ypjr, zpjr = transprojr.transform(xpj, ypj, zpj, radians=True)
        >>> ""%.3f %.3f %.3f"" % (xpjr, ypjr, zpjr)
        '-2704026.010 -4253051.810 3895878.820'
        >>> transformer = Transformer.from_proj(""epsg:4326"", 4326, skip_equivalent=True)
        >>> xeq, yeq = transformer.transform(33, 98)
        >>> ""%.0f  %.0f"" % (xeq, yeq)
        '33  98'

        """"""
        # process inputs, making copies that support buffer API.
        inx, xisfloat, xislist, xistuple = _copytobuffer(xx)
        iny, yisfloat, yislist, yistuple = _copytobuffer(yy)
        if zz is not None:
            inz, zisfloat, zislist, zistuple = _copytobuffer(zz)
        else:
            inz = None
        if tt is not None:
            intime, tisfloat, tislist, tistuple = _copytobuffer(tt)
        else:
            intime = None
        # call pj_transform.  inx,iny,inz buffers modified in place.
        self._transformer._transform(
            inx,
            iny,
            inz=inz,
            intime=intime,
            direction=direction,
            radians=radians,
            errcheck=errcheck,
        )
        # if inputs were lists, tuples or floats, convert back.
        outx = _convertback(xisfloat, xislist, xistuple, inx)
        outy = _convertback(yisfloat, yislist, xistuple, iny)
        return_data = (outx, outy)
        if inz is not None:
            return_data += (  # type: ignore
                _convertback(zisfloat, zislist, zistuple, inz),
            )
        if intime is not None:
            return_data += (  # type: ignore
                _convertback(tisfloat, tislist, tistuple, intime),
            )
        return return_data"
"    def itransform(
        self,
        points: Any,
        switch: bool = False,
        time_3rd: bool = False,
        radians: bool = False,
        errcheck: bool = False,
        direction: Union[TransformDirection, str] = TransformDirection.FORWARD,
    ) -> Iterator[Iterable]:
        """"""
        Iterator/generator version of the function pyproj.Transformer.transform.

        .. versionadded:: 2.1.1 errcheck
        .. versionadded:: 2.2.0 direction

        Parameters
        ----------
        points: list
            List of point tuples.
        switch: boolean, optional
            If True x, y or lon,lat coordinates of points are switched to y, x
            or lat, lon. Default is False.
        time_3rd: boolean, optional
            If the input coordinates are 3 dimensional and the 3rd dimension is time.
        radians: boolean, optional
            If True, will expect input data to be in radians and will return radians
            if the projection is geographic. Default is False (degrees). Ignored for
            pipeline transformations.
        errcheck: boolean, optional (default False)
            If True an exception is raised if the transformation is invalid.
            By default errcheck=False and an invalid transformation
            returns ``inf`` and no exception is raised.
        direction: pyproj.enums.TransformDirection, optional
            The direction of the transform.
            Default is :attr:`pyproj.enums.TransformDirection.FORWARD`.


        Example:

        >>> from pyproj import Transformer
        >>> transformer = Transformer.from_crs(4326, 2100)
        >>> points = [(22.95, 40.63), (22.81, 40.53), (23.51, 40.86)]
        >>> for pt in transformer.itransform(points): '{:.3f} {:.3f}'.format(*pt)
        '2221638.801 2637034.372'
        '2212924.125 2619851.898'
        '2238294.779 2703763.736'
        >>> pipeline_str = (
        ...     ""+proj=pipeline +step +proj=longlat +ellps=WGS84 ""
        ...     ""+step +proj=unitconvert +xy_in=rad +xy_out=deg""
        ... )
        >>> pipe_trans = Transformer.from_pipeline(pipeline_str)
        >>> for pt in pipe_trans.itransform([(2.1, 0.001)]):
        ...     '{:.3f} {:.3f}'.format(*pt)
        '2.100 0.001'
        >>> transproj = Transformer.from_crs(
        ...     {""proj"":'geocent', ""ellps"":'WGS84', ""datum"":'WGS84'},
        ...     ""EPSG:4326"",
        ...     always_xy=True,
        ... )
        >>> for pt in transproj.itransform(
        ...     [(-2704026.010, -4253051.810, 3895878.820)],
        ...     radians=True,
        ... ):
        ...     '{:.3f} {:.3f} {:.3f}'.format(*pt)
        '-2.137 0.661 -20.531'
        >>> transprojr = Transformer.from_crs(
        ...     ""EPSG:4326"",
        ...     {""proj"":'geocent', ""ellps"":'WGS84', ""datum"":'WGS84'},
        ...     always_xy=True,
        ... )
        >>> for pt in transprojr.itransform(
        ...     [(-2.137, 0.661, -20.531)],
        ...     radians=True
        ... ):
        ...     '{:.3f} {:.3f} {:.3f}'.format(*pt)
        '-2704214.394 -4254414.478 3894270.731'
        >>> transproj_eq = Transformer.from_proj(
        ...     'EPSG:4326',
        ...     '+proj=longlat +datum=WGS84 +no_defs +type=crs',
        ...     always_xy=True,
        ...     skip_equivalent=True
        ... )
        >>> for pt in transproj_eq.itransform([(-2.137, 0.661)]):
        ...     '{:.3f} {:.3f}'.format(*pt)
        '-2.137 0.661'

        """"""
        it = iter(points)  # point iterator
        # get first point to check stride
        try:
            fst_pt = next(it)
        except StopIteration:
            raise ValueError(""iterable must contain at least one point"")

        stride = len(fst_pt)
        if stride not in (2, 3, 4):
            raise ValueError(""points can contain up to 4 coordinates"")

        if time_3rd and stride != 3:
            raise ValueError(""'time_3rd' is only valid for 3 coordinates."")

        # create a coordinate sequence generator etc. x1,y1,z1,x2,y2,z2,....
        # chain so the generator returns the first point that was already acquired
        coord_gen = chain(fst_pt, (coords[c] for coords in it for c in range(stride)))

        while True:
            # create a temporary buffer storage for
            # the next 64 points (64*stride*8 bytes)
            buff = array(""d"", islice(coord_gen, 0, 64 * stride))
            if len(buff) == 0:
                break

            self._transformer._transform_sequence(
                stride,
                buff,
                switch=switch,
                direction=direction,
                time_3rd=time_3rd,
                radians=radians,
                errcheck=errcheck,
            )

            for pt in zip(*([iter(buff)] * stride)):
                yield pt","    def itransform(
        self,
        points: Any,
        switch: bool = False,
        time_3rd: bool = False,
        radians: bool = False,
        errcheck: bool = False,
        direction: Union[TransformDirection, str] = TransformDirection.FORWARD,
    ) -> Iterator[Iterable]:
        """"""
        Iterator/generator version of the function pyproj.Transformer.transform.

        .. versionadded:: 2.1.1 errcheck
        .. versionadded:: 2.2.0 direction

        Parameters
        ----------
        points: list
            List of point tuples.
        switch: boolean, optional
            If True x, y or lon,lat coordinates of points are switched to y, x
            or lat, lon. Default is False.
        time_3rd: boolean, optional
            If the input coordinates are 3 dimensional and the 3rd dimension is time.
        radians: boolean, optional
            If True, will expect input data to be in radians and will return radians
            if the projection is geographic. Default is False (degrees). Ignored for
            pipeline transformations.
        errcheck: boolean, optional (default False)
            If True an exception is raised if the transformation is invalid.
            By default errcheck=False and an invalid transformation
            returns ``inf`` and no exception is raised.
        direction: pyproj.enums.TransformDirection, optional
            The direction of the transform.
            Default is :attr:`pyproj.enums.TransformDirection.FORWARD`.


        Example:

        >>> from pyproj import Transformer
        >>> transformer = Transformer.from_crs(4326, 2100)
        >>> points = [(22.95, 40.63), (22.81, 40.53), (23.51, 40.86)]
        >>> for pt in transformer.itransform(points): '{:.3f} {:.3f}'.format(*pt)
        '2221638.801 2637034.372'
        '2212924.125 2619851.898'
        '2238294.779 2703763.736'
        >>> pipeline_str = (
        ...     ""+proj=pipeline +step +proj=longlat +ellps=WGS84 ""
        ...     ""+step +proj=unitconvert +xy_in=rad +xy_out=deg""
        ... )
        >>> pipe_trans = Transformer.from_pipeline(pipeline_str)
        >>> for pt in pipe_trans.itransform([(2.1, 0.001)]):
        ...     '{:.3f} {:.3f}'.format(*pt)
        '120.321 0.057'
        >>> transproj = Transformer.from_crs(
        ...     {""proj"":'geocent', ""ellps"":'WGS84', ""datum"":'WGS84'},
        ...     ""EPSG:4326"",
        ...     always_xy=True,
        ... )
        >>> for pt in transproj.itransform(
        ...     [(-2704026.010, -4253051.810, 3895878.820)],
        ...     radians=True,
        ... ):
        ...     '{:.3f} {:.3f} {:.3f}'.format(*pt)
        '-2.137 0.661 -20.531'
        >>> transprojr = Transformer.from_crs(
        ...     ""EPSG:4326"",
        ...     {""proj"":'geocent', ""ellps"":'WGS84', ""datum"":'WGS84'},
        ...     always_xy=True,
        ... )
        >>> for pt in transprojr.itransform(
        ...     [(-2.137, 0.661, -20.531)],
        ...     radians=True
        ... ):
        ...     '{:.3f} {:.3f} {:.3f}'.format(*pt)
        '-2704214.394 -4254414.478 3894270.731'
        >>> transproj_eq = Transformer.from_proj(
        ...     'EPSG:4326',
        ...     '+proj=longlat +datum=WGS84 +no_defs +type=crs',
        ...     always_xy=True,
        ...     skip_equivalent=True
        ... )
        >>> for pt in transproj_eq.itransform([(-2.137, 0.661)]):
        ...     '{:.3f} {:.3f}'.format(*pt)
        '-2.137 0.661'

        """"""
        it = iter(points)  # point iterator
        # get first point to check stride
        try:
            fst_pt = next(it)
        except StopIteration:
            raise ValueError(""iterable must contain at least one point"")

        stride = len(fst_pt)
        if stride not in (2, 3, 4):
            raise ValueError(""points can contain up to 4 coordinates"")

        if time_3rd and stride != 3:
            raise ValueError(""'time_3rd' is only valid for 3 coordinates."")

        # create a coordinate sequence generator etc. x1,y1,z1,x2,y2,z2,....
        # chain so the generator returns the first point that was already acquired
        coord_gen = chain(fst_pt, (coords[c] for coords in it for c in range(stride)))

        while True:
            # create a temporary buffer storage for
            # the next 64 points (64*stride*8 bytes)
            buff = array(""d"", islice(coord_gen, 0, 64 * stride))
            if len(buff) == 0:
                break

            self._transformer._transform_sequence(
                stride,
                buff,
                switch=switch,
                direction=direction,
                time_3rd=time_3rd,
                radians=radians,
                errcheck=errcheck,
            )

            for pt in zip(*([iter(buff)] * stride)):
                yield pt"
"    def from_user_input(value: Any) -> ""CRS"":
        """"""
        Initialize a CRS class instance with:
          - PROJ string
          - Dictionary of PROJ parameters
          - PROJ keyword arguments for parameters
          - JSON string with PROJ parameters
          - CRS WKT string
          - An authority string [i.e. 'epsg:4326']
          - An EPSG integer code [i.e. 4326]
          - A tuple of (""auth_name"": ""auth_code"") [i.e ('epsg', '4326')]
          - An object with a `to_wkt` method.
          - A :class:`pyproj.crs.CRS` class

        Parameters
        ----------
        value : obj
            A Python int, dict, or str.

        Returns
        -------
        CRS
        """"""
        if isinstance(value, CRS):
            return value
        return CRS(value)","    def from_user_input(value: str) -> ""CRS"":
        """"""
        Initialize a CRS class instance with:
          - PROJ string
          - Dictionary of PROJ parameters
          - PROJ keyword arguments for parameters
          - JSON string with PROJ parameters
          - CRS WKT string
          - An authority string [i.e. 'epsg:4326']
          - An EPSG integer code [i.e. 4326]
          - A tuple of (""auth_name"": ""auth_code"") [i.e ('epsg', '4326')]
          - An object with a `to_wkt` method.
          - A :class:`pyproj.crs.CRS` class

        Parameters
        ----------
        value : obj
            A Python int, dict, or str.

        Returns
        -------
        CRS
        """"""
        if isinstance(value, CRS):
            return value
        return CRS(value)"
"    def __init__(
        self,
        name: str = ""undefined"",
        datum: Any = ""urn:ogc:def:datum:EPSG::6326"",
        ellipsoidal_cs: Any = None,
    ) -> None:
        """"""
        Parameters
        ----------
        name: str, optional
            Name of the CRS. Default is undefined.
        datum: Any, optional
            Anything accepted by :meth:`pyproj.crs.Datum.from_user_input` or
            a :class:`pyproj.crs.datum.CustomDatum`.
        ellipsoidal_cs: Any, optional
            Input to create an Ellipsoidal Coordinate System.
            Anything accepted by :meth:`pyproj.crs.CoordinateSystem.from_user_input`
            or an Ellipsoidal Coordinate System created from :ref:`coordinate_system`.
        """"""
        geographic_crs_json = {
            ""$schema"": ""https://proj.org/schemas/v0.2/projjson.schema.json"",
            ""type"": ""GeographicCRS"",
            ""name"": name,
            ""datum"": Datum.from_user_input(datum).to_json_dict(),
            ""coordinate_system"": CoordinateSystem.from_user_input(
                ellipsoidal_cs or Ellipsoidal2DCS()
            ).to_json_dict(),
        }
        super().__init__(geographic_crs_json)","    def __init__(
        self,
        name: str = ""undefined"",
        datum: Any = ""urn:ogc:def:datum:EPSG::6326"",
        ellipsoidal_cs: Any = Ellipsoidal2DCS(),
    ) -> None:
        """"""
        Parameters
        ----------
        name: str, optional
            Name of the CRS. Default is undefined.
        datum: Any, optional
            Anything accepted by :meth:`pyproj.crs.Datum.from_user_input` or
            a :class:`pyproj.crs.datum.CustomDatum`.
        ellipsoidal_cs: Any, optional
            Input to create an Ellipsoidal Coordinate System.
            Anything accepted by :meth:`pyproj.crs.CoordinateSystem.from_user_input`
            or an Ellipsoidal Coordinate System created from :ref:`coordinate_system`.
        """"""
        geographic_crs_json = {
            ""$schema"": ""https://proj.org/schemas/v0.2/projjson.schema.json"",
            ""type"": ""GeographicCRS"",
            ""name"": name,
            ""datum"": Datum.from_user_input(datum).to_json_dict(),
            ""coordinate_system"": CoordinateSystem.from_user_input(
                ellipsoidal_cs
            ).to_json_dict(),
        }
        super().__init__(geographic_crs_json)"
"    def __init__(
        self,
        base_crs: Any,
        conversion: Any,
        ellipsoidal_cs: Any = None,
        name: str = ""undefined"",
    ) -> None:
        """"""
        Parameters
        ----------
        base_crs: Any
            Input to create the Geodetic CRS, a :class:`GeographicCRS` or
            anything accepted by :meth:`pyproj.crs.CRS.from_user_input`.
        conversion: Any
            Anything accepted by :meth:`pyproj.crs.CoordinateSystem.from_user_input`
            or a conversion from :ref:`coordinate_operation`.
        ellipsoidal_cs: Any, optional
            Input to create an Ellipsoidal Coordinate System.
            Anything accepted by :meth:`pyproj.crs.CoordinateSystem.from_user_input`
            or an Ellipsoidal Coordinate System created from :ref:`coordinate_system`.
        name: str, optional
            Name of the CRS. Default is undefined.
        """"""
        derived_geographic_crs_json = {
            ""$schema"": ""https://proj.org/schemas/v0.2/projjson.schema.json"",
            ""type"": ""DerivedGeographicCRS"",
            ""name"": name,
            ""base_crs"": CRS.from_user_input(base_crs).to_json_dict(),
            ""conversion"": CoordinateOperation.from_user_input(
                conversion
            ).to_json_dict(),
            ""coordinate_system"": CoordinateSystem.from_user_input(
                ellipsoidal_cs or Ellipsoidal2DCS()
            ).to_json_dict(),
        }
        super().__init__(derived_geographic_crs_json)","    def __init__(
        self,
        base_crs: Any,
        conversion: Any,
        ellipsoidal_cs: Any = Ellipsoidal2DCS(),
        name: str = ""undefined"",
    ) -> None:
        """"""
        Parameters
        ----------
        base_crs: Any
            Input to create the Geodetic CRS, a :class:`GeographicCRS` or
            anything accepted by :meth:`pyproj.crs.CRS.from_user_input`.
        conversion: Any
            Anything accepted by :meth:`pyproj.crs.CoordinateSystem.from_user_input`
            or a conversion from :ref:`coordinate_operation`.
        ellipsoidal_cs: Any, optional
            Input to create an Ellipsoidal Coordinate System.
            Anything accepted by :meth:`pyproj.crs.CoordinateSystem.from_user_input`
            or an Ellipsoidal Coordinate System created from :ref:`coordinate_system`.
        name: str, optional
            Name of the CRS. Default is undefined.
        """"""
        derived_geographic_crs_json = {
            ""$schema"": ""https://proj.org/schemas/v0.2/projjson.schema.json"",
            ""type"": ""DerivedGeographicCRS"",
            ""name"": name,
            ""base_crs"": CRS.from_user_input(base_crs).to_json_dict(),
            ""conversion"": CoordinateOperation.from_user_input(
                conversion
            ).to_json_dict(),
            ""coordinate_system"": CoordinateSystem.from_user_input(
                ellipsoidal_cs
            ).to_json_dict(),
        }
        super().__init__(derived_geographic_crs_json)"
"    def __init__(
        self,
        conversion: Any,
        name: str = ""undefined"",
        cartesian_cs: Any = None,
        geodetic_crs: Any = None,
    ) -> None:
        """"""
        Parameters
        ----------
        conversion: Any
            Anything accepted by :meth:`pyproj.crs.CoordinateSystem.from_user_input`
            or a conversion from :ref:`coordinate_operation`.
        name: str, optional
            The name of the Projected CRS. Default is undefined.
        cartesian_cs: Any, optional
            Input to create a Cartesian Coordinate System.
            Anything accepted by :meth:`pyproj.crs.CoordinateSystem.from_user_input`
            or :class:`pyproj.crs.coordinate_system.Cartesian2DCS`.
        geodetic_crs: Any, optional
            Input to create the Geodetic CRS, a :class:`GeographicCRS` or
            anything accepted by :meth:`pyproj.crs.CRS.from_user_input`.
        """"""
        proj_crs_json = {
            ""$schema"": ""https://proj.org/schemas/v0.2/projjson.schema.json"",
            ""type"": ""ProjectedCRS"",
            ""name"": name,
            ""base_crs"": CRS.from_user_input(
                geodetic_crs or GeographicCRS()
            ).to_json_dict(),
            ""conversion"": CoordinateOperation.from_user_input(
                conversion
            ).to_json_dict(),
            ""coordinate_system"": CoordinateSystem.from_user_input(
                cartesian_cs or Cartesian2DCS()
            ).to_json_dict(),
        }
        super().__init__(proj_crs_json)","    def __init__(
        self,
        conversion: Any,
        name: str = ""undefined"",
        cartesian_cs: Any = Cartesian2DCS(),
        geodetic_crs: Any = GeographicCRS(),
    ) -> None:
        """"""
        Parameters
        ----------
        conversion: Any
            Anything accepted by :meth:`pyproj.crs.CoordinateSystem.from_user_input`
            or a conversion from :ref:`coordinate_operation`.
        name: str, optional
            The name of the Projected CRS. Default is undefined.
        cartesian_cs: Any, optional
            Input to create a Cartesian Coordinate System.
            Anything accepted by :meth:`pyproj.crs.CoordinateSystem.from_user_input`
            or :class:`pyproj.crs.coordinate_system.Cartesian2DCS`.
        geodetic_crs: Any, optional
            Input to create the Geodetic CRS, a :class:`GeographicCRS` or
            anything accepted by :meth:`pyproj.crs.CRS.from_user_input`.
        """"""
        proj_crs_json = {
            ""$schema"": ""https://proj.org/schemas/v0.2/projjson.schema.json"",
            ""type"": ""ProjectedCRS"",
            ""name"": name,
            ""base_crs"": CRS.from_user_input(geodetic_crs).to_json_dict(),
            ""conversion"": CoordinateOperation.from_user_input(
                conversion
            ).to_json_dict(),
            ""coordinate_system"": CoordinateSystem.from_user_input(
                cartesian_cs
            ).to_json_dict(),
        }
        super().__init__(proj_crs_json)"
"    def __init__(
        self,
        name: str,
        datum: Any,
        vertical_cs: Any = None,
        geoid_model: Optional[str] = None,
    ) -> None:
        """"""
        Parameters
        ----------
        name: str
            The name of the Vertical CRS (e.g. NAVD88 height).
        datum: Any
            Anything accepted by :meth:`pyproj.crs.Datum.from_user_input`
        vertical_cs: Any, optional
            Input to create a Vertical Coordinate System accepted by
            :meth:`pyproj.crs.CoordinateSystem.from_user_input`
            or :class:`pyproj.crs.coordinate_system.VerticalCS`
        geoid_model: str, optional
            The name of the GEOID Model (e.g. GEOID12B).
        """"""
        vert_crs_json = {
            ""$schema"": ""https://proj.org/schemas/v0.2/projjson.schema.json"",
            ""type"": ""VerticalCRS"",
            ""name"": name,
            ""datum"": Datum.from_user_input(datum).to_json_dict(),
            ""coordinate_system"": CoordinateSystem.from_user_input(
                vertical_cs or VerticalCS()
            ).to_json_dict(),
        }
        if geoid_model is not None:
            vert_crs_json[""geoid_model""] = {""name"": geoid_model}

        super().__init__(vert_crs_json)","    def __init__(
        self,
        name: str,
        datum: Any,
        vertical_cs: Any = VerticalCS(),
        geoid_model: str = None,
    ) -> None:
        """"""
        Parameters
        ----------
        name: str
            The name of the Vertical CRS (e.g. NAVD88 height).
        datum: Any
            Anything accepted by :meth:`pyproj.crs.Datum.from_user_input`
        vertical_cs: Any, optional
            Input to create a Vertical Coordinate System accepted by
            :meth:`pyproj.crs.CoordinateSystem.from_user_input`
            or :class:`pyproj.crs.coordinate_system.VerticalCS`
        geoid_model: str, optional
            The name of the GEOID Model (e.g. GEOID12B).
        """"""
        vert_crs_json = {
            ""$schema"": ""https://proj.org/schemas/v0.2/projjson.schema.json"",
            ""type"": ""VerticalCRS"",
            ""name"": name,
            ""datum"": Datum.from_user_input(datum).to_json_dict(),
            ""coordinate_system"": CoordinateSystem.from_user_input(
                vertical_cs
            ).to_json_dict(),
        }
        if geoid_model is not None:
            vert_crs_json[""geoid_model""] = {""name"": geoid_model}

        super().__init__(vert_crs_json)"
"def set_data_dir(proj_data_dir):
    """"""
    Set the data directory for PROJ to use.

    Parameters
    ----------
    proj_data_dir: str
        The path to rhe PROJ data directory.
    """"""
    global _USER_PROJ_DATA
    _USER_PROJ_DATA = proj_data_dir
    # reset search paths
    from pyproj._datadir import PYPROJ_CONTEXT

    PYPROJ_CONTEXT.set_search_paths(reset=True)","def set_data_dir(proj_data_dir):
    """"""
    Set the data directory for PROJ to use.

    Parameters
    ----------
    proj_data_dir: str
        The path to rhe PROJ data directory.
    """"""
    global _USER_PROJ_DATA
    _USER_PROJ_DATA = proj_data_dir
    # reset search paths
    from pyproj._datadir import PYPROJ_CONTEXT

    PYPROJ_CONTEXT.set_search_paths()"
"def set_data_dir(proj_data_dir):
    """"""
    Set the data directory for PROJ to use.

    Parameters
    ----------
    proj_data_dir: str
        The path to rhe PROJ data directory.
    """"""
    global _USER_PROJ_DATA
    _USER_PROJ_DATA = proj_data_dir
    # reset search paths
    from pyproj._datadir import PYPROJ_CONTEXT

    PYPROJ_CONTEXT.set_search_paths()","def set_data_dir(proj_data_dir):
    """"""
    Set the data directory for PROJ to use.

    Parameters
    ----------
    proj_data_dir: str
        The path to rhe PROJ data directory.
    """"""
    global _USER_PROJ_DATA
    global _VALIDATED_PROJ_DATA
    _USER_PROJ_DATA = proj_data_dir
    # set to none to re-validate
    _VALIDATED_PROJ_DATA = None"
"def get_data_dir():
    """"""
    The order of preference for the data directory is:

    1. The one set by pyproj.datadir.set_data_dir (if exists & valid)
    2. The internal proj directory (if exists & valid)
    3. The directory in PROJ_LIB (if exists & valid)
    4. The directory on the PATH (if exists & valid)

    Returns
    -------
    str: The valid data directory.

    """"""
    global _USER_PROJ_DATA
    internal_datadir = os.path.join(
        os.path.dirname(os.path.abspath(__file__)), ""proj_dir"", ""share"", ""proj""
    )
    proj_lib_dirs = os.environ.get(""PROJ_LIB"", """")

    def valid_data_dir(potential_data_dir):
        if potential_data_dir is not None and os.path.exists(
            os.path.join(potential_data_dir, ""proj.db"")
        ):
            return True
        return False

    def valid_data_dirs(potential_data_dirs):
        if potential_data_dirs is None:
            return False
        for proj_data_dir in potential_data_dirs.split(os.pathsep):
            if valid_data_dir(proj_data_dir):
                return True
                break
        return None

    validated_proj_data = None
    if valid_data_dirs(_USER_PROJ_DATA):
        validated_proj_data = _USER_PROJ_DATA
    elif valid_data_dir(internal_datadir):
        validated_proj_data = internal_datadir
    elif valid_data_dirs(proj_lib_dirs):
        validated_proj_data = proj_lib_dirs
    else:
        proj_exe = find_executable(""proj"")
        if proj_exe is not None:
            system_proj_dir = os.path.join(
                os.path.dirname(os.path.dirname(proj_exe)), ""share"", ""proj""
            )
            if valid_data_dir(system_proj_dir):
                validated_proj_data = system_proj_dir

    if validated_proj_data is None:
        raise DataDirError(
            ""Valid PROJ data directory not found. ""
            ""Either set the path using the environmental variable PROJ_LIB or ""
            ""with `pyproj.datadir.set_data_dir`.""
        )
    return validated_proj_data","def get_data_dir():
    """"""
    The order of preference for the data directory is:

    1. The one set by pyproj.datadir.set_data_dir (if exists & valid)
    2. The internal proj directory (if exists & valid)
    3. The directory in PROJ_LIB (if exists & valid)
    4. The directory on the PATH (if exists & valid)

    Returns
    -------
    str: The valid data directory.

    """"""
    # to avoid re-validating
    global _VALIDATED_PROJ_DATA
    if _VALIDATED_PROJ_DATA is not None:
        return _VALIDATED_PROJ_DATA

    global _USER_PROJ_DATA
    internal_datadir = os.path.join(
        os.path.dirname(os.path.abspath(__file__)), ""proj_dir"", ""share"", ""proj""
    )
    proj_lib_dirs = os.environ.get(""PROJ_LIB"", """")

    def valid_data_dir(potential_data_dir):
        if potential_data_dir is not None and os.path.exists(
            os.path.join(potential_data_dir, ""proj.db"")
        ):
            return True
        return False

    def valid_data_dirs(potential_data_dirs):
        if potential_data_dirs is None:
            return False
        for proj_data_dir in potential_data_dirs.split(os.pathsep):
            if valid_data_dir(proj_data_dir):
                return True
                break
        return None

    if valid_data_dirs(_USER_PROJ_DATA):
        _VALIDATED_PROJ_DATA = _USER_PROJ_DATA
    elif valid_data_dir(internal_datadir):
        _VALIDATED_PROJ_DATA = internal_datadir
    elif valid_data_dirs(proj_lib_dirs):
        _VALIDATED_PROJ_DATA = proj_lib_dirs
    else:
        proj_exe = find_executable(""proj"")
        if proj_exe is not None:
            system_proj_dir = os.path.join(
                os.path.dirname(os.path.dirname(proj_exe)), ""share"", ""proj""
            )
            if valid_data_dir(system_proj_dir):
                _VALIDATED_PROJ_DATA = system_proj_dir

    if _VALIDATED_PROJ_DATA is None:
        raise DataDirError(
            ""Valid PROJ data directory not found. ""
            ""Either set the path using the environmental variable PROJ_LIB or ""
            ""with `pyproj.datadir.set_data_dir`.""
        )
    return _VALIDATED_PROJ_DATA"
"    def from_proj(proj_from, proj_to, skip_equivalent=False, always_xy=False):
        """"""Make a Transformer from a :obj:`~pyproj.proj.Proj` or input used to create one.

        Parameters
        ----------
        proj_from: :obj:`~pyproj.proj.Proj` or input used to create one
            Projection of input data.
        proj_to: :obj:`~pyproj.proj.Proj` or input used to create one
            Projection of output data.
        skip_equivalent: bool, optional
            If true, will skip the transformation operation if input and output 
            projections are equivalent. Default is false.
        always_xy: bool, optional
            If true, the transform method will accept as input and return as output
            coordinates using the traditional GIS order, that is longitude, latitude
            for geographic CRS and easting, northing for most projected CRS.
            Default is false.

        Returns
        -------
        :obj:`~Transformer`

        """"""
        if not isinstance(proj_from, Proj):
            proj_from = Proj(proj_from)
        if not isinstance(proj_to, Proj):
            proj_to = Proj(proj_to)

        return Transformer(
            _Transformer.from_crs(
                proj_from.crs,
                proj_to.crs,
                skip_equivalent=skip_equivalent,
                always_xy=always_xy,
            )
        )","    def from_proj(proj_from, proj_to, skip_equivalent=False, always_xy=False):
        """"""Make a Transformer from a :obj:`~pyproj.proj.Proj` or input used to create one.

        Parameters
        ----------
        proj_from: :obj:`~pyproj.proj.Proj` or input used to create one
            Projection of input data.
        proj_to: :obj:`~pyproj.proj.Proj` or input used to create one
            Projection of output data.
        skip_equivalent: bool, optional
            If true, will skip the transformation operation if input and output 
            projections are equivalent. Default is false.
        always_xy: bool, optional
            If true, the transform method will accept as input and return as output
            coordinates using the traditional GIS order, that is longitude, latitude
            for geographic CRS and easting, northing for most projected CRS.
            Default is false.

        Returns
        -------
        :obj:`~Transformer`

        """"""
        if not isinstance(proj_from, Proj):
            proj_from = Proj(proj_from)
        if not isinstance(proj_to, Proj):
            proj_to = Proj(proj_to)

        transformer = Transformer()
        transformer._transformer = _Transformer.from_crs(
            proj_from.crs,
            proj_to.crs,
            skip_equivalent=skip_equivalent,
            always_xy=always_xy,
        )
        return transformer"
"    def from_crs(crs_from, crs_to, skip_equivalent=False, always_xy=False):
        """"""Make a Transformer from a :obj:`~pyproj.crs.CRS` or input used to create one.

        Parameters
        ----------
        crs_from: ~pyproj.crs.CRS or input used to create one
            Projection of input data.
        crs_to: ~pyproj.crs.CRS or input used to create one
            Projection of output data.
        skip_equivalent: bool, optional
            If true, will skip the transformation operation if input and output
            projections are equivalent. Default is false.
        always_xy: bool, optional
            If true, the transform method will accept as input and return as output
            coordinates using the traditional GIS order, that is longitude, latitude
            for geographic CRS and easting, northing for most projected CRS.
            Default is false.

        Returns
        -------
        :obj:`~Transformer`

        """"""
        transformer = Transformer(
            _Transformer.from_crs(
                CRS.from_user_input(crs_from),
                CRS.from_user_input(crs_to),
                skip_equivalent=skip_equivalent,
                always_xy=always_xy,
            )
        )
        return transformer","    def from_crs(crs_from, crs_to, skip_equivalent=False, always_xy=False):
        """"""Make a Transformer from a :obj:`~pyproj.crs.CRS` or input used to create one.

        Parameters
        ----------
        crs_from: ~pyproj.crs.CRS or input used to create one
            Projection of input data.
        crs_to: ~pyproj.crs.CRS or input used to create one
            Projection of output data.
        skip_equivalent: bool, optional
            If true, will skip the transformation operation if input and output
            projections are equivalent. Default is false.
        always_xy: bool, optional
            If true, the transform method will accept as input and return as output
            coordinates using the traditional GIS order, that is longitude, latitude
            for geographic CRS and easting, northing for most projected CRS.
            Default is false.

        Returns
        -------
        :obj:`~Transformer`

        """"""
        transformer = Transformer()
        transformer._transformer = _Transformer.from_crs(
            CRS.from_user_input(crs_from),
            CRS.from_user_input(crs_to),
            skip_equivalent=skip_equivalent,
            always_xy=always_xy,
        )
        return transformer"
"    def from_pipeline(proj_pipeline):
        """"""Make a Transformer from a PROJ pipeline string.

        https://proj4.org/operations/pipeline.html

        Parameters
        ----------
        proj_pipeline: str
            Projection pipeline string.

        Returns
        -------
        ~Transformer

        """"""
        return Transformer(_Transformer.from_pipeline(cstrencode(proj_pipeline)))","    def from_pipeline(proj_pipeline):
        """"""Make a Transformer from a PROJ pipeline string.

        https://proj4.org/operations/pipeline.html

        Parameters
        ----------
        proj_pipeline: str
            Projection pipeline string.

        Returns
        -------
        ~Transformer

        """"""
        transformer = Transformer()
        transformer._transformer = _Transformer.from_pipeline(cstrencode(proj_pipeline))
        return transformer"
"def _dict2string(projparams):
    # convert a dict to a proj4 string.
    pjargs = []
    proj_inserted = False
    for key, value in projparams.items():
        # the towgs84 as list
        if isinstance(value, (list, tuple)):
            value = "","".join([str(val) for val in value])
        # issue 183 (+ no_rot)
        if value is None or value is True:
            pjargs.append(""+{key}"".format(key=key))
        elif value is False:
            pass
        # make sure string starts with proj or init
        elif not proj_inserted and key in (""init"", ""proj""):
            pjargs.insert(0, ""+{key}={value}"".format(key=key, value=value))
            proj_inserted = True
        else:
            pjargs.append(""+{key}={value}"".format(key=key, value=value))
    return "" "".join(pjargs)","def _dict2string(projparams):
    # convert a dict to a proj4 string.
    pjargs = []
    for key, value in projparams.items():
        # the towgs84 as list
        if isinstance(value, (list, tuple)):
            value = "","".join([str(val) for val in value])
        # issue 183 (+ no_rot)
        if value is None or value is True:
            pjargs.append(""+"" + key + "" "")
        elif value is False:
            pass
        else:
            pjargs.append(""+"" + key + ""="" + str(value) + "" "")
    return """".join(pjargs)"
"    def __init__(self, projparams=None, preserve_units=True, **kwargs):
        """"""
        initialize a Proj class instance.

        See the proj documentation (https://github.com/OSGeo/proj.4/wiki)
        for more information about projection parameters.

        Parameters
        ----------
        projparams: int, str, dict, pyproj.CRS
            A proj.4 or WKT string, proj.4 dict, EPSG integer, or a pyproj.CRS instnace.
        preserve_units: bool
            If false, will ensure +units=m.
        **kwargs:
            proj.4 projection parameters.


        Example usage:

        >>> from pyproj import Proj
        >>> p = Proj(proj='utm',zone=10,ellps='WGS84', preserve_units=False) # use kwargs
        >>> x,y = p(-120.108, 34.36116666)
        >>> 'x=%9.3f y=%11.3f' % (x,y)
        'x=765975.641 y=3805993.134'
        >>> 'lon=%8.3f lat=%5.3f' % p(x,y,inverse=True)
        'lon=-120.108 lat=34.361'
        >>> # do 3 cities at a time in a tuple (Fresno, LA, SF)
        >>> lons = (-119.72,-118.40,-122.38)
        >>> lats = (36.77, 33.93, 37.62 )
        >>> x,y = p(lons, lats)
        >>> 'x: %9.3f %9.3f %9.3f' % x
        'x: 792763.863 925321.537 554714.301'
        >>> 'y: %9.3f %9.3f %9.3f' % y
        'y: 4074377.617 3763936.941 4163835.303'
        >>> lons, lats = p(x, y, inverse=True) # inverse transform
        >>> 'lons: %8.3f %8.3f %8.3f' % lons
        'lons: -119.720 -118.400 -122.380'
        >>> 'lats: %8.3f %8.3f %8.3f' % lats
        'lats:   36.770   33.930   37.620'
        >>> p2 = Proj('+proj=utm +zone=10 +ellps=WGS84', preserve_units=False) # use proj4 string
        >>> x,y = p2(-120.108, 34.36116666)
        >>> 'x=%9.3f y=%11.3f' % (x,y)
        'x=765975.641 y=3805993.134'
        >>> p = Proj(init=""epsg:32667"", preserve_units=False)
        >>> 'x=%12.3f y=%12.3f (meters)' % p(-114.057222, 51.045)
        'x=-1783506.250 y= 6193827.033 (meters)'
        >>> p = Proj(""+init=epsg:32667"")
        >>> 'x=%12.3f y=%12.3f (feet)' % p(-114.057222, 51.045)
        'x=-5851386.754 y=20320914.191 (feet)'
        >>> # test data with radian inputs
        >>> p1 = Proj(init=""epsg:4214"")
        >>> x1, y1 = p1(116.366, 39.867)
        >>> '{:.3f} {:.3f}'.format(x1, y1)
        '2.031 0.696'
        >>> x2, y2 = p1(x1, y1, inverse=True)
        >>> '{:.3f} {:.3f}'.format(x2, y2)
        '116.366 39.867'
        """"""
        self.crs = CRS.from_user_input(projparams if projparams is not None else kwargs)
        # make sure units are meters if preserve_units is False.
        if not preserve_units and ""foot"" in self.crs.axis_info[0].unit_name:
            projstring = self.crs.to_proj4(4)
            projstring = re.sub(r""\\s\\+units=[\\w-]+"", """", projstring)
            projstring += "" +units=m""
            self.crs = CRS(projstring)
        super(Proj, self).__init__(
            cstrencode(
                (self.crs.to_proj4() or self.crs.srs).replace(""+type=crs"", """").strip()
            )
        )","    def __init__(self, projparams=None, preserve_units=True, **kwargs):
        """"""
        initialize a Proj class instance.

        See the proj documentation (https://github.com/OSGeo/proj.4/wiki)
        for more information about projection parameters.

        Parameters
        ----------
        projparams: int, str, dict, pyproj.CRS
            A proj.4 or WKT string, proj.4 dict, EPSG integer, or a pyproj.CRS instnace.
        preserve_units: bool
            If false, will ensure +units=m.
        **kwargs:
            proj.4 projection parameters.


        Example usage:

        >>> from pyproj import Proj
        >>> p = Proj(proj='utm',zone=10,ellps='WGS84', preserve_units=False) # use kwargs
        >>> x,y = p(-120.108, 34.36116666)
        >>> 'x=%9.3f y=%11.3f' % (x,y)
        'x=765975.641 y=3805993.134'
        >>> 'lon=%8.3f lat=%5.3f' % p(x,y,inverse=True)
        'lon=-120.108 lat=34.361'
        >>> # do 3 cities at a time in a tuple (Fresno, LA, SF)
        >>> lons = (-119.72,-118.40,-122.38)
        >>> lats = (36.77, 33.93, 37.62 )
        >>> x,y = p(lons, lats)
        >>> 'x: %9.3f %9.3f %9.3f' % x
        'x: 792763.863 925321.537 554714.301'
        >>> 'y: %9.3f %9.3f %9.3f' % y
        'y: 4074377.617 3763936.941 4163835.303'
        >>> lons, lats = p(x, y, inverse=True) # inverse transform
        >>> 'lons: %8.3f %8.3f %8.3f' % lons
        'lons: -119.720 -118.400 -122.380'
        >>> 'lats: %8.3f %8.3f %8.3f' % lats
        'lats:   36.770   33.930   37.620'
        >>> p2 = Proj('+proj=utm +zone=10 +ellps=WGS84', preserve_units=False) # use proj4 string
        >>> x,y = p2(-120.108, 34.36116666)
        >>> 'x=%9.3f y=%11.3f' % (x,y)
        'x=765975.641 y=3805993.134'
        >>> p = Proj(init=""epsg:32667"", preserve_units=False)
        >>> 'x=%12.3f y=%12.3f (meters)' % p(-114.057222, 51.045)
        'x=-1783506.250 y= 6193827.033 (meters)'
        >>> p = Proj(""+init=epsg:32667"")
        >>> 'x=%12.3f y=%12.3f (feet)' % p(-114.057222, 51.045)
        'x=-5851386.754 y=20320914.191 (feet)'
        >>> # test data with radian inputs
        >>> p1 = Proj(init=""epsg:4214"")
        >>> x1, y1 = p1(116.366, 39.867)
        >>> '{:.3f} {:.3f}'.format(x1, y1)
        '2.031 0.696'
        >>> x2, y2 = p1(x1, y1, inverse=True)
        >>> '{:.3f} {:.3f}'.format(x2, y2)
        '116.366 39.867'
        """"""
        self.crs = CRS.from_user_input(projparams if projparams is not None else kwargs)
        # make sure units are meters if preserve_units is False.
        if not preserve_units and ""foot"" in self.crs.axis_info[0].unit_name:
            projstring = self.crs.to_proj4(4)
            projstring = re.sub(r""\\s\\+units=[\\w-]+"", """", projstring)
            projstring += "" +units=m""
            self.crs = CRS(projstring)
        super(Proj, self).__init__(
            cstrencode(self.crs.to_proj4().replace(""+type=crs"", """").strip())
        )"
"def Kuf_conv_patch(inducing_variable, kernel, Xnew):
    Xp = kernel.get_patches(Xnew)  # [N, num_patches, patch_len]
    bigKzx = kernel.base_kernel.K(
        inducing_variable.Z, Xp
    )  # [M, N, P] -- thanks to broadcasting of kernels
    Kzx = tf.reduce_sum(bigKzx * kernel.weights if hasattr(kernel, ""weights"") else bigKzx, [2])
    return Kzx / kernel.num_patches","def Kuf_conv_patch(feat, kern, Xnew):
    Xp = kern.get_patches(Xnew)  # [N, num_patches, patch_len]
    bigKzx = kern.base_kernel.K(feat.Z, Xp)  # [M, N, P] -- thanks to broadcasting of kernels
    Kzx = tf.reduce_sum(bigKzx * kern.weights if hasattr(kern, ""weights"") else bigKzx, [2])
    return Kzx / kern.num_patches"
"def Kuu_kernel_inducingpoints(inducing_variable: InducingPoints, kernel: Kernel, *, jitter=0.0):
    Kzz = kernel(inducing_variable.Z)
    Kzz += jitter * tf.eye(inducing_variable.num_inducing, dtype=Kzz.dtype)
    return Kzz","def Kuu_kernel_inducingpoints(inducing_variable: InducingPoints, kernel: Kernel, *, jitter=0.0):
    Kzz = kernel(inducing_variable.Z)
    Kzz += jitter * tf.eye(len(inducing_variable), dtype=Kzz.dtype)
    return Kzz"
"def Kuu_sqexp_multiscale(inducing_variable: Multiscale, kernel: SquaredExponential, *, jitter=0.0):
    Zmu, Zlen = kernel.slice(inducing_variable.Z, inducing_variable.scales)
    idlengthscales2 = tf.square(kernel.lengthscales + Zlen)
    sc = tf.sqrt(
        idlengthscales2[None, ...] + idlengthscales2[:, None, ...] - kernel.lengthscales ** 2
    )
    d = inducing_variable._cust_square_dist(Zmu, Zmu, sc)
    Kzz = kernel.variance * tf.exp(-d / 2) * tf.reduce_prod(kernel.lengthscales / sc, 2)
    Kzz += jitter * tf.eye(inducing_variable.num_inducing, dtype=Kzz.dtype)
    return Kzz","def Kuu_sqexp_multiscale(inducing_variable: Multiscale, kernel: SquaredExponential, *, jitter=0.0):
    Zmu, Zlen = kernel.slice(inducing_variable.Z, inducing_variable.scales)
    idlengthscales2 = tf.square(kernel.lengthscales + Zlen)
    sc = tf.sqrt(
        idlengthscales2[None, ...] + idlengthscales2[:, None, ...] - kernel.lengthscales ** 2
    )
    d = inducing_variable._cust_square_dist(Zmu, Zmu, sc)
    Kzz = kernel.variance * tf.exp(-d / 2) * tf.reduce_prod(kernel.lengthscales / sc, 2)
    Kzz += jitter * tf.eye(len(inducing_variable), dtype=Kzz.dtype)
    return Kzz"
"def Kuu_conv_patch(inducing_variable, kernel, jitter=0.0):
    return kernel.base_kernel.K(inducing_variable.Z) + jitter * tf.eye(
        inducing_variable.num_inducing, dtype=default_float()
    )","def Kuu_conv_patch(feat, kern, jitter=0.0):
    return kern.base_kernel.K(feat.Z) + jitter * tf.eye(len(feat), dtype=default_float())"
"def _Kuu(
    inducing_variable: FallbackSeparateIndependentInducingVariables,
    kernel: Union[SeparateIndependent, LinearCoregionalization],
    *,
    jitter=0.0,
):
    Kmms = [Kuu(f, k) for f, k in zip(inducing_variable.inducing_variable_list, kernel.kernels)]
    Kmm = tf.stack(Kmms, axis=0)  # [L, M, M]
    jittermat = tf.eye(inducing_variable.num_inducing, dtype=Kmm.dtype)[None, :, :] * jitter
    return Kmm + jittermat","def _Kuu(
    inducing_variable: FallbackSeparateIndependentInducingVariables,
    kernel: Union[SeparateIndependent, LinearCoregionalization],
    *,
    jitter=0.0,
):
    Kmms = [Kuu(f, k) for f, k in zip(inducing_variable.inducing_variable_list, kernel.kernels)]
    Kmm = tf.stack(Kmms, axis=0)  # [L, M, M]
    jittermat = tf.eye(len(inducing_variable), dtype=Kmm.dtype)[None, :, :] * jitter
    return Kmm + jittermat"
"    def __init__(self, Z: TensorData, name: Optional[str] = None):
        """"""
        :param Z: the initial positions of the inducing points, size [M, D]
        """"""
        super().__init__(name=name)
        if not isinstance(Z, (tf.Variable, tfp.util.TransformedVariable)):
            Z = Parameter(Z)
        self.Z = Z","    def __init__(self, Z: TensorData, name: Optional[str] = None):
        """"""
        :param Z: the initial positions of the inducing points, size [M, D]
        """"""
        super().__init__(name=name)
        self.Z = Parameter(Z, dtype=default_float())"
"    def __len__(self) -> int:
        return tf.shape(self.Z)[0]","    def __len__(self) -> int:
        return self.Z.shape[0]"
"    def __len__(self) -> int:
        return self.inducing_variable.num_inducing","    def __len__(self) -> int:
        return len(self.inducing_variable)"
"    def __len__(self) -> int:
        # TODO(st--) we should check that they all have the same length...
        return self.inducing_variable_list[0].num_inducing","    def __len__(self) -> int:
        return len(self.inducing_variable_list[0])"
"    def __init__(
        self,
        distribution_class: Type[tfp.distributions.Distribution] = tfp.distributions.Normal,
        scale_transform: Optional[tfp.bijectors.Bijector] = None,
        **kwargs,
    ):
        """"""
        :param distribution_class: distribution class parameterized by `loc` and `scale`
            as first and second argument, respectively.
        :param scale_transform: callable/bijector applied to the latent
            function modelling the scale to ensure its positivity.
            Typically, `tf.exp` or `tf.softplus`, but can be any function f: R -> R^+. Defaults to exp if not explicitly specified. 
        """"""
        if scale_transform is None:
            scale_transform = positive(base=""exp"")
        self.scale_transform = scale_transform

        def conditional_distribution(Fs) -> tfp.distributions.Distribution:
            tf.debugging.assert_equal(tf.shape(Fs)[-1], 2)
            loc = Fs[..., :1]
            scale = self.scale_transform(Fs[..., 1:])
            return distribution_class(loc, scale)

        super().__init__(
            latent_dim=2, conditional_distribution=conditional_distribution, **kwargs,
        )","    def __init__(
        self,
        distribution_class: Type[tfp.distributions.Distribution] = tfp.distributions.Normal,
        scale_transform: tfp.bijectors.Bijector = positive(base=""exp""),
        **kwargs,
    ):
        """"""
        :param distribution_class: distribution class parameterized by `loc` and `scale`
            as first and second argument, respectively.
        :param scale_transform: callable/bijector applied to the latent
            function modelling the scale to ensure its positivity.
            Typically, `tf.exp` or `tf.softplus`, but can be any function f: R -> R^+.
        """"""

        def conditional_distribution(Fs) -> tfp.distributions.Distribution:
            tf.debugging.assert_equal(tf.shape(Fs)[-1], 2)
            loc = Fs[..., :1]
            scale = scale_transform(Fs[..., 1:])
            return distribution_class(loc, scale)

        super().__init__(
            latent_dim=2, conditional_distribution=conditional_distribution, **kwargs,
        )"
"        def conditional_distribution(Fs) -> tfp.distributions.Distribution:
            tf.debugging.assert_equal(tf.shape(Fs)[-1], 2)
            loc = Fs[..., :1]
            scale = self.scale_transform(Fs[..., 1:])
            return distribution_class(loc, scale)","        def conditional_distribution(Fs) -> tfp.distributions.Distribution:
            tf.debugging.assert_equal(tf.shape(Fs)[-1], 2)
            loc = Fs[..., :1]
            scale = scale_transform(Fs[..., 1:])
            return distribution_class(loc, scale)"
"    def elbo(self) -> tf.Tensor:
        """"""
        Construct a tensorflow function to compute the bound on the marginal
        likelihood.
        """"""
        Y_data = self.data

        pX = DiagonalGaussian(self.X_data_mean, self.X_data_var)

        num_inducing = self.inducing_variable.num_inducing
        psi0 = tf.reduce_sum(expectation(pX, self.kernel))
        psi1 = expectation(pX, (self.kernel, self.inducing_variable))
        psi2 = tf.reduce_sum(
            expectation(
                pX, (self.kernel, self.inducing_variable), (self.kernel, self.inducing_variable)
            ),
            axis=0,
        )
        cov_uu = covariances.Kuu(self.inducing_variable, self.kernel, jitter=default_jitter())
        L = tf.linalg.cholesky(cov_uu)
        sigma2 = self.likelihood.variance
        sigma = tf.sqrt(sigma2)

        # Compute intermediate matrices
        A = tf.linalg.triangular_solve(L, tf.transpose(psi1), lower=True) / sigma
        tmp = tf.linalg.triangular_solve(L, psi2, lower=True)
        AAT = tf.linalg.triangular_solve(L, tf.transpose(tmp), lower=True) / sigma2
        B = AAT + tf.eye(num_inducing, dtype=default_float())
        LB = tf.linalg.cholesky(B)
        log_det_B = 2.0 * tf.reduce_sum(tf.math.log(tf.linalg.diag_part(LB)))
        c = tf.linalg.triangular_solve(LB, tf.linalg.matmul(A, Y_data), lower=True) / sigma

        # KL[q(x) || p(x)]
        dX_data_var = (
            self.X_data_var
            if self.X_data_var.shape.ndims == 2
            else tf.linalg.diag_part(self.X_data_var)
        )
        NQ = to_default_float(tf.size(self.X_data_mean))
        D = to_default_float(tf.shape(Y_data)[1])
        KL = -0.5 * tf.reduce_sum(tf.math.log(dX_data_var))
        KL += 0.5 * tf.reduce_sum(tf.math.log(self.X_prior_var))
        KL -= 0.5 * NQ
        KL += 0.5 * tf.reduce_sum(
            (tf.square(self.X_data_mean - self.X_prior_mean) + dX_data_var) / self.X_prior_var
        )

        # compute log marginal bound
        ND = to_default_float(tf.size(Y_data))
        bound = -0.5 * ND * tf.math.log(2 * np.pi * sigma2)
        bound += -0.5 * D * log_det_B
        bound += -0.5 * tf.reduce_sum(tf.square(Y_data)) / sigma2
        bound += 0.5 * tf.reduce_sum(tf.square(c))
        bound += -0.5 * D * (tf.reduce_sum(psi0) / sigma2 - tf.reduce_sum(tf.linalg.diag_part(AAT)))
        bound -= KL
        return bound","    def elbo(self) -> tf.Tensor:
        """"""
        Construct a tensorflow function to compute the bound on the marginal
        likelihood.
        """"""
        Y_data = self.data

        pX = DiagonalGaussian(self.X_data_mean, self.X_data_var)

        num_inducing = len(self.inducing_variable)
        psi0 = tf.reduce_sum(expectation(pX, self.kernel))
        psi1 = expectation(pX, (self.kernel, self.inducing_variable))
        psi2 = tf.reduce_sum(
            expectation(
                pX, (self.kernel, self.inducing_variable), (self.kernel, self.inducing_variable)
            ),
            axis=0,
        )
        cov_uu = covariances.Kuu(self.inducing_variable, self.kernel, jitter=default_jitter())
        L = tf.linalg.cholesky(cov_uu)
        sigma2 = self.likelihood.variance
        sigma = tf.sqrt(sigma2)

        # Compute intermediate matrices
        A = tf.linalg.triangular_solve(L, tf.transpose(psi1), lower=True) / sigma
        tmp = tf.linalg.triangular_solve(L, psi2, lower=True)
        AAT = tf.linalg.triangular_solve(L, tf.transpose(tmp), lower=True) / sigma2
        B = AAT + tf.eye(num_inducing, dtype=default_float())
        LB = tf.linalg.cholesky(B)
        log_det_B = 2.0 * tf.reduce_sum(tf.math.log(tf.linalg.diag_part(LB)))
        c = tf.linalg.triangular_solve(LB, tf.linalg.matmul(A, Y_data), lower=True) / sigma

        # KL[q(x) || p(x)]
        dX_data_var = (
            self.X_data_var
            if self.X_data_var.shape.ndims == 2
            else tf.linalg.diag_part(self.X_data_var)
        )
        NQ = to_default_float(tf.size(self.X_data_mean))
        D = to_default_float(tf.shape(Y_data)[1])
        KL = -0.5 * tf.reduce_sum(tf.math.log(dX_data_var))
        KL += 0.5 * tf.reduce_sum(tf.math.log(self.X_prior_var))
        KL -= 0.5 * NQ
        KL += 0.5 * tf.reduce_sum(
            (tf.square(self.X_data_mean - self.X_prior_mean) + dX_data_var) / self.X_prior_var
        )

        # compute log marginal bound
        ND = to_default_float(tf.size(Y_data))
        bound = -0.5 * ND * tf.math.log(2 * np.pi * sigma2)
        bound += -0.5 * D * log_det_B
        bound += -0.5 * tf.reduce_sum(tf.square(Y_data)) / sigma2
        bound += 0.5 * tf.reduce_sum(tf.square(c))
        bound += -0.5 * D * (tf.reduce_sum(psi0) / sigma2 - tf.reduce_sum(tf.linalg.diag_part(AAT)))
        bound -= KL
        return bound"
"    def predict_f(
        self, Xnew: InputData, full_cov: bool = False, full_output_cov: bool = False
    ) -> MeanAndVariance:
        """"""
        Compute the mean and variance of the latent function at some new points.
        Note that this is very similar to the SGPR prediction, for which
        there are notes in the SGPR notebook.

        Note: This model does not allow full output covariances.

        :param Xnew: points at which to predict
        """"""
        if full_output_cov:
            raise NotImplementedError

        pX = DiagonalGaussian(self.X_data_mean, self.X_data_var)

        Y_data = self.data
        num_inducing = self.inducing_variable.num_inducing
        psi1 = expectation(pX, (self.kernel, self.inducing_variable))
        psi2 = tf.reduce_sum(
            expectation(
                pX, (self.kernel, self.inducing_variable), (self.kernel, self.inducing_variable)
            ),
            axis=0,
        )
        jitter = default_jitter()
        Kus = covariances.Kuf(self.inducing_variable, self.kernel, Xnew)
        sigma2 = self.likelihood.variance
        sigma = tf.sqrt(sigma2)
        L = tf.linalg.cholesky(covariances.Kuu(self.inducing_variable, self.kernel, jitter=jitter))

        A = tf.linalg.triangular_solve(L, tf.transpose(psi1), lower=True) / sigma
        tmp = tf.linalg.triangular_solve(L, psi2, lower=True)
        AAT = tf.linalg.triangular_solve(L, tf.transpose(tmp), lower=True) / sigma2
        B = AAT + tf.eye(num_inducing, dtype=default_float())
        LB = tf.linalg.cholesky(B)
        c = tf.linalg.triangular_solve(LB, tf.linalg.matmul(A, Y_data), lower=True) / sigma
        tmp1 = tf.linalg.triangular_solve(L, Kus, lower=True)
        tmp2 = tf.linalg.triangular_solve(LB, tmp1, lower=True)
        mean = tf.linalg.matmul(tmp2, c, transpose_a=True)
        if full_cov:
            var = (
                self.kernel(Xnew)
                + tf.linalg.matmul(tmp2, tmp2, transpose_a=True)
                - tf.linalg.matmul(tmp1, tmp1, transpose_a=True)
            )
            shape = tf.stack([1, 1, tf.shape(Y_data)[1]])
            var = tf.tile(tf.expand_dims(var, 2), shape)
        else:
            var = (
                self.kernel(Xnew, full_cov=False)
                + tf.reduce_sum(tf.square(tmp2), axis=0)
                - tf.reduce_sum(tf.square(tmp1), axis=0)
            )
            shape = tf.stack([1, tf.shape(Y_data)[1]])
            var = tf.tile(tf.expand_dims(var, 1), shape)
        return mean + self.mean_function(Xnew), var","    def predict_f(
        self, Xnew: InputData, full_cov: bool = False, full_output_cov: bool = False
    ) -> MeanAndVariance:
        """"""
        Compute the mean and variance of the latent function at some new points.
        Note that this is very similar to the SGPR prediction, for which
        there are notes in the SGPR notebook.

        Note: This model does not allow full output covariances.

        :param Xnew: points at which to predict
        """"""
        if full_output_cov:
            raise NotImplementedError

        pX = DiagonalGaussian(self.X_data_mean, self.X_data_var)

        Y_data = self.data
        num_inducing = len(self.inducing_variable)
        psi1 = expectation(pX, (self.kernel, self.inducing_variable))
        psi2 = tf.reduce_sum(
            expectation(
                pX, (self.kernel, self.inducing_variable), (self.kernel, self.inducing_variable)
            ),
            axis=0,
        )
        jitter = default_jitter()
        Kus = covariances.Kuf(self.inducing_variable, self.kernel, Xnew)
        sigma2 = self.likelihood.variance
        sigma = tf.sqrt(sigma2)
        L = tf.linalg.cholesky(covariances.Kuu(self.inducing_variable, self.kernel, jitter=jitter))

        A = tf.linalg.triangular_solve(L, tf.transpose(psi1), lower=True) / sigma
        tmp = tf.linalg.triangular_solve(L, psi2, lower=True)
        AAT = tf.linalg.triangular_solve(L, tf.transpose(tmp), lower=True) / sigma2
        B = AAT + tf.eye(num_inducing, dtype=default_float())
        LB = tf.linalg.cholesky(B)
        c = tf.linalg.triangular_solve(LB, tf.linalg.matmul(A, Y_data), lower=True) / sigma
        tmp1 = tf.linalg.triangular_solve(L, Kus, lower=True)
        tmp2 = tf.linalg.triangular_solve(LB, tmp1, lower=True)
        mean = tf.linalg.matmul(tmp2, c, transpose_a=True)
        if full_cov:
            var = (
                self.kernel(Xnew)
                + tf.linalg.matmul(tmp2, tmp2, transpose_a=True)
                - tf.linalg.matmul(tmp1, tmp1, transpose_a=True)
            )
            shape = tf.stack([1, 1, tf.shape(Y_data)[1]])
            var = tf.tile(tf.expand_dims(var, 2), shape)
        else:
            var = (
                self.kernel(Xnew, full_cov=False)
                + tf.reduce_sum(tf.square(tmp2), axis=0)
                - tf.reduce_sum(tf.square(tmp1), axis=0)
            )
            shape = tf.stack([1, tf.shape(Y_data)[1]])
            var = tf.tile(tf.expand_dims(var, 1), shape)
        return mean + self.mean_function(Xnew), var"
"    def __init__(
        self,
        data: RegressionData,
        kernel: Kernel,
        likelihood: Likelihood,
        mean_function: Optional[MeanFunction] = None,
        num_latent_gps: Optional[int] = None,
        inducing_variable: Optional[InducingPoints] = None,
    ):
        """"""
        data is a tuple of X, Y with X, a data matrix, size [N, D] and Y, a data matrix, size [N, R]
        Z is a data matrix, of inducing inputs, size [M, D]
        kernel, likelihood, mean_function are appropriate GPflow objects
        """"""
        if num_latent_gps is None:
            num_latent_gps = self.calc_num_latent_gps_from_data(data, kernel, likelihood)
        super().__init__(kernel, likelihood, mean_function, num_latent_gps=num_latent_gps)
        self.data = data_input_to_tensor(data)
        self.num_data = data[0].shape[0]
        self.inducing_variable = inducingpoint_wrapper(inducing_variable)
        self.V = Parameter(np.zeros((self.inducing_variable.num_inducing, self.num_latent_gps)))
        self.V.prior = tfp.distributions.Normal(
            loc=to_default_float(0.0), scale=to_default_float(1.0)
        )","    def __init__(
        self,
        data: RegressionData,
        kernel: Kernel,
        likelihood: Likelihood,
        mean_function: Optional[MeanFunction] = None,
        num_latent_gps: Optional[int] = None,
        inducing_variable: Optional[InducingPoints] = None,
    ):
        """"""
        data is a tuple of X, Y with X, a data matrix, size [N, D] and Y, a data matrix, size [N, R]
        Z is a data matrix, of inducing inputs, size [M, D]
        kernel, likelihood, mean_function are appropriate GPflow objects
        """"""
        if num_latent_gps is None:
            num_latent_gps = self.calc_num_latent_gps_from_data(data, kernel, likelihood)
        super().__init__(kernel, likelihood, mean_function, num_latent_gps=num_latent_gps)
        self.data = data_input_to_tensor(data)
        self.num_data = data[0].shape[0]
        self.inducing_variable = inducingpoint_wrapper(inducing_variable)
        self.V = Parameter(np.zeros((len(self.inducing_variable), self.num_latent_gps)))
        self.V.prior = tfp.distributions.Normal(
            loc=to_default_float(0.0), scale=to_default_float(1.0)
        )"
"    def upper_bound(self) -> tf.Tensor:
        """"""
        Upper bound for the sparse GP regression marginal likelihood.  Note that
        the same inducing points are used for calculating the upper bound, as are
        used for computing the likelihood approximation. This may not lead to the
        best upper bound. The upper bound can be tightened by optimising Z, just
        like the lower bound. This is especially important in FITC, as FITC is
        known to produce poor inducing point locations. An optimisable upper bound
        can be found in https://github.com/markvdw/gp_upper.

        The key reference is

        ::

          @misc{titsias_2014,
            title={Variational Inference for Gaussian and Determinantal Point Processes},
            url={http://www2.aueb.gr/users/mtitsias/papers/titsiasNipsVar14.pdf},
            publisher={Workshop on Advances in Variational Inference (NIPS 2014)},
            author={Titsias, Michalis K.},
            year={2014},
            month={Dec}
          }

        The key quantity, the trace term, can be computed via

        >>> _, v = conditionals.conditional(X, model.inducing_variable.Z, model.kernel,
        ...                                 np.zeros((model.inducing_variable.num_inducing, 1)))

        which computes each individual element of the trace term.
        """"""
        X_data, Y_data = self.data
        num_data = to_default_float(tf.shape(Y_data)[0])

        Kdiag = self.kernel(X_data, full_cov=False)
        kuu = Kuu(self.inducing_variable, self.kernel, jitter=default_jitter())
        kuf = Kuf(self.inducing_variable, self.kernel, X_data)

        I = tf.eye(tf.shape(kuu)[0], dtype=default_float())

        L = tf.linalg.cholesky(kuu)
        A = tf.linalg.triangular_solve(L, kuf, lower=True)
        AAT = tf.linalg.matmul(A, A, transpose_b=True)
        B = I + AAT / self.likelihood.variance
        LB = tf.linalg.cholesky(B)

        # Using the Trace bound, from Titsias' presentation
        c = tf.reduce_sum(Kdiag) - tf.reduce_sum(tf.square(A))

        # Alternative bound on max eigenval:
        corrected_noise = self.likelihood.variance + c

        const = -0.5 * num_data * tf.math.log(2 * np.pi * self.likelihood.variance)
        logdet = -tf.reduce_sum(tf.math.log(tf.linalg.diag_part(LB)))

        err = Y_data - self.mean_function(X_data)
        LC = tf.linalg.cholesky(I + AAT / corrected_noise)
        v = tf.linalg.triangular_solve(LC, tf.linalg.matmul(A, err) / corrected_noise, lower=True)
        quad = -0.5 * tf.reduce_sum(tf.square(err)) / corrected_noise + 0.5 * tf.reduce_sum(
            tf.square(v)
        )

        return const + logdet + quad","    def upper_bound(self) -> tf.Tensor:
        """"""
        Upper bound for the sparse GP regression marginal likelihood.  Note that
        the same inducing points are used for calculating the upper bound, as are
        used for computing the likelihood approximation. This may not lead to the
        best upper bound. The upper bound can be tightened by optimising Z, just
        like the lower bound. This is especially important in FITC, as FITC is
        known to produce poor inducing point locations. An optimisable upper bound
        can be found in https://github.com/markvdw/gp_upper.

        The key reference is

        ::

          @misc{titsias_2014,
            title={Variational Inference for Gaussian and Determinantal Point Processes},
            url={http://www2.aueb.gr/users/mtitsias/papers/titsiasNipsVar14.pdf},
            publisher={Workshop on Advances in Variational Inference (NIPS 2014)},
            author={Titsias, Michalis K.},
            year={2014},
            month={Dec}
          }

        The key quantity, the trace term, can be computed via

        >>> _, v = conditionals.conditional(X, model.inducing_variable.Z, model.kernel,
        ...                                 np.zeros((len(model.inducing_variable), 1)))

        which computes each individual element of the trace term.
        """"""
        X_data, Y_data = self.data
        num_data = to_default_float(tf.shape(Y_data)[0])

        Kdiag = self.kernel(X_data, full_cov=False)
        kuu = Kuu(self.inducing_variable, self.kernel, jitter=default_jitter())
        kuf = Kuf(self.inducing_variable, self.kernel, X_data)

        I = tf.eye(tf.shape(kuu)[0], dtype=default_float())

        L = tf.linalg.cholesky(kuu)
        A = tf.linalg.triangular_solve(L, kuf, lower=True)
        AAT = tf.linalg.matmul(A, A, transpose_b=True)
        B = I + AAT / self.likelihood.variance
        LB = tf.linalg.cholesky(B)

        # Using the Trace bound, from Titsias' presentation
        c = tf.reduce_sum(Kdiag) - tf.reduce_sum(tf.square(A))

        # Alternative bound on max eigenval:
        corrected_noise = self.likelihood.variance + c

        const = -0.5 * num_data * tf.math.log(2 * np.pi * self.likelihood.variance)
        logdet = -tf.reduce_sum(tf.math.log(tf.linalg.diag_part(LB)))

        err = Y_data - self.mean_function(X_data)
        LC = tf.linalg.cholesky(I + AAT / corrected_noise)
        v = tf.linalg.triangular_solve(LC, tf.linalg.matmul(A, err) / corrected_noise, lower=True)
        quad = -0.5 * tf.reduce_sum(tf.square(err)) / corrected_noise + 0.5 * tf.reduce_sum(
            tf.square(v)
        )

        return const + logdet + quad"
"    def elbo(self) -> tf.Tensor:
        """"""
        Construct a tensorflow function to compute the bound on the marginal
        likelihood. For a derivation of the terms in here, see the associated
        SGPR notebook.
        """"""
        X_data, Y_data = self.data

        num_inducing = self.inducing_variable.num_inducing
        num_data = to_default_float(tf.shape(Y_data)[0])
        output_dim = to_default_float(tf.shape(Y_data)[1])

        err = Y_data - self.mean_function(X_data)
        Kdiag = self.kernel(X_data, full_cov=False)
        kuf = Kuf(self.inducing_variable, self.kernel, X_data)
        kuu = Kuu(self.inducing_variable, self.kernel, jitter=default_jitter())
        L = tf.linalg.cholesky(kuu)
        sigma = tf.sqrt(self.likelihood.variance)

        # Compute intermediate matrices
        A = tf.linalg.triangular_solve(L, kuf, lower=True) / sigma
        AAT = tf.linalg.matmul(A, A, transpose_b=True)
        B = AAT + tf.eye(num_inducing, dtype=default_float())
        LB = tf.linalg.cholesky(B)
        Aerr = tf.linalg.matmul(A, err)
        c = tf.linalg.triangular_solve(LB, Aerr, lower=True) / sigma

        # compute log marginal bound
        bound = -0.5 * num_data * output_dim * np.log(2 * np.pi)
        bound += tf.negative(output_dim) * tf.reduce_sum(tf.math.log(tf.linalg.diag_part(LB)))
        bound -= 0.5 * num_data * output_dim * tf.math.log(self.likelihood.variance)
        bound += -0.5 * tf.reduce_sum(tf.square(err)) / self.likelihood.variance
        bound += 0.5 * tf.reduce_sum(tf.square(c))
        bound += -0.5 * output_dim * tf.reduce_sum(Kdiag) / self.likelihood.variance
        bound += 0.5 * output_dim * tf.reduce_sum(tf.linalg.diag_part(AAT))

        return bound","    def elbo(self) -> tf.Tensor:
        """"""
        Construct a tensorflow function to compute the bound on the marginal
        likelihood. For a derivation of the terms in here, see the associated
        SGPR notebook.
        """"""
        X_data, Y_data = self.data

        num_inducing = len(self.inducing_variable)
        num_data = to_default_float(tf.shape(Y_data)[0])
        output_dim = to_default_float(tf.shape(Y_data)[1])

        err = Y_data - self.mean_function(X_data)
        Kdiag = self.kernel(X_data, full_cov=False)
        kuf = Kuf(self.inducing_variable, self.kernel, X_data)
        kuu = Kuu(self.inducing_variable, self.kernel, jitter=default_jitter())
        L = tf.linalg.cholesky(kuu)
        sigma = tf.sqrt(self.likelihood.variance)

        # Compute intermediate matrices
        A = tf.linalg.triangular_solve(L, kuf, lower=True) / sigma
        AAT = tf.linalg.matmul(A, A, transpose_b=True)
        B = AAT + tf.eye(num_inducing, dtype=default_float())
        LB = tf.linalg.cholesky(B)
        Aerr = tf.linalg.matmul(A, err)
        c = tf.linalg.triangular_solve(LB, Aerr, lower=True) / sigma

        # compute log marginal bound
        bound = -0.5 * num_data * output_dim * np.log(2 * np.pi)
        bound += tf.negative(output_dim) * tf.reduce_sum(tf.math.log(tf.linalg.diag_part(LB)))
        bound -= 0.5 * num_data * output_dim * tf.math.log(self.likelihood.variance)
        bound += -0.5 * tf.reduce_sum(tf.square(err)) / self.likelihood.variance
        bound += 0.5 * tf.reduce_sum(tf.square(c))
        bound += -0.5 * output_dim * tf.reduce_sum(Kdiag) / self.likelihood.variance
        bound += 0.5 * output_dim * tf.reduce_sum(tf.linalg.diag_part(AAT))

        return bound"
"    def predict_f(self, Xnew: InputData, full_cov=False, full_output_cov=False) -> MeanAndVariance:
        """"""
        Compute the mean and variance of the latent function at some new points
        Xnew. For a derivation of the terms in here, see the associated SGPR
        notebook.
        """"""
        X_data, Y_data = self.data
        num_inducing = self.inducing_variable.num_inducing
        err = Y_data - self.mean_function(X_data)
        kuf = Kuf(self.inducing_variable, self.kernel, X_data)
        kuu = Kuu(self.inducing_variable, self.kernel, jitter=default_jitter())
        Kus = Kuf(self.inducing_variable, self.kernel, Xnew)
        sigma = tf.sqrt(self.likelihood.variance)
        L = tf.linalg.cholesky(kuu)
        A = tf.linalg.triangular_solve(L, kuf, lower=True) / sigma
        B = tf.linalg.matmul(A, A, transpose_b=True) + tf.eye(num_inducing, dtype=default_float())
        LB = tf.linalg.cholesky(B)
        Aerr = tf.linalg.matmul(A, err)
        c = tf.linalg.triangular_solve(LB, Aerr, lower=True) / sigma
        tmp1 = tf.linalg.triangular_solve(L, Kus, lower=True)
        tmp2 = tf.linalg.triangular_solve(LB, tmp1, lower=True)
        mean = tf.linalg.matmul(tmp2, c, transpose_a=True)
        if full_cov:
            var = (
                self.kernel(Xnew)
                + tf.linalg.matmul(tmp2, tmp2, transpose_a=True)
                - tf.linalg.matmul(tmp1, tmp1, transpose_a=True)
            )
            var = tf.tile(var[None, ...], [self.num_latent_gps, 1, 1])  # [P, N, N]
        else:
            var = (
                self.kernel(Xnew, full_cov=False)
                + tf.reduce_sum(tf.square(tmp2), 0)
                - tf.reduce_sum(tf.square(tmp1), 0)
            )
            var = tf.tile(var[:, None], [1, self.num_latent_gps])
        return mean + self.mean_function(Xnew), var","    def predict_f(self, Xnew: InputData, full_cov=False, full_output_cov=False) -> MeanAndVariance:
        """"""
        Compute the mean and variance of the latent function at some new points
        Xnew. For a derivation of the terms in here, see the associated SGPR
        notebook.
        """"""
        X_data, Y_data = self.data
        num_inducing = len(self.inducing_variable)
        err = Y_data - self.mean_function(X_data)
        kuf = Kuf(self.inducing_variable, self.kernel, X_data)
        kuu = Kuu(self.inducing_variable, self.kernel, jitter=default_jitter())
        Kus = Kuf(self.inducing_variable, self.kernel, Xnew)
        sigma = tf.sqrt(self.likelihood.variance)
        L = tf.linalg.cholesky(kuu)
        A = tf.linalg.triangular_solve(L, kuf, lower=True) / sigma
        B = tf.linalg.matmul(A, A, transpose_b=True) + tf.eye(num_inducing, dtype=default_float())
        LB = tf.linalg.cholesky(B)
        Aerr = tf.linalg.matmul(A, err)
        c = tf.linalg.triangular_solve(LB, Aerr, lower=True) / sigma
        tmp1 = tf.linalg.triangular_solve(L, Kus, lower=True)
        tmp2 = tf.linalg.triangular_solve(LB, tmp1, lower=True)
        mean = tf.linalg.matmul(tmp2, c, transpose_a=True)
        if full_cov:
            var = (
                self.kernel(Xnew)
                + tf.linalg.matmul(tmp2, tmp2, transpose_a=True)
                - tf.linalg.matmul(tmp1, tmp1, transpose_a=True)
            )
            var = tf.tile(var[None, ...], [self.num_latent_gps, 1, 1])  # [P, N, N]
        else:
            var = (
                self.kernel(Xnew, full_cov=False)
                + tf.reduce_sum(tf.square(tmp2), 0)
                - tf.reduce_sum(tf.square(tmp1), 0)
            )
            var = tf.tile(var[:, None], [1, self.num_latent_gps])
        return mean + self.mean_function(Xnew), var"
"    def common_terms(self):
        X_data, Y_data = self.data
        num_inducing = self.inducing_variable.num_inducing
        err = Y_data - self.mean_function(X_data)  # size [N, R]
        Kdiag = self.kernel(X_data, full_cov=False)
        kuf = Kuf(self.inducing_variable, self.kernel, X_data)
        kuu = Kuu(self.inducing_variable, self.kernel, jitter=default_jitter())

        Luu = tf.linalg.cholesky(kuu)  # => Luu Luu^T = kuu
        V = tf.linalg.triangular_solve(Luu, kuf)  # => V^T V = Qff = kuf^T kuu^-1 kuf

        diagQff = tf.reduce_sum(tf.square(V), 0)
        nu = Kdiag - diagQff + self.likelihood.variance

        B = tf.eye(num_inducing, dtype=default_float()) + tf.linalg.matmul(
            V / nu, V, transpose_b=True
        )
        L = tf.linalg.cholesky(B)
        beta = err / tf.expand_dims(nu, 1)  # size [N, R]
        alpha = tf.linalg.matmul(V, beta)  # size [N, R]

        gamma = tf.linalg.triangular_solve(L, alpha, lower=True)  # size [N, R]

        return err, nu, Luu, L, alpha, beta, gamma","    def common_terms(self):
        X_data, Y_data = self.data
        num_inducing = len(self.inducing_variable)
        err = Y_data - self.mean_function(X_data)  # size [N, R]
        Kdiag = self.kernel(X_data, full_cov=False)
        kuf = Kuf(self.inducing_variable, self.kernel, X_data)
        kuu = Kuu(self.inducing_variable, self.kernel, jitter=default_jitter())

        Luu = tf.linalg.cholesky(kuu)  # => Luu Luu^T = kuu
        V = tf.linalg.triangular_solve(Luu, kuf)  # => V^T V = Qff = kuf^T kuu^-1 kuf

        diagQff = tf.reduce_sum(tf.square(V), 0)
        nu = Kdiag - diagQff + self.likelihood.variance

        B = tf.eye(num_inducing, dtype=default_float()) + tf.linalg.matmul(
            V / nu, V, transpose_b=True
        )
        L = tf.linalg.cholesky(B)
        beta = err / tf.expand_dims(nu, 1)  # size [N, R]
        alpha = tf.linalg.matmul(V, beta)  # size [N, R]

        gamma = tf.linalg.triangular_solve(L, alpha, lower=True)  # size [N, R]

        return err, nu, Luu, L, alpha, beta, gamma"
"    def __init__(
        self,
        kernel,
        likelihood,
        inducing_variable,
        *,
        mean_function=None,
        num_latent_gps: int = 1,
        q_diag: bool = False,
        q_mu=None,
        q_sqrt=None,
        whiten: bool = True,
        num_data=None,
    ):
        """"""
        - kernel, likelihood, inducing_variables, mean_function are appropriate
          GPflow objects
        - num_latent_gps is the number of latent processes to use, defaults to 1
        - q_diag is a boolean. If True, the covariance is approximated by a
          diagonal matrix.
        - whiten is a boolean. If True, we use the whitened representation of
          the inducing points.
        - num_data is the total number of observations, defaults to X.shape[0]
          (relevant when feeding in external minibatches)
        """"""
        # init the super class, accept args
        super().__init__(kernel, likelihood, mean_function, num_latent_gps)
        self.num_data = num_data
        self.q_diag = q_diag
        self.whiten = whiten
        self.inducing_variable = inducingpoint_wrapper(inducing_variable)

        # init variational parameters
        num_inducing = self.inducing_variable.num_inducing
        self._init_variational_parameters(num_inducing, q_mu, q_sqrt, q_diag)","    def __init__(
        self,
        kernel,
        likelihood,
        inducing_variable,
        *,
        mean_function=None,
        num_latent_gps: int = 1,
        q_diag: bool = False,
        q_mu=None,
        q_sqrt=None,
        whiten: bool = True,
        num_data=None,
    ):
        """"""
        - kernel, likelihood, inducing_variables, mean_function are appropriate
          GPflow objects
        - num_latent_gps is the number of latent processes to use, defaults to 1
        - q_diag is a boolean. If True, the covariance is approximated by a
          diagonal matrix.
        - whiten is a boolean. If True, we use the whitened representation of
          the inducing points.
        - num_data is the total number of observations, defaults to X.shape[0]
          (relevant when feeding in external minibatches)
        """"""
        # init the super class, accept args
        super().__init__(kernel, likelihood, mean_function, num_latent_gps)
        self.num_data = num_data
        self.q_diag = q_diag
        self.whiten = whiten
        self.inducing_variable = inducingpoint_wrapper(inducing_variable)

        # init variational parameters
        num_inducing = len(self.inducing_variable)
        self._init_variational_parameters(num_inducing, q_mu, q_sqrt, q_diag)"
"def ndiagquad(funcs, H: int, Fmu, Fvar, logspace: bool = False, **Ys):
    """"""
    Computes N Gaussian expectation integrals of one or more functions
    using Gauss-Hermite quadrature. The Gaussians must be independent.

    The means and variances of the Gaussians are specified by Fmu and Fvar.
    The N-integrals are assumed to be taken wrt the last dimensions of Fmu, Fvar.

    :param funcs: the integrand(s):
        Callable or Iterable of Callables that operates elementwise
    :param H: number of Gauss-Hermite quadrature points
    :param Fmu: array/tensor or `Din`-tuple/list thereof
    :param Fvar: array/tensor or `Din`-tuple/list thereof
    :param logspace: if True, funcs are the log-integrands and this calculates
        the log-expectation of exp(funcs)
    :param **Ys: arrays/tensors; deterministic arguments to be passed by name

    Fmu, Fvar, Ys should all have same shape, with overall size `N`
    :return: shape is the same as that of the first Fmu
    """"""
    n_gh = H
    if isinstance(Fmu, (tuple, list)):
        dim = len(Fmu)
        shape = tf.shape(Fmu[0])
        Fmu = tf.stack(Fmu, axis=-1)
        Fvar = tf.stack(Fvar, axis=-1)
    else:
        dim = 1
        shape = tf.shape(Fmu)

    Fmu = tf.reshape(Fmu, (-1, dim))
    Fvar = tf.reshape(Fvar, (-1, dim))

    Ys = {Yname: tf.reshape(Y, (-1, 1)) for Yname, Y in Ys.items()}

    def wrapper(old_fun):
        def new_fun(X, **Ys):
            Xs = tf.unstack(X, axis=-1)
            fun_eval = old_fun(*Xs, **Ys)
            return tf.cond(
                pred=tf.less(tf.rank(fun_eval), tf.rank(X)),
                true_fn=lambda: fun_eval[..., tf.newaxis],
                false_fn=lambda: fun_eval,
            )

        return new_fun

    if isinstance(funcs, Iterable):
        funcs = [wrapper(f) for f in funcs]
    else:
        funcs = wrapper(funcs)

    quadrature = NDiagGHQuadrature(dim, n_gh)
    if logspace:
        result = quadrature.logspace(funcs, Fmu, Fvar, **Ys)
    else:
        result = quadrature(funcs, Fmu, Fvar, **Ys)

    if isinstance(result, list):
        result = [tf.reshape(r, shape) for r in result]
    else:
        result = tf.reshape(result, shape)

    return result","def ndiagquad(funcs, H: int, Fmu, Fvar, logspace: bool = False, **Ys):
    """"""
    Computes N Gaussian expectation integrals of one or more functions
    using Gauss-Hermite quadrature. The Gaussians must be independent.

    The means and variances of the Gaussians are specified by Fmu and Fvar.
    The N-integrals are assumed to be taken wrt the last dimensions of Fmu, Fvar.

    :param funcs: the integrand(s):
        Callable or Iterable of Callables that operates elementwise
    :param H: number of Gauss-Hermite quadrature points
    :param Fmu: array/tensor or `Din`-tuple/list thereof
    :param Fvar: array/tensor or `Din`-tuple/list thereof
    :param logspace: if True, funcs are the log-integrands and this calculates
        the log-expectation of exp(funcs)
    :param **Ys: arrays/tensors; deterministic arguments to be passed by name

    Fmu, Fvar, Ys should all have same shape, with overall size `N`
    :return: shape is the same as that of the first Fmu
    """"""
    n_gh = H
    if isinstance(Fmu, (tuple, list)):
        dim = len(Fmu)
        shape = tf.shape(Fmu[0])
        Fmu = tf.stack(Fmu, axis=-1)
        Fvar = tf.stack(Fvar, axis=-1)
    else:
        dim = 1
        shape = tf.shape(Fmu)

    Fmu = tf.reshape(Fmu, (-1, dim))
    Fvar = tf.reshape(Fvar, (-1, dim))

    Ys = {Yname: tf.reshape(Y, (-1, 1)) for Yname, Y in Ys.items()}

    def wrapper(old_fun):
        def new_fun(X, **Ys):
            Xs = tf.unstack(X, axis=-1)
            fun_eval = old_fun(*Xs, **Ys)
            if tf.rank(fun_eval) < tf.rank(X):
                fun_eval = tf.expand_dims(fun_eval, axis=-1)
            return fun_eval

        return new_fun

    if isinstance(funcs, Iterable):
        funcs = [wrapper(f) for f in funcs]
    else:
        funcs = wrapper(funcs)

    quadrature = NDiagGHQuadrature(dim, n_gh)
    if logspace:
        result = quadrature.logspace(funcs, Fmu, Fvar, **Ys)
    else:
        result = quadrature(funcs, Fmu, Fvar, **Ys)

    if isinstance(result, list):
        result = [tf.reshape(r, shape) for r in result]
    else:
        result = tf.reshape(result, shape)

    return result"
"    def wrapper(old_fun):
        def new_fun(X, **Ys):
            Xs = tf.unstack(X, axis=-1)
            fun_eval = old_fun(*Xs, **Ys)
            return tf.cond(
                pred=tf.less(tf.rank(fun_eval), tf.rank(X)),
                true_fn=lambda: fun_eval[..., tf.newaxis],
                false_fn=lambda: fun_eval,
            )

        return new_fun","    def wrapper(old_fun):
        def new_fun(X, **Ys):
            Xs = tf.unstack(X, axis=-1)
            fun_eval = old_fun(*Xs, **Ys)
            if tf.rank(fun_eval) < tf.rank(X):
                fun_eval = tf.expand_dims(fun_eval, axis=-1)
            return fun_eval

        return new_fun"
"        def new_fun(X, **Ys):
            Xs = tf.unstack(X, axis=-1)
            fun_eval = old_fun(*Xs, **Ys)
            return tf.cond(
                pred=tf.less(tf.rank(fun_eval), tf.rank(X)),
                true_fn=lambda: fun_eval[..., tf.newaxis],
                false_fn=lambda: fun_eval,
            )","        def new_fun(X, **Ys):
            Xs = tf.unstack(X, axis=-1)
            fun_eval = old_fun(*Xs, **Ys)
            if tf.rank(fun_eval) < tf.rank(X):
                fun_eval = tf.expand_dims(fun_eval, axis=-1)
            return fun_eval"
"    def __init__(
        self,
        data: OutputData,
        latent_dim: int,
        X_data_mean: Optional[tf.Tensor] = None,
        kernel: Optional[Kernel] = None,
        mean_function: Optional[MeanFunction] = None,
    ):
        """"""
        Initialise GPLVM object. This method only works with a Gaussian likelihood.

        :param data: y data matrix, size N (number of points) x D (dimensions)
        :param latent_dim: the number of latent dimensions (Q)
        :param X_data_mean: latent positions ([N, Q]), for the initialisation of the latent space.
        :param kernel: kernel specification, by default Squared Exponential
        :param mean_function: mean function, by default None.
        """"""
        if X_data_mean is None:
            X_data_mean = pca_reduce(data, latent_dim)

        num_latent_gps = X_data_mean.shape[1]
        if num_latent_gps != latent_dim:
            msg = ""Passed in number of latent {0} does not match initial X {1}.""
            raise ValueError(msg.format(latent_dim, num_latent_gps))

        if mean_function is None:
            mean_function = Zero()

        if kernel is None:
            kernel = kernels.SquaredExponential(lengthscales=tf.ones((latent_dim,)))

        if data.shape[1] < num_latent_gps:
            raise ValueError(""More latent dimensions than observed."")

        gpr_data = (Parameter(X_data_mean), data_input_to_tensor(data))
        super().__init__(gpr_data, kernel, mean_function=mean_function)","    def __init__(
        self,
        data: OutputData,
        latent_dim: int,
        X_data_mean: Optional[tf.Tensor] = None,
        kernel: Optional[Kernel] = None,
        mean_function: Optional[MeanFunction] = None,
    ):
        """"""
        Initialise GPLVM object. This method only works with a Gaussian likelihood.

        :param data: y data matrix, size N (number of points) x D (dimensions)
        :param latent_dim: the number of latent dimensions (Q)
        :param X_data_mean: latent positions ([N, Q]), for the initialisation of the latent space.
        :param kernel: kernel specification, by default Squared Exponential
        :param mean_function: mean function, by default None.
        """"""
        if X_data_mean is None:
            X_data_mean = pca_reduce(data, latent_dim)

        num_latent_gps = X_data_mean.shape[1]
        if num_latent_gps != latent_dim:
            msg = ""Passed in number of latent {0} does not match initial X {1}.""
            raise ValueError(msg.format(latent_dim, num_latent_gps))

        if mean_function is None:
            mean_function = Zero()

        if kernel is None:
            kernel = kernels.SquaredExponential(lengthscales=tf.ones((latent_dim,)))

        if data.shape[1] < num_latent_gps:
            raise ValueError(""More latent dimensions than observed."")

        gpr_data = (Parameter(X_data_mean), data)
        super().__init__(gpr_data, kernel, mean_function=mean_function)"
"    def __init__(
        self,
        data: OutputData,
        X_data_mean: tf.Tensor,
        X_data_var: tf.Tensor,
        kernel: Kernel,
        num_inducing_variables: Optional[int] = None,
        inducing_variable=None,
        X_prior_mean=None,
        X_prior_var=None,
    ):
        """"""
        Initialise Bayesian GPLVM object. This method only works with a Gaussian likelihood.

        :param data: data matrix, size N (number of points) x D (dimensions)
        :param X_data_mean: initial latent positions, size N (number of points) x Q (latent dimensions).
        :param X_data_var: variance of latent positions ([N, Q]), for the initialisation of the latent space.
        :param kernel: kernel specification, by default Squared Exponential
        :param num_inducing_variables: number of inducing points, M
        :param inducing_variable: matrix of inducing points, size M (inducing points) x Q (latent dimensions). By default
            random permutation of X_data_mean.
        :param X_prior_mean: prior mean used in KL term of bound. By default 0. Same size as X_data_mean.
        :param X_prior_var: prior variance used in KL term of bound. By default 1.
        """"""
        num_data, num_latent_gps = X_data_mean.shape
        super().__init__(kernel, likelihoods.Gaussian(), num_latent_gps=num_latent_gps)
        self.data = data_input_to_tensor(data)
        assert X_data_var.ndim == 2

        self.X_data_mean = Parameter(X_data_mean)
        self.X_data_var = Parameter(X_data_var, transform=positive())

        self.num_data = num_data
        self.output_dim = self.data.shape[-1]

        assert np.all(X_data_mean.shape == X_data_var.shape)
        assert X_data_mean.shape[0] == self.data.shape[0], ""X mean and Y must be same size.""
        assert X_data_var.shape[0] == self.data.shape[0], ""X var and Y must be same size.""

        if (inducing_variable is None) == (num_inducing_variables is None):
            raise ValueError(
                ""BayesianGPLVM needs exactly one of `inducing_variable` and `num_inducing_variables`""
            )

        if inducing_variable is None:
            # By default we initialize by subset of initial latent points
            # Note that tf.random.shuffle returns a copy, it does not shuffle in-place
            Z = tf.random.shuffle(X_data_mean)[:num_inducing_variables]
            inducing_variable = InducingPoints(Z)

        self.inducing_variable = inducingpoint_wrapper(inducing_variable)

        assert X_data_mean.shape[1] == self.num_latent_gps

        # deal with parameters for the prior mean variance of X
        if X_prior_mean is None:
            X_prior_mean = tf.zeros((self.num_data, self.num_latent_gps), dtype=default_float())
        if X_prior_var is None:
            X_prior_var = tf.ones((self.num_data, self.num_latent_gps))

        self.X_prior_mean = tf.convert_to_tensor(np.atleast_1d(X_prior_mean), dtype=default_float())
        self.X_prior_var = tf.convert_to_tensor(np.atleast_1d(X_prior_var), dtype=default_float())

        assert self.X_prior_mean.shape[0] == self.num_data
        assert self.X_prior_mean.shape[1] == self.num_latent_gps
        assert self.X_prior_var.shape[0] == self.num_data
        assert self.X_prior_var.shape[1] == self.num_latent_gps","    def __init__(
        self,
        data: OutputData,
        X_data_mean: tf.Tensor,
        X_data_var: tf.Tensor,
        kernel: Kernel,
        num_inducing_variables: Optional[int] = None,
        inducing_variable=None,
        X_prior_mean=None,
        X_prior_var=None,
    ):
        """"""
        Initialise Bayesian GPLVM object. This method only works with a Gaussian likelihood.

        :param data: data matrix, size N (number of points) x D (dimensions)
        :param X_data_mean: initial latent positions, size N (number of points) x Q (latent dimensions).
        :param X_data_var: variance of latent positions ([N, Q]), for the initialisation of the latent space.
        :param kernel: kernel specification, by default Squared Exponential
        :param num_inducing_variables: number of inducing points, M
        :param inducing_variable: matrix of inducing points, size M (inducing points) x Q (latent dimensions). By default
            random permutation of X_data_mean.
        :param X_prior_mean: prior mean used in KL term of bound. By default 0. Same size as X_data_mean.
        :param X_prior_var: prior variance used in KL term of bound. By default 1.
        """"""
        num_data, num_latent_gps = X_data_mean.shape
        super().__init__(kernel, likelihoods.Gaussian(), num_latent_gps=num_latent_gps)
        self.data = data
        assert X_data_var.ndim == 2

        self.X_data_mean = Parameter(X_data_mean)
        self.X_data_var = Parameter(X_data_var, transform=positive())

        self.num_data = num_data
        self.output_dim = data.shape[-1]

        assert np.all(X_data_mean.shape == X_data_var.shape)
        assert X_data_mean.shape[0] == data.shape[0], ""X mean and Y must be same size.""
        assert X_data_var.shape[0] == data.shape[0], ""X var and Y must be same size.""

        if (inducing_variable is None) == (num_inducing_variables is None):
            raise ValueError(
                ""BayesianGPLVM needs exactly one of `inducing_variable` and `num_inducing_variables`""
            )

        if inducing_variable is None:
            # By default we initialize by subset of initial latent points
            # Note that tf.random.shuffle returns a copy, it does not shuffle in-place
            Z = tf.random.shuffle(X_data_mean)[:num_inducing_variables]
            inducing_variable = InducingPoints(Z)

        self.inducing_variable = inducingpoint_wrapper(inducing_variable)

        assert X_data_mean.shape[1] == self.num_latent_gps

        # deal with parameters for the prior mean variance of X
        if X_prior_mean is None:
            X_prior_mean = tf.zeros((self.num_data, self.num_latent_gps), dtype=default_float())
        if X_prior_var is None:
            X_prior_var = tf.ones((self.num_data, self.num_latent_gps))

        self.X_prior_mean = tf.convert_to_tensor(np.atleast_1d(X_prior_mean), dtype=default_float())
        self.X_prior_var = tf.convert_to_tensor(np.atleast_1d(X_prior_var), dtype=default_float())

        assert self.X_prior_mean.shape[0] == self.num_data
        assert self.X_prior_mean.shape[1] == self.num_latent_gps
        assert self.X_prior_var.shape[0] == self.num_data
        assert self.X_prior_var.shape[1] == self.num_latent_gps"
"    def __init__(
        self,
        data: RegressionData,
        kernel: Kernel,
        mean_function: Optional[MeanFunction] = None,
        noise_variance: float = 1.0,
    ):
        likelihood = gpflow.likelihoods.Gaussian(noise_variance)
        _, Y_data = data
        super().__init__(kernel, likelihood, mean_function, num_latent_gps=Y_data.shape[-1])
        self.data = data_input_to_tensor(data)","    def __init__(
        self,
        data: RegressionData,
        kernel: Kernel,
        mean_function: Optional[MeanFunction] = None,
        noise_variance: float = 1.0,
    ):
        likelihood = gpflow.likelihoods.Gaussian(noise_variance)
        _, Y_data = data
        super().__init__(kernel, likelihood, mean_function, num_latent_gps=Y_data.shape[-1])
        self.data = data"
"    def __init__(
        self,
        data: RegressionData,
        kernel: Kernel,
        likelihood: Likelihood,
        mean_function: Optional[MeanFunction] = None,
        num_latent_gps: Optional[int] = None,
        inducing_variable: Optional[InducingPoints] = None,
    ):
        """"""
        data is a tuple of X, Y with X, a data matrix, size [N, D] and Y, a data matrix, size [N, R]
        Z is a data matrix, of inducing inputs, size [M, D]
        kernel, likelihood, mean_function are appropriate GPflow objects
        """"""
        if num_latent_gps is None:
            num_latent_gps = self.calc_num_latent_gps_from_data(data, kernel, likelihood)
        super().__init__(kernel, likelihood, mean_function, num_latent_gps=num_latent_gps)
        self.data = data_input_to_tensor(data)
        self.num_data = data[0].shape[0]
        self.inducing_variable = inducingpoint_wrapper(inducing_variable)
        self.V = Parameter(np.zeros((len(self.inducing_variable), self.num_latent_gps)))
        self.V.prior = tfp.distributions.Normal(
            loc=to_default_float(0.0), scale=to_default_float(1.0)
        )","    def __init__(
        self,
        data: RegressionData,
        kernel: Kernel,
        likelihood: Likelihood,
        mean_function: Optional[MeanFunction] = None,
        num_latent_gps: Optional[int] = None,
        inducing_variable: Optional[InducingPoints] = None,
    ):
        """"""
        data is a tuple of X, Y with X, a data matrix, size [N, D] and Y, a data matrix, size [N, R]
        Z is a data matrix, of inducing inputs, size [M, D]
        kernel, likelihood, mean_function are appropriate GPflow objects
        """"""
        if num_latent_gps is None:
            num_latent_gps = self.calc_num_latent_gps_from_data(data, kernel, likelihood)
        super().__init__(kernel, likelihood, mean_function, num_latent_gps=num_latent_gps)
        self.data = data
        self.num_data = data[0].shape[0]
        self.inducing_variable = inducingpoint_wrapper(inducing_variable)
        self.V = Parameter(np.zeros((len(self.inducing_variable), self.num_latent_gps)))
        self.V.prior = tfp.distributions.Normal(
            loc=to_default_float(0.0), scale=to_default_float(1.0)
        )"
"    def __init__(
        self,
        data: RegressionData,
        kernel: Kernel,
        inducing_variable: InducingPoints,
        *,
        mean_function: Optional[MeanFunction] = None,
        num_latent_gps: Optional[int] = None,
        noise_variance: float = 1.0,
    ):
        """"""
        `data`:  a tuple of (X, Y), where the inputs X has shape [N, D]
            and the outputs Y has shape [N, R].
        `inducing_variable`:  an InducingPoints instance or a matrix of
            the pseudo inputs Z, of shape [M, D].
        `kernel`, `mean_function` are appropriate GPflow objects

        This method only works with a Gaussian likelihood, its variance is
        initialized to `noise_variance`.
        """"""
        likelihood = likelihoods.Gaussian(noise_variance)
        X_data, Y_data = data_input_to_tensor(data)
        num_latent_gps = Y_data.shape[-1] if num_latent_gps is None else num_latent_gps
        super().__init__(kernel, likelihood, mean_function, num_latent_gps=num_latent_gps)

        self.data = X_data, Y_data
        self.num_data = X_data.shape[0]

        self.inducing_variable = inducingpoint_wrapper(inducing_variable)","    def __init__(
        self,
        data: RegressionData,
        kernel: Kernel,
        inducing_variable: InducingPoints,
        *,
        mean_function: Optional[MeanFunction] = None,
        num_latent_gps: Optional[int] = None,
        noise_variance: float = 1.0,
    ):
        """"""
        `data`:  a tuple of (X, Y), where the inputs X has shape [N, D]
            and the outputs Y has shape [N, R].
        `inducing_variable`:  an InducingPoints instance or a matrix of
            the pseudo inputs Z, of shape [M, D].
        `kernel`, `mean_function` are appropriate GPflow objects

        This method only works with a Gaussian likelihood, its variance is
        initialized to `noise_variance`.
        """"""
        likelihood = likelihoods.Gaussian(noise_variance)
        X_data, Y_data = data
        num_latent_gps = Y_data.shape[-1] if num_latent_gps is None else num_latent_gps
        super().__init__(kernel, likelihood, mean_function, num_latent_gps=num_latent_gps)

        self.data = data
        self.num_data = X_data.shape[0]

        self.inducing_variable = inducingpoint_wrapper(inducing_variable)"
"    def __init__(
        self,
        data: RegressionData,
        kernel: Kernel,
        likelihood: Likelihood,
        mean_function: Optional[MeanFunction] = None,
        num_latent_gps: Optional[int] = None,
    ):
        """"""
        data = (X, Y) contains the input points [N, D] and the observations [N, P]
        kernel, likelihood, mean_function are appropriate GPflow objects
        """"""
        if num_latent_gps is None:
            num_latent_gps = self.calc_num_latent_gps_from_data(data, kernel, likelihood)
        super().__init__(kernel, likelihood, mean_function, num_latent_gps)

        self.data = data_input_to_tensor(data)
        X_data, Y_data = self.data
        num_data = X_data.shape[0]
        self.num_data = num_data

        self.q_mu = Parameter(np.zeros((num_data, self.num_latent_gps)))
        q_sqrt = np.array([np.eye(num_data) for _ in range(self.num_latent_gps)])
        self.q_sqrt = Parameter(q_sqrt, transform=triangular())","    def __init__(
        self,
        data: RegressionData,
        kernel: Kernel,
        likelihood: Likelihood,
        mean_function: Optional[MeanFunction] = None,
        num_latent_gps: Optional[int] = None,
    ):
        """"""
        data = (X, Y) contains the input points [N, D] and the observations [N, P]
        kernel, likelihood, mean_function are appropriate GPflow objects
        """"""
        if num_latent_gps is None:
            num_latent_gps = self.calc_num_latent_gps_from_data(data, kernel, likelihood)
        super().__init__(kernel, likelihood, mean_function, num_latent_gps)

        X_data, Y_data = data
        num_data = X_data.shape[0]
        self.num_data = num_data
        self.data = data

        self.q_mu = Parameter(np.zeros((num_data, self.num_latent_gps)))
        q_sqrt = np.array([np.eye(num_data) for _ in range(self.num_latent_gps)])
        self.q_sqrt = Parameter(q_sqrt, transform=triangular())"
"    def __init__(
        self,
        data: RegressionData,
        kernel: Kernel,
        likelihood: Likelihood,
        mean_function: Optional[MeanFunction] = None,
        num_latent_gps: Optional[int] = None,
    ):
        """"""
        data = (X, Y) contains the input points [N, D] and the observations [N, P]
        kernel, likelihood, mean_function are appropriate GPflow objects
        """"""
        if num_latent_gps is None:
            num_latent_gps = self.calc_num_latent_gps_from_data(data, kernel, likelihood)
        super().__init__(kernel, likelihood, mean_function, num_latent_gps)

        self.data = data_input_to_tensor(data)
        X_data, Y_data = self.data
        self.num_data = X_data.shape[0]
        self.q_alpha = Parameter(np.zeros((self.num_data, self.num_latent_gps)))
        self.q_lambda = Parameter(
            np.ones((self.num_data, self.num_latent_gps)), transform=gpflow.utilities.positive()
        )","    def __init__(
        self,
        data: RegressionData,
        kernel: Kernel,
        likelihood: Likelihood,
        mean_function: Optional[MeanFunction] = None,
        num_latent_gps: Optional[int] = None,
    ):
        """"""
        data = (X, Y) contains the input points [N, D] and the observations [N, P]
        kernel, likelihood, mean_function are appropriate GPflow objects
        """"""
        if num_latent_gps is None:
            num_latent_gps = self.calc_num_latent_gps_from_data(data, kernel, likelihood)
        super().__init__(kernel, likelihood, mean_function, num_latent_gps)

        X_data, Y_data = data
        self.data = data
        self.num_data = X_data.shape[0]
        self.q_alpha = Parameter(np.zeros((self.num_data, self.num_latent_gps)))
        self.q_lambda = Parameter(
            np.ones((self.num_data, self.num_latent_gps)), transform=gpflow.utilities.positive()
        )"
"    def __init__(
        self,
        data: RegressionData,
        kernel: Kernel,
        likelihood: Likelihood,
        mean_function: Optional[MeanFunction] = None,
        num_latent_gps: Optional[int] = None,
    ):
        """"""
        data is a tuple of X, Y with X, a data matrix, size [N, D] and Y, a data matrix, size [N, R]
        kernel, likelihood, mean_function are appropriate GPflow objects

        This is a vanilla implementation of a GP with a non-Gaussian
        likelihood. The latent function values are represented by centered
        (whitened) variables, so

            v ~ N(0, I)
            f = Lv + m(x)

        with

            L L^T = K

        """"""
        if num_latent_gps is None:
            num_latent_gps = self.calc_num_latent_gps_from_data(data, kernel, likelihood)
        super().__init__(kernel, likelihood, mean_function, num_latent_gps)
        self.data = data_input_to_tensor(data)
        self.num_data = self.data[0].shape[0]
        self.V = Parameter(np.zeros((self.num_data, self.num_latent_gps)))
        self.V.prior = tfp.distributions.Normal(
            loc=to_default_float(0.0), scale=to_default_float(1.0)
        )","    def __init__(
        self,
        data: RegressionData,
        kernel: Kernel,
        likelihood: Likelihood,
        mean_function: Optional[MeanFunction] = None,
        num_latent_gps: Optional[int] = None,
    ):
        """"""
        data is a tuple of X, Y with X, a data matrix, size [N, D] and Y, a data matrix, size [N, R]
        kernel, likelihood, mean_function are appropriate GPflow objects

        This is a vanilla implementation of a GP with a non-Gaussian
        likelihood. The latent function values are represented by centered
        (whitened) variables, so

            v ~ N(0, I)
            f = Lv + m(x)

        with

            L L^T = K

        """"""
        if num_latent_gps is None:
            num_latent_gps = self.calc_num_latent_gps_from_data(data, kernel, likelihood)
        super().__init__(kernel, likelihood, mean_function, num_latent_gps)
        self.data = data
        self.num_data = data[0].shape[0]
        self.V = Parameter(np.zeros((self.num_data, self.num_latent_gps)))
        self.V.prior = tfp.distributions.Normal(
            loc=to_default_float(0.0), scale=to_default_float(1.0)
        )"
"    def K(self, X: tf.Tensor, X2: Optional[tf.Tensor] = None) -> tf.Tensor:
        sig_X = self._sigmoids(X)  # N1 x 1 x Ncp
        sig_X2 = self._sigmoids(X2) if X2 is not None else sig_X  # N2 x 1 x Ncp

        # `starters` are the sigmoids going from 0 -> 1, whilst `stoppers` go
        # from 1 -> 0, dimensions are N1 x N2 x Ncp
        starters = sig_X * tf.transpose(sig_X2, perm=(1, 0, 2))
        stoppers = (1 - sig_X) * tf.transpose((1 - sig_X2), perm=(1, 0, 2))

        # prepend `starters` with ones and append ones to `stoppers` since the
        # first kernel has no start and the last kernel has no end
        N1 = tf.shape(X)[0]
        N2 = tf.shape(X2)[0] if X2 is not None else N1
        ones = tf.ones((N1, N2, 1), dtype=X.dtype)
        starters = tf.concat([ones, starters], axis=2)
        stoppers = tf.concat([stoppers, ones], axis=2)

        # now combine with the underlying kernels
        kernel_stack = tf.stack([k(X, X2) for k in self.kernels], axis=2)
        return tf.reduce_sum(kernel_stack * starters * stoppers, axis=2)","    def K(self, X: tf.Tensor, X2: Optional[tf.Tensor] = None) -> tf.Tensor:
        sig_X = self._sigmoids(X)  # N x 1 x Ncp
        sig_X2 = self._sigmoids(X2) if X2 is not None else sig_X

        # `starters` are the sigmoids going from 0 -> 1, whilst `stoppers` go
        # from 1 -> 0, dimensions are N x N x Ncp
        starters = sig_X * tf.transpose(sig_X2, perm=(1, 0, 2))
        stoppers = (1 - sig_X) * tf.transpose((1 - sig_X2), perm=(1, 0, 2))

        # prepend `starters` with ones and append ones to `stoppers` since the
        # first kernel has no start and the last kernel has no end
        N = tf.shape(X)[0]
        ones = tf.ones((N, N, 1), dtype=X.dtype)
        starters = tf.concat([ones, starters], axis=2)
        stoppers = tf.concat([stoppers, ones], axis=2)

        # now combine with the underlying kernels
        kernel_stack = tf.stack([k(X, X2) for k in self.kernels], axis=2)
        return tf.reduce_sum(kernel_stack * starters * stoppers, axis=2)"
"def autoflow(*af_args, **af_kwargs):
    def autoflow_wrapper(method):
        @functools.wraps(method)
        def runnable(obj, *args, **kwargs):
            if not isinstance(obj, Node):
                raise GPflowError(
                    'AutoFlow works only with node-like objects.')
            if obj.is_built_coherence(obj.graph) is Build.NO:
                raise GPflowError('Not built with ""{graph}"".'.format(graph=obj.graph))
            name = method.__name__
            store = AutoFlow.get_autoflow(obj, name)
            session = kwargs.pop('session', None)
            session = obj.enquire_session(session=session)

            scope_name = _name_scope_name(obj, name)
            with session.graph.as_default(), tf.name_scope(scope_name):
                if not store:
                    _setup_storage(store, *af_args, **af_kwargs)
                    _build_method(method, obj, store)
                return _session_run(session, obj, store, *args, **kwargs)
        return runnable
    return autoflow_wrapper","def autoflow(*af_args, **af_kwargs):
    def autoflow_wrapper(method):
        @functools.wraps(method)
        def runnable(obj, *args, **kwargs):
            if not isinstance(obj, Node):
                raise GPflowError(
                    'AutoFlow works only with node-like objects.')
            if obj.is_built_coherence(obj.graph) is Build.NO:
                raise GPflowError('Not built with ""{graph}"".'.format(graph=obj.graph))
            name = method.__name__
            store = AutoFlow.get_autoflow(obj, name)
            session = kwargs.pop('session', None)
            session = obj.enquire_session(session=session)
            if not store:
                scope_name = _name_scope_name(obj, name)
                with session.graph.as_default(), tf.name_scope(scope_name):
                    _setup_storage(store, *af_args, **af_kwargs)
                    _build_method(method, obj, store)
            return _session_run(session, obj, store, *args, **kwargs)
        return runnable
    return autoflow_wrapper"
"    def autoflow_wrapper(method):
        @functools.wraps(method)
        def runnable(obj, *args, **kwargs):
            if not isinstance(obj, Node):
                raise GPflowError(
                    'AutoFlow works only with node-like objects.')
            if obj.is_built_coherence(obj.graph) is Build.NO:
                raise GPflowError('Not built with ""{graph}"".'.format(graph=obj.graph))
            name = method.__name__
            store = AutoFlow.get_autoflow(obj, name)
            session = kwargs.pop('session', None)
            session = obj.enquire_session(session=session)

            scope_name = _name_scope_name(obj, name)
            with session.graph.as_default(), tf.name_scope(scope_name):
                if not store:
                    _setup_storage(store, *af_args, **af_kwargs)
                    _build_method(method, obj, store)
                return _session_run(session, obj, store, *args, **kwargs)
        return runnable","    def autoflow_wrapper(method):
        @functools.wraps(method)
        def runnable(obj, *args, **kwargs):
            if not isinstance(obj, Node):
                raise GPflowError(
                    'AutoFlow works only with node-like objects.')
            if obj.is_built_coherence(obj.graph) is Build.NO:
                raise GPflowError('Not built with ""{graph}"".'.format(graph=obj.graph))
            name = method.__name__
            store = AutoFlow.get_autoflow(obj, name)
            session = kwargs.pop('session', None)
            session = obj.enquire_session(session=session)
            if not store:
                scope_name = _name_scope_name(obj, name)
                with session.graph.as_default(), tf.name_scope(scope_name):
                    _setup_storage(store, *af_args, **af_kwargs)
                    _build_method(method, obj, store)
            return _session_run(session, obj, store, *args, **kwargs)
        return runnable"
"        def runnable(obj, *args, **kwargs):
            if not isinstance(obj, Node):
                raise GPflowError(
                    'AutoFlow works only with node-like objects.')
            if obj.is_built_coherence(obj.graph) is Build.NO:
                raise GPflowError('Not built with ""{graph}"".'.format(graph=obj.graph))
            name = method.__name__
            store = AutoFlow.get_autoflow(obj, name)
            session = kwargs.pop('session', None)
            session = obj.enquire_session(session=session)

            scope_name = _name_scope_name(obj, name)
            with session.graph.as_default(), tf.name_scope(scope_name):
                if not store:
                    _setup_storage(store, *af_args, **af_kwargs)
                    _build_method(method, obj, store)
                return _session_run(session, obj, store, *args, **kwargs)","        def runnable(obj, *args, **kwargs):
            if not isinstance(obj, Node):
                raise GPflowError(
                    'AutoFlow works only with node-like objects.')
            if obj.is_built_coherence(obj.graph) is Build.NO:
                raise GPflowError('Not built with ""{graph}"".'.format(graph=obj.graph))
            name = method.__name__
            store = AutoFlow.get_autoflow(obj, name)
            session = kwargs.pop('session', None)
            session = obj.enquire_session(session=session)
            if not store:
                scope_name = _name_scope_name(obj, name)
                with session.graph.as_default(), tf.name_scope(scope_name):
                    _setup_storage(store, *af_args, **af_kwargs)
                    _build_method(method, obj, store)
            return _session_run(session, obj, store, *args, **kwargs)"
"def initialize_variables(variables=None, session=None, force=False, **run_kwargs):
    session = tf.get_default_session() if session is None else session
    if variables is None:
        initializer = tf.global_variables_initializer()
    else:
        if force:
            vars_for_init = list(_initializable_tensors(variables))
        else:
            vars_for_init = list(_find_initializable_tensors(variables, session))
        if not vars_for_init:
            return
        initializer = tf.variables_initializer(vars_for_init)
    session.run(initializer, **run_kwargs)","def initialize_variables(variables=None, session=None, force=False, **run_kwargs):
    session = tf.get_default_session() if session is None else session
    if variables is None:
        initializer = tf.global_variables_initializer()
    else:
        if force:
            initializer = tf.variables_initializer(variables)
        else:
            uninitialized = tf.report_uninitialized_variables(var_list=variables)
            def uninitialized_names():
                for uv in session.run(uninitialized):
                    yield uv.decode('utf-8')
                    # if isinstance(uv, bytes):
                    #     yield uv.decode('utf-8')
                    # elif isinstance(uv, str):
                    #     yield uv
                    # else:
                    #     msg = 'Unknown output type ""{}"" from `tf.report_uninitialized_variables`'
                    #     raise ValueError(msg.format(type(uv)))
            names = set(uninitialized_names())
            vars_for_init = [v for v in variables if v.name.split(':')[0] in names]
            initializer = tf.variables_initializer(vars_for_init)
    session.run(initializer, **run_kwargs)"
"    def _clear(self):
        self._reset_name()
        self._initial_value_tensor = None
        self._dataholder_tensor = None
        self._is_initialized_tensor = None","    def _clear(self):
        self._reset_name()
        self._initial_value_tensor = None
        self._dataholder_tensor = None"
"    def _build(self):
        tensor = self._build_parameter()
        self._dataholder_tensor = tensor
        self._is_initialized_tensor = tf.is_variable_initialized(tensor)","    def _build(self):
        self._dataholder_tensor = self._build_parameter()  # pylint: disable=W0201"
"    def _init_parameter_defaults(self):
        self._initial_value_tensor = None
        self._dataholder_tensor = None
        self._is_initialized_tensor = None","    def _init_parameter_defaults(self):
        self._initial_value_tensor = None
        self._dataholder_tensor = None"
"    def initializables(self):
        if self._externally_defined:
            return None
        return [(self.parameter_tensor, self.is_initialized_tensor)]","    def initializables(self):
        if self._externally_defined:
            return None
        return [self.parameter_tensor]"
"    def read_value(self, session=None):
        if session is not None and not isinstance(session, tf.Session):
            raise ValueError('TensorFlow session expected as an argument.')
        if session is None and self._externally_defined:
            raise GPflowError('Externally defined parameter requires session.')
        elif session:
            is_built = self.is_built_coherence(session.graph)
            if is_built is Build.YES:
                return self._read_parameter_tensor(session)
        return self._value","    def read_value(self, session=None):
        if session is not None:
            if not isinstance(session, tf.Session):
                raise ValueError('TensorFlow session expected as session argument.')
            is_built = self.is_built_coherence(session.graph)
            if is_built is Build.YES:
                return self._read_parameter_tensor(session)
        elif self._externally_defined:
            raise GPflowError('Externally defined parameter requires session.')
        return self._value"
"    def _clear(self):
        self._reset_name()
        self._externally_defined = False
        self._is_initialized_tensor = None
        self._initial_value_tensor = None
        self._unconstrained_tensor = None
        self._constrained_tensor = None
        self._prior_tensor = None","    def _clear(self):
        self._reset_name()
        self._externally_defined = False   # pylint: disable=W0201
        self._initial_value_tensor = None  # pylint: disable=W0201
        self._unconstrained_tensor = None  # pylint: disable=W0201
        self._constrained_tensor = None    # pylint: disable=W0201
        self._prior_tensor = None          # pylint: disable=W0201"
"    def _build(self):
        unconstrained = self._build_parameter()
        constrained = self._build_constrained(unconstrained)
        prior = self._build_prior(unconstrained, constrained)

        self._is_initialized_tensor = tf.is_variable_initialized(unconstrained)
        self._unconstrained_tensor = unconstrained
        self._constrained_tensor = constrained
        self._prior_tensor = prior","    def _build(self):
        unconstrained = self._build_parameter()
        constrained = self._build_constrained(unconstrained)
        prior = self._build_prior(unconstrained, constrained)
        self._unconstrained_tensor = unconstrained  # pylint: disable=W0201
        self._constrained_tensor = constrained      # pylint: disable=W0201
        self._prior_tensor = prior                  # pylint: disable=W0201"
"    def _build_parameter(self):
        if self._externally_defined:
            self._check_tensor_trainable(self.parameter_tensor)
            return self.parameter_tensor

        name = self._parameter_name()
        tensor = misc.get_variable_by_name(name)
        if tensor is not None:
            raise GPflowError('Tensor with name ""{name}"" already exists, {tensor}.'
                              .format(name=name, tensor=tensor))

        value = self._apply_transform(self._value)
        shape = value.shape if self.fixed_shape else None
        init = tf.placeholder(self.dtype, shape=shape, name='initial_unconstrained_value')
        self._initial_value_tensor = init
        if self.fixed_shape:
            args = dict(trainable=self.trainable)
        else:
            args = dict(validate_shape=False, trainable=self.trainable)
        variable = tf.get_variable(name, initializer=init, **args)
        return variable","    def _build_parameter(self):
        if self._externally_defined:
            self._check_tensor_trainable(self.parameter_tensor)
            return self.parameter_tensor

        name = self._parameter_name()
        tensor = misc.get_variable_by_name(name)
        if tensor is not None:
            raise GPflowError('Tensor with name ""{name}"" already exists, {tensor}.'
                              .format(name=name, tensor=tensor))

        value = self._apply_transform(self._value)
        shape = value.shape if self.fixed_shape else None
        init = tf.placeholder(self.dtype, shape=shape, name='initial_unconstrained_value')
        self._initial_value_tensor = init
        if self.fixed_shape:
            return tf.get_variable(name, initializer=init, trainable=self.trainable)
        return tf.get_variable(name, initializer=init,
                               validate_shape=False,
                               trainable=self.trainable)"
"    def _init_parameter_defaults(self):
        self._is_initialized_tensor = None
        self._initial_value_tensor = None
        self._unconstrained_tensor = None
        self._prior_tensor = None
        self._constrained_tensor = None","    def _init_parameter_defaults(self):
        self._initial_value_tensor = None
        self._unconstrained_tensor = None
        self._prior_tensor = None
        self._constrained_tensor = None"
"    def minimize(self, model, session=None, var_list=None, feed_dict=None,
                 maxiter=1000, initialize=False, anchor=True, **kwargs):
        """"""
        Minimizes objective function of the model.

        :param model: GPflow model with objective tensor.
        :param session: Session where optimization will be run.
        :param var_list: List of extra variables which should be trained during optimization.
        :param feed_dict: Feed dictionary of tensors passed to session run method.
        :param maxiter: Number of run interation.
        :param initialize: If `True` model parameters will be re-initialized even if they were
            initialized before for gotten session.
        :param anchor: If `True` trained variable values computed during optimization at
            particular session will be synchronized with internal parameter values.
        :param kwargs: This is a dictionary of extra parameters for session run method.
        """"""

        if model is None or not isinstance(model, Model):
            raise ValueError('Unknown type passed for optimization.')

        session = model.enquire_session(session)

        self._model = model
        objective = model.objective

        with session.graph.as_default():
            full_var_list = self._gen_var_list(model, var_list)

            # Create optimizer variables before initialization.
            self._minimize_operation = self.optimizer.minimize(
                objective, var_list=full_var_list, **kwargs)

            model.initialize(session=session, force=initialize)
            self._initialize_optimizer(session, full_var_list)

            feed_dict = self._gen_feed_dict(model, feed_dict)
            for _i in range(maxiter):
                session.run(self.minimize_operation, feed_dict=feed_dict)

        if anchor:
            model.anchor(session)","    def minimize(self, model, session=None, var_list=None, feed_dict=None,
                 maxiter=1000, initialize=True, anchor=True, **kwargs):
        """"""
        Minimizes objective function of the model.

        :param model: GPflow model with objective tensor.
        :param session: Session where optimization will be run.
        :param var_list: List of extra variables which should be trained during optimization.
        :param feed_dict: Feed dictionary of tensors passed to session run method.
        :param maxiter: Number of run interation.
        :param initialize: If `True` model parameters will be re-initialized even if they were
            initialized before for gotten session.
        :param anchor: If `True` trained variable values computed during optimization at
            particular session will be synchronized with internal parameter values.
        :param kwargs: This is a dictionary of extra parameters for session run method.
        """"""

        if model is None or not isinstance(model, Model):
            raise ValueError('Unknown type passed for optimization.')

        session = model.enquire_session(session)

        self._model = model
        objective = model.objective

        with session.graph.as_default():
            full_var_list = self._gen_var_list(model, var_list)

            # Create optimizer variables before initialization.
            self._minimize_operation = self.optimizer.minimize(
                objective, var_list=full_var_list, **kwargs)

            model.initialize(session=session, force=initialize)
            self._initialize_optimizer(session, full_var_list)

            feed_dict = self._gen_feed_dict(model, feed_dict)
            for _i in range(maxiter):
                session.run(self.minimize_operation, feed_dict=feed_dict)

        if anchor:
            model.anchor(session)"
"    def prob_is_largest(self, Y, mu, var, gh_x, gh_w):
        # work out what the mean and variance is of the indicated latent function.
        oh_on = tf.cast(tf.one_hot(tf.reshape(Y, (-1,)), self.num_classes, 1., 0.), tf.float64)
        mu_selected = tf.reduce_sum(oh_on * mu, 1)
        var_selected = tf.reduce_sum(oh_on * var, 1)

        # generate Gauss Hermite grid
        X = tf.reshape(mu_selected, (-1, 1)) + gh_x * tf.reshape(tf.sqrt(tf.clip_by_value(2. * var_selected, 1e-10, np.inf)), (-1, 1))

        # compute the CDF of the Gaussian between the latent functions and the grid (including the selected function)
        dist = (tf.expand_dims(X, 1) - tf.expand_dims(mu, 2)) / tf.expand_dims(tf.sqrt(tf.clip_by_value(var, 1e-10, np.inf)), 2)
        cdfs = 0.5 * (1.0 + tf.erf(dist/np.sqrt(2.0)))

        cdfs = cdfs * (1-2e-4) + 1e-4

        # blank out all the distances on the selected latent function
        oh_off = tf.cast(tf.one_hot(tf.reshape(Y, (-1,)), self.num_classes, 0., 1.), tf.float64)
        cdfs = cdfs * tf.expand_dims(oh_off, 2) + tf.expand_dims(oh_on, 2)

        # take the product over the latent functions, and the sum over the GH grid.
        return tf.matmul(tf.reduce_prod(cdfs, reduction_indices=[1]), tf.reshape(gh_w/np.sqrt(np.pi), (-1, 1)))","    def prob_is_largest(self, Y, mu, var, gh_x, gh_w):
        # work out what the mean and variance is of the indicated latent function.
        oh_on = tf.cast(tf.one_hot(tf.reshape(Y, (-1,)), self.num_classes, 1., 0.), tf.float64)
        mu_selected = tf.reduce_sum(oh_on * mu, 1)
        var_selected = tf.reduce_sum(oh_on * var, 1)

        # generate Gauss Hermite grid
        X = tf.reshape(mu_selected, (-1, 1)) + gh_x * tf.reshape(tf.sqrt(tf.clip_by_value(2. * var_selected, 1e-10, np.inf)), (-1, 1))

        # compute the CDF of the Gaussian between the latent functions and the grid (including the selected function)
        dist = (tf.expand_dims(X, 1) - tf.expand_dims(mu, 2)) / tf.expand_dims(tf.sqrt(tf.clip_by_value(var, 1e-10, np.inf)), 2)
        cdfs = 0.5 * (1.0 + tf.erf(dist/np.sqrt(2.0)))

        cdfs = cdfs * (1-2e-4) + 1e-4

        # blank out all the distances on the selected latent function
        oh_off = tf.cast(tf.one_hot(tf.reshape(Y, (-1,)), self.num_classes, 0., 1.), tf.float64)
        cdfs = cdfs * tf.expand_dims(oh_off, 2) + tf.expand_dims(oh_on, 2)

        # take the product over the latent functions, and the sum over the GH grid.
        return tf.matmul(tf.reduce_prod(cdfs, 1), tf.reshape(gh_w/np.sqrt(np.pi), (-1, 1)))"
"    def __call__(self, tf_method):
        @wraps(tf_method)
        def runnable(instance, *np_args):
            graph_name = '_' + tf_method.__name__ + '_graph'
            if not hasattr(instance, graph_name):
                if instance._needs_recompile:
                    instance._compile()  # ensures free_vars is up-to-date.
                self.tf_args = [tf.placeholder(*a) for a in self.tf_arg_tuples]
                with instance.tf_mode():
                    graph = tf_method(instance, *self.tf_args)
                setattr(instance, graph_name, graph)
            feed_dict = dict(zip(self.tf_args, np_args))
            feed_dict[instance._free_vars] = instance.get_free_state()
            graph = getattr(instance, graph_name)
            return instance._session.run(graph, feed_dict=feed_dict)
        return runnable","    def __call__(self, tf_method):
        @wraps(tf_method)
        def runnable(instance, *np_args):
            graph_name = '_' + tf_method.__name__ + '_graph'
            if not hasattr(instance, graph_name):
                instance._compile()
                self.tf_args = [tf.placeholder(*a) for a in self.tf_arg_tuples]
                with instance.tf_mode():
                    graph = tf_method(instance, *self.tf_args)
                setattr(instance, graph_name, graph)
            feed_dict = dict(zip(self.tf_args, np_args))
            feed_dict[instance._free_vars] = instance.get_free_state()
            graph = getattr(instance, graph_name)
            return instance._session.run(graph, feed_dict=feed_dict)
        return runnable"
"        def runnable(instance, *np_args):
            graph_name = '_' + tf_method.__name__ + '_graph'
            if not hasattr(instance, graph_name):
                if instance._needs_recompile:
                    instance._compile()  # ensures free_vars is up-to-date.
                self.tf_args = [tf.placeholder(*a) for a in self.tf_arg_tuples]
                with instance.tf_mode():
                    graph = tf_method(instance, *self.tf_args)
                setattr(instance, graph_name, graph)
            feed_dict = dict(zip(self.tf_args, np_args))
            feed_dict[instance._free_vars] = instance.get_free_state()
            graph = getattr(instance, graph_name)
            return instance._session.run(graph, feed_dict=feed_dict)","        def runnable(instance, *np_args):
            graph_name = '_' + tf_method.__name__ + '_graph'
            if not hasattr(instance, graph_name):
                instance._compile()
                self.tf_args = [tf.placeholder(*a) for a in self.tf_arg_tuples]
                with instance.tf_mode():
                    graph = tf_method(instance, *self.tf_args)
                setattr(instance, graph_name, graph)
            feed_dict = dict(zip(self.tf_args, np_args))
            feed_dict[instance._free_vars] = instance.get_free_state()
            graph = getattr(instance, graph_name)
            return instance._session.run(graph, feed_dict=feed_dict)"
"    def browse(self, uri):
        logger.debug(""Browsing files at: %s"", uri)
        result = []
        local_path = path.uri_to_path(uri)

        if str(local_path) == ""root"":
            return list(self._get_media_dirs_refs())

        if not self._is_in_basedir(local_path):
            logger.warning(
                ""Rejected attempt to browse path (%s) outside dirs defined ""
                ""in file/media_dirs config."",
                uri,
            )
            return []
        if path.uri_to_path(uri).is_file():
            logger.error(""Rejected attempt to browse file (%s)"", uri)
            return []

        for dir_entry in local_path.iterdir():
            child_path = dir_entry.resolve()
            uri = path.path_to_uri(child_path)

            if not self._show_dotfiles and dir_entry.name.startswith("".""):
                continue

            if (
                self._excluded_file_extensions
                and dir_entry.suffix in self._excluded_file_extensions
            ):
                continue

            if child_path.is_symlink() and not self._follow_symlinks:
                logger.debug(""Ignoring symlink: %s"", uri)
                continue

            if not self._is_in_basedir(child_path):
                logger.debug(""Ignoring symlink to outside base dir: %s"", uri)
                continue

            if child_path.is_dir():
                result.append(
                    models.Ref.directory(name=dir_entry.name, uri=uri)
                )
            elif child_path.is_file():
                result.append(models.Ref.track(name=dir_entry.name, uri=uri))

        def order(item):
            return (item.type != models.Ref.DIRECTORY, item.name)

        result.sort(key=order)

        return result","    def browse(self, uri):
        logger.debug(""Browsing files at: %s"", uri)
        result = []
        local_path = path.uri_to_path(uri)

        if str(local_path) == ""root"":
            return list(self._get_media_dirs_refs())

        if not self._is_in_basedir(local_path):
            logger.warning(
                ""Rejected attempt to browse path (%s) outside dirs defined ""
                ""in file/media_dirs config."",
                uri,
            )
            return []

        for dir_entry in local_path.iterdir():
            child_path = dir_entry.resolve()
            uri = path.path_to_uri(child_path)

            if not self._show_dotfiles and dir_entry.name.startswith("".""):
                continue

            if (
                self._excluded_file_extensions
                and dir_entry.suffix in self._excluded_file_extensions
            ):
                continue

            if child_path.is_symlink() and not self._follow_symlinks:
                logger.debug(""Ignoring symlink: %s"", uri)
                continue

            if not self._is_in_basedir(child_path):
                logger.debug(""Ignoring symlink to outside base dir: %s"", uri)
                continue

            if child_path.is_dir():
                result.append(
                    models.Ref.directory(name=dir_entry.name, uri=uri)
                )
            elif child_path.is_file():
                result.append(models.Ref.track(name=dir_entry.name, uri=uri))

        def order(item):
            return (item.type != models.Ref.DIRECTORY, item.name)

        result.sort(key=order)

        return result"
"    def on_error(self, error, debug):
        gst_logger.error(f""GStreamer error: {error.message}"")
        gst_logger.debug(
            f""Got ERROR bus message: error={error!r} debug={debug!r}""
        )

        # TODO: is this needed?
        self._audio.stop_playback()","    def on_error(self, error, debug):
        error_msg = str(error).decode()
        debug_msg = debug.decode()
        gst_logger.debug(
            ""Got ERROR bus message: error=%r debug=%r"", error_msg, debug_msg
        )
        gst_logger.error(""GStreamer error: %s"", error_msg)
        # TODO: is this needed?
        self._audio.stop_playback()"
"    def on_warning(self, error, debug):
        gst_logger.warning(f""GStreamer warning: {error.message}"")
        gst_logger.debug(
            f""Got WARNING bus message: error={error!r} debug={debug!r}""
        )","    def on_warning(self, error, debug):
        error_msg = str(error).decode()
        debug_msg = debug.decode()
        gst_logger.warning(""GStreamer warning: %s"", error_msg)
        gst_logger.debug(
            ""Got WARNING bus message: error=%r debug=%r"", error_msg, debug_msg
        )"
"def _unwrap_stream(uri, timeout, scanner, requests_session):
    """"""
    Get a stream URI from a playlist URI, ``uri``.

    Unwraps nested playlists until something that's not a playlist is found or
    the ``timeout`` is reached.
    """"""

    original_uri = uri
    seen_uris = set()
    deadline = time.time() + timeout

    while time.time() < deadline:
        if uri in seen_uris:
            logger.info(
                'Unwrapping stream from URI (%s) failed: '
                'playlist referenced itself', uri)
            return None, None
        else:
            seen_uris.add(uri)

        logger.debug('Unwrapping stream from URI: %s', uri)

        try:
            scan_timeout = deadline - time.time()
            if scan_timeout < 0:
                logger.info(
                    'Unwrapping stream from URI (%s) failed: '
                    'timed out in %sms', uri, timeout)
                return None, None
            scan_result = scanner.scan(uri, timeout=scan_timeout)
        except exceptions.ScannerError as exc:
            logger.debug('GStreamer failed scanning URI (%s): %s', uri, exc)
            scan_result = None

        if scan_result is not None:
            has_interesting_mime = (
                scan_result.mime is not None and
                not scan_result.mime.startswith('text/') and
                not scan_result.mime.startswith('application/')
            )
            if scan_result.playable or has_interesting_mime:
                logger.debug(
                    'Unwrapped potential %s stream: %s', scan_result.mime, uri)
                return uri, scan_result

        download_timeout = deadline - time.time()
        if download_timeout < 0:
            logger.info(
                'Unwrapping stream from URI (%s) failed: timed out in %sms',
                uri, timeout)
            return None, None
        content = http.download(
            requests_session, uri, timeout=download_timeout / 1000)

        if content is None:
            logger.info(
                'Unwrapping stream from URI (%s) failed: '
                'error downloading URI %s', original_uri, uri)
            return None, None

        uris = playlists.parse(content)
        if not uris:
            logger.debug(
                'Failed parsing URI (%s) as playlist; found potential stream.',
                uri)
            return uri, None

        # TODO Test streams and return first that seems to be playable
        logger.debug(
            'Parsed playlist (%s) and found new URI: %s', uri, uris[0])
        uri = urllib.parse.urljoin(uri, uris[0])","def _unwrap_stream(uri, timeout, scanner, requests_session):
    """"""
    Get a stream URI from a playlist URI, ``uri``.

    Unwraps nested playlists until something that's not a playlist is found or
    the ``timeout`` is reached.
    """"""

    original_uri = uri
    seen_uris = set()
    deadline = time.time() + timeout

    while time.time() < deadline:
        if uri in seen_uris:
            logger.info(
                'Unwrapping stream from URI (%s) failed: '
                'playlist referenced itself', uri)
            return None, None
        else:
            seen_uris.add(uri)

        logger.debug('Unwrapping stream from URI: %s', uri)

        try:
            scan_timeout = deadline - time.time()
            if scan_timeout < 0:
                logger.info(
                    'Unwrapping stream from URI (%s) failed: '
                    'timed out in %sms', uri, timeout)
                return None, None
            scan_result = scanner.scan(uri, timeout=scan_timeout)
        except exceptions.ScannerError as exc:
            logger.debug('GStreamer failed scanning URI (%s): %s', uri, exc)
            scan_result = None

        if scan_result is not None:
            if scan_result.playable or (
                not scan_result.mime.startswith('text/') and
                not scan_result.mime.startswith('application/')
            ):
                logger.debug(
                    'Unwrapped potential %s stream: %s', scan_result.mime, uri)
                return uri, scan_result

        download_timeout = deadline - time.time()
        if download_timeout < 0:
            logger.info(
                'Unwrapping stream from URI (%s) failed: timed out in %sms',
                uri, timeout)
            return None, None
        content = http.download(
            requests_session, uri, timeout=download_timeout / 1000)

        if content is None:
            logger.info(
                'Unwrapping stream from URI (%s) failed: '
                'error downloading URI %s', original_uri, uri)
            return None, None

        uris = playlists.parse(content)
        if not uris:
            logger.debug(
                'Failed parsing URI (%s) as playlist; found potential stream.',
                uri)
            return uri, None

        # TODO Test streams and return first that seems to be playable
        logger.debug(
            'Parsed playlist (%s) and found new URI: %s', uri, uris[0])
        uri = urllib.parse.urljoin(uri, uris[0])"
"def listplaylist(context, name):
    """"""
    *musicpd.org, stored playlists section:*

        ``listplaylist {NAME}``

        Lists the files in the playlist ``NAME.m3u``.

    Output format::

        file: relative/path/to/file1.flac
        file: relative/path/to/file2.ogg
        file: relative/path/to/file3.mp3
    """"""
    playlist = _get_playlist(context, name)
    return [translator.uri_to_mpd_format(t.uri) for t in playlist.tracks]","def listplaylist(context, name):
    """"""
    *musicpd.org, stored playlists section:*

        ``listplaylist {NAME}``

        Lists the files in the playlist ``NAME.m3u``.

    Output format::

        file: relative/path/to/file1.flac
        file: relative/path/to/file2.ogg
        file: relative/path/to/file3.mp3
    """"""
    playlist = _get_playlist(context, name)
    return ['file: %s' % t.uri for t in playlist.tracks]"
"def track_to_mpd_format(track, position=None, stream_title=None):
    """"""
    Format track for output to MPD client.

    :param track: the track
    :type track: :class:`mopidy.models.Track` or :class:`mopidy.models.TlTrack`
    :param position: track's position in playlist
    :type position: integer
    :param stream_title: the current streams title
    :type position: string
    :rtype: list of two-tuples
    """"""
    if isinstance(track, TlTrack):
        (tlid, track) = track
    else:
        (tlid, track) = (None, track)

    if not track.uri:
        logger.warning('Ignoring track without uri')
        return []

    result = [
        uri_to_mpd_format(track.uri),
        ('Time', track.length and (track.length // 1000) or 0),
        ('Artist', concat_multi_values(track.artists, 'name')),
        ('Album', track.album and track.album.name or ''),
    ]

    if stream_title is not None:
        result.append(('Title', stream_title))
        if track.name:
            result.append(('Name', track.name))
    else:
        result.append(('Title', track.name or ''))

    if track.date:
        result.append(('Date', track.date))

    if track.album is not None and track.album.num_tracks is not None:
        result.append(('Track', '%d/%d' % (
            track.track_no or 0, track.album.num_tracks)))
    else:
        result.append(('Track', track.track_no or 0))
    if position is not None and tlid is not None:
        result.append(('Pos', position))
        result.append(('Id', tlid))
    if track.album is not None and track.album.musicbrainz_id is not None:
        result.append(('MUSICBRAINZ_ALBUMID', track.album.musicbrainz_id))

    if track.album is not None and track.album.artists:
        result.append(
            ('AlbumArtist', concat_multi_values(track.album.artists, 'name')))
        musicbrainz_ids = concat_multi_values(
            track.album.artists, 'musicbrainz_id')
        if musicbrainz_ids:
            result.append(('MUSICBRAINZ_ALBUMARTISTID', musicbrainz_ids))

    if track.artists:
        musicbrainz_ids = concat_multi_values(track.artists, 'musicbrainz_id')
        if musicbrainz_ids:
            result.append(('MUSICBRAINZ_ARTISTID', musicbrainz_ids))

    if track.composers:
        result.append(
            ('Composer', concat_multi_values(track.composers, 'name')))

    if track.performers:
        result.append(
            ('Performer', concat_multi_values(track.performers, 'name')))

    if track.genre:
        result.append(('Genre', track.genre))

    if track.disc_no:
        result.append(('Disc', track.disc_no))

    if track.last_modified:
        datestring = datetime.datetime.utcfromtimestamp(
            track.last_modified // 1000).isoformat()
        result.append(('Last-Modified', datestring + 'Z'))

    if track.musicbrainz_id is not None:
        result.append(('MUSICBRAINZ_TRACKID', track.musicbrainz_id))

    if track.album and track.album.uri:
        result.append(('X-AlbumUri', track.album.uri))
    if track.album and track.album.images:
        images = ';'.join(i for i in track.album.images if i != '')
        result.append(('X-AlbumImage', images))

    result = [element for element in result if _has_value(*element)]

    return result","def track_to_mpd_format(track, position=None, stream_title=None):
    """"""
    Format track for output to MPD client.

    :param track: the track
    :type track: :class:`mopidy.models.Track` or :class:`mopidy.models.TlTrack`
    :param position: track's position in playlist
    :type position: integer
    :param stream_title: the current streams title
    :type position: string
    :rtype: list of two-tuples
    """"""
    if isinstance(track, TlTrack):
        (tlid, track) = track
    else:
        (tlid, track) = (None, track)

    if not track.uri:
        logger.warning('Ignoring track without uri')
        return []

    result = [
        ('file', track.uri),
        ('Time', track.length and (track.length // 1000) or 0),
        ('Artist', concat_multi_values(track.artists, 'name')),
        ('Album', track.album and track.album.name or ''),
    ]

    if stream_title is not None:
        result.append(('Title', stream_title))
        if track.name:
            result.append(('Name', track.name))
    else:
        result.append(('Title', track.name or ''))

    if track.date:
        result.append(('Date', track.date))

    if track.album is not None and track.album.num_tracks is not None:
        result.append(('Track', '%d/%d' % (
            track.track_no or 0, track.album.num_tracks)))
    else:
        result.append(('Track', track.track_no or 0))
    if position is not None and tlid is not None:
        result.append(('Pos', position))
        result.append(('Id', tlid))
    if track.album is not None and track.album.musicbrainz_id is not None:
        result.append(('MUSICBRAINZ_ALBUMID', track.album.musicbrainz_id))

    if track.album is not None and track.album.artists:
        result.append(
            ('AlbumArtist', concat_multi_values(track.album.artists, 'name')))
        musicbrainz_ids = concat_multi_values(
            track.album.artists, 'musicbrainz_id')
        if musicbrainz_ids:
            result.append(('MUSICBRAINZ_ALBUMARTISTID', musicbrainz_ids))

    if track.artists:
        musicbrainz_ids = concat_multi_values(track.artists, 'musicbrainz_id')
        if musicbrainz_ids:
            result.append(('MUSICBRAINZ_ARTISTID', musicbrainz_ids))

    if track.composers:
        result.append(
            ('Composer', concat_multi_values(track.composers, 'name')))

    if track.performers:
        result.append(
            ('Performer', concat_multi_values(track.performers, 'name')))

    if track.genre:
        result.append(('Genre', track.genre))

    if track.disc_no:
        result.append(('Disc', track.disc_no))

    if track.last_modified:
        datestring = datetime.datetime.utcfromtimestamp(
            track.last_modified // 1000).isoformat()
        result.append(('Last-Modified', datestring + 'Z'))

    if track.musicbrainz_id is not None:
        result.append(('MUSICBRAINZ_TRACKID', track.musicbrainz_id))

    if track.album and track.album.uri:
        result.append(('X-AlbumUri', track.album.uri))
    if track.album and track.album.images:
        images = ';'.join(i for i in track.album.images if i != '')
        result.append(('X-AlbumImage', images))

    result = [element for element in result if _has_value(*element)]

    return result"
"def _get_user_dirs(xdg_config_dir):
    """"""Returns a dict of XDG dirs read from
    ``$XDG_CONFIG_HOME/user-dirs.dirs``.

    This is used at import time for most users of :mod:`mopidy`. By rolling our
    own implementation instead of using :meth:`glib.get_user_special_dir` we
    make it possible for many extensions to run their test suites, which are
    importing parts of :mod:`mopidy`, in a virtualenv with global site-packages
    disabled, and thus no :mod:`glib` available.
    """"""

    dirs_file = os.path.join(xdg_config_dir, b'user-dirs.dirs')

    if not os.path.exists(dirs_file):
        return {}

    with open(dirs_file, 'rb') as fh:
        data = fh.read()

    data = b'[XDG_USER_DIRS]\\n' + data
    data = data.replace(b'$HOME', os.path.expanduser(b'~'))
    data = data.replace(b'""', b'')

    config = configparser.RawConfigParser()
    config.readfp(io.BytesIO(data))

    return {
        k.upper().decode('utf-8'): os.path.abspath(v)
        for k, v in config.items('XDG_USER_DIRS') if v is not None
    }","def _get_user_dirs(xdg_config_dir):
    """"""Returns a dict of XDG dirs read from
    ``$XDG_CONFIG_HOME/user-dirs.dirs``.

    This is used at import time for most users of :mod:`mopidy`. By rolling our
    own implementation instead of using :meth:`glib.get_user_special_dir` we
    make it possible for many extensions to run their test suites, which are
    importing parts of :mod:`mopidy`, in a virtualenv with global site-packages
    disabled, and thus no :mod:`glib` available.
    """"""

    dirs_file = os.path.join(xdg_config_dir, b'user-dirs.dirs')

    if not os.path.exists(dirs_file):
        return {}

    with open(dirs_file, 'rb') as fh:
        data = fh.read().decode('utf-8')

    data = '[XDG_USER_DIRS]\\n' + data
    data = data.replace('$HOME', os.path.expanduser('~'))
    data = data.replace('""', '')

    config = configparser.RawConfigParser()
    config.readfp(io.StringIO(data))

    return {
        k.upper(): os.path.abspath(v)
        for k, v in config.items('XDG_USER_DIRS') if v is not None}"
"    def validate(self, value):
        value = super(Identifier, self).validate(value)
        if isinstance(value, compat.text_type):
            value = value.encode('utf-8')
        return compat.intern(value)","    def validate(self, value):
        return compat.intern(str(super(Identifier, self).validate(value)))"
"    def on_stream_start(self):
        gst_logger.debug('Got STREAM_START bus message')
        uri = self._audio._pending_uri
        logger.debug('Audio event: stream_changed(uri=%r)', uri)
        AudioListener.send('stream_changed', uri=uri)

        # Emit any postponed tags that we got after about-to-finish.
        tags, self._audio._pending_tags = self._audio._pending_tags, None
        self._audio._tags = tags or {}

        if tags:
            logger.debug('Audio event: tags_changed(tags=%r)', tags.keys())
            AudioListener.send('tags_changed', tags=tags.keys())","    def on_stream_start(self):
        gst_logger.debug('Got STREAM_START bus message')
        uri = self._audio._pending_uri
        logger.debug('Audio event: stream_changed(uri=%r)', uri)
        AudioListener.send('stream_changed', uri=uri)

        # Emit any postponed tags that we got after about-to-finish.
        tags, self._audio._pending_tags = self._audio._pending_tags, None
        self._audio._tags = tags

        if tags:
            logger.debug('Audio event: tags_changed(tags=%r)', tags.keys())
            AudioListener.send('tags_changed', tags=tags.keys())"
"    def on_playbin_state_changed(self, old_state, new_state, pending_state):
        gst_logger.debug(
            'Got STATE_CHANGED bus message: old=%s new=%s pending=%s',
            old_state.value_name, new_state.value_name,
            pending_state.value_name)

        if new_state == Gst.State.READY and pending_state == Gst.State.NULL:
            # XXX: We're not called on the last state change when going down to
            # NULL, so we rewrite the second to last call to get the expected
            # behavior.
            new_state = Gst.State.NULL
            pending_state = Gst.State.VOID_PENDING

        if pending_state != Gst.State.VOID_PENDING:
            return  # Ignore intermediate state changes

        if new_state == Gst.State.READY:
            return  # Ignore READY state as it's GStreamer specific

        new_state = _GST_STATE_MAPPING[new_state]
        old_state, self._audio.state = self._audio.state, new_state

        target_state = _GST_STATE_MAPPING.get(self._audio._target_state)
        if target_state is None:
            # XXX: Workaround for #1430, to be fixed properly by #1222.
            logger.debug('Race condition happened. See #1222 and #1430.')
            return
        if target_state == new_state:
            target_state = None

        logger.debug('Audio event: state_changed(old_state=%s, new_state=%s, '
                     'target_state=%s)', old_state, new_state, target_state)
        AudioListener.send('state_changed', old_state=old_state,
                           new_state=new_state, target_state=target_state)
        if new_state == PlaybackState.STOPPED:
            logger.debug('Audio event: stream_changed(uri=None)')
            AudioListener.send('stream_changed', uri=None)

        if 'GST_DEBUG_DUMP_DOT_DIR' in os.environ:
            Gst.debug_bin_to_dot_file(
                self._audio._playbin, Gst.DebugGraphDetails.ALL, 'mopidy')","    def on_playbin_state_changed(self, old_state, new_state, pending_state):
        gst_logger.debug(
            'Got STATE_CHANGED bus message: old=%s new=%s pending=%s',
            old_state.value_name, new_state.value_name,
            pending_state.value_name)

        if new_state == Gst.State.READY and pending_state == Gst.State.NULL:
            # XXX: We're not called on the last state change when going down to
            # NULL, so we rewrite the second to last call to get the expected
            # behavior.
            new_state = Gst.State.NULL
            pending_state = Gst.State.VOID_PENDING

        if pending_state != Gst.State.VOID_PENDING:
            return  # Ignore intermediate state changes

        if new_state == Gst.State.READY:
            return  # Ignore READY state as it's GStreamer specific

        new_state = _GST_STATE_MAPPING[new_state]
        old_state, self._audio.state = self._audio.state, new_state

        target_state = _GST_STATE_MAPPING[self._audio._target_state]
        if target_state == new_state:
            target_state = None

        logger.debug('Audio event: state_changed(old_state=%s, new_state=%s, '
                     'target_state=%s)', old_state, new_state, target_state)
        AudioListener.send('state_changed', old_state=old_state,
                           new_state=new_state, target_state=target_state)
        if new_state == PlaybackState.STOPPED:
            logger.debug('Audio event: stream_changed(uri=None)')
            AudioListener.send('stream_changed', uri=None)

        if 'GST_DEBUG_DUMP_DOT_DIR' in os.environ:
            Gst.debug_bin_to_dot_file(
                self._audio._playbin, Gst.DebugGraphDetails.ALL, 'mopidy')"
"    def playlist_uri_from_name(self, name):
        """"""
        Helper function to retrieve a playlist URI from its unique MPD name.
        """"""
        if name not in self._uri_from_name:
            self.refresh_playlists_mapping()
        return self._uri_from_name.get(name)","    def playlist_uri_from_name(self, name):
        """"""
        Helper function to retrieve a playlist URI from its unique MPD name.
        """"""
        if not self._uri_from_name:
            self.refresh_playlists_mapping()
        return self._uri_from_name.get(name)"
"def _get_library(args, config):
    libraries = dict((l.name, l) for l in args.registry['local:library'])
    library_name = config['local']['library']

    if library_name not in libraries:
        logger.error('Local library %s not found', library_name)
        return None

    logger.debug('Using %s as the local library', library_name)
    return libraries[library_name](config)","def _get_library(args, config):
    libraries = dict((l.name, l) for l in args.registry['local:library'])
    library_name = config['local']['library']

    if library_name not in libraries:
        logger.warning('Local library %s not found', library_name)
        return 1

    logger.debug('Using %s as the local library', library_name)
    return libraries[library_name](config)"
"    def run(self, args, config):
        library = _get_library(args, config)
        if library is None:
            return 1

        prompt = '\\nAre you sure you want to clear the library? [y/N] '

        if compat.input(prompt).lower() != 'y':
            print('Clearing library aborted.')
            return 0

        if library.clear():
            print('Library successfully cleared.')
            return 0

        print('Unable to clear library.')
        return 1","    def run(self, args, config):
        library = _get_library(args, config)
        prompt = '\\nAre you sure you want to clear the library? [y/N] '

        if compat.input(prompt).lower() != 'y':
            print('Clearing library aborted.')
            return 0

        if library.clear():
            print('Library successfully cleared.')
            return 0

        print('Unable to clear library.')
        return 1"
"    def run(self, args, config):
        media_dir = config['local']['media_dir']
        scan_timeout = config['local']['scan_timeout']
        flush_threshold = config['local']['scan_flush_threshold']
        excluded_file_extensions = config['local']['excluded_file_extensions']
        excluded_file_extensions = tuple(
            bytes(file_ext.lower()) for file_ext in excluded_file_extensions)

        library = _get_library(args, config)
        if library is None:
            return 1

        file_mtimes, file_errors = path.find_mtimes(
            media_dir, follow=config['local']['scan_follow_symlinks'])

        logger.info('Found %d files in media_dir.', len(file_mtimes))

        if file_errors:
            logger.warning('Encountered %d errors while scanning media_dir.',
                           len(file_errors))
        for name in file_errors:
            logger.debug('Scan error %r for %r', file_errors[name], name)

        num_tracks = library.load()
        logger.info('Checking %d tracks from library.', num_tracks)

        uris_to_update = set()
        uris_to_remove = set()
        uris_in_library = set()

        for track in library.begin():
            abspath = translator.local_track_uri_to_path(track.uri, media_dir)
            mtime = file_mtimes.get(abspath)
            if mtime is None:
                logger.debug('Missing file %s', track.uri)
                uris_to_remove.add(track.uri)
            elif mtime > track.last_modified or args.force:
                uris_to_update.add(track.uri)
            uris_in_library.add(track.uri)

        logger.info('Removing %d missing tracks.', len(uris_to_remove))
        for uri in uris_to_remove:
            library.remove(uri)

        for abspath in file_mtimes:
            relpath = os.path.relpath(abspath, media_dir)
            uri = translator.path_to_local_track_uri(relpath)

            if b'/.' in relpath:
                logger.debug('Skipped %s: Hidden directory/file.', uri)
            elif relpath.lower().endswith(excluded_file_extensions):
                logger.debug('Skipped %s: File extension excluded.', uri)
            elif uri not in uris_in_library:
                uris_to_update.add(uri)

        logger.info(
            'Found %d tracks which need to be updated.', len(uris_to_update))
        logger.info('Scanning...')

        uris_to_update = sorted(uris_to_update, key=lambda v: v.lower())
        uris_to_update = uris_to_update[:args.limit]

        scanner = scan.Scanner(scan_timeout)
        progress = _Progress(flush_threshold, len(uris_to_update))

        for uri in uris_to_update:
            try:
                relpath = translator.local_track_uri_to_path(uri, media_dir)
                file_uri = path.path_to_uri(os.path.join(media_dir, relpath))
                result = scanner.scan(file_uri)
                tags, duration = result.tags, result.duration
                if not result.playable:
                    logger.warning('Failed %s: No audio found in file.', uri)
                elif duration < MIN_DURATION_MS:
                    logger.warning('Failed %s: Track shorter than %dms',
                                   uri, MIN_DURATION_MS)
                else:
                    mtime = file_mtimes.get(os.path.join(media_dir, relpath))
                    track = utils.convert_tags_to_track(tags).replace(
                        uri=uri, length=duration, last_modified=mtime)
                    if library.add_supports_tags_and_duration:
                        library.add(track, tags=tags, duration=duration)
                    else:
                        library.add(track)
                    logger.debug('Added %s', track.uri)
            except exceptions.ScannerError as error:
                logger.warning('Failed %s: %s', uri, error)

            if progress.increment():
                progress.log()
                if library.flush():
                    logger.debug('Progress flushed.')

        progress.log()
        library.close()
        logger.info('Done scanning.')
        return 0","    def run(self, args, config):
        media_dir = config['local']['media_dir']
        scan_timeout = config['local']['scan_timeout']
        flush_threshold = config['local']['scan_flush_threshold']
        excluded_file_extensions = config['local']['excluded_file_extensions']
        excluded_file_extensions = tuple(
            bytes(file_ext.lower()) for file_ext in excluded_file_extensions)

        library = _get_library(args, config)

        file_mtimes, file_errors = path.find_mtimes(
            media_dir, follow=config['local']['scan_follow_symlinks'])

        logger.info('Found %d files in media_dir.', len(file_mtimes))

        if file_errors:
            logger.warning('Encountered %d errors while scanning media_dir.',
                           len(file_errors))
        for name in file_errors:
            logger.debug('Scan error %r for %r', file_errors[name], name)

        num_tracks = library.load()
        logger.info('Checking %d tracks from library.', num_tracks)

        uris_to_update = set()
        uris_to_remove = set()
        uris_in_library = set()

        for track in library.begin():
            abspath = translator.local_track_uri_to_path(track.uri, media_dir)
            mtime = file_mtimes.get(abspath)
            if mtime is None:
                logger.debug('Missing file %s', track.uri)
                uris_to_remove.add(track.uri)
            elif mtime > track.last_modified or args.force:
                uris_to_update.add(track.uri)
            uris_in_library.add(track.uri)

        logger.info('Removing %d missing tracks.', len(uris_to_remove))
        for uri in uris_to_remove:
            library.remove(uri)

        for abspath in file_mtimes:
            relpath = os.path.relpath(abspath, media_dir)
            uri = translator.path_to_local_track_uri(relpath)

            if b'/.' in relpath:
                logger.debug('Skipped %s: Hidden directory/file.', uri)
            elif relpath.lower().endswith(excluded_file_extensions):
                logger.debug('Skipped %s: File extension excluded.', uri)
            elif uri not in uris_in_library:
                uris_to_update.add(uri)

        logger.info(
            'Found %d tracks which need to be updated.', len(uris_to_update))
        logger.info('Scanning...')

        uris_to_update = sorted(uris_to_update, key=lambda v: v.lower())
        uris_to_update = uris_to_update[:args.limit]

        scanner = scan.Scanner(scan_timeout)
        progress = _Progress(flush_threshold, len(uris_to_update))

        for uri in uris_to_update:
            try:
                relpath = translator.local_track_uri_to_path(uri, media_dir)
                file_uri = path.path_to_uri(os.path.join(media_dir, relpath))
                result = scanner.scan(file_uri)
                tags, duration = result.tags, result.duration
                if not result.playable:
                    logger.warning('Failed %s: No audio found in file.', uri)
                elif duration < MIN_DURATION_MS:
                    logger.warning('Failed %s: Track shorter than %dms',
                                   uri, MIN_DURATION_MS)
                else:
                    mtime = file_mtimes.get(os.path.join(media_dir, relpath))
                    track = utils.convert_tags_to_track(tags).replace(
                        uri=uri, length=duration, last_modified=mtime)
                    if library.add_supports_tags_and_duration:
                        library.add(track, tags=tags, duration=duration)
                    else:
                        library.add(track)
                    logger.debug('Added %s', track.uri)
            except exceptions.ScannerError as error:
                logger.warning('Failed %s: %s', uri, error)

            if progress.increment():
                progress.log()
                if library.flush():
                    logger.debug('Progress flushed.')

        progress.log()
        library.close()
        logger.info('Done scanning.')
        return 0"
"def parse_urilist(data):
    result = []
    for line in data.splitlines():
        if not line.strip() or line.startswith(b'#'):
            continue
        try:
            validation.check_uri(line)
        except ValueError:
            return []
        result.append(line)
    return result","def parse_urilist(data):
    result = []
    for line in data.splitlines():
        if not line.strip() or line.startswith('#'):
            continue
        try:
            validation.check_uri(line)
        except ValueError:
            return []
        result.append(line)
    return result"
"    def send(self, data):
        """"""Send data to client, return any unsent data.""""""
        try:
            sent = self.sock.send(data)
            return data[sent:]
        except socket.error as e:
            if e.errno in (errno.EWOULDBLOCK, errno.EINTR):
                return data
            self.stop(
                'Unexpected client error: %s' % encoding.locale_decode(e))
            return b''","    def send(self, data):
        """"""Send data to client, return any unsent data.""""""
        try:
            sent = self.sock.send(data)
            return data[sent:]
        except socket.error as e:
            if e.errno in (errno.EWOULDBLOCK, errno.EINTR):
                return data
            self.stop('Unexpected client error: %s' % e)
            return b''"
"def _find_worker(relative, follow, done, work, results, errors):
    """"""Worker thread for collecting stat() results.

    :param str relative: directory to make results relative to
    :param bool follow: if symlinks should be followed
    :param threading.Event done: event indicating that all work has been done
    :param queue.Queue work: queue of paths to process
    :param dict results: shared dictionary for storing all the stat() results
    :param dict errors: shared dictionary for storing any per path errors
    """"""
    while not done.is_set():
        try:
            entry, parents = work.get(block=False)
        except queue.Empty:
            continue

        if relative:
            path = os.path.relpath(entry, relative)
        else:
            path = entry

        try:
            if follow:
                st = os.stat(entry)
            else:
                st = os.lstat(entry)

            if (st.st_dev, st.st_ino) in parents:
                errors[path] = exceptions.FindError('Sym/hardlink loop found.')
                continue

            parents = parents + [(st.st_dev, st.st_ino)]
            if stat.S_ISDIR(st.st_mode):
                for e in os.listdir(entry):
                    work.put((os.path.join(entry, e), parents))
            elif stat.S_ISREG(st.st_mode):
                results[path] = st
            elif stat.S_ISLNK(st.st_mode):
                errors[path] = exceptions.FindError('Not following symlinks.')
            else:
                errors[path] = exceptions.FindError('Not a file or directory.')

        except OSError as e:
            errors[path] = exceptions.FindError(
                encoding.locale_decode(e.strerror), e.errno)
        finally:
            work.task_done()","def _find_worker(relative, follow, done, work, results, errors):
    """"""Worker thread for collecting stat() results.

    :param str relative: directory to make results relative to
    :param bool follow: if symlinks should be followed
    :param threading.Event done: event indicating that all work has been done
    :param queue.Queue work: queue of paths to process
    :param dict results: shared dictionary for storing all the stat() results
    :param dict errors: shared dictionary for storing any per path errors
    """"""
    while not done.is_set():
        try:
            entry, parents = work.get(block=False)
        except queue.Empty:
            continue

        if relative:
            path = os.path.relpath(entry, relative)
        else:
            path = entry

        try:
            if follow:
                st = os.stat(entry)
            else:
                st = os.lstat(entry)

            if (st.st_dev, st.st_ino) in parents:
                errors[path] = exceptions.FindError('Sym/hardlink loop found.')
                continue

            parents = parents + [(st.st_dev, st.st_ino)]
            if stat.S_ISDIR(st.st_mode):
                for e in os.listdir(entry):
                    work.put((os.path.join(entry, e), parents))
            elif stat.S_ISREG(st.st_mode):
                results[path] = st
            elif stat.S_ISLNK(st.st_mode):
                errors[path] = exceptions.FindError('Not following symlinks.')
            else:
                errors[path] = exceptions.FindError('Not a file or directory.')

        except OSError as e:
            errors[path] = exceptions.FindError(e.strerror, e.errno)
        finally:
            work.task_done()"
"    def push(self, buffer_):
        if self._source is None:
            return False

        if buffer_ is None:
            gst_logger.debug('Sending appsrc end-of-stream event.')
            return self._source.emit('end-of-stream') == gst.FLOW_OK
        else:
            return self._source.emit('push-buffer', buffer_) == gst.FLOW_OK","    def push(self, buffer_):
        if buffer_ is None:
            gst_logger.debug('Sending appsrc end-of-stream event.')
            return self._source.emit('end-of-stream') == gst.FLOW_OK
        else:
            return self._source.emit('push-buffer', buffer_) == gst.FLOW_OK"
"def find_exact(tracks, query=None, uris=None):
    # TODO Only return results within URI roots given by ``uris``

    if query is None:
        query = {}

    _validate_query(query)

    for (field, values) in query.items():
        if not hasattr(values, '__iter__'):
            values = [values]
        # FIXME this is bound to be slow for large libraries
        for value in values:
            if field == 'track_no':
                q = _convert_to_int(value)
            else:
                q = value.strip()

            uri_filter = lambda t: q == t.uri
            track_name_filter = lambda t: q == t.name
            album_filter = lambda t: q == getattr(
                getattr(t, 'album', None), 'name', None)
            artist_filter = lambda t: filter(
                lambda a: q == a.name, t.artists)
            albumartist_filter = lambda t: any([
                q == a.name
                for a in getattr(t.album, 'artists', [])])
            composer_filter = lambda t: any([
                q == a.name
                for a in getattr(t, 'composers', [])])
            performer_filter = lambda t: any([
                q == a.name
                for a in getattr(t, 'performers', [])])
            track_no_filter = lambda t: q == t.track_no
            genre_filter = lambda t: t.genre and q == t.genre
            date_filter = lambda t: q == t.date
            comment_filter = lambda t: q == t.comment
            any_filter = lambda t: (
                uri_filter(t) or
                track_name_filter(t) or
                album_filter(t) or
                artist_filter(t) or
                albumartist_filter(t) or
                composer_filter(t) or
                performer_filter(t) or
                track_no_filter(t) or
                genre_filter(t) or
                date_filter(t) or
                comment_filter(t))

            if field == 'uri':
                tracks = filter(uri_filter, tracks)
            elif field == 'track_name':
                tracks = filter(track_name_filter, tracks)
            elif field == 'album':
                tracks = filter(album_filter, tracks)
            elif field == 'artist':
                tracks = filter(artist_filter, tracks)
            elif field == 'albumartist':
                tracks = filter(albumartist_filter, tracks)
            elif field == 'composer':
                tracks = filter(composer_filter, tracks)
            elif field == 'performer':
                tracks = filter(performer_filter, tracks)
            elif field == 'track_no':
                tracks = filter(track_no_filter, tracks)
            elif field == 'genre':
                tracks = filter(genre_filter, tracks)
            elif field == 'date':
                tracks = filter(date_filter, tracks)
            elif field == 'comment':
                tracks = filter(comment_filter, tracks)
            elif field == 'any':
                tracks = filter(any_filter, tracks)
            else:
                raise LookupError('Invalid lookup field: %s' % field)

    # TODO: add local:search:<query>
    return SearchResult(uri='local:search', tracks=tracks)","def find_exact(tracks, query=None, uris=None):
    # TODO Only return results within URI roots given by ``uris``

    if query is None:
        query = {}

    _validate_query(query)

    for (field, values) in query.items():
        if not hasattr(values, '__iter__'):
            values = [values]
        # FIXME this is bound to be slow for large libraries
        for value in values:
            if field == 'track_no':
                q = _convert_to_int(value)
            else:
                q = value.strip()

            uri_filter = lambda t: q == t.uri
            track_name_filter = lambda t: q == t.name
            album_filter = lambda t: q == getattr(t, 'album', Album()).name
            artist_filter = lambda t: filter(
                lambda a: q == a.name, t.artists)
            albumartist_filter = lambda t: any([
                q == a.name
                for a in getattr(t.album, 'artists', [])])
            composer_filter = lambda t: any([
                q == a.name
                for a in getattr(t, 'composers', [])])
            performer_filter = lambda t: any([
                q == a.name
                for a in getattr(t, 'performers', [])])
            track_no_filter = lambda t: q == t.track_no
            genre_filter = lambda t: t.genre and q == t.genre
            date_filter = lambda t: q == t.date
            comment_filter = lambda t: q == t.comment
            any_filter = lambda t: (
                uri_filter(t) or
                track_name_filter(t) or
                album_filter(t) or
                artist_filter(t) or
                albumartist_filter(t) or
                composer_filter(t) or
                performer_filter(t) or
                track_no_filter(t) or
                genre_filter(t) or
                date_filter(t) or
                comment_filter(t))

            if field == 'uri':
                tracks = filter(uri_filter, tracks)
            elif field == 'track_name':
                tracks = filter(track_name_filter, tracks)
            elif field == 'album':
                tracks = filter(album_filter, tracks)
            elif field == 'artist':
                tracks = filter(artist_filter, tracks)
            elif field == 'albumartist':
                tracks = filter(albumartist_filter, tracks)
            elif field == 'composer':
                tracks = filter(composer_filter, tracks)
            elif field == 'performer':
                tracks = filter(performer_filter, tracks)
            elif field == 'track_no':
                tracks = filter(track_no_filter, tracks)
            elif field == 'genre':
                tracks = filter(genre_filter, tracks)
            elif field == 'date':
                tracks = filter(date_filter, tracks)
            elif field == 'comment':
                tracks = filter(comment_filter, tracks)
            elif field == 'any':
                tracks = filter(any_filter, tracks)
            else:
                raise LookupError('Invalid lookup field: %s' % field)

    # TODO: add local:search:<query>
    return SearchResult(uri='local:search', tracks=tracks)"
"def validate_extension(extension):
    """"""Verify extension's dependencies and environment.

    :param extensions: an extension to check
    :returns: if extension should be run
    """"""

    logger.debug('Validating extension: %s', extension.ext_name)

    if extension.ext_name != extension.entry_point.name:
        logger.warning(
            'Disabled extension %(ep)s: entry point name (%(ep)s) '
            'does not match extension name (%(ext)s)',
            {'ep': extension.entry_point.name, 'ext': extension.ext_name})
        return False

    try:
        extension.entry_point.require()
    except pkg_resources.DistributionNotFound as ex:
        logger.info(
            'Disabled extension %s: Dependency %s not found',
            extension.ext_name, ex)
        return False
    except pkg_resources.VersionConflict as ex:
        if len(ex.args) == 2:
            found, required = ex.args
            logger.info(
                'Disabled extension %s: %s required, but found %s at %s',
                extension.ext_name, required, found, found.location)
        else:
            logger.info('Disabled extension %s: %s', extension.ext_name, ex)
        return False

    try:
        extension.validate_environment()
    except exceptions.ExtensionError as ex:
        logger.info(
            'Disabled extension %s: %s', extension.ext_name, ex.message)
        return False

    return True","def validate_extension(extension):
    """"""Verify extension's dependencies and environment.

    :param extensions: an extension to check
    :returns: if extension should be run
    """"""

    logger.debug('Validating extension: %s', extension.ext_name)

    if extension.ext_name != extension.entry_point.name:
        logger.warning(
            'Disabled extension %(ep)s: entry point name (%(ep)s) '
            'does not match extension name (%(ext)s)',
            {'ep': extension.entry_point.name, 'ext': extension.ext_name})
        return False

    try:
        extension.entry_point.require()
    except pkg_resources.DistributionNotFound as ex:
        logger.info(
            'Disabled extension %s: Dependency %s not found',
            extension.ext_name, ex)
        return False
    except pkg_resources.VersionConflict as ex:
        found, required = ex.args
        logger.info(
            'Disabled extension %s: %s required, but found %s at %s',
            extension.ext_name, required, found, found.location)
        return False

    try:
        extension.validate_environment()
    except exceptions.ExtensionError as ex:
        logger.info(
            'Disabled extension %s: %s', extension.ext_name, ex.message)
        return False

    return True"
"    def recv_callback(self, fd, flags):
        if flags & (gobject.IO_ERR | gobject.IO_HUP):
            self.stop('Bad client flags: %s' % flags)
            return True

        try:
            data = self.sock.recv(4096)
        except socket.error as e:
            if e.errno not in (errno.EWOULDBLOCK, errno.EINTR):
                self.stop('Unexpected client error: %s' % e)
            return True

        if not data:
            self.disable_recv()
            self.actor_ref.tell({'close': True})
            return True

        try:
            self.actor_ref.tell({'received': data})
        except pykka.ActorDeadError:
            self.stop('Actor is dead.')

        return True","    def recv_callback(self, fd, flags):
        if flags & (gobject.IO_ERR | gobject.IO_HUP):
            self.stop('Bad client flags: %s' % flags)
            return True

        try:
            data = self.sock.recv(4096)
        except socket.error as e:
            if e.errno not in (errno.EWOULDBLOCK, errno.EINTR):
                self.stop('Unexpected client error: %s' % e)
            return True

        if not data:
            self.actor_ref.tell({'close': True})
            self.disable_recv()
            return True

        try:
            self.actor_ref.tell({'received': data})
        except pykka.ActorDeadError:
            self.stop('Actor is dead.')

        return True"
"    def publish(self):
        if not dbus:
            logger.debug('Zeroconf publish failed: dbus not installed.')
            return False

        try:
            bus = dbus.SystemBus()
        except dbus.exceptions.DBusException as e:
            logger.debug('Zeroconf publish failed: %s', e)
            return False

        if not bus.name_has_owner('org.freedesktop.Avahi'):
            logger.debug('Zeroconf publish failed: Avahi service not running.')
            return False

        server = dbus.Interface(bus.get_object('org.freedesktop.Avahi', '/'),
                                'org.freedesktop.Avahi.Server')

        self.group = dbus.Interface(
            bus.get_object('org.freedesktop.Avahi', server.EntryGroupNew()),
            'org.freedesktop.Avahi.EntryGroup')

        try:
            text = [_convert_text_to_dbus_bytes(t) for t in self.text]
            self.group.AddService(
                _AVAHI_IF_UNSPEC, _AVAHI_PROTO_UNSPEC,
                dbus.UInt32(_AVAHI_PUBLISHFLAGS_NONE), self.name, self.stype,
                self.domain, self.host, dbus.UInt16(self.port), text)
        except dbus.exceptions.DBusException as e:
            logger.debug('Zeroconf publish failed: %s', e)
            return False

        self.group.Commit()
        return True","    def publish(self):
        if not dbus:
            logger.debug('Zeroconf publish failed: dbus not installed.')
            return False

        try:
            bus = dbus.SystemBus()
        except dbus.exceptions.DBusException as e:
            logger.debug('Zeroconf publish failed: %s', e)
            return False

        if not bus.name_has_owner('org.freedesktop.Avahi'):
            logger.debug('Zeroconf publish failed: Avahi service not running.')
            return False

        server = dbus.Interface(bus.get_object('org.freedesktop.Avahi', '/'),
                                'org.freedesktop.Avahi.Server')

        self.group = dbus.Interface(
            bus.get_object('org.freedesktop.Avahi', server.EntryGroupNew()),
            'org.freedesktop.Avahi.EntryGroup')

        text = [_convert_text_to_dbus_bytes(t) for t in self.text]
        self.group.AddService(_AVAHI_IF_UNSPEC, _AVAHI_PROTO_UNSPEC,
                              dbus.UInt32(_AVAHI_PUBLISHFLAGS_NONE),
                              self.name, self.stype, self.domain, self.host,
                              dbus.UInt16(self.port), text)

        self.group.Commit()
        return True"
"def parse_options():
    parser = optparse.OptionParser(
        version='Mopidy %s' % versioning.get_version())
    # NOTE Python 2.6: To support Python versions < 2.6.2rc1 we must use
    # bytestrings for the first argument to ``add_option``
    # See https://github.com/mopidy/mopidy/issues/302 for details
    parser.add_option(
        b'--help-gst',
        action='store_true', dest='help_gst',
        help='show GStreamer help options')
    parser.add_option(
        b'-i', '--interactive',
        action='store_true', dest='interactive',
        help='ask interactively for required settings which are missing')
    parser.add_option(
        b'-q', '--quiet',
        action='store_const', const=0, dest='verbosity_level',
        help='less output (warning level)')
    parser.add_option(
        b'-v', '--verbose',
        action='count', default=1, dest='verbosity_level',
        help='more output (debug level)')
    parser.add_option(
        b'--save-debug-log',
        action='store_true', dest='save_debug_log',
        help='save debug log to ""./mopidy.log""')
    parser.add_option(
        b'--list-settings',
        action='callback',
        callback=settings_utils.list_settings_optparse_callback,
        help='list current settings')
    parser.add_option(
        b'--list-deps',
        action='callback', callback=deps.list_deps_optparse_callback,
        help='list dependencies and their versions')
    parser.add_option(
        b'--debug-thread',
        action='store_true', dest='debug_thread',
        help='run background thread that dumps tracebacks on SIGUSR1')
    return parser.parse_args(args=mopidy_args)[0]","def parse_options():
    parser = optparse.OptionParser(
        version='Mopidy %s' % versioning.get_version())
    parser.add_option(
        '--help-gst',
        action='store_true', dest='help_gst',
        help='show GStreamer help options')
    parser.add_option(
        '-i', '--interactive',
        action='store_true', dest='interactive',
        help='ask interactively for required settings which are missing')
    parser.add_option(
        '-q', '--quiet',
        action='store_const', const=0, dest='verbosity_level',
        help='less output (warning level)')
    parser.add_option(
        '-v', '--verbose',
        action='count', default=1, dest='verbosity_level',
        help='more output (debug level)')
    parser.add_option(
        '--save-debug-log',
        action='store_true', dest='save_debug_log',
        help='save debug log to ""./mopidy.log""')
    parser.add_option(
        '--list-settings',
        action='callback',
        callback=settings_utils.list_settings_optparse_callback,
        help='list current settings')
    parser.add_option(
        '--list-deps',
        action='callback', callback=deps.list_deps_optparse_callback,
        help='list dependencies and their versions')
    parser.add_option(
        '--debug-thread',
        action='store_true', dest='debug_thread',
        help='run background thread that dumps tracebacks on SIGUSR1')
    return parser.parse_args(args=mopidy_args)[0]"
"def _convert_mpd_data(data, tracks, music_dir):
    if not data:
        return

    # NOTE: kwargs are explicitly made bytestrings to work on Python
    # 2.6.0/2.6.1. See https://github.com/mopidy/mopidy/issues/302 for details.

    track_kwargs = {}
    album_kwargs = {}
    artist_kwargs = {}
    albumartist_kwargs = {}

    if 'track' in data:
        if '/' in data['track']:
            album_kwargs[b'num_tracks'] = int(data['track'].split('/')[1])
            track_kwargs[b'track_no'] = int(data['track'].split('/')[0])
        else:
            track_kwargs[b'track_no'] = int(data['track'])

    if 'artist' in data:
        artist_kwargs[b'name'] = data['artist']
        albumartist_kwargs[b'name'] = data['artist']

    if 'albumartist' in data:
        albumartist_kwargs[b'name'] = data['albumartist']

    if 'album' in data:
        album_kwargs[b'name'] = data['album']

    if 'title' in data:
        track_kwargs[b'name'] = data['title']

    if 'date' in data:
        track_kwargs[b'date'] = data['date']

    if 'musicbrainz_trackid' in data:
        track_kwargs[b'musicbrainz_id'] = data['musicbrainz_trackid']

    if 'musicbrainz_albumid' in data:
        album_kwargs[b'musicbrainz_id'] = data['musicbrainz_albumid']

    if 'musicbrainz_artistid' in data:
        artist_kwargs[b'musicbrainz_id'] = data['musicbrainz_artistid']

    if 'musicbrainz_albumartistid' in data:
        albumartist_kwargs[b'musicbrainz_id'] = (
            data['musicbrainz_albumartistid'])

    if data['file'][0] == '/':
        path = data['file'][1:]
    else:
        path = data['file']
    path = urllib.unquote(path)

    if artist_kwargs:
        artist = Artist(**artist_kwargs)
        track_kwargs[b'artists'] = [artist]

    if albumartist_kwargs:
        albumartist = Artist(**albumartist_kwargs)
        album_kwargs[b'artists'] = [albumartist]

    if album_kwargs:
        album = Album(**album_kwargs)
        track_kwargs[b'album'] = album

    track_kwargs[b'uri'] = path_to_uri(music_dir, path)
    track_kwargs[b'length'] = int(data.get('time', 0)) * 1000

    track = Track(**track_kwargs)
    tracks.add(track)","def _convert_mpd_data(data, tracks, music_dir):
    if not data:
        return

    track_kwargs = {}
    album_kwargs = {}
    artist_kwargs = {}
    albumartist_kwargs = {}

    if 'track' in data:
        if '/' in data['track']:
            album_kwargs['num_tracks'] = int(data['track'].split('/')[1])
            track_kwargs['track_no'] = int(data['track'].split('/')[0])
        else:
            track_kwargs['track_no'] = int(data['track'])

    if 'artist' in data:
        artist_kwargs['name'] = data['artist']
        albumartist_kwargs['name'] = data['artist']

    if 'albumartist' in data:
        albumartist_kwargs['name'] = data['albumartist']

    if 'album' in data:
        album_kwargs['name'] = data['album']

    if 'title' in data:
        track_kwargs['name'] = data['title']

    if 'date' in data:
        track_kwargs['date'] = data['date']

    if 'musicbrainz_trackid' in data:
        track_kwargs['musicbrainz_id'] = data['musicbrainz_trackid']

    if 'musicbrainz_albumid' in data:
        album_kwargs['musicbrainz_id'] = data['musicbrainz_albumid']

    if 'musicbrainz_artistid' in data:
        artist_kwargs['musicbrainz_id'] = data['musicbrainz_artistid']

    if 'musicbrainz_albumartistid' in data:
        albumartist_kwargs['musicbrainz_id'] = (
            data['musicbrainz_albumartistid'])

    if data['file'][0] == '/':
        path = data['file'][1:]
    else:
        path = data['file']
    path = urllib.unquote(path)

    if artist_kwargs:
        artist = Artist(**artist_kwargs)
        track_kwargs['artists'] = [artist]

    if albumartist_kwargs:
        albumartist = Artist(**albumartist_kwargs)
        album_kwargs['artists'] = [albumartist]

    if album_kwargs:
        album = Album(**album_kwargs)
        track_kwargs['album'] = album

    track_kwargs['uri'] = path_to_uri(music_dir, path)
    track_kwargs['length'] = int(data.get('time', 0)) * 1000

    track = Track(**track_kwargs)
    tracks.add(track)"
"def translator(data):
    albumartist_kwargs = {}
    album_kwargs = {}
    artist_kwargs = {}
    track_kwargs = {}

    # NOTE: kwargs are explicitly made bytestrings to work on Python
    # 2.6.0/2.6.1. See https://github.com/mopidy/mopidy/issues/302 for
    # details.

    def _retrieve(source_key, target_key, target):
        if source_key in data:
            target[str(target_key)] = data[source_key]

    _retrieve(gst.TAG_ALBUM, 'name', album_kwargs)
    _retrieve(gst.TAG_TRACK_COUNT, 'num_tracks', album_kwargs)
    _retrieve(gst.TAG_ARTIST, 'name', artist_kwargs)

    if gst.TAG_DATE in data and data[gst.TAG_DATE]:
        date = data[gst.TAG_DATE]
        try:
            date = datetime.date(date.year, date.month, date.day)
        except ValueError:
            pass  # Ignore invalid dates
        else:
            track_kwargs[b'date'] = date.isoformat()

    _retrieve(gst.TAG_TITLE, 'name', track_kwargs)
    _retrieve(gst.TAG_TRACK_NUMBER, 'track_no', track_kwargs)

    # Following keys don't seem to have TAG_* constant.
    _retrieve('album-artist', 'name', albumartist_kwargs)
    _retrieve('musicbrainz-trackid', 'musicbrainz_id', track_kwargs)
    _retrieve('musicbrainz-artistid', 'musicbrainz_id', artist_kwargs)
    _retrieve('musicbrainz-albumid', 'musicbrainz_id', album_kwargs)
    _retrieve(
        'musicbrainz-albumartistid', 'musicbrainz_id', albumartist_kwargs)

    if albumartist_kwargs:
        album_kwargs[b'artists'] = [Artist(**albumartist_kwargs)]

    track_kwargs['uri'] = data['uri']
    track_kwargs['length'] = data[gst.TAG_DURATION]
    track_kwargs['album'] = Album(**album_kwargs)
    track_kwargs['artists'] = [Artist(**artist_kwargs)]

    return Track(**track_kwargs)","def translator(data):
    albumartist_kwargs = {}
    album_kwargs = {}
    artist_kwargs = {}
    track_kwargs = {}

    def _retrieve(source_key, target_key, target):
        if source_key in data:
            target[target_key] = data[source_key]

    _retrieve(gst.TAG_ALBUM, 'name', album_kwargs)
    _retrieve(gst.TAG_TRACK_COUNT, 'num_tracks', album_kwargs)
    _retrieve(gst.TAG_ARTIST, 'name', artist_kwargs)

    if gst.TAG_DATE in data and data[gst.TAG_DATE]:
        date = data[gst.TAG_DATE]
        try:
            date = datetime.date(date.year, date.month, date.day)
        except ValueError:
            pass  # Ignore invalid dates
        else:
            track_kwargs['date'] = date.isoformat()

    _retrieve(gst.TAG_TITLE, 'name', track_kwargs)
    _retrieve(gst.TAG_TRACK_NUMBER, 'track_no', track_kwargs)

    # Following keys don't seem to have TAG_* constant.
    _retrieve('album-artist', 'name', albumartist_kwargs)
    _retrieve('musicbrainz-trackid', 'musicbrainz_id', track_kwargs)
    _retrieve('musicbrainz-artistid', 'musicbrainz_id', artist_kwargs)
    _retrieve('musicbrainz-albumid', 'musicbrainz_id', album_kwargs)
    _retrieve(
        'musicbrainz-albumartistid', 'musicbrainz_id', albumartist_kwargs)

    if albumartist_kwargs:
        album_kwargs['artists'] = [Artist(**albumartist_kwargs)]

    track_kwargs['uri'] = data['uri']
    track_kwargs['length'] = data[gst.TAG_DURATION]
    track_kwargs['album'] = Album(**album_kwargs)
    track_kwargs['artists'] = [Artist(**artist_kwargs)]

    return Track(**track_kwargs)"
"    def _retrieve(source_key, target_key, target):
        if source_key in data:
            target[str(target_key)] = data[source_key]","    def _retrieve(source_key, target_key, target):
        if source_key in data:
            target[target_key] = data[source_key]"
"def translator(data):
    albumartist_kwargs = {}
    album_kwargs = {}
    artist_kwargs = {}
    track_kwargs = {}

    # NOTE: kwargs are explicitly made bytestrings to work on Python
    # 2.6.0/2.6.1. See https://github.com/mopidy/mopidy/issues/302 for
    # details.

    def _retrieve(source_key, target_key, target):
        if source_key in data:
            target[str(target_key)] = data[source_key]

    _retrieve(gst.TAG_ALBUM, 'name', album_kwargs)
    _retrieve(gst.TAG_TRACK_COUNT, 'num_tracks', album_kwargs)
    _retrieve(gst.TAG_ARTIST, 'name', artist_kwargs)

    if gst.TAG_DATE in data and data[gst.TAG_DATE]:
        date = data[gst.TAG_DATE]
        try:
            date = datetime.date(date.year, date.month, date.day)
        except ValueError:
            pass  # Ignore invalid dates
        else:
            track_kwargs[b'date'] = date.isoformat()

    _retrieve(gst.TAG_TITLE, 'name', track_kwargs)
    _retrieve(gst.TAG_TRACK_NUMBER, 'track_no', track_kwargs)

    # Following keys don't seem to have TAG_* constant.
    _retrieve('album-artist', 'name', albumartist_kwargs)
    _retrieve('musicbrainz-trackid', 'musicbrainz_id', track_kwargs)
    _retrieve('musicbrainz-artistid', 'musicbrainz_id', artist_kwargs)
    _retrieve('musicbrainz-albumid', 'musicbrainz_id', album_kwargs)
    _retrieve(
        'musicbrainz-albumartistid', 'musicbrainz_id', albumartist_kwargs)

    if albumartist_kwargs:
        album_kwargs[b'artists'] = [Artist(**albumartist_kwargs)]

    track_kwargs[b'uri'] = data['uri']
    track_kwargs[b'length'] = data[gst.TAG_DURATION]
    track_kwargs[b'album'] = Album(**album_kwargs)
    track_kwargs[b'artists'] = [Artist(**artist_kwargs)]

    return Track(**track_kwargs)","def translator(data):
    albumartist_kwargs = {}
    album_kwargs = {}
    artist_kwargs = {}
    track_kwargs = {}

    # NOTE: kwargs are explicitly made bytestrings to work on Python
    # 2.6.0/2.6.1. See https://github.com/mopidy/mopidy/issues/302 for
    # details.

    def _retrieve(source_key, target_key, target):
        if source_key in data:
            target[str(target_key)] = data[source_key]

    _retrieve(gst.TAG_ALBUM, 'name', album_kwargs)
    _retrieve(gst.TAG_TRACK_COUNT, 'num_tracks', album_kwargs)
    _retrieve(gst.TAG_ARTIST, 'name', artist_kwargs)

    if gst.TAG_DATE in data and data[gst.TAG_DATE]:
        date = data[gst.TAG_DATE]
        try:
            date = datetime.date(date.year, date.month, date.day)
        except ValueError:
            pass  # Ignore invalid dates
        else:
            track_kwargs[b'date'] = date.isoformat()

    _retrieve(gst.TAG_TITLE, 'name', track_kwargs)
    _retrieve(gst.TAG_TRACK_NUMBER, 'track_no', track_kwargs)

    # Following keys don't seem to have TAG_* constant.
    _retrieve('album-artist', 'name', albumartist_kwargs)
    _retrieve('musicbrainz-trackid', 'musicbrainz_id', track_kwargs)
    _retrieve('musicbrainz-artistid', 'musicbrainz_id', artist_kwargs)
    _retrieve('musicbrainz-albumid', 'musicbrainz_id', album_kwargs)
    _retrieve(
        'musicbrainz-albumartistid', 'musicbrainz_id', albumartist_kwargs)

    if albumartist_kwargs:
        album_kwargs[b'artists'] = [Artist(**albumartist_kwargs)]

    track_kwargs['uri'] = data['uri']
    track_kwargs['length'] = data[gst.TAG_DURATION]
    track_kwargs['album'] = Album(**album_kwargs)
    track_kwargs['artists'] = [Artist(**artist_kwargs)]

    return Track(**track_kwargs)"
"def parse_options():
    parser = optparse.OptionParser(
        version='Mopidy %s' % versioning.get_version())
    # NOTE Python 2.6: To support Python versions < 2.6.2rc1 we must use
    # bytestrings for the first argument to ``add_option``
    # See https://github.com/mopidy/mopidy/issues/302 for details
    parser.add_option(
        b'-q', '--quiet',
        action='store_const', const=0, dest='verbosity_level',
        help='less output (warning level)')
    parser.add_option(
        b'-v', '--verbose',
        action='count', default=1, dest='verbosity_level',
        help='more output (debug level)')
    return parser.parse_args(args=mopidy_args)[0]","def parse_options():
    parser = optparse.OptionParser(
        version='Mopidy %s' % versioning.get_version())
    parser.add_option(
        '-q', '--quiet',
        action='store_const', const=0, dest='verbosity_level',
        help='less output (warning level)')
    parser.add_option(
        '-v', '--verbose',
        action='count', default=1, dest='verbosity_level',
        help='more output (debug level)')
    return parser.parse_args(args=mopidy_args)[0]"
"    def __init__(self, core):
        super(MpdFrontend, self).__init__()
        hostname = network.format_hostname(settings.MPD_SERVER_HOSTNAME)
        port = settings.MPD_SERVER_PORT

        # NOTE: dict key must be bytestring to work on Python < 2.6.5
        # See https://github.com/mopidy/mopidy/issues/302 for details
        try:
            network.Server(
                hostname, port,
                protocol=session.MpdSession, protocol_kwargs={b'core': core},
                max_connections=settings.MPD_SERVER_MAX_CONNECTIONS,
                timeout=settings.MPD_SERVER_CONNECTION_TIMEOUT)
        except IOError as error:
            logger.error(
                'MPD server startup failed: %s',
                encoding.locale_decode(error))
            sys.exit(1)

        logger.info('MPD server running at [%s]:%s', hostname, port)","    def __init__(self, core):
        super(MpdFrontend, self).__init__()
        hostname = network.format_hostname(settings.MPD_SERVER_HOSTNAME)
        port = settings.MPD_SERVER_PORT

        try:
            network.Server(
                hostname, port,
                protocol=session.MpdSession, protocol_kwargs={'core': core},
                max_connections=settings.MPD_SERVER_MAX_CONNECTIONS,
                timeout=settings.MPD_SERVER_CONNECTION_TIMEOUT)
        except IOError as error:
            logger.error(
                'MPD server startup failed: %s',
                encoding.locale_decode(error))
            sys.exit(1)

        logger.info('MPD server running at [%s]:%s', hostname, port)"
"def handle_request(pattern, auth_required=True):
    """"""
    Decorator for connecting command handlers to command requests.

    If you use named groups in the pattern, the decorated method will get the
    groups as keyword arguments. If the group is optional, remember to give the
    argument a default value.

    For example, if the command is ``do that thing`` the ``what`` argument will
    be ``this thing``::

        @handle_request('^do (?P<what>.+)$')
        def do(what):
            ...

    :param pattern: regexp pattern for matching commands
    :type pattern: string
    """"""
    def decorator(func):
        match = re.search('([a-z_]+)', pattern)
        if match is not None:
            mpd_commands.add(
                MpdCommand(name=match.group(), auth_required=auth_required))
        # NOTE: Make pattern a bytestring to get bytestring keys in the dict
        # returned from matches.groupdict(), which is again used as a **kwargs
        # dict. This is needed to work on Python < 2.6.5. See
        # https://github.com/mopidy/mopidy/issues/302 for details.
        bytestring_pattern = pattern.encode('utf-8')
        compiled_pattern = re.compile(bytestring_pattern, flags=re.UNICODE)
        if compiled_pattern in request_handlers:
            raise ValueError('Tried to redefine handler for %s with %s' % (
                pattern, func))
        request_handlers[compiled_pattern] = func
        func.__doc__ = '    - *Pattern:* ``%s``\\n\\n%s' % (
            pattern, func.__doc__ or '')
        return func
    return decorator","def handle_request(pattern, auth_required=True):
    """"""
    Decorator for connecting command handlers to command requests.

    If you use named groups in the pattern, the decorated method will get the
    groups as keyword arguments. If the group is optional, remember to give the
    argument a default value.

    For example, if the command is ``do that thing`` the ``what`` argument will
    be ``this thing``::

        @handle_request('^do (?P<what>.+)$')
        def do(what):
            ...

    :param pattern: regexp pattern for matching commands
    :type pattern: string
    """"""
    def decorator(func):
        match = re.search('([a-z_]+)', pattern)
        if match is not None:
            mpd_commands.add(
                MpdCommand(name=match.group(), auth_required=auth_required))
        compiled_pattern = re.compile(pattern, flags=re.UNICODE)
        if compiled_pattern in request_handlers:
            raise ValueError('Tried to redefine handler for %s with %s' % (
                pattern, func))
        request_handlers[compiled_pattern] = func
        func.__doc__ = '    - *Pattern:* ``%s``\\n\\n%s' % (
            pattern, func.__doc__ or '')
        return func
    return decorator"
"    def decorator(func):
        match = re.search('([a-z_]+)', pattern)
        if match is not None:
            mpd_commands.add(
                MpdCommand(name=match.group(), auth_required=auth_required))
        # NOTE: Make pattern a bytestring to get bytestring keys in the dict
        # returned from matches.groupdict(), which is again used as a **kwargs
        # dict. This is needed to work on Python < 2.6.5. See
        # https://github.com/mopidy/mopidy/issues/302 for details.
        bytestring_pattern = pattern.encode('utf-8')
        compiled_pattern = re.compile(bytestring_pattern, flags=re.UNICODE)
        if compiled_pattern in request_handlers:
            raise ValueError('Tried to redefine handler for %s with %s' % (
                pattern, func))
        request_handlers[compiled_pattern] = func
        func.__doc__ = '    - *Pattern:* ``%s``\\n\\n%s' % (
            pattern, func.__doc__ or '')
        return func","    def decorator(func):
        match = re.search('([a-z_]+)', pattern)
        if match is not None:
            mpd_commands.add(
                MpdCommand(name=match.group(), auth_required=auth_required))
        compiled_pattern = re.compile(pattern, flags=re.UNICODE)
        if compiled_pattern in request_handlers:
            raise ValueError('Tried to redefine handler for %s with %s' % (
                pattern, func))
        request_handlers[compiled_pattern] = func
        func.__doc__ = '    - *Pattern:* ``%s``\\n\\n%s' % (
            pattern, func.__doc__ or '')
        return func"
"def parse_options():
    parser = optparse.OptionParser(
        version='Mopidy %s' % versioning.get_version())
    # NOTE First argument to add_option must be bytestrings on Python < 2.6.2
    # See https://github.com/mopidy/mopidy/issues/302 for details
    parser.add_option(
        b'--help-gst',
        action='store_true', dest='help_gst',
        help='show GStreamer help options')
    parser.add_option(
        b'-i', '--interactive',
        action='store_true', dest='interactive',
        help='ask interactively for required settings which are missing')
    parser.add_option(
        b'-q', '--quiet',
        action='store_const', const=0, dest='verbosity_level',
        help='less output (warning level)')
    parser.add_option(
        b'-v', '--verbose',
        action='count', default=1, dest='verbosity_level',
        help='more output (debug level)')
    parser.add_option(
        b'--save-debug-log',
        action='store_true', dest='save_debug_log',
        help='save debug log to ""./mopidy.log""')
    parser.add_option(
        b'--list-settings',
        action='callback',
        callback=settings_utils.list_settings_optparse_callback,
        help='list current settings')
    parser.add_option(
        b'--list-deps',
        action='callback', callback=deps.list_deps_optparse_callback,
        help='list dependencies and their versions')
    parser.add_option(
        b'--debug-thread',
        action='store_true', dest='debug_thread',
        help='run background thread that dumps tracebacks on SIGUSR1')
    return parser.parse_args(args=mopidy_args)[0]","def parse_options():
    parser = optparse.OptionParser(
        version='Mopidy %s' % versioning.get_version())
    # NOTE Python 2.6: To support Python versions < 2.6.2rc1 we must use
    # bytestrings for the first argument to ``add_option``
    # See https://github.com/mopidy/mopidy/issues/302 for details
    parser.add_option(
        b'--help-gst',
        action='store_true', dest='help_gst',
        help='show GStreamer help options')
    parser.add_option(
        b'-i', '--interactive',
        action='store_true', dest='interactive',
        help='ask interactively for required settings which are missing')
    parser.add_option(
        b'-q', '--quiet',
        action='store_const', const=0, dest='verbosity_level',
        help='less output (warning level)')
    parser.add_option(
        b'-v', '--verbose',
        action='count', default=1, dest='verbosity_level',
        help='more output (debug level)')
    parser.add_option(
        b'--save-debug-log',
        action='store_true', dest='save_debug_log',
        help='save debug log to ""./mopidy.log""')
    parser.add_option(
        b'--list-settings',
        action='callback',
        callback=settings_utils.list_settings_optparse_callback,
        help='list current settings')
    parser.add_option(
        b'--list-deps',
        action='callback', callback=deps.list_deps_optparse_callback,
        help='list dependencies and their versions')
    parser.add_option(
        b'--debug-thread',
        action='store_true', dest='debug_thread',
        help='run background thread that dumps tracebacks on SIGUSR1')
    return parser.parse_args(args=mopidy_args)[0]"
"def _convert_mpd_data(data, tracks, music_dir):
    if not data:
        return

    # NOTE kwargs dict keys must be bytestrings to work on Python < 2.6.5
    # See https://github.com/mopidy/mopidy/issues/302 for details.

    track_kwargs = {}
    album_kwargs = {}
    artist_kwargs = {}
    albumartist_kwargs = {}

    if 'track' in data:
        if '/' in data['track']:
            album_kwargs[b'num_tracks'] = int(data['track'].split('/')[1])
            track_kwargs[b'track_no'] = int(data['track'].split('/')[0])
        else:
            track_kwargs[b'track_no'] = int(data['track'])

    if 'artist' in data:
        artist_kwargs[b'name'] = data['artist']
        albumartist_kwargs[b'name'] = data['artist']

    if 'albumartist' in data:
        albumartist_kwargs[b'name'] = data['albumartist']

    if 'album' in data:
        album_kwargs[b'name'] = data['album']

    if 'title' in data:
        track_kwargs[b'name'] = data['title']

    if 'date' in data:
        track_kwargs[b'date'] = data['date']

    if 'musicbrainz_trackid' in data:
        track_kwargs[b'musicbrainz_id'] = data['musicbrainz_trackid']

    if 'musicbrainz_albumid' in data:
        album_kwargs[b'musicbrainz_id'] = data['musicbrainz_albumid']

    if 'musicbrainz_artistid' in data:
        artist_kwargs[b'musicbrainz_id'] = data['musicbrainz_artistid']

    if 'musicbrainz_albumartistid' in data:
        albumartist_kwargs[b'musicbrainz_id'] = (
            data['musicbrainz_albumartistid'])

    if data['file'][0] == '/':
        path = data['file'][1:]
    else:
        path = data['file']
    path = urllib.unquote(path)

    if artist_kwargs:
        artist = Artist(**artist_kwargs)
        track_kwargs[b'artists'] = [artist]

    if albumartist_kwargs:
        albumartist = Artist(**albumartist_kwargs)
        album_kwargs[b'artists'] = [albumartist]

    if album_kwargs:
        album = Album(**album_kwargs)
        track_kwargs[b'album'] = album

    track_kwargs[b'uri'] = path_to_uri(music_dir, path)
    track_kwargs[b'length'] = int(data.get('time', 0)) * 1000

    track = Track(**track_kwargs)
    tracks.add(track)","def _convert_mpd_data(data, tracks, music_dir):
    if not data:
        return

    # NOTE: kwargs are explicitly made bytestrings to work on Python
    # 2.6.0/2.6.1. See https://github.com/mopidy/mopidy/issues/302 for details.

    track_kwargs = {}
    album_kwargs = {}
    artist_kwargs = {}
    albumartist_kwargs = {}

    if 'track' in data:
        if '/' in data['track']:
            album_kwargs[b'num_tracks'] = int(data['track'].split('/')[1])
            track_kwargs[b'track_no'] = int(data['track'].split('/')[0])
        else:
            track_kwargs[b'track_no'] = int(data['track'])

    if 'artist' in data:
        artist_kwargs[b'name'] = data['artist']
        albumartist_kwargs[b'name'] = data['artist']

    if 'albumartist' in data:
        albumartist_kwargs[b'name'] = data['albumartist']

    if 'album' in data:
        album_kwargs[b'name'] = data['album']

    if 'title' in data:
        track_kwargs[b'name'] = data['title']

    if 'date' in data:
        track_kwargs[b'date'] = data['date']

    if 'musicbrainz_trackid' in data:
        track_kwargs[b'musicbrainz_id'] = data['musicbrainz_trackid']

    if 'musicbrainz_albumid' in data:
        album_kwargs[b'musicbrainz_id'] = data['musicbrainz_albumid']

    if 'musicbrainz_artistid' in data:
        artist_kwargs[b'musicbrainz_id'] = data['musicbrainz_artistid']

    if 'musicbrainz_albumartistid' in data:
        albumartist_kwargs[b'musicbrainz_id'] = (
            data['musicbrainz_albumartistid'])

    if data['file'][0] == '/':
        path = data['file'][1:]
    else:
        path = data['file']
    path = urllib.unquote(path)

    if artist_kwargs:
        artist = Artist(**artist_kwargs)
        track_kwargs[b'artists'] = [artist]

    if albumartist_kwargs:
        albumartist = Artist(**albumartist_kwargs)
        album_kwargs[b'artists'] = [albumartist]

    if album_kwargs:
        album = Album(**album_kwargs)
        track_kwargs[b'album'] = album

    track_kwargs[b'uri'] = path_to_uri(music_dir, path)
    track_kwargs[b'length'] = int(data.get('time', 0)) * 1000

    track = Track(**track_kwargs)
    tracks.add(track)"
"    def __init__(self, core):
        super(MpdFrontend, self).__init__()
        hostname = network.format_hostname(settings.MPD_SERVER_HOSTNAME)
        port = settings.MPD_SERVER_PORT

        # NOTE kwargs dict keys must be bytestrings to work on Python < 2.6.5
        # See https://github.com/mopidy/mopidy/issues/302 for details.
        try:
            network.Server(
                hostname, port,
                protocol=session.MpdSession, protocol_kwargs={b'core': core},
                max_connections=settings.MPD_SERVER_MAX_CONNECTIONS,
                timeout=settings.MPD_SERVER_CONNECTION_TIMEOUT)
        except IOError as error:
            logger.error(
                'MPD server startup failed: %s',
                encoding.locale_decode(error))
            sys.exit(1)

        logger.info('MPD server running at [%s]:%s', hostname, port)","    def __init__(self, core):
        super(MpdFrontend, self).__init__()
        hostname = network.format_hostname(settings.MPD_SERVER_HOSTNAME)
        port = settings.MPD_SERVER_PORT

        # NOTE: dict key must be bytestring to work on Python < 2.6.5
        # See https://github.com/mopidy/mopidy/issues/302 for details
        try:
            network.Server(
                hostname, port,
                protocol=session.MpdSession, protocol_kwargs={b'core': core},
                max_connections=settings.MPD_SERVER_MAX_CONNECTIONS,
                timeout=settings.MPD_SERVER_CONNECTION_TIMEOUT)
        except IOError as error:
            logger.error(
                'MPD server startup failed: %s',
                encoding.locale_decode(error))
            sys.exit(1)

        logger.info('MPD server running at [%s]:%s', hostname, port)"
"def handle_request(pattern, auth_required=True):
    """"""
    Decorator for connecting command handlers to command requests.

    If you use named groups in the pattern, the decorated method will get the
    groups as keyword arguments. If the group is optional, remember to give the
    argument a default value.

    For example, if the command is ``do that thing`` the ``what`` argument will
    be ``this thing``::

        @handle_request('^do (?P<what>.+)$')
        def do(what):
            ...

    :param pattern: regexp pattern for matching commands
    :type pattern: string
    """"""
    def decorator(func):
        match = re.search('([a-z_]+)', pattern)
        if match is not None:
            mpd_commands.add(
                MpdCommand(name=match.group(), auth_required=auth_required))
        # NOTE Make pattern a bytestring to get bytestring keys in the dict
        # returned from matches.groupdict(), which is again used as a **kwargs
        # dict. This is needed to work on Python < 2.6.5.
        # See https://github.com/mopidy/mopidy/issues/302 for details.
        bytestring_pattern = pattern.encode('utf-8')
        compiled_pattern = re.compile(bytestring_pattern, flags=re.UNICODE)
        if compiled_pattern in request_handlers:
            raise ValueError('Tried to redefine handler for %s with %s' % (
                pattern, func))
        request_handlers[compiled_pattern] = func
        func.__doc__ = '    - *Pattern:* ``%s``\\n\\n%s' % (
            pattern, func.__doc__ or '')
        return func
    return decorator","def handle_request(pattern, auth_required=True):
    """"""
    Decorator for connecting command handlers to command requests.

    If you use named groups in the pattern, the decorated method will get the
    groups as keyword arguments. If the group is optional, remember to give the
    argument a default value.

    For example, if the command is ``do that thing`` the ``what`` argument will
    be ``this thing``::

        @handle_request('^do (?P<what>.+)$')
        def do(what):
            ...

    :param pattern: regexp pattern for matching commands
    :type pattern: string
    """"""
    def decorator(func):
        match = re.search('([a-z_]+)', pattern)
        if match is not None:
            mpd_commands.add(
                MpdCommand(name=match.group(), auth_required=auth_required))
        # NOTE: Make pattern a bytestring to get bytestring keys in the dict
        # returned from matches.groupdict(), which is again used as a **kwargs
        # dict. This is needed to work on Python < 2.6.5. See
        # https://github.com/mopidy/mopidy/issues/302 for details.
        bytestring_pattern = pattern.encode('utf-8')
        compiled_pattern = re.compile(bytestring_pattern, flags=re.UNICODE)
        if compiled_pattern in request_handlers:
            raise ValueError('Tried to redefine handler for %s with %s' % (
                pattern, func))
        request_handlers[compiled_pattern] = func
        func.__doc__ = '    - *Pattern:* ``%s``\\n\\n%s' % (
            pattern, func.__doc__ or '')
        return func
    return decorator"
"    def decorator(func):
        match = re.search('([a-z_]+)', pattern)
        if match is not None:
            mpd_commands.add(
                MpdCommand(name=match.group(), auth_required=auth_required))
        # NOTE Make pattern a bytestring to get bytestring keys in the dict
        # returned from matches.groupdict(), which is again used as a **kwargs
        # dict. This is needed to work on Python < 2.6.5.
        # See https://github.com/mopidy/mopidy/issues/302 for details.
        bytestring_pattern = pattern.encode('utf-8')
        compiled_pattern = re.compile(bytestring_pattern, flags=re.UNICODE)
        if compiled_pattern in request_handlers:
            raise ValueError('Tried to redefine handler for %s with %s' % (
                pattern, func))
        request_handlers[compiled_pattern] = func
        func.__doc__ = '    - *Pattern:* ``%s``\\n\\n%s' % (
            pattern, func.__doc__ or '')
        return func","    def decorator(func):
        match = re.search('([a-z_]+)', pattern)
        if match is not None:
            mpd_commands.add(
                MpdCommand(name=match.group(), auth_required=auth_required))
        # NOTE: Make pattern a bytestring to get bytestring keys in the dict
        # returned from matches.groupdict(), which is again used as a **kwargs
        # dict. This is needed to work on Python < 2.6.5. See
        # https://github.com/mopidy/mopidy/issues/302 for details.
        bytestring_pattern = pattern.encode('utf-8')
        compiled_pattern = re.compile(bytestring_pattern, flags=re.UNICODE)
        if compiled_pattern in request_handlers:
            raise ValueError('Tried to redefine handler for %s with %s' % (
                pattern, func))
        request_handlers[compiled_pattern] = func
        func.__doc__ = '    - *Pattern:* ``%s``\\n\\n%s' % (
            pattern, func.__doc__ or '')
        return func"
"def query_from_mpd_list_format(field, mpd_query):
    """"""
    Converts an MPD ``list`` query to a Mopidy query.
    """"""
    # NOTE kwargs dict keys must be bytestrings to work on Python < 2.6.5
    # See https://github.com/mopidy/mopidy/issues/302 for details
    if mpd_query is None:
        return {}
    try:
        # shlex does not seem to be friends with unicode objects
        tokens = shlex.split(mpd_query.encode('utf-8'))
    except ValueError as error:
        if str(error) == 'No closing quotation':
            raise MpdArgError('Invalid unquoted character', command='list')
        else:
            raise
    tokens = [t.decode('utf-8') for t in tokens]
    if len(tokens) == 1:
        if field == 'album':
            if not tokens[0]:
                raise ValueError
            return {b'artist': [tokens[0]]}  # See above NOTE
        else:
            raise MpdArgError(
                'should be ""Album"" for 3 arguments', command='list')
    elif len(tokens) % 2 == 0:
        query = {}
        while tokens:
            key = str(tokens[0].lower())  # See above NOTE
            value = tokens[1]
            tokens = tokens[2:]
            if key not in ('artist', 'album', 'date', 'genre'):
                raise MpdArgError('not able to parse args', command='list')
            if not value:
                raise ValueError
            if key in query:
                query[key].append(value)
            else:
                query[key] = [value]
        return query
    else:
        raise MpdArgError('not able to parse args', command='list')","def query_from_mpd_list_format(field, mpd_query):
    """"""
    Converts an MPD ``list`` query to a Mopidy query.
    """"""
    if mpd_query is None:
        return {}
    try:
        # shlex does not seem to be friends with unicode objects
        tokens = shlex.split(mpd_query.encode('utf-8'))
    except ValueError as error:
        if str(error) == 'No closing quotation':
            raise MpdArgError('Invalid unquoted character', command='list')
        else:
            raise
    tokens = [t.decode('utf-8') for t in tokens]
    if len(tokens) == 1:
        if field == 'album':
            if not tokens[0]:
                raise ValueError
            return {'artist': [tokens[0]]}
        else:
            raise MpdArgError(
                'should be ""Album"" for 3 arguments', command='list')
    elif len(tokens) % 2 == 0:
        query = {}
        while tokens:
            key = tokens[0].lower()
            key = str(key)  # Needed for kwargs keys on OS X and Windows
            value = tokens[1]
            tokens = tokens[2:]
            if key not in ('artist', 'album', 'date', 'genre'):
                raise MpdArgError('not able to parse args', command='list')
            if not value:
                raise ValueError
            if key in query:
                query[key].append(value)
            else:
                query[key] = [value]
        return query
    else:
        raise MpdArgError('not able to parse args', command='list')"
"    def copy(self, **values):
        """"""
        Copy the model with ``field`` updated to new value.

        Examples::

            # Returns a track with a new name
            Track(name='foo').copy(name='bar')
            # Return an album with a new number of tracks
            Album(num_tracks=2).copy(num_tracks=5)

        :param values: the model fields to modify
        :type values: dict
        :rtype: new instance of the model being copied
        """"""
        # NOTE kwargs dict keys must be bytestrings to work on Python < 2.6.5
        # See https://github.com/mopidy/mopidy/issues/302 for details
        data = {}
        for key in self.__dict__.keys():
            public_key = key.lstrip('_')
            data[str(public_key)] = values.pop(public_key, self.__dict__[key])
        for key in values.keys():
            if hasattr(self, key):
                data[str(key)] = values.pop(key)
        if values:
            raise TypeError(
                'copy() got an unexpected keyword argument ""%s""' % key)
        return self.__class__(**data)","    def copy(self, **values):
        """"""
        Copy the model with ``field`` updated to new value.

        Examples::

            # Returns a track with a new name
            Track(name='foo').copy(name='bar')
            # Return an album with a new number of tracks
            Album(num_tracks=2).copy(num_tracks=5)

        :param values: the model fields to modify
        :type values: dict
        :rtype: new instance of the model being copied
        """"""
        data = {}
        for key in self.__dict__.keys():
            public_key = key.lstrip('_')
            data[public_key] = values.pop(public_key, self.__dict__[key])
        for key in values.keys():
            if hasattr(self, key):
                data[key] = values.pop(key)
        if values:
            raise TypeError(
                'copy() got an unexpected keyword argument ""%s""' % key)
        return self.__class__(**data)"
"    def __init__(self, *args, **kwargs):
        # NOTE kwargs dict keys must be bytestrings to work on Python < 2.6.5
        # See https://github.com/mopidy/mopidy/issues/302 for details
        self.__dict__[b'artists'] = frozenset(kwargs.pop('artists', []))
        super(Album, self).__init__(*args, **kwargs)","    def __init__(self, *args, **kwargs):
        self.__dict__['artists'] = frozenset(kwargs.pop('artists', []))
        super(Album, self).__init__(*args, **kwargs)"
"    def __init__(self, *args, **kwargs):
        # NOTE kwargs dict keys must be bytestrings to work on Python < 2.6.5
        # See https://github.com/mopidy/mopidy/issues/302 for details
        self.__dict__[b'artists'] = frozenset(kwargs.pop('artists', []))
        super(Track, self).__init__(*args, **kwargs)","    def __init__(self, *args, **kwargs):
        self.__dict__['artists'] = frozenset(kwargs.pop('artists', []))
        super(Track, self).__init__(*args, **kwargs)"
"    def __init__(self, *args, **kwargs):
        # NOTE kwargs dict keys must be bytestrings to work on Python < 2.6.5
        # See https://github.com/mopidy/mopidy/issues/302 for details
        if len(args) == 2 and len(kwargs) == 0:
            kwargs[b'tlid'] = args[0]
            kwargs[b'track'] = args[1]
            args = []
        super(TlTrack, self).__init__(*args, **kwargs)","    def __init__(self, *args, **kwargs):
        if len(args) == 2 and len(kwargs) == 0:
            kwargs['tlid'] = args[0]
            kwargs['track'] = args[1]
            args = []
        super(TlTrack, self).__init__(*args, **kwargs)"
"    def __init__(self, *args, **kwargs):
        # NOTE kwargs dict keys must be bytestrings to work on Python < 2.6.5
        # See https://github.com/mopidy/mopidy/issues/302 for details
        self.__dict__[b'tracks'] = tuple(kwargs.pop('tracks', []))
        super(Playlist, self).__init__(*args, **kwargs)","    def __init__(self, *args, **kwargs):
        self.__dict__['tracks'] = tuple(kwargs.pop('tracks', []))
        super(Playlist, self).__init__(*args, **kwargs)"
"    def __init__(self, *args, **kwargs):
        # NOTE kwargs dict keys must be bytestrings to work on Python < 2.6.5
        # See https://github.com/mopidy/mopidy/issues/302 for details
        self.__dict__[b'tracks'] = tuple(kwargs.pop('tracks', []))
        self.__dict__[b'artists'] = tuple(kwargs.pop('artists', []))
        self.__dict__[b'albums'] = tuple(kwargs.pop('albums', []))
        super(SearchResult, self).__init__(*args, **kwargs)","    def __init__(self, *args, **kwargs):
        self.__dict__['tracks'] = tuple(kwargs.pop('tracks', []))
        self.__dict__['artists'] = tuple(kwargs.pop('artists', []))
        self.__dict__['albums'] = tuple(kwargs.pop('albums', []))
        super(SearchResult, self).__init__(*args, **kwargs)"
"def parse_options():
    parser = optparse.OptionParser(
        version='Mopidy %s' % versioning.get_version())
    # NOTE First argument to add_option must be bytestrings on Python < 2.6.2
    # See https://github.com/mopidy/mopidy/issues/302 for details
    parser.add_option(
        b'-q', '--quiet',
        action='store_const', const=0, dest='verbosity_level',
        help='less output (warning level)')
    parser.add_option(
        b'-v', '--verbose',
        action='count', default=1, dest='verbosity_level',
        help='more output (debug level)')
    return parser.parse_args(args=mopidy_args)[0]","def parse_options():
    parser = optparse.OptionParser(
        version='Mopidy %s' % versioning.get_version())
    # NOTE Python 2.6: To support Python versions < 2.6.2rc1 we must use
    # bytestrings for the first argument to ``add_option``
    # See https://github.com/mopidy/mopidy/issues/302 for details
    parser.add_option(
        b'-q', '--quiet',
        action='store_const', const=0, dest='verbosity_level',
        help='less output (warning level)')
    parser.add_option(
        b'-v', '--verbose',
        action='count', default=1, dest='verbosity_level',
        help='more output (debug level)')
    return parser.parse_args(args=mopidy_args)[0]"
"def translator(data):
    albumartist_kwargs = {}
    album_kwargs = {}
    artist_kwargs = {}
    track_kwargs = {}

    # NOTE kwargs dict keys must be bytestrings to work on Python < 2.6.5
    # See https://github.com/mopidy/mopidy/issues/302 for details.

    def _retrieve(source_key, target_key, target):
        if source_key in data:
            target[str(target_key)] = data[source_key]

    _retrieve(gst.TAG_ALBUM, 'name', album_kwargs)
    _retrieve(gst.TAG_TRACK_COUNT, 'num_tracks', album_kwargs)
    _retrieve(gst.TAG_ARTIST, 'name', artist_kwargs)

    if gst.TAG_DATE in data and data[gst.TAG_DATE]:
        date = data[gst.TAG_DATE]
        try:
            date = datetime.date(date.year, date.month, date.day)
        except ValueError:
            pass  # Ignore invalid dates
        else:
            track_kwargs[b'date'] = date.isoformat()

    _retrieve(gst.TAG_TITLE, 'name', track_kwargs)
    _retrieve(gst.TAG_TRACK_NUMBER, 'track_no', track_kwargs)

    # Following keys don't seem to have TAG_* constant.
    _retrieve('album-artist', 'name', albumartist_kwargs)
    _retrieve('musicbrainz-trackid', 'musicbrainz_id', track_kwargs)
    _retrieve('musicbrainz-artistid', 'musicbrainz_id', artist_kwargs)
    _retrieve('musicbrainz-albumid', 'musicbrainz_id', album_kwargs)
    _retrieve(
        'musicbrainz-albumartistid', 'musicbrainz_id', albumartist_kwargs)

    if albumartist_kwargs:
        album_kwargs[b'artists'] = [Artist(**albumartist_kwargs)]

    track_kwargs[b'uri'] = data['uri']
    track_kwargs[b'length'] = data[gst.TAG_DURATION]
    track_kwargs[b'album'] = Album(**album_kwargs)
    track_kwargs[b'artists'] = [Artist(**artist_kwargs)]

    return Track(**track_kwargs)","def translator(data):
    albumartist_kwargs = {}
    album_kwargs = {}
    artist_kwargs = {}
    track_kwargs = {}

    # NOTE: kwargs are explicitly made bytestrings to work on Python
    # 2.6.0/2.6.1. See https://github.com/mopidy/mopidy/issues/302 for
    # details.

    def _retrieve(source_key, target_key, target):
        if source_key in data:
            target[str(target_key)] = data[source_key]

    _retrieve(gst.TAG_ALBUM, 'name', album_kwargs)
    _retrieve(gst.TAG_TRACK_COUNT, 'num_tracks', album_kwargs)
    _retrieve(gst.TAG_ARTIST, 'name', artist_kwargs)

    if gst.TAG_DATE in data and data[gst.TAG_DATE]:
        date = data[gst.TAG_DATE]
        try:
            date = datetime.date(date.year, date.month, date.day)
        except ValueError:
            pass  # Ignore invalid dates
        else:
            track_kwargs[b'date'] = date.isoformat()

    _retrieve(gst.TAG_TITLE, 'name', track_kwargs)
    _retrieve(gst.TAG_TRACK_NUMBER, 'track_no', track_kwargs)

    # Following keys don't seem to have TAG_* constant.
    _retrieve('album-artist', 'name', albumartist_kwargs)
    _retrieve('musicbrainz-trackid', 'musicbrainz_id', track_kwargs)
    _retrieve('musicbrainz-artistid', 'musicbrainz_id', artist_kwargs)
    _retrieve('musicbrainz-albumid', 'musicbrainz_id', album_kwargs)
    _retrieve(
        'musicbrainz-albumartistid', 'musicbrainz_id', albumartist_kwargs)

    if albumartist_kwargs:
        album_kwargs[b'artists'] = [Artist(**albumartist_kwargs)]

    track_kwargs[b'uri'] = data['uri']
    track_kwargs[b'length'] = data[gst.TAG_DURATION]
    track_kwargs[b'album'] = Album(**album_kwargs)
    track_kwargs[b'artists'] = [Artist(**artist_kwargs)]

    return Track(**track_kwargs)"
"def model_json_decoder(dct):
    """"""
    Automatically deserialize Mopidy models from JSON.

    Usage::

        >>> import json
        >>> json.loads(
        ...     '{""a_track"": {""__model__"": ""Track"", ""name"": ""name""}}',
        ...     object_hook=model_json_decoder)
        {u'a_track': Track(artists=[], name=u'name')}

    """"""
    # NOTE kwargs dict keys must be bytestrings to work on Python < 2.6.5
    # See https://github.com/mopidy/mopidy/issues/302 for details.
    if '__model__' in dct:
        model_name = dct.pop('__model__')
        cls = globals().get(model_name, None)
        if issubclass(cls, ImmutableObject):
            kwargs = {}
            for key, value in dct.items():
                kwargs[str(key)] = value
            return cls(**kwargs)
    return dct","def model_json_decoder(dct):
    """"""
    Automatically deserialize Mopidy models from JSON.

    Usage::

        >>> import json
        >>> json.loads(
        ...     '{""a_track"": {""__model__"": ""Track"", ""name"": ""name""}}',
        ...     object_hook=model_json_decoder)
        {u'a_track': Track(artists=[], name=u'name')}

    """"""
    if '__model__' in dct:
        model_name = dct.pop('__model__')
        cls = globals().get(model_name, None)
        if issubclass(cls, ImmutableObject):
            return cls(**dct)
    return dct"
"def _convert_mpd_data(data, tracks, music_dir):
    if not data:
        return

    track_kwargs = {}
    album_kwargs = {}
    artist_kwargs = {}
    albumartist_kwargs = {}

    if 'track' in data:
        if '/' in data['track']:
            album_kwargs['num_tracks'] = int(data['track'].split('/')[1])
            track_kwargs['track_no'] = int(data['track'].split('/')[0])
        else:
            track_kwargs['track_no'] = int(data['track'])

    if 'artist' in data:
        artist_kwargs['name'] = data['artist']
        albumartist_kwargs['name'] = data['artist']

    if 'albumartist' in data:
        albumartist_kwargs['name'] = data['albumartist']

    if 'album' in data:
        album_kwargs['name'] = data['album']

    if 'title' in data:
        track_kwargs['name'] = data['title']

    if 'date' in data:
        track_kwargs['date'] = data['date']

    if 'musicbrainz_trackid' in data:
        track_kwargs['musicbrainz_id'] = data['musicbrainz_trackid']

    if 'musicbrainz_albumid' in data:
        album_kwargs['musicbrainz_id'] = data['musicbrainz_albumid']

    if 'musicbrainz_artistid' in data:
        artist_kwargs['musicbrainz_id'] = data['musicbrainz_artistid']

    if 'musicbrainz_albumartistid' in data:
        albumartist_kwargs['musicbrainz_id'] = (
            data['musicbrainz_albumartistid'])

    if artist_kwargs:
        artist = Artist(**artist_kwargs)
        track_kwargs['artists'] = [artist]

    if albumartist_kwargs:
        albumartist = Artist(**albumartist_kwargs)
        album_kwargs['artists'] = [albumartist]

    if album_kwargs:
        album = Album(**album_kwargs)
        track_kwargs['album'] = album

    if data['file'][0] == '/':
        path = data['file'][1:]
    else:
        path = data['file']
    path = urllib.unquote(path.encode('utf-8'))

    if isinstance(music_dir, unicode):
        music_dir = music_dir.encode('utf-8')

    # Make sure we only pass bytestrings to path_to_uri to avoid implicit
    # decoding of bytestrings to unicode strings
    track_kwargs['uri'] = path_to_uri(music_dir, path)

    track_kwargs['length'] = int(data.get('time', 0)) * 1000

    track = Track(**track_kwargs)
    tracks.add(track)","def _convert_mpd_data(data, tracks, music_dir):
    if not data:
        return

    # NOTE kwargs dict keys must be bytestrings to work on Python < 2.6.5
    # See https://github.com/mopidy/mopidy/issues/302 for details.

    track_kwargs = {}
    album_kwargs = {}
    artist_kwargs = {}
    albumartist_kwargs = {}

    if 'track' in data:
        if '/' in data['track']:
            album_kwargs[b'num_tracks'] = int(data['track'].split('/')[1])
            track_kwargs[b'track_no'] = int(data['track'].split('/')[0])
        else:
            track_kwargs[b'track_no'] = int(data['track'])

    if 'artist' in data:
        artist_kwargs[b'name'] = data['artist']
        albumartist_kwargs[b'name'] = data['artist']

    if 'albumartist' in data:
        albumartist_kwargs[b'name'] = data['albumartist']

    if 'album' in data:
        album_kwargs[b'name'] = data['album']

    if 'title' in data:
        track_kwargs[b'name'] = data['title']

    if 'date' in data:
        track_kwargs[b'date'] = data['date']

    if 'musicbrainz_trackid' in data:
        track_kwargs[b'musicbrainz_id'] = data['musicbrainz_trackid']

    if 'musicbrainz_albumid' in data:
        album_kwargs[b'musicbrainz_id'] = data['musicbrainz_albumid']

    if 'musicbrainz_artistid' in data:
        artist_kwargs[b'musicbrainz_id'] = data['musicbrainz_artistid']

    if 'musicbrainz_albumartistid' in data:
        albumartist_kwargs[b'musicbrainz_id'] = (
            data['musicbrainz_albumartistid'])

    if artist_kwargs:
        artist = Artist(**artist_kwargs)
        track_kwargs[b'artists'] = [artist]

    if albumartist_kwargs:
        albumartist = Artist(**albumartist_kwargs)
        album_kwargs[b'artists'] = [albumartist]

    if album_kwargs:
        album = Album(**album_kwargs)
        track_kwargs[b'album'] = album

    if data['file'][0] == '/':
        path = data['file'][1:]
    else:
        path = data['file']
    path = urllib.unquote(path.encode('utf-8'))

    if isinstance(music_dir, unicode):
        music_dir = music_dir.encode('utf-8')

    # Make sure we only pass bytestrings to path_to_uri to avoid implicit
    # decoding of bytestrings to unicode strings
    track_kwargs[b'uri'] = path_to_uri(music_dir, path)

    track_kwargs[b'length'] = int(data.get('time', 0)) * 1000

    track = Track(**track_kwargs)
    tracks.add(track)"
"    def __init__(self, config, core):
        super(MpdFrontend, self).__init__()
        hostname = network.format_hostname(config['mpd']['hostname'])
        port = config['mpd']['port']

        try:
            network.Server(
                hostname, port,
                protocol=session.MpdSession,
                protocol_kwargs={
                    'config': config,
                    'core': core,
                },
                max_connections=config['mpd']['max_connections'],
                timeout=config['mpd']['connection_timeout'])
        except IOError as error:
            logger.error(
                'MPD server startup failed: %s',
                encoding.locale_decode(error))
            sys.exit(1)

        logger.info('MPD server running at [%s]:%s', hostname, port)","    def __init__(self, config, core):
        super(MpdFrontend, self).__init__()
        hostname = network.format_hostname(config['mpd']['hostname'])
        port = config['mpd']['port']

        # NOTE kwargs dict keys must be bytestrings to work on Python < 2.6.5
        # See https://github.com/mopidy/mopidy/issues/302 for details.
        try:
            network.Server(
                hostname, port,
                protocol=session.MpdSession,
                protocol_kwargs={
                    b'config': config,
                    b'core': core,
                },
                max_connections=config['mpd']['max_connections'],
                timeout=config['mpd']['connection_timeout'])
        except IOError as error:
            logger.error(
                'MPD server startup failed: %s',
                encoding.locale_decode(error))
            sys.exit(1)

        logger.info('MPD server running at [%s]:%s', hostname, port)"
"def handle_request(pattern, auth_required=True):
    """"""
    Decorator for connecting command handlers to command requests.

    If you use named groups in the pattern, the decorated method will get the
    groups as keyword arguments. If the group is optional, remember to give the
    argument a default value.

    For example, if the command is ``do that thing`` the ``what`` argument will
    be ``this thing``::

        @handle_request('^do (?P<what>.+)$')
        def do(what):
            ...

    :param pattern: regexp pattern for matching commands
    :type pattern: string
    """"""
    def decorator(func):
        match = re.search('([a-z_]+)', pattern)
        if match is not None:
            mpd_commands.add(
                MpdCommand(name=match.group(), auth_required=auth_required))
        compiled_pattern = re.compile(pattern, flags=re.UNICODE)
        if compiled_pattern in request_handlers:
            raise ValueError('Tried to redefine handler for %s with %s' % (
                pattern, func))
        request_handlers[compiled_pattern] = func
        func.__doc__ = '    - *Pattern:* ``%s``\\n\\n%s' % (
            pattern, func.__doc__ or '')
        return func
    return decorator","def handle_request(pattern, auth_required=True):
    """"""
    Decorator for connecting command handlers to command requests.

    If you use named groups in the pattern, the decorated method will get the
    groups as keyword arguments. If the group is optional, remember to give the
    argument a default value.

    For example, if the command is ``do that thing`` the ``what`` argument will
    be ``this thing``::

        @handle_request('^do (?P<what>.+)$')
        def do(what):
            ...

    :param pattern: regexp pattern for matching commands
    :type pattern: string
    """"""
    def decorator(func):
        match = re.search('([a-z_]+)', pattern)
        if match is not None:
            mpd_commands.add(
                MpdCommand(name=match.group(), auth_required=auth_required))
        # NOTE Make pattern a bytestring to get bytestring keys in the dict
        # returned from matches.groupdict(), which is again used as a **kwargs
        # dict. This is needed to work on Python < 2.6.5.
        # See https://github.com/mopidy/mopidy/issues/302 for details.
        bytestring_pattern = pattern.encode('utf-8')
        compiled_pattern = re.compile(bytestring_pattern, flags=re.UNICODE)
        if compiled_pattern in request_handlers:
            raise ValueError('Tried to redefine handler for %s with %s' % (
                pattern, func))
        request_handlers[compiled_pattern] = func
        func.__doc__ = '    - *Pattern:* ``%s``\\n\\n%s' % (
            pattern, func.__doc__ or '')
        return func
    return decorator"
"    def decorator(func):
        match = re.search('([a-z_]+)', pattern)
        if match is not None:
            mpd_commands.add(
                MpdCommand(name=match.group(), auth_required=auth_required))
        compiled_pattern = re.compile(pattern, flags=re.UNICODE)
        if compiled_pattern in request_handlers:
            raise ValueError('Tried to redefine handler for %s with %s' % (
                pattern, func))
        request_handlers[compiled_pattern] = func
        func.__doc__ = '    - *Pattern:* ``%s``\\n\\n%s' % (
            pattern, func.__doc__ or '')
        return func","    def decorator(func):
        match = re.search('([a-z_]+)', pattern)
        if match is not None:
            mpd_commands.add(
                MpdCommand(name=match.group(), auth_required=auth_required))
        # NOTE Make pattern a bytestring to get bytestring keys in the dict
        # returned from matches.groupdict(), which is again used as a **kwargs
        # dict. This is needed to work on Python < 2.6.5.
        # See https://github.com/mopidy/mopidy/issues/302 for details.
        bytestring_pattern = pattern.encode('utf-8')
        compiled_pattern = re.compile(bytestring_pattern, flags=re.UNICODE)
        if compiled_pattern in request_handlers:
            raise ValueError('Tried to redefine handler for %s with %s' % (
                pattern, func))
        request_handlers[compiled_pattern] = func
        func.__doc__ = '    - *Pattern:* ``%s``\\n\\n%s' % (
            pattern, func.__doc__ or '')
        return func"
"def query_from_mpd_list_format(field, mpd_query):
    """"""
    Converts an MPD ``list`` query to a Mopidy query.
    """"""
    if mpd_query is None:
        return {}
    try:
        # shlex does not seem to be friends with unicode objects
        tokens = shlex.split(mpd_query.encode('utf-8'))
    except ValueError as error:
        if str(error) == 'No closing quotation':
            raise MpdArgError('Invalid unquoted character', command='list')
        else:
            raise
    tokens = [t.decode('utf-8') for t in tokens]
    if len(tokens) == 1:
        if field == 'album':
            if not tokens[0]:
                raise ValueError
            return {'artist': [tokens[0]]}  # See above NOTE
        else:
            raise MpdArgError(
                'should be ""Album"" for 3 arguments', command='list')
    elif len(tokens) % 2 == 0:
        query = {}
        while tokens:
            key = str(tokens[0].lower())  # See above NOTE
            value = tokens[1]
            tokens = tokens[2:]
            if key not in ('artist', 'album', 'date', 'genre'):
                raise MpdArgError('not able to parse args', command='list')
            if not value:
                raise ValueError
            if key in query:
                query[key].append(value)
            else:
                query[key] = [value]
        return query
    else:
        raise MpdArgError('not able to parse args', command='list')","def query_from_mpd_list_format(field, mpd_query):
    """"""
    Converts an MPD ``list`` query to a Mopidy query.
    """"""
    # NOTE kwargs dict keys must be bytestrings to work on Python < 2.6.5
    # See https://github.com/mopidy/mopidy/issues/302 for details
    if mpd_query is None:
        return {}
    try:
        # shlex does not seem to be friends with unicode objects
        tokens = shlex.split(mpd_query.encode('utf-8'))
    except ValueError as error:
        if str(error) == 'No closing quotation':
            raise MpdArgError('Invalid unquoted character', command='list')
        else:
            raise
    tokens = [t.decode('utf-8') for t in tokens]
    if len(tokens) == 1:
        if field == 'album':
            if not tokens[0]:
                raise ValueError
            return {b'artist': [tokens[0]]}  # See above NOTE
        else:
            raise MpdArgError(
                'should be ""Album"" for 3 arguments', command='list')
    elif len(tokens) % 2 == 0:
        query = {}
        while tokens:
            key = str(tokens[0].lower())  # See above NOTE
            value = tokens[1]
            tokens = tokens[2:]
            if key not in ('artist', 'album', 'date', 'genre'):
                raise MpdArgError('not able to parse args', command='list')
            if not value:
                raise ValueError
            if key in query:
                query[key].append(value)
            else:
                query[key] = [value]
        return query
    else:
        raise MpdArgError('not able to parse args', command='list')"
"    def copy(self, **values):
        """"""
        Copy the model with ``field`` updated to new value.

        Examples::

            # Returns a track with a new name
            Track(name='foo').copy(name='bar')
            # Return an album with a new number of tracks
            Album(num_tracks=2).copy(num_tracks=5)

        :param values: the model fields to modify
        :type values: dict
        :rtype: new instance of the model being copied
        """"""
        data = {}
        for key in self.__dict__.keys():
            public_key = key.lstrip('_')
            data[public_key] = values.pop(public_key, self.__dict__[key])
        for key in values.keys():
            if hasattr(self, key):
                data[key] = values.pop(key)
        if values:
            raise TypeError(
                'copy() got an unexpected keyword argument ""%s""' % key)
        return self.__class__(**data)","    def copy(self, **values):
        """"""
        Copy the model with ``field`` updated to new value.

        Examples::

            # Returns a track with a new name
            Track(name='foo').copy(name='bar')
            # Return an album with a new number of tracks
            Album(num_tracks=2).copy(num_tracks=5)

        :param values: the model fields to modify
        :type values: dict
        :rtype: new instance of the model being copied
        """"""
        # NOTE kwargs dict keys must be bytestrings to work on Python < 2.6.5
        # See https://github.com/mopidy/mopidy/issues/302 for details
        data = {}
        for key in self.__dict__.keys():
            public_key = key.lstrip('_')
            data[str(public_key)] = values.pop(public_key, self.__dict__[key])
        for key in values.keys():
            if hasattr(self, key):
                data[str(key)] = values.pop(key)
        if values:
            raise TypeError(
                'copy() got an unexpected keyword argument ""%s""' % key)
        return self.__class__(**data)"
"def model_json_decoder(dct):
    """"""
    Automatically deserialize Mopidy models from JSON.

    Usage::

        >>> import json
        >>> json.loads(
        ...     '{""a_track"": {""__model__"": ""Track"", ""name"": ""name""}}',
        ...     object_hook=model_json_decoder)
        {u'a_track': Track(artists=[], name=u'name')}

    """"""
    if '__model__' in dct:
        model_name = dct.pop('__model__')
        cls = globals().get(model_name, None)
        if issubclass(cls, ImmutableObject):
            kwargs = {}
            for key, value in dct.items():
                kwargs[key] = value
            return cls(**kwargs)
    return dct","def model_json_decoder(dct):
    """"""
    Automatically deserialize Mopidy models from JSON.

    Usage::

        >>> import json
        >>> json.loads(
        ...     '{""a_track"": {""__model__"": ""Track"", ""name"": ""name""}}',
        ...     object_hook=model_json_decoder)
        {u'a_track': Track(artists=[], name=u'name')}

    """"""
    # NOTE kwargs dict keys must be bytestrings to work on Python < 2.6.5
    # See https://github.com/mopidy/mopidy/issues/302 for details.
    if '__model__' in dct:
        model_name = dct.pop('__model__')
        cls = globals().get(model_name, None)
        if issubclass(cls, ImmutableObject):
            kwargs = {}
            for key, value in dct.items():
                kwargs[str(key)] = value
            return cls(**kwargs)
    return dct"
"    def __init__(self, *args, **kwargs):
        self.__dict__['artists'] = frozenset(kwargs.pop('artists', []))
        self.__dict__['images'] = frozenset(kwargs.pop('images', []))
        super(Album, self).__init__(*args, **kwargs)","    def __init__(self, *args, **kwargs):
        # NOTE kwargs dict keys must be bytestrings to work on Python < 2.6.5
        # See https://github.com/mopidy/mopidy/issues/302 for details
        self.__dict__[b'artists'] = frozenset(kwargs.pop('artists', []))
        self.__dict__[b'images'] = frozenset(kwargs.pop('images', []))
        super(Album, self).__init__(*args, **kwargs)"
"    def __init__(self, *args, **kwargs):
        self.__dict__['artists'] = frozenset(kwargs.pop('artists', []))
        super(Track, self).__init__(*args, **kwargs)","    def __init__(self, *args, **kwargs):
        # NOTE kwargs dict keys must be bytestrings to work on Python < 2.6.5
        # See https://github.com/mopidy/mopidy/issues/302 for details
        self.__dict__[b'artists'] = frozenset(kwargs.pop('artists', []))
        super(Track, self).__init__(*args, **kwargs)"
"    def __init__(self, *args, **kwargs):
        if len(args) == 2 and len(kwargs) == 0:
            kwargs['tlid'] = args[0]
            kwargs['track'] = args[1]
            args = []
        super(TlTrack, self).__init__(*args, **kwargs)","    def __init__(self, *args, **kwargs):
        # NOTE kwargs dict keys must be bytestrings to work on Python < 2.6.5
        # See https://github.com/mopidy/mopidy/issues/302 for details
        if len(args) == 2 and len(kwargs) == 0:
            kwargs[b'tlid'] = args[0]
            kwargs[b'track'] = args[1]
            args = []
        super(TlTrack, self).__init__(*args, **kwargs)"
"    def __init__(self, *args, **kwargs):
        self.__dict__['tracks'] = tuple(kwargs.pop('tracks', []))
        super(Playlist, self).__init__(*args, **kwargs)","    def __init__(self, *args, **kwargs):
        # NOTE kwargs dict keys must be bytestrings to work on Python < 2.6.5
        # See https://github.com/mopidy/mopidy/issues/302 for details
        self.__dict__[b'tracks'] = tuple(kwargs.pop('tracks', []))
        super(Playlist, self).__init__(*args, **kwargs)"
"    def __init__(self, *args, **kwargs):
        self.__dict__['tracks'] = tuple(kwargs.pop('tracks', []))
        self.__dict__['artists'] = tuple(kwargs.pop('artists', []))
        self.__dict__['albums'] = tuple(kwargs.pop('albums', []))
        super(SearchResult, self).__init__(*args, **kwargs)","    def __init__(self, *args, **kwargs):
        # NOTE kwargs dict keys must be bytestrings to work on Python < 2.6.5
        # See https://github.com/mopidy/mopidy/issues/302 for details
        self.__dict__[b'tracks'] = tuple(kwargs.pop('tracks', []))
        self.__dict__[b'artists'] = tuple(kwargs.pop('artists', []))
        self.__dict__[b'albums'] = tuple(kwargs.pop('albums', []))
        super(SearchResult, self).__init__(*args, **kwargs)"
"def parse_options():
    parser = optparse.OptionParser(
        version='Mopidy %s' % versioning.get_version())
    parser.add_option(
        '-q', '--quiet',
        action='store_const', const=0, dest='verbosity_level',
        help='less output (warning level)')
    parser.add_option(
        '-v', '--verbose',
        action='count', default=1, dest='verbosity_level',
        help='more output (debug level)')
    return parser.parse_args(args=mopidy_args)[0]","def parse_options():
    parser = optparse.OptionParser(
        version='Mopidy %s' % versioning.get_version())
    # NOTE First argument to add_option must be bytestrings on Python < 2.6.2
    # See https://github.com/mopidy/mopidy/issues/302 for details
    parser.add_option(
        b'-q', '--quiet',
        action='store_const', const=0, dest='verbosity_level',
        help='less output (warning level)')
    parser.add_option(
        b'-v', '--verbose',
        action='count', default=1, dest='verbosity_level',
        help='more output (debug level)')
    return parser.parse_args(args=mopidy_args)[0]"
"def translator(data):
    albumartist_kwargs = {}
    album_kwargs = {}
    artist_kwargs = {}
    track_kwargs = {}

    def _retrieve(source_key, target_key, target):
        if source_key in data:
            target[target_key] = data[source_key]

    _retrieve(gst.TAG_ALBUM, 'name', album_kwargs)
    _retrieve(gst.TAG_TRACK_COUNT, 'num_tracks', album_kwargs)
    _retrieve(gst.TAG_ARTIST, 'name', artist_kwargs)

    if gst.TAG_DATE in data and data[gst.TAG_DATE]:
        date = data[gst.TAG_DATE]
        try:
            date = datetime.date(date.year, date.month, date.day)
        except ValueError:
            pass  # Ignore invalid dates
        else:
            track_kwargs['date'] = date.isoformat()

    _retrieve(gst.TAG_TITLE, 'name', track_kwargs)
    _retrieve(gst.TAG_TRACK_NUMBER, 'track_no', track_kwargs)

    # Following keys don't seem to have TAG_* constant.
    _retrieve('album-artist', 'name', albumartist_kwargs)
    _retrieve('musicbrainz-trackid', 'musicbrainz_id', track_kwargs)
    _retrieve('musicbrainz-artistid', 'musicbrainz_id', artist_kwargs)
    _retrieve('musicbrainz-albumid', 'musicbrainz_id', album_kwargs)
    _retrieve(
        'musicbrainz-albumartistid', 'musicbrainz_id', albumartist_kwargs)

    if albumartist_kwargs:
        album_kwargs['artists'] = [Artist(**albumartist_kwargs)]

    track_kwargs['uri'] = data['uri']
    track_kwargs['length'] = data[gst.TAG_DURATION]
    track_kwargs['album'] = Album(**album_kwargs)
    track_kwargs['artists'] = [Artist(**artist_kwargs)]

    return Track(**track_kwargs)","def translator(data):
    albumartist_kwargs = {}
    album_kwargs = {}
    artist_kwargs = {}
    track_kwargs = {}

    # NOTE kwargs dict keys must be bytestrings to work on Python < 2.6.5
    # See https://github.com/mopidy/mopidy/issues/302 for details.

    def _retrieve(source_key, target_key, target):
        if source_key in data:
            target[str(target_key)] = data[source_key]

    _retrieve(gst.TAG_ALBUM, 'name', album_kwargs)
    _retrieve(gst.TAG_TRACK_COUNT, 'num_tracks', album_kwargs)
    _retrieve(gst.TAG_ARTIST, 'name', artist_kwargs)

    if gst.TAG_DATE in data and data[gst.TAG_DATE]:
        date = data[gst.TAG_DATE]
        try:
            date = datetime.date(date.year, date.month, date.day)
        except ValueError:
            pass  # Ignore invalid dates
        else:
            track_kwargs[b'date'] = date.isoformat()

    _retrieve(gst.TAG_TITLE, 'name', track_kwargs)
    _retrieve(gst.TAG_TRACK_NUMBER, 'track_no', track_kwargs)

    # Following keys don't seem to have TAG_* constant.
    _retrieve('album-artist', 'name', albumartist_kwargs)
    _retrieve('musicbrainz-trackid', 'musicbrainz_id', track_kwargs)
    _retrieve('musicbrainz-artistid', 'musicbrainz_id', artist_kwargs)
    _retrieve('musicbrainz-albumid', 'musicbrainz_id', album_kwargs)
    _retrieve(
        'musicbrainz-albumartistid', 'musicbrainz_id', albumartist_kwargs)

    if albumartist_kwargs:
        album_kwargs[b'artists'] = [Artist(**albumartist_kwargs)]

    track_kwargs[b'uri'] = data['uri']
    track_kwargs[b'length'] = data[gst.TAG_DURATION]
    track_kwargs[b'album'] = Album(**album_kwargs)
    track_kwargs[b'artists'] = [Artist(**artist_kwargs)]

    return Track(**track_kwargs)"
"    def _retrieve(source_key, target_key, target):
        if source_key in data:
            target[target_key] = data[source_key]","    def _retrieve(source_key, target_key, target):
        if source_key in data:
            target[str(target_key)] = data[source_key]"
"    def _select_mixer_track(self, mixer, track_label):
        # Ignore tracks without volumes, then look for track with
        # label == settings.MIXER_TRACK, otherwise fallback to first usable
        # track hoping the mixer gave them to us in a sensible order.

        usable_tracks = []
        for track in mixer.list_tracks():
            if not mixer.get_volume(track):
                continue

            if track_label and track.label == track_label:
                return track
            elif track.flags & (gst.interfaces.MIXER_TRACK_MASTER |
                                gst.interfaces.MIXER_TRACK_OUTPUT):
                usable_tracks.append(track)

        if usable_tracks:
            return usable_tracks[0]","    def _select_mixer_track(self, mixer, track_label):
        # Look for track with label == MIXER_TRACK, otherwise fallback to
        # master track which is also an output.
        for track in mixer.list_tracks():
            if track_label:
                if track.label == track_label:
                    return track
            elif track.flags & (gst.interfaces.MIXER_TRACK_MASTER |
                                gst.interfaces.MIXER_TRACK_OUTPUT):
                return track"
"def parse_m3u(file_path, music_folder):
    """"""
    Convert M3U file list of uris

    Example M3U data::

        # This is a comment
        Alternative\\Band - Song.mp3
        Classical\\Other Band - New Song.mp3
        Stuff.mp3
        D:\\More Music\\Foo.mp3
        http://www.example.com:8000/Listen.pls
        http://www.example.com/~user/Mine.mp3

    - Relative paths of songs should be with respect to location of M3U.
    - Paths are normaly platform specific.
    - Lines starting with # should be ignored.
    - m3u files are latin-1.
    - This function does not bother with Extended M3U directives.
    """"""

    uris = []
    try:
        with open(file_path) as m3u:
            contents = m3u.readlines()
    except IOError as error:
        logger.error('Couldn\\'t open m3u: %s', locale_decode(error))
        return uris

    for line in contents:
        line = line.strip().decode('latin1')

        if line.startswith('#'):
            continue

        # FIXME what about other URI types?
        if line.startswith('file://'):
            uris.append(line)
        else:
            path = path_to_uri(music_folder, line)
            uris.append(path)

    return uris","def parse_m3u(file_path):
    """"""
    Convert M3U file list of uris

    Example M3U data::

        # This is a comment
        Alternative\\Band - Song.mp3
        Classical\\Other Band - New Song.mp3
        Stuff.mp3
        D:\\More Music\\Foo.mp3
        http://www.example.com:8000/Listen.pls
        http://www.example.com/~user/Mine.mp3

    - Relative paths of songs should be with respect to location of M3U.
    - Paths are normaly platform specific.
    - Lines starting with # should be ignored.
    - m3u files are latin-1.
    - This function does not bother with Extended M3U directives.
    """"""

    uris = []
    folder = os.path.dirname(file_path)

    try:
        with open(file_path) as m3u:
            contents = m3u.readlines()
    except IOError as error:
        logger.error('Couldn\\'t open m3u: %s', locale_decode(error))
        return uris

    for line in contents:
        line = line.strip().decode('latin1')

        if line.startswith('#'):
            continue

        # FIXME what about other URI types?
        if line.startswith('file://'):
            uris.append(line)
        else:
            path = path_to_uri(folder, line)
            uris.append(path)

    return uris"
"    def __init__(self, folder, data_callback, error_callback=None):
        self.files = find_files(folder)
        self.data_callback = data_callback
        self.error_callback = error_callback
        self.loop = gobject.MainLoop()

        fakesink = gst.element_factory_make('fakesink')

        self.uribin = gst.element_factory_make('uridecodebin')
        self.uribin.set_property('caps', gst.Caps('audio/x-raw-int'))
        self.uribin.connect('pad-added', self.process_new_pad,
            fakesink.get_pad('sink'))

        self.pipe = gst.element_factory_make('pipeline')
        self.pipe.add(self.uribin)
        self.pipe.add(fakesink)

        bus = self.pipe.get_bus()
        bus.add_signal_watch()
        bus.connect('message::tag', self.process_tags)
        bus.connect('message::error', self.process_error)","    def __init__(self, folder, data_callback, error_callback=None):
        self.uris = [path_to_uri(f) for f in find_files(folder)]
        self.data_callback = data_callback
        self.error_callback = error_callback
        self.loop = gobject.MainLoop()

        fakesink = gst.element_factory_make('fakesink')

        self.uribin = gst.element_factory_make('uridecodebin')
        self.uribin.set_property('caps', gst.Caps('audio/x-raw-int'))
        self.uribin.connect('pad-added', self.process_new_pad,
            fakesink.get_pad('sink'))

        self.pipe = gst.element_factory_make('pipeline')
        self.pipe.add(self.uribin)
        self.pipe.add(fakesink)

        bus = self.pipe.get_bus()
        bus.add_signal_watch()
        bus.connect('message::tag', self.process_tags)
        bus.connect('message::error', self.process_error)"
"    def next_uri(self):
        try:
            uri = path_to_uri(self.files.next())
        except StopIteration:
            self.stop()
            return False
        self.pipe.set_state(gst.STATE_NULL)
        self.uribin.set_property('uri', uri)
        self.pipe.set_state(gst.STATE_PAUSED)
        return True","    def next_uri(self):
        if not self.uris:
            return self.stop()

        self.pipe.set_state(gst.STATE_NULL)
        self.uribin.set_property('uri', self.uris.pop())
        self.pipe.set_state(gst.STATE_PAUSED)"
"    def start(self):
        if self.next_uri():
            self.loop.run()","    def start(self):
        if not self.uris:
            return
        self.next_uri()
        self.loop.run()"
"def import_targets(request):
    context = {}
    context['import_target_li'] = 'active'
    context['target_data_active'] = 'true'
    if request.method == 'POST':
        if 'txtFile' in request.FILES:
            txt_file = request.FILES['txtFile']
            if txt_file.content_type == 'text/plain':
                target_count = 0
                txt_content = txt_file.read().decode('UTF-8')
                io_string = io.StringIO(txt_content)
                for target in io_string:
                    if validators.domain(target):
                        Domain.objects.create(
                            domain_name=target.rstrip(""\\n""), insert_date=timezone.now())
                        target_count += 1
                if target_count:
                    messages.add_message(request, messages.SUCCESS, str(
                        target_count) + ' targets added successfully!')
                    return http.HttpResponseRedirect(reverse('list_target'))
                else:
                    messages.add_message(
                        request,
                        messages.ERROR,
                        'Oops! File format was invalid, could not import any targets.')
            else:
                messages.add_message(
                    request, messages.ERROR, 'Invalid File type!')
        elif 'csvFile' in request.FILES:
            csv_file = request.FILES['csvFile']
            if csv_file.content_type == 'text/csv':
                target_count = 0
                csv_content = csv_file.read().decode('UTF-8')
                io_string = io.StringIO(csv_content)
                for column in csv.reader(io_string, delimiter=','):
                    if validators.domain(column[0]):
                        Domain.objects.create(
                            domain_name=column[0],
                            domain_description=column[1],
                            insert_date=timezone.now())
                        target_count += 1
                if target_count:
                    messages.add_message(request, messages.SUCCESS, str(
                        target_count) + ' targets added successfully!')
                    return http.HttpResponseRedirect(reverse('list_target'))
                else:
                    messages.add_message(
                        request,
                        messages.ERROR,
                        'Oops! File format was invalid, could not import any targets.')
            else:
                messages.add_message(
                    request, messages.ERROR, 'Invalid File type!')
    return render(request, 'target/import.html', context)","def import_targets(request):
    context = {}
    context['import_target_li'] = 'active'
    context['target_data_active'] = 'true'
    if request.method == 'POST':
        if 'txtFile' in request.FILES:
            txt_file = request.FILES['txtFile']
            if txt_file.content_type == 'text/plain':
                target_count = 0
                txt_content = txt_file.read().decode('UTF-8')
                io_string = io.StringIO(txt_content)
                for target in io_string:
                    if validators.domain(target):
                        Domain.objects.create(
                            domain_name=target, insert_date=timezone.now())
                        target_count += 1
                if target_count:
                    messages.add_message(request, messages.SUCCESS, str(
                        target_count) + ' targets added successfully!')
                    return http.HttpResponseRedirect(reverse('list_target'))
                else:
                    messages.add_message(
                        request,
                        messages.ERROR,
                        'Oops! File format was invalid, could not import any targets.')
            else:
                messages.add_message(
                    request, messages.ERROR, 'Invalid File type!')
        elif 'csvFile' in request.FILES:
            csv_file = request.FILES['csvFile']
            if csv_file.content_type == 'text/csv':
                target_count = 0
                csv_content = csv_file.read().decode('UTF-8')
                io_string = io.StringIO(csv_content)
                for column in csv.reader(io_string, delimiter=','):
                    if validators.domain(column[0]):
                        Domain.objects.create(
                            domain_name=column[0],
                            domain_description=column[1],
                            insert_date=timezone.now())
                        target_count += 1
                if target_count:
                    messages.add_message(request, messages.SUCCESS, str(
                        target_count) + ' targets added successfully!')
                    return http.HttpResponseRedirect(reverse('list_target'))
                else:
                    messages.add_message(
                        request,
                        messages.ERROR,
                        'Oops! File format was invalid, could not import any targets.')
            else:
                messages.add_message(
                    request, messages.ERROR, 'Invalid File type!')
    return render(request, 'target/import.html', context)"
"    def __init__(self, key_prefixes, runtime_dirs=get_runtime_dirs()):
        key_prefixes = map(self._sanitize, key_prefixes)
        # compute read and write dirs from base runtime dirs: the first base
        # dir is selected for writes and prefered for reads
        self._read_dirs = [os.path.join(x, *key_prefixes) for x in runtime_dirs]
        self._write_dir = self._read_dirs[0]
        os.makedirs(self._write_dir, exist_ok=True)
        if sys.platform == 'linux':
            # set the sticky bit to prevent removal during cleanup
            os.chmod(self._write_dir, 0o1700)
        _LOGGER.debug('data in %s', self._write_dir)","    def __init__(self, key_prefixes):
        key_prefixes = map(self._sanitize, key_prefixes)
        # compute read and write dirs from base runtime dirs: the first base
        # dir is selected for writes and prefered for reads
        self._read_dirs = [os.path.join(x, *key_prefixes) for x in get_runtime_dirs()]
        self._write_dir = self._read_dirs[0]
        os.makedirs(self._write_dir, exist_ok=True)
        if sys.platform == 'linux':
            # set the sticky bit to prevent removal during cleanup
            os.chmod(self._write_dir, 0o1700)
        _LOGGER.debug('data in %s', self._write_dir)"
"    def load(self, key):
        for base in self._read_dirs:
            path = os.path.join(base, key)
            if not os.path.isfile(path):
                continue
            try:
                with open(path, mode='r') as f:
                    data = f.read().strip()
                    if len(data) == 0:
                        value = None
                    else:
                        value = literal_eval(data)
                    _LOGGER.debug('loaded %s=%r (from %s)', key, value, path)
            except OSError as err:
                _LOGGER.warning('%s exists but could not be read: %s', path, err)
            except ValueError as err:
                _LOGGER.warning('%s exists but was corrupted: %s', key, err)
            else:
                return value

        _LOGGER.debug('no data (file) found for %s', key)
        return None","    def load(self, key):
        for base in self._read_dirs:
            path = os.path.join(base, key)
            if not os.path.isfile(path):
                continue
            try:
                with open(path, mode='r') as f:
                    data = f.read().strip()
                    if len(data) == 0:
                        value = None
                    else:
                        value = literal_eval(data)
                    _LOGGER.debug('loaded %s=%r (from %s)', key, value, path)
            except OSError as err:
                _LOGGER.warning('%s exists but cannot be read: %s', path, err)
                continue
            return value
        _LOGGER.debug('no data (file) found for %s', key)
        return None"
"    def _write(self, data):
        assert len(data) <= _REPORT_LENGTH
        packet = bytearray(1 + _REPORT_LENGTH)
        packet[1 : 1 + len(data)] = data  # device doesn't use numbered reports
        self.device.write(packet)","    def _write(self, data):
        padding = [0x0]*(_WRITE_LENGTH - len(data))
        self.device.write(data + padding)"
"    def _read(self):
        return self.device.read(_REPORT_LENGTH)","    def _read(self):
        return self.device.read(_READ_LENGTH)"
"    def connect(self, **kwargs):
        """"""Connect to the device.""""""
        super().connect(**kwargs)
        ids = f'vid{self.vendor_id:04x}_pid{self.product_id:04x}'
        # must use the HID path because there is no serial number; however,
        # these can be quite long on Windows and macOS, so only take the
        # numbers, since they are likely the only parts that vary between two
        # devices of the same model
        loc = 'loc' + '_'.join(re.findall(r'\\d+', self.address))
        self._data = RuntimeStorage(key_prefixes=[ids, loc])
        self._sequence = _sequence(self._data)","    def connect(self, **kwargs):
        """"""Connect to the device.""""""
        super().connect(**kwargs)
        ids = f'vid{self.vendor_id:04x}_pid{self.product_id:04x}'
        # must use the HID path because there is no serial number; however,
        # these can be quite long on Windows and macOS, so only take the
        # numbers, since they are likely the only parts that vary between two
        # devices of the same model
        loc = 'loc' + '_'.join((num.decode() for num in re.findall(b'\\\\d+', self.address)))
        self._data = RuntimeStorage(key_prefixes=[ids, loc])
        self._sequence = _sequence(self._data)"
"    def connect(self, **kwargs):
        """"""Connect to the device.

        Enables the device to send data to the host.""""""
        super().connect(**kwargs)
        self._configure_flow_control(clear_to_send=True)","    def connect(self, **kwargs):
        """"""Connect to the device.""""""
        super().connect()
        try:
            self._open()
        except usb.core.USBError as err:
            LOGGER.warning('report: failed to open right away, will close first')
            LOGGER.debug(err, exc_info=True)
            self._close()
            self._open()
        finally:
            self.device.release()"
"    def disconnect(self, **kwargs):
        """"""Disconnect from the device.

        Implementation note: unlike SI_Close is supposed to do,¹ do not send
        _USBXPRESS_NOT_CLEAR_TO_SEND to the device.  This allows one program to
        disconnect without sotping reads from another.

        Surrounding device.read() with _USBXPRESS_[NOT_]CLEAR_TO_SEND would
        make more sense, but there seems to be a yet unknown minimum delay
        necessary for that to work well.

        ¹ https://github.com/craigshelley/SiUSBXp/blob/master/SiUSBXp.c
        """"""
        super().disconnect(**kwargs)","    def disconnect(self, **kwargs):
        """"""Disconnect from the device.""""""
        self._close()
        super().disconnect()
        self.device.release()"
"    def dump(self):
        """"""
        Returns the string that represents the nyan file.
        """"""
        fileinfo_str = f""# NYAN FILE\\nversion {FILE_VERSION}\\n\\n""
        import_str = """"
        objects_str = """"

        for nyan_object in self.nyan_objects:
            objects_str += nyan_object.dump(import_tree=self.import_tree)

        # Removes one empty newline at the end of the objects definition
        objects_str = objects_str[:-1]

        import_aliases = self.import_tree.get_import_dict()
        self.import_tree.clear_marks()

        for alias, fqon in import_aliases.items():
            import_str += ""import ""

            import_str += ""."".join(fqon)

            import_str += f"" as {alias}\\n""

        import_str += ""\\n""

        output_str = fileinfo_str + import_str + objects_str

        return output_str","    def dump(self):
        """"""
        Returns the string that represents the nyan file.
        """"""
        output_str = f""# NYAN FILE\\nversion {FILE_VERSION}\\n\\n""

        import_aliases = self.import_tree.establish_import_dict(self,
                                                                ignore_names=[""type"", ""types""])

        for alias, fqon in import_aliases.items():
            output_str += ""import ""

            output_str += ""."".join(fqon)

            output_str += f"" as {alias}\\n""

        output_str += ""\\n""

        for nyan_object in self.nyan_objects:
            output_str += nyan_object.dump(import_tree=self.import_tree)

        self.import_tree.clear_marks()

        # Removes one empty line at the end of the file
        output_str = output_str[:-1]

        return output_str"
"def convert_assets(assets, args, srcdir=None, prev_source_dir_path=None):
    """"""
    Perform asset conversion.

    Requires original assets and stores them in usable and free formats.

    assets must be a filesystem-like object pointing at the game's asset dir.
    srcdir must be None, or point at some source directory.

    If gen_extra_files is True, some more files, mostly for debugging purposes,
    are created.

    This method prepares srcdir and targetdir to allow a pleasant, unified
    conversion experience, then passes them to .driver.convert().
    """"""
    # acquire conversion source directory
    if srcdir is None:
        srcdir = acquire_conversion_source_dir(prev_source_dir_path)

    converted_path = assets / ""converted""
    converted_path.mkdirs()
    targetdir = DirectoryCreator(converted_path).root

    # Set compression level for media output if it was not set
    if ""compression_level"" not in vars(args):
        args.compression_level = 1

    # Set verbosity for debug output
    if ""debug_info"" not in vars(args) or not args.debug_info:
        if args.devmode:
            args.debug_info = 3

        else:
            args.debug_info = 0

    # add a dir for debug info
    debug_log_path = converted_path / ""debug"" / datetime.now().strftime(""%Y-%m-%d-%H-%M-%S"")
    debugdir = DirectoryCreator(debug_log_path).root
    args.debugdir = AccessSynchronizer(debugdir).root

    # Create CLI args info
    debug_cli_args(args.debugdir, args.debug_info, args)

    # Initialize game versions data
    auxiliary_files_dir = args.cfg_dir / ""converter"" / ""games""
    args.avail_game_eds, args.avail_game_exps = create_version_objects(auxiliary_files_dir)

    # Acquire game version info
    args.game_version = get_game_version(srcdir, args.avail_game_eds, args.avail_game_exps)
    debug_game_version(args.debugdir, args.debug_info, args)

    # Mount assets into conversion folder
    data_dir = mount_asset_dirs(srcdir, args.game_version)
    if not data_dir:
        return None

    # make srcdir and targetdir safe for threaded conversion
    args.srcdir = AccessSynchronizer(data_dir).root
    args.targetdir = AccessSynchronizer(targetdir).root

    # Create mountpoint info
    debug_mounts(args.debugdir, args.debug_info, args)

    def flag(name):
        """"""
        Convenience function for accessing boolean flags in args.
        Flags default to False if they don't exist.
        """"""
        return getattr(args, name, False)

    args.flag = flag

    # import here so codegen.py doesn't depend on it.
    from .tool.driver import convert

    converted_count = 0
    total_count = None
    for current_item in convert(args):
        if isinstance(current_item, int):
            # convert is informing us about the estimated number of remaining
            # items.
            total_count = current_item + converted_count
            continue

        # TODO a GUI would be nice here.

        if total_count is None:
            info(""[%s] %s"", converted_count, current_item)
        else:
            info(""[%s] %s"", format_progress(converted_count, total_count), current_item)

        converted_count += 1

    # clean args
    del args.srcdir
    del args.targetdir

    return data_dir.resolve_native_path()","def convert_assets(assets, args, srcdir=None, prev_source_dir_path=None):
    """"""
    Perform asset conversion.

    Requires original assets and stores them in usable and free formats.

    assets must be a filesystem-like object pointing at the game's asset dir.
    srcdir must be None, or point at some source directory.

    If gen_extra_files is True, some more files, mostly for debugging purposes,
    are created.

    This method prepares srcdir and targetdir to allow a pleasant, unified
    conversion experience, then passes them to .driver.convert().
    """"""
    # acquire conversion source directory
    if srcdir is None:
        srcdir = acquire_conversion_source_dir(prev_source_dir_path)

    converted_path = assets / ""converted""
    converted_path.mkdirs()
    targetdir = DirectoryCreator(converted_path).root

    # Set compression level for media output if it was not set
    if ""compression_level"" not in vars(args):
        args.compression_level = 1

    # Set verbosity for debug output
    if ""debug_info"" not in vars(args):
        if args.devmode:
            args.debug_info = 3

        else:
            args.debug_info = 0

    # add a dir for debug info
    debug_log_path = converted_path / ""debug"" / datetime.now().strftime(""%Y-%m-%d-%H-%M-%S"")
    debugdir = DirectoryCreator(debug_log_path).root
    args.debugdir = AccessSynchronizer(debugdir).root

    # Create CLI args info
    debug_cli_args(args.debugdir, args.debug_info, args)

    # Initialize game versions data
    auxiliary_files_dir = args.cfg_dir / ""converter"" / ""games""
    args.avail_game_eds, args.avail_game_exps = create_version_objects(auxiliary_files_dir)

    # Acquire game version info
    args.game_version = get_game_version(srcdir, args.avail_game_eds, args.avail_game_exps)
    debug_game_version(args.debugdir, args.debug_info, args)

    # Mount assets into conversion folder
    data_dir = mount_asset_dirs(srcdir, args.game_version)
    if not data_dir:
        return None

    # make srcdir and targetdir safe for threaded conversion
    args.srcdir = AccessSynchronizer(data_dir).root
    args.targetdir = AccessSynchronizer(targetdir).root

    # Create mountpoint info
    debug_mounts(args.debugdir, args.debug_info, args)

    def flag(name):
        """"""
        Convenience function for accessing boolean flags in args.
        Flags default to False if they don't exist.
        """"""
        return getattr(args, name, False)

    args.flag = flag

    # import here so codegen.py doesn't depend on it.
    from .tool.driver import convert

    converted_count = 0
    total_count = None
    for current_item in convert(args):
        if isinstance(current_item, int):
            # convert is informing us about the estimated number of remaining
            # items.
            total_count = current_item + converted_count
            continue

        # TODO a GUI would be nice here.

        if total_count is None:
            info(""[%s] %s"", converted_count, current_item)
        else:
            info(""[%s] %s"", format_progress(converted_count, total_count), current_item)

        converted_count += 1

    # clean args
    del args.srcdir
    del args.targetdir

    return data_dir.resolve_native_path()"
"    def _get_aoe2_base(cls, gamedata):
        """"""
        Create the aoe2-base modpack.
        """"""
        modpack = Modpack(""aoe2_base"")

        mod_def = modpack.get_info()

        mod_def.set_version(""1.0c"")
        mod_def.set_uid(2000)

        mod_def.add_assets_to_load(""data/*"")

        cls.organize_nyan_objects(modpack, gamedata)
        cls.organize_media_objects(modpack, gamedata)

        return modpack","    def _get_aoe2_base(cls, gamedata):
        """"""
        Create the aoe2-base modpack.
        """"""
        modpack = Modpack(""aoe2-base"")

        mod_def = modpack.get_info()

        mod_def.set_version(""1.0c"")
        mod_def.set_uid(2000)

        mod_def.add_assets_to_load(""data/*"")

        cls.organize_nyan_objects(modpack, gamedata)
        cls.organize_media_objects(modpack, gamedata)

        return modpack"
"    def organize_nyan_objects(modpack, full_data_set):
        """"""
        Put available nyan objects into a given modpack.
        """"""
        created_nyan_files = {}

        # Access all raw API objects
        raw_api_objects = []
        raw_api_objects.extend(full_data_set.pregen_nyan_objects.values())

        for unit_line in full_data_set.unit_lines.values():
            raw_api_objects.extend(unit_line.get_raw_api_objects().values())

        for building_line in full_data_set.building_lines.values():
            raw_api_objects.extend(building_line.get_raw_api_objects().values())

        for ambient_group in full_data_set.ambient_groups.values():
            raw_api_objects.extend(ambient_group.get_raw_api_objects().values())

        for variant_group in full_data_set.variant_groups.values():
            raw_api_objects.extend(variant_group.get_raw_api_objects().values())

        for tech_group in full_data_set.tech_groups.values():
            raw_api_objects.extend(tech_group.get_raw_api_objects().values())

        for terrain_group in full_data_set.terrain_groups.values():
            raw_api_objects.extend(terrain_group.get_raw_api_objects().values())

        for civ_group in full_data_set.civ_groups.values():
            raw_api_objects.extend(civ_group.get_raw_api_objects().values())

        # Create the files
        for raw_api_object in raw_api_objects:
            obj_location = raw_api_object.get_location()

            if isinstance(obj_location, ForwardRef):
                # Resolve location and add nested object
                nyan_object = obj_location.resolve()
                nyan_object.add_nested_object(raw_api_object.get_nyan_object())
                continue

            obj_filename = raw_api_object.get_filename()
            nyan_file_path = f""{modpack.info.name}/{obj_location}{obj_filename}""

            if nyan_file_path in created_nyan_files.keys():
                nyan_file = created_nyan_files[nyan_file_path]

            else:
                nyan_file = NyanFile(obj_location, obj_filename,
                                     modpack.info.name)
                created_nyan_files.update({nyan_file.get_relative_file_path(): nyan_file})
                modpack.add_data_export(nyan_file)

            nyan_file.add_nyan_object(raw_api_object.get_nyan_object())

        # Create an import tree from the files
        import_tree = ImportTree()

        for nyan_file in created_nyan_files.values():
            import_tree.expand_from_file(nyan_file)

        for nyan_object in full_data_set.nyan_api_objects.values():
            import_tree.expand_from_object(nyan_object)

        for nyan_file in created_nyan_files.values():
            nyan_file.set_import_tree(import_tree)

        AoCModpackSubprocessor._set_static_aliases(modpack, import_tree)","    def organize_nyan_objects(modpack, full_data_set):
        """"""
        Put available nyan objects into a given modpack.
        """"""
        created_nyan_files = {}

        # Access all raw API objects
        raw_api_objects = []
        raw_api_objects.extend(full_data_set.pregen_nyan_objects.values())

        for unit_line in full_data_set.unit_lines.values():
            raw_api_objects.extend(unit_line.get_raw_api_objects().values())

        for building_line in full_data_set.building_lines.values():
            raw_api_objects.extend(building_line.get_raw_api_objects().values())

        for ambient_group in full_data_set.ambient_groups.values():
            raw_api_objects.extend(ambient_group.get_raw_api_objects().values())

        for variant_group in full_data_set.variant_groups.values():
            raw_api_objects.extend(variant_group.get_raw_api_objects().values())

        for tech_group in full_data_set.tech_groups.values():
            raw_api_objects.extend(tech_group.get_raw_api_objects().values())

        for terrain_group in full_data_set.terrain_groups.values():
            raw_api_objects.extend(terrain_group.get_raw_api_objects().values())

        for civ_group in full_data_set.civ_groups.values():
            raw_api_objects.extend(civ_group.get_raw_api_objects().values())

        # Create the files
        for raw_api_object in raw_api_objects:
            obj_location = raw_api_object.get_location()

            if isinstance(obj_location, ForwardRef):
                # Resolve location and add nested object
                nyan_object = obj_location.resolve()
                nyan_object.add_nested_object(raw_api_object.get_nyan_object())
                continue

            obj_filename = raw_api_object.get_filename()
            nyan_file_path = f""{modpack.info.name}/{obj_location}{obj_filename}""

            if nyan_file_path in created_nyan_files.keys():
                nyan_file = created_nyan_files[nyan_file_path]

            else:
                nyan_file = NyanFile(obj_location, obj_filename,
                                     modpack.info.name)
                created_nyan_files.update({nyan_file.get_relative_file_path(): nyan_file})
                modpack.add_data_export(nyan_file)

            nyan_file.add_nyan_object(raw_api_object.get_nyan_object())

        # Create an import tree from the files
        import_tree = ImportTree()

        for nyan_file in created_nyan_files.values():
            import_tree.expand_from_file(nyan_file)

        for nyan_object in full_data_set.nyan_api_objects.values():
            import_tree.expand_from_object(nyan_object)

        for nyan_file in created_nyan_files.values():
            nyan_file.set_import_tree(import_tree)"
"    def generate_modifiers(full_data_set, pregen_converter_group):
        """"""
        Generate standard modifiers.

        :param full_data_set: GenieObjectContainer instance that
                              contains all relevant data for the conversion
                              process.
        :type full_data_set: ...dataformat.aoc.genie_object_container.GenieObjectContainer
        :param pregen_converter_group: GenieObjectGroup instance that stores
                                       pregenerated API objects for referencing with
                                       ForwardRef
        :type pregen_converter_group: ...dataformat.aoc.genie_object_container.GenieObjectGroup
        """"""
        pregen_nyan_objects = full_data_set.pregen_nyan_objects
        api_objects = full_data_set.nyan_api_objects

        modifier_parent = ""engine.modifier.multiplier.MultiplierModifier""
        type_parent = ""engine.modifier.multiplier.effect.flat_attribute_change.type.Flyover""
        types_location = ""data/aux/modifier/flyover_cliff/""

        # =======================================================================
        # Flyover effect multiplier
        # =======================================================================
        modifier_ref_in_modpack = ""aux.modifier.flyover_cliff.AttackMultiplierFlyover""
        modifier_raw_api_object = RawAPIObject(modifier_ref_in_modpack,
                                               ""AttackMultiplierFlyover"", api_objects,
                                               types_location)
        modifier_raw_api_object.set_filename(""flyover_cliff"")
        modifier_raw_api_object.add_raw_parent(type_parent)

        pregen_converter_group.add_raw_api_object(modifier_raw_api_object)
        pregen_nyan_objects.update({modifier_ref_in_modpack: modifier_raw_api_object})

        # Increases effect value by 25%
        modifier_raw_api_object.add_raw_member(""multiplier"",
                                               1.25,
                                               modifier_parent)

        # Relative angle to cliff must not be larger than 90°
        modifier_raw_api_object.add_raw_member(""relative_angle"",
                                               90,
                                               type_parent)

        # Affects all cliffs
        types = [ForwardRef(pregen_converter_group, ""aux.game_entity_type.types.Cliff"")]
        modifier_raw_api_object.add_raw_member(""flyover_types"",
                                               types,
                                               type_parent)
        modifier_raw_api_object.add_raw_member(""blacklisted_entities"",
                                               [],
                                               type_parent)

        # =======================================================================
        # Elevation difference effect multiplier (higher unit)
        # =======================================================================
        modifier_parent = ""engine.modifier.multiplier.MultiplierModifier""
        type_parent = ""engine.modifier.multiplier.effect.flat_attribute_change.type.ElevationDifferenceHigh""
        types_location = ""data/aux/modifier/elevation_difference/""

        modifier_ref_in_modpack = ""aux.modifier.elevation_difference.AttackMultiplierHigh""
        modifier_raw_api_object = RawAPIObject(modifier_ref_in_modpack,
                                               ""AttackMultiplierHigh"", api_objects,
                                               types_location)
        modifier_raw_api_object.set_filename(""elevation_difference"")
        modifier_raw_api_object.add_raw_parent(type_parent)

        pregen_converter_group.add_raw_api_object(modifier_raw_api_object)
        pregen_nyan_objects.update({modifier_ref_in_modpack: modifier_raw_api_object})

        # Increases effect value to 125%
        modifier_raw_api_object.add_raw_member(""multiplier"",
                                               1.25,
                                               modifier_parent)

        # Min elevation difference is not set

        # =======================================================================
        # Elevation difference effect multiplier (lower unit)
        # =======================================================================
        modifier_parent = ""engine.modifier.multiplier.MultiplierModifier""
        type_parent = ""engine.modifier.multiplier.effect.flat_attribute_change.type.ElevationDifferenceLow""
        types_location = ""data/aux/modifier/elevation_difference/""

        modifier_ref_in_modpack = ""aux.modifier.elevation_difference.AttackMultiplierLow""
        modifier_raw_api_object = RawAPIObject(modifier_ref_in_modpack,
                                               ""AttackMultiplierLow"", api_objects,
                                               types_location)
        modifier_raw_api_object.set_filename(""elevation_difference"")
        modifier_raw_api_object.add_raw_parent(type_parent)

        pregen_converter_group.add_raw_api_object(modifier_raw_api_object)
        pregen_nyan_objects.update({modifier_ref_in_modpack: modifier_raw_api_object})

        # Decreases effect value to 75%
        modifier_raw_api_object.add_raw_member(""multiplier"",
                                               0.75,
                                               modifier_parent)","    def generate_modifiers(full_data_set, pregen_converter_group):
        """"""
        Generate standard modifiers.

        :param full_data_set: GenieObjectContainer instance that
                              contains all relevant data for the conversion
                              process.
        :type full_data_set: ...dataformat.aoc.genie_object_container.GenieObjectContainer
        :param pregen_converter_group: GenieObjectGroup instance that stores
                                       pregenerated API objects for referencing with
                                       ForwardRef
        :type pregen_converter_group: ...dataformat.aoc.genie_object_container.GenieObjectGroup
        """"""
        pregen_nyan_objects = full_data_set.pregen_nyan_objects
        api_objects = full_data_set.nyan_api_objects

        modifier_parent = ""engine.modifier.multiplier.MultiplierModifier""
        type_parent = ""engine.modifier.multiplier.effect.flat_attribute_change.type.Flyover""
        types_location = ""data/aux/modifier/flyover_cliff""

        # =======================================================================
        # Flyover effect multiplier
        # =======================================================================
        modifier_ref_in_modpack = ""aux.modifier.flyover_cliff.AttackMultiplierFlyover""
        modifier_raw_api_object = RawAPIObject(modifier_ref_in_modpack,
                                               ""AttackMultiplierFlyover"", api_objects,
                                               types_location)
        modifier_raw_api_object.set_filename(""flyover_cliff"")
        modifier_raw_api_object.add_raw_parent(type_parent)

        pregen_converter_group.add_raw_api_object(modifier_raw_api_object)
        pregen_nyan_objects.update({modifier_ref_in_modpack: modifier_raw_api_object})

        # Increases effect value by 25%
        modifier_raw_api_object.add_raw_member(""multiplier"",
                                               1.25,
                                               modifier_parent)

        # Relative angle to cliff must not be larger than 90°
        modifier_raw_api_object.add_raw_member(""relative_angle"",
                                               90,
                                               type_parent)

        # Affects all cliffs
        types = [ForwardRef(pregen_converter_group, ""aux.game_entity_type.types.Cliff"")]
        modifier_raw_api_object.add_raw_member(""flyover_types"",
                                               types,
                                               type_parent)
        modifier_raw_api_object.add_raw_member(""blacklisted_entities"",
                                               [],
                                               type_parent)

        # =======================================================================
        # Elevation difference effect multiplier (higher unit)
        # =======================================================================
        modifier_parent = ""engine.modifier.multiplier.MultiplierModifier""
        type_parent = ""engine.modifier.multiplier.effect.flat_attribute_change.type.ElevationDifferenceHigh""
        types_location = ""data/aux/modifier/elevation_difference""

        modifier_ref_in_modpack = ""aux.modifier.elevation_difference.AttackMultiplierHigh""
        modifier_raw_api_object = RawAPIObject(modifier_ref_in_modpack,
                                               ""AttackMultiplierHigh"", api_objects,
                                               types_location)
        modifier_raw_api_object.set_filename(""elevation_difference"")
        modifier_raw_api_object.add_raw_parent(type_parent)

        pregen_converter_group.add_raw_api_object(modifier_raw_api_object)
        pregen_nyan_objects.update({modifier_ref_in_modpack: modifier_raw_api_object})

        # Increases effect value to 125%
        modifier_raw_api_object.add_raw_member(""multiplier"",
                                               1.25,
                                               modifier_parent)

        # Min elevation difference is not set

        # =======================================================================
        # Elevation difference effect multiplier (lower unit)
        # =======================================================================
        modifier_parent = ""engine.modifier.multiplier.MultiplierModifier""
        type_parent = ""engine.modifier.multiplier.effect.flat_attribute_change.type.ElevationDifferenceLow""
        types_location = ""data/aux/modifier/elevation_difference""

        modifier_ref_in_modpack = ""aux.modifier.elevation_difference.AttackMultiplierLow""
        modifier_raw_api_object = RawAPIObject(modifier_ref_in_modpack,
                                               ""AttackMultiplierLow"", api_objects,
                                               types_location)
        modifier_raw_api_object.set_filename(""elevation_difference"")
        modifier_raw_api_object.add_raw_parent(type_parent)

        pregen_converter_group.add_raw_api_object(modifier_raw_api_object)
        pregen_nyan_objects.update({modifier_ref_in_modpack: modifier_raw_api_object})

        # Decreases effect value to 75%
        modifier_raw_api_object.add_raw_member(""multiplier"",
                                               0.75,
                                               modifier_parent)"
"    def live_ability(converter_group, line, container_obj_ref, diff=None):
        """"""
        Creates a patch for the Live ability of a line.

        :param converter_group: Group that gets the patch.
        :type converter_group: ...dataformat.converter_object.ConverterObjectGroup
        :param line: Unit/Building line that has the ability.
        :type line: ...dataformat.converter_object.ConverterObjectGroup
        :param container_obj_ref: Reference of the raw API object the patch is nested in.
        :type container_obj_ref: str
        :param diff: A diff between two ConvertObject instances.
        :type diff: ...dataformat.converter_object.ConverterObject
        :returns: The forward references for the generated patches.
        :rtype: list
        """"""
        head_unit_id = line.get_head_unit_id()
        tech_id = converter_group.get_id()
        dataset = line.data

        patches = []

        name_lookup_dict = internal_name_lookups.get_entity_lookups(dataset.game_version)
        tech_lookup_dict = internal_name_lookups.get_tech_lookups(dataset.game_version)

        game_entity_name = name_lookup_dict[head_unit_id][0]

        if diff:
            diff_hp = diff[""hit_points""]
            if isinstance(diff_hp, NoDiffMember):
                return patches

            diff_hp_value = diff_hp.get_value()

        else:
            return patches

        patch_target_ref = f""{game_entity_name}.Live.Health""
        patch_target_forward_ref = ForwardRef(line, patch_target_ref)

        # Wrapper
        wrapper_name = f""Change{game_entity_name}HealthWrapper""
        wrapper_ref = f""{container_obj_ref}.{wrapper_name}""
        wrapper_raw_api_object = RawAPIObject(wrapper_ref,
                                              wrapper_name,
                                              dataset.nyan_api_objects)
        wrapper_raw_api_object.add_raw_parent(""engine.aux.patch.Patch"")

        if isinstance(line, GenieBuildingLineGroup):
            # Store building upgrades next to their game entity definition,
            # not in the Age up techs.
            wrapper_raw_api_object.set_location(""data/game_entity/generic/%s/""
                                                % (name_lookup_dict[head_unit_id][1]))
            wrapper_raw_api_object.set_filename(f""{tech_lookup_dict[tech_id][1]}_upgrade"")

        else:
            wrapper_raw_api_object.set_location(ForwardRef(converter_group, container_obj_ref))

        # Nyan patch
        nyan_patch_name = f""Change{game_entity_name}Health""
        nyan_patch_ref = f""{container_obj_ref}.{wrapper_name}.{nyan_patch_name}""
        nyan_patch_location = ForwardRef(converter_group, wrapper_ref)
        nyan_patch_raw_api_object = RawAPIObject(nyan_patch_ref,
                                                 nyan_patch_name,
                                                 dataset.nyan_api_objects,
                                                 nyan_patch_location)
        nyan_patch_raw_api_object.add_raw_parent(""engine.aux.patch.NyanPatch"")
        nyan_patch_raw_api_object.set_patch_target(patch_target_forward_ref)

        # HP max value
        nyan_patch_raw_api_object.add_raw_patch_member(""max_value"",
                                                       diff_hp_value,
                                                       ""engine.aux.attribute.AttributeSetting"",
                                                       MemberOperator.ADD)

        # HP starting value
        nyan_patch_raw_api_object.add_raw_patch_member(""starting_value"",
                                                       diff_hp_value,
                                                       ""engine.aux.attribute.AttributeSetting"",
                                                       MemberOperator.ADD)

        patch_forward_ref = ForwardRef(converter_group, nyan_patch_ref)
        wrapper_raw_api_object.add_raw_member(""patch"",
                                              patch_forward_ref,
                                              ""engine.aux.patch.Patch"")

        converter_group.add_raw_api_object(wrapper_raw_api_object)
        converter_group.add_raw_api_object(nyan_patch_raw_api_object)

        wrapper_forward_ref = ForwardRef(converter_group, wrapper_ref)
        patches.append(wrapper_forward_ref)

        return patches","    def live_ability(converter_group, line, container_obj_ref, diff=None):
        """"""
        Creates a patch for the Live ability of a line.

        :param converter_group: Group that gets the patch.
        :type converter_group: ...dataformat.converter_object.ConverterObjectGroup
        :param line: Unit/Building line that has the ability.
        :type line: ...dataformat.converter_object.ConverterObjectGroup
        :param container_obj_ref: Reference of the raw API object the patch is nested in.
        :type container_obj_ref: str
        :param diff: A diff between two ConvertObject instances.
        :type diff: ...dataformat.converter_object.ConverterObject
        :returns: The forward references for the generated patches.
        :rtype: list
        """"""
        head_unit_id = line.get_head_unit_id()
        tech_id = converter_group.get_id()
        dataset = line.data

        patches = []

        name_lookup_dict = internal_name_lookups.get_entity_lookups(dataset.game_version)
        tech_lookup_dict = internal_name_lookups.get_tech_lookups(dataset.game_version)

        game_entity_name = name_lookup_dict[head_unit_id][0]

        if diff:
            diff_hp = diff[""hit_points""]
            if isinstance(diff_hp, NoDiffMember):
                return patches

            diff_hp_value = diff_hp.get_value()

        else:
            return patches

        patch_target_ref = f""{game_entity_name}.Live.Health""
        patch_target_forward_ref = ForwardRef(line, patch_target_ref)

        # Wrapper
        wrapper_name = f""Change{game_entity_name}HealthWrapper""
        wrapper_ref = f""{container_obj_ref}.{wrapper_name}""
        wrapper_raw_api_object = RawAPIObject(wrapper_ref,
                                              wrapper_name,
                                              dataset.nyan_api_objects)
        wrapper_raw_api_object.add_raw_parent(""engine.aux.patch.Patch"")

        if isinstance(line, GenieBuildingLineGroup):
            # Store building upgrades next to their game entity definition,
            # not in the Age up techs.
            wrapper_raw_api_object.set_location(""data/game_entity/generic/%s/""
                                                % (name_lookup_dict[head_unit_id][1]))
            wrapper_raw_api_object.set_filename(f""{tech_lookup_dict[tech_id][1]}_upgrade"")

        else:
            wrapper_raw_api_object.set_location(ForwardRef(converter_group, container_obj_ref))

        # Nyan patch
        nyan_patch_name = f""Change{game_entity_name}Health""
        nyan_patch_ref = f""{container_obj_ref}.{wrapper_name}.{nyan_patch_name}""
        nyan_patch_location = ForwardRef(converter_group, wrapper_ref)
        nyan_patch_raw_api_object = RawAPIObject(nyan_patch_ref,
                                                 nyan_patch_name,
                                                 dataset.nyan_api_objects,
                                                 nyan_patch_location)
        nyan_patch_raw_api_object.add_raw_parent(""engine.aux.patch.NyanPatch"")
        nyan_patch_raw_api_object.set_patch_target(patch_target_forward_ref)

        # HP max value
        nyan_patch_raw_api_object.add_raw_patch_member(""max_value"",
                                                       diff_hp_value,
                                                       ""engine.aux.attribute.AttributeSetting"",
                                                       MemberOperator.ADD)

        patch_forward_ref = ForwardRef(converter_group, nyan_patch_ref)
        wrapper_raw_api_object.add_raw_member(""patch"",
                                              patch_forward_ref,
                                              ""engine.aux.patch.Patch"")

        converter_group.add_raw_api_object(wrapper_raw_api_object)
        converter_group.add_raw_api_object(nyan_patch_raw_api_object)

        wrapper_forward_ref = ForwardRef(converter_group, wrapper_ref)
        patches.append(wrapper_forward_ref)

        return patches"
"    def hp_upgrade(converter_group, line, value, operator, team=False):
        """"""
        Creates a patch for the HP modify effect (ID: 0).

        :param converter_group: Tech/Civ that gets the patch.
        :type converter_group: ...dataformat.converter_object.ConverterObjectGroup
        :param line: Unit/Building line that has the ability.
        :type line: ...dataformat.converter_object.ConverterObjectGroup
        :param value: Value used for patching the member.
        :type value: MemberOperator
        :param operator: Operator used for patching the member.
        :type operator: MemberOperator
        :returns: The forward references for the generated patches.
        :rtype: list
        """"""
        head_unit_id = line.get_head_unit_id()
        dataset = line.data

        patches = []

        obj_id = converter_group.get_id()
        if isinstance(converter_group, GenieTechEffectBundleGroup):
            tech_lookup_dict = internal_name_lookups.get_tech_lookups(dataset.game_version)
            obj_name = tech_lookup_dict[obj_id][0]

        else:
            civ_lookup_dict = internal_name_lookups.get_civ_lookups(dataset.game_version)
            obj_name = civ_lookup_dict[obj_id][0]

        name_lookup_dict = internal_name_lookups.get_entity_lookups(dataset.game_version)

        game_entity_name = name_lookup_dict[head_unit_id][0]

        patch_target_ref = f""{game_entity_name}.Live.Health""
        patch_target_forward_ref = ForwardRef(line, patch_target_ref)

        # Wrapper
        wrapper_name = f""Change{game_entity_name}MaxHealthWrapper""
        wrapper_ref = f""{obj_name}.{wrapper_name}""
        wrapper_location = ForwardRef(converter_group, obj_name)
        wrapper_raw_api_object = RawAPIObject(wrapper_ref,
                                              wrapper_name,
                                              dataset.nyan_api_objects,
                                              wrapper_location)
        wrapper_raw_api_object.add_raw_parent(""engine.aux.patch.Patch"")

        # Nyan patch
        nyan_patch_name = f""Change{game_entity_name}MaxHealth""
        nyan_patch_ref = f""{obj_name}.{wrapper_name}.{nyan_patch_name}""
        nyan_patch_location = ForwardRef(converter_group, wrapper_ref)
        nyan_patch_raw_api_object = RawAPIObject(nyan_patch_ref,
                                                 nyan_patch_name,
                                                 dataset.nyan_api_objects,
                                                 nyan_patch_location)
        nyan_patch_raw_api_object.add_raw_parent(""engine.aux.patch.NyanPatch"")
        nyan_patch_raw_api_object.set_patch_target(patch_target_forward_ref)

        nyan_patch_raw_api_object.add_raw_patch_member(""max_value"",
                                                       value,
                                                       ""engine.aux.attribute.AttributeSetting"",
                                                       operator)

        nyan_patch_raw_api_object.add_raw_patch_member(""starting_value"",
                                                       value,
                                                       ""engine.aux.attribute.AttributeSetting"",
                                                       operator)

        patch_forward_ref = ForwardRef(converter_group, nyan_patch_ref)
        wrapper_raw_api_object.add_raw_member(""patch"",
                                              patch_forward_ref,
                                              ""engine.aux.patch.Patch"")

        if team:
            wrapper_raw_api_object.add_raw_parent(""engine.aux.patch.type.DiplomaticPatch"")
            stances = [
                dataset.nyan_api_objects[""engine.aux.diplomatic_stance.type.Self""],
                dataset.pregen_nyan_objects[""aux.diplomatic_stance.types.Friendly""].get_nyan_object()
            ]
            wrapper_raw_api_object.add_raw_member(""stances"",
                                                  stances,
                                                  ""engine.aux.patch.type.DiplomaticPatch"")

        converter_group.add_raw_api_object(wrapper_raw_api_object)
        converter_group.add_raw_api_object(nyan_patch_raw_api_object)

        wrapper_forward_ref = ForwardRef(converter_group, wrapper_ref)
        patches.append(wrapper_forward_ref)

        return patches","    def hp_upgrade(converter_group, line, value, operator, team=False):
        """"""
        Creates a patch for the HP modify effect (ID: 0).

        :param converter_group: Tech/Civ that gets the patch.
        :type converter_group: ...dataformat.converter_object.ConverterObjectGroup
        :param line: Unit/Building line that has the ability.
        :type line: ...dataformat.converter_object.ConverterObjectGroup
        :param value: Value used for patching the member.
        :type value: MemberOperator
        :param operator: Operator used for patching the member.
        :type operator: MemberOperator
        :returns: The forward references for the generated patches.
        :rtype: list
        """"""
        head_unit_id = line.get_head_unit_id()
        dataset = line.data

        patches = []

        obj_id = converter_group.get_id()
        if isinstance(converter_group, GenieTechEffectBundleGroup):
            tech_lookup_dict = internal_name_lookups.get_tech_lookups(dataset.game_version)
            obj_name = tech_lookup_dict[obj_id][0]

        else:
            civ_lookup_dict = internal_name_lookups.get_civ_lookups(dataset.game_version)
            obj_name = civ_lookup_dict[obj_id][0]

        name_lookup_dict = internal_name_lookups.get_entity_lookups(dataset.game_version)

        game_entity_name = name_lookup_dict[head_unit_id][0]

        patch_target_ref = f""{game_entity_name}.Live.Health""
        patch_target_forward_ref = ForwardRef(line, patch_target_ref)

        # Wrapper
        wrapper_name = f""Change{game_entity_name}MaxHealthWrapper""
        wrapper_ref = f""{obj_name}.{wrapper_name}""
        wrapper_location = ForwardRef(converter_group, obj_name)
        wrapper_raw_api_object = RawAPIObject(wrapper_ref,
                                              wrapper_name,
                                              dataset.nyan_api_objects,
                                              wrapper_location)
        wrapper_raw_api_object.add_raw_parent(""engine.aux.patch.Patch"")

        # Nyan patch
        nyan_patch_name = f""Change{game_entity_name}MaxHealth""
        nyan_patch_ref = f""{obj_name}.{wrapper_name}.{nyan_patch_name}""
        nyan_patch_location = ForwardRef(converter_group, wrapper_ref)
        nyan_patch_raw_api_object = RawAPIObject(nyan_patch_ref,
                                                 nyan_patch_name,
                                                 dataset.nyan_api_objects,
                                                 nyan_patch_location)
        nyan_patch_raw_api_object.add_raw_parent(""engine.aux.patch.NyanPatch"")
        nyan_patch_raw_api_object.set_patch_target(patch_target_forward_ref)

        nyan_patch_raw_api_object.add_raw_patch_member(""max_value"",
                                                       value,
                                                       ""engine.aux.attribute.AttributeSetting"",
                                                       operator)

        patch_forward_ref = ForwardRef(converter_group, nyan_patch_ref)
        wrapper_raw_api_object.add_raw_member(""patch"",
                                              patch_forward_ref,
                                              ""engine.aux.patch.Patch"")

        if team:
            wrapper_raw_api_object.add_raw_parent(""engine.aux.patch.type.DiplomaticPatch"")
            stances = [
                dataset.nyan_api_objects[""engine.aux.diplomatic_stance.type.Self""],
                dataset.pregen_nyan_objects[""aux.diplomatic_stance.types.Friendly""].get_nyan_object()
            ]
            wrapper_raw_api_object.add_raw_member(""stances"",
                                                  stances,
                                                  ""engine.aux.patch.type.DiplomaticPatch"")

        converter_group.add_raw_api_object(wrapper_raw_api_object)
        converter_group.add_raw_api_object(nyan_patch_raw_api_object)

        wrapper_forward_ref = ForwardRef(converter_group, wrapper_ref)
        patches.append(wrapper_forward_ref)

        return patches"
"    def _get_aoe2_base(cls, gamedata):
        """"""
        Create the aoe2-base modpack.
        """"""
        modpack = Modpack(""de2_base"")

        mod_def = modpack.get_info()

        mod_def.set_version(""TODO"")
        mod_def.set_uid(2000)

        mod_def.add_assets_to_load(""data/*"")

        AoCModpackSubprocessor.organize_nyan_objects(modpack, gamedata)
        AoCModpackSubprocessor.organize_media_objects(modpack, gamedata)

        return modpack","    def _get_aoe2_base(cls, gamedata):
        """"""
        Create the aoe2-base modpack.
        """"""
        modpack = Modpack(""de2-base"")

        mod_def = modpack.get_info()

        mod_def.set_version(""TODO"")
        mod_def.set_uid(2000)

        mod_def.add_assets_to_load(""data/*"")

        AoCModpackSubprocessor.organize_nyan_objects(modpack, gamedata)
        AoCModpackSubprocessor.organize_media_objects(modpack, gamedata)

        return modpack"
"    def _get_aoe1_base(cls, gamedata):
        """"""
        Create the aoe1-base modpack.
        """"""
        modpack = Modpack(""aoe1_base"")

        mod_def = modpack.get_info()

        mod_def.set_version(""1.0B"")
        mod_def.set_uid(1000)

        mod_def.add_assets_to_load(""data/*"")

        AoCModpackSubprocessor.organize_nyan_objects(modpack, gamedata)
        AoCModpackSubprocessor.organize_media_objects(modpack, gamedata)

        return modpack","    def _get_aoe1_base(cls, gamedata):
        """"""
        Create the aoe1-base modpack.
        """"""
        modpack = Modpack(""aoe1-base"")

        mod_def = modpack.get_info()

        mod_def.set_version(""1.0B"")
        mod_def.set_uid(1000)

        mod_def.add_assets_to_load(""data/*"")

        AoCModpackSubprocessor.organize_nyan_objects(modpack, gamedata)
        AoCModpackSubprocessor.organize_media_objects(modpack, gamedata)

        return modpack"
"    def _get_swgb_base(cls, gamedata):
        """"""
        Create the swgb-base modpack.
        """"""
        modpack = Modpack(""swgb_base"")

        mod_def = modpack.get_info()

        mod_def.set_version(""GOG"")
        mod_def.set_uid(5000)

        mod_def.add_assets_to_load(""data/*"")

        AoCModpackSubprocessor.organize_nyan_objects(modpack, gamedata)
        AoCModpackSubprocessor.organize_media_objects(modpack, gamedata)

        return modpack","    def _get_swgb_base(cls, gamedata):
        """"""
        Create the swgb-base modpack.
        """"""
        modpack = Modpack(""swgb-base"")

        mod_def = modpack.get_info()

        mod_def.set_version(""GOG"")
        mod_def.set_uid(5000)

        mod_def.add_assets_to_load(""data/*"")

        AoCModpackSubprocessor.organize_nyan_objects(modpack, gamedata)
        AoCModpackSubprocessor.organize_media_objects(modpack, gamedata)

        return modpack"
"    def expand_from_file(self, nyan_file):
        """"""
        Expands the tree from a nyan file.

        :param nyan_file: File with nyan objects.
        :type nyan_file: .convert.export.formats.nyan_file.NyanFile
        """"""
        current_node = self.root
        fqon = nyan_file.get_fqon()
        node_type = NodeType.FILESYS

        for node_str in fqon:
            if current_node.has_child(node_str):
                # Choose the already created node
                current_node = current_node.get_child(node_str)

            else:
                # Add a new node
                new_node = Node(node_str, node_type, current_node)
                current_node.add_child(new_node)
                current_node = new_node

        # Process fqons of the contained objects
        for nyan_object in nyan_file.nyan_objects:
            self.expand_from_object(nyan_object)","    def expand_from_file(self, nyan_file):
        """"""
        Expands the tree from a nyan file.

        :param nyan_file: File with nyan objects.
        :type nyan_file: .convert.export.formats.nyan_file.NyanFile
        """"""
        # Process fqon of the file
        current_node = self.root
        fqon = nyan_file.get_fqon()
        node_type = NodeType.FILESYS

        for node_str in fqon:
            if current_node.has_child(node_str):
                # Choose the already created node
                current_node = current_node.get_child(node_str)

            else:
                # Add a new node
                new_node = Node(node_str, node_type, current_node)
                current_node.add_child(new_node)
                current_node = new_node

        # Process fqons of the contained objects
        for nyan_object in nyan_file.nyan_objects:
            self.expand_from_object(nyan_object)"
"    def get_alias_fqon(self, fqon, namespace=None):
        """"""
        Find the (shortened) fqon by traversing the tree to the fqon node and
        then going upwards until an alias is found.

        :param fqon: Object reference for which an alias should be found.
        :type fqon: tuple
        :param namespace: Identifier of a namespace. If this is a (nested) object,
                          we check if the fqon is in the namespace before
                          searching for an alias.
        :type namespace: tuple
        """"""
        if namespace:
            current_node = self.root

            if len(namespace) <= len(fqon):
                # Check if the fqon is in the namespace by comparing their identifiers
                for index in range(len(namespace)):
                    current_node = current_node.get_child(namespace[index])

                    if namespace[index] != fqon[index]:
                        break

                else:
                    # Check if the namespace node is an object
                    if current_node.node_type in (NodeType.OBJECT, NodeType.NESTED):
                        # The object with the fqon is nested and we don't have to look
                        # up an alias
                        return (fqon[-1],)

        # Traverse the tree downwards
        current_node = self.root
        for part in fqon:
            current_node = current_node.get_child(part)

        # Traverse the tree upwards
        sfqon = []
        while current_node.depth > 0:
            if current_node.alias:
                sfqon.insert(0, current_node.alias)
                current_node.mark()
                break

            sfqon.insert(0, current_node.name)

            current_node = current_node.parent

        if not current_node.alias:
            print(fqon)

        return tuple(sfqon)","    def get_alias_fqon(self, fqon):
        """"""
        Find the (shortened) fqon by traversing the tree to the fqon node and
        then going upwards until a marked node is found.
        """"""
        # Traverse the tree downwards
        current_node = self.root
        for part in fqon:
            current_node = current_node.get_child(part)

        # Traverse the tree upwards
        sfqon = []
        while current_node.depth > 0:
            sfqon.insert(0, current_node.name)

            if current_node.alias:
                break

            current_node = current_node.parent

        return tuple(sfqon)"
"    def __init__(self, name, node_type, parent):
        """"""
        Create a node for an import tree.

        :param name: Name of the node.
        :type name: str
        :param node_type: Type of the node.
        :type node_type: NodeType
        :param parent: Parent node of this node.
        :type parent: Node
        """"""

        self.name = name
        self.node_type = node_type
        self.parent = parent

        if not self.parent and self.node_type is not NodeType.ROOT:
            raise Exception(""Only node with type ROOT are allowed to have no parent"")

        self.depth = 0
        if self.node_type is NodeType.ROOT:
            self.depth = 0

        else:
            self.depth = self.parent.depth + 1

        self.children = {}

        self.marked = False
        self.alias = """"","    def __init__(self, name, node_type, parent):
        """"""
        Create a node for an import tree.

        :param name: Name of the node.
        :type name: str
        :param node_type: Type of the node.
        :type node_type: NodeType
        :param parent: Parent node of this node.
        :type parent: Node
        """"""

        self.name = name
        self.node_type = node_type
        self.parent = parent

        if not self.parent and self.node_type is not NodeType.ROOT:
            raise Exception(""Only node with type ROOT are allowed to have no parent"")

        self.depth = 0
        if self.node_type is NodeType.ROOT:
            self.depth = 0

        else:
            self.depth = self.parent.depth + 1

        self.children = {}

        self.alias = False"
"    def mark(self):
        """"""
        Mark this node as an alias node.
        """"""
        self.marked = True","    def mark(self):
        """"""
        Mark this node as an alias node.
        """"""
        self.alias = True"
"    def unmark(self):
        """"""
        Unmark this node as an alias node.
        """"""
        self.marked = False","    def unmark(self):
        """"""
        Unmark this node as an alias node.
        """"""
        self.alias = False"
"    def _prepare_object_content(self, indent_depth, import_tree=None):
        """"""
        Returns a string containing the nyan object's content
        (members, nested objects).

        Subroutine of dump().
        """"""
        output_str = """"
        empty = True

        if len(self._inherited_members) > 0:
            for inherited_member in self._inherited_members:
                if inherited_member.has_value():
                    empty = False
                    output_str += ""%s%s\\n"" % (
                        (indent_depth + 1) * INDENT,
                        inherited_member.dump(
                            indent_depth + 1,
                            import_tree=import_tree,
                            namespace=self.get_fqon()
                        )
                    )
            if not empty:
                output_str += ""\\n""

        if len(self._members) > 0:
            empty = False
            for member in self._members:
                if self.is_patch():
                    # Patches do not need the type definition
                    output_str += ""%s%s\\n"" % (
                        (indent_depth + 1) * INDENT,
                        member.dump_short(
                            indent_depth + 1,
                            import_tree=import_tree,
                            namespace=self.get_fqon()
                        )
                    )
                else:
                    output_str += ""%s%s\\n"" % (
                        (indent_depth + 1) * INDENT,
                        member.dump(
                            indent_depth + 1,
                            import_tree=import_tree,
                            namespace=self.get_fqon()
                        )
                    )

            output_str += ""\\n""

        # Nested objects
        if len(self._nested_objects) > 0:
            empty = False
            for nested_object in self._nested_objects:
                output_str += ""%s%s"" % (
                    (indent_depth + 1) * INDENT,
                    nested_object.dump(
                        indent_depth + 1,
                        import_tree=import_tree
                    )
                )

            output_str += """"

        # Empty objects need a 'pass' line
        if empty:
            output_str += f""{(indent_depth + 1) * INDENT}pass\\n\\n""

        return output_str","    def _prepare_object_content(self, indent_depth, import_tree=None):
        """"""
        Returns a string containing the nyan object's content
        (members, nested objects).

        Subroutine of dump().
        """"""
        output_str = """"
        empty = True

        if len(self._inherited_members) > 0:
            for inherited_member in self._inherited_members:
                if inherited_member.has_value():
                    empty = False
                    output_str += ""%s%s\\n"" % (
                        (indent_depth + 1) * INDENT,
                        inherited_member.dump(import_tree=import_tree)
                    )
            if not empty:
                output_str += ""\\n""

        if len(self._members) > 0:
            empty = False
            for member in self._members:
                if self.is_patch():
                    # Patches do not need the type definition
                    output_str += ""%s%s\\n"" % (
                        (indent_depth + 1) * INDENT,
                        member.dump_short(import_tree=import_tree)
                    )
                else:
                    output_str += ""%s%s\\n"" % (
                        (indent_depth + 1) * INDENT,
                        member.dump(import_tree=import_tree)
                    )

            output_str += ""\\n""

        # Nested objects
        if len(self._nested_objects) > 0:
            empty = False
            for nested_object in self._nested_objects:
                output_str += ""%s%s"" % (
                    (indent_depth + 1) * INDENT,
                    nested_object.dump(
                        indent_depth + 1,
                        import_tree
                    )
                )

            output_str += """"

        # Empty objects need a 'pass' line
        if empty:
            output_str += f""{(indent_depth + 1) * INDENT}pass\\n\\n""

        return output_str"
"    def _prepare_inheritance_content(self, import_tree=None):
        """"""
        Returns a string containing the nyan object's inheritance set
        in the header.

        Subroutine of dump().
        """"""
        output_str = ""(""

        if len(self._parents) > 0:
            for parent in self._parents:
                if import_tree:
                    sfqon = ""."".join(import_tree.get_alias_fqon(
                        parent.get_fqon(),
                        namespace=self.get_fqon()
                    ))

                else:
                    sfqon = ""."".join(parent.get_fqon())

                output_str += f""{sfqon}, ""

            output_str = output_str[:-2]

        output_str += ""):\\n""

        return output_str","    def _prepare_inheritance_content(self, import_tree=None):
        """"""
        Returns a string containing the nyan object's inheritance set
        in the header.

        Subroutine of dump().
        """"""
        output_str = ""(""

        if len(self._parents) > 0:
            for parent in self._parents:
                if import_tree:
                    sfqon = ""."".join(import_tree.get_alias_fqon(parent.get_fqon()))

                else:
                    sfqon = ""."".join(parent.get_fqon())

                output_str += f""{sfqon}, ""

            output_str = output_str[:-2]

        output_str += ""):\\n""

        return output_str"
"    def dump(self, indent_depth, import_tree=None, namespace=None):
        """"""
        Returns the nyan string representation of the member.
        """"""
        output_str = f""{self.name}""

        type_str = """"

        if isinstance(self._member_type, NyanObject):
            if import_tree:
                sfqon = ""."".join(import_tree.get_alias_fqon(
                    self._member_type.get_fqon(),
                    namespace
                ))

            else:
                sfqon = ""."".join(self._member_type.get_fqon())

            type_str = sfqon

        else:
            type_str = self._member_type.value

        if self._optional:
            output_str += f"" : optional({type_str})""

        else:
            output_str += f"" : {type_str}""

        if self.is_complex():
            if isinstance(self._set_type, NyanObject):
                if import_tree:
                    sfqon = ""."".join(import_tree.get_alias_fqon(
                        self._set_type.get_fqon(),
                        namespace
                    ))

                else:
                    sfqon = ""."".join(self._set_type.get_fqon())

                output_str += f""({sfqon})""

            else:
                output_str += f""({self._set_type.value})""

        if self.is_initialized():
            output_str += "" %s%s %s"" % (""@"" * self._override_depth,
                                        self._operator.value,
                                        self._get_str_representation(
                                            indent_depth,
                                            import_tree=import_tree,
                                            namespace=namespace
                                        ))

        return output_str","    def dump(self, import_tree=None):
        """"""
        Returns the nyan string representation of the member.
        """"""
        output_str = f""{self.name}""

        type_str = """"

        if isinstance(self._member_type, NyanObject):
            if import_tree:
                sfqon = ""."".join(import_tree.get_alias_fqon(self._member_type.get_fqon()))

            else:
                sfqon = ""."".join(self._member_type.get_fqon())

            type_str = sfqon

        else:
            type_str = self._member_type.value

        if self._optional:
            output_str += f"" : optional({type_str})""

        else:
            output_str += f"" : {type_str}""

        if self.is_complex():
            if isinstance(self._set_type, NyanObject):
                if import_tree:
                    sfqon = ""."".join(import_tree.get_alias_fqon(self._set_type.get_fqon()))

                else:
                    sfqon = ""."".join(self._set_type.get_fqon())

                output_str += f""({sfqon})""

            else:
                output_str += f""({self._set_type.value})""

        if self.is_initialized():
            output_str += "" %s%s %s"" % (""@"" * self._override_depth,
                                        self._operator.value,
                                        self._get_str_representation(import_tree=import_tree))

        return output_str"
"    def dump_short(self, indent_depth, import_tree=None, namespace=None):
        """"""
        Returns the nyan string representation of the member, but
        without the type definition.
        """"""
        return ""%s %s%s %s"" % (
            self.get_name(),
            ""@"" * self._override_depth,
            self._operator.value,
            self._get_str_representation(
                indent_depth,
                import_tree=import_tree,
                namespace=namespace
            )
        )","    def dump_short(self, import_tree=None):
        """"""
        Returns the nyan string representation of the member, but
        without the type definition.
        """"""
        return ""%s %s%s %s"" % (self.get_name(),
                               ""@"" * self._override_depth,
                               self._operator.value,
                               self._get_str_representation(import_tree=import_tree))"
"    def _get_primitive_value_str(self, member_type, value, import_tree=None, namespace=None):
        """"""
        Returns the nyan string representation of primitive values.

        Subroutine of _get_str_representation()
        """"""
        if member_type in (MemberType.TEXT, MemberType.FILE):
            return f""\\""{value}\\""""

        elif isinstance(member_type, NyanObject):
            if import_tree:
                sfqon = ""."".join(import_tree.get_alias_fqon(
                    value.get_fqon(),
                    namespace=namespace
                ))

            else:
                sfqon = ""."".join(value.get_fqon())

            return sfqon

        return f""{value}""","    def _get_primitive_value_str(self, member_type, value, import_tree=None):
        """"""
        Returns the nyan string representation of primitive values.

        Subroutine of _get_str_representation()
        """"""
        if member_type is MemberType.FLOAT:
            return f""{value}f""

        elif member_type in (MemberType.TEXT, MemberType.FILE):
            return f""\\""{value}\\""""

        elif isinstance(member_type, NyanObject):
            if import_tree:
                sfqon = ""."".join(import_tree.get_alias_fqon(value.get_fqon()))

            else:
                sfqon = ""."".join(value.get_fqon())

            return sfqon

        return f""{value}"""
"    def _get_str_representation(self, indent_depth, import_tree=None, namespace=None):
        """"""
        Returns the nyan string representation of the value.
        """"""
        if not self.is_initialized():
            return f""UNINITIALIZED VALUE {self.__repr__()}""

        if self._optional and self.value is MemberSpecialValue.NYAN_NONE:
            return MemberSpecialValue.NYAN_NONE.value

        if self.value is MemberSpecialValue.NYAN_INF:
            return MemberSpecialValue.NYAN_INF.value

        if self._member_type in (MemberType.INT, MemberType.FLOAT,
                                 MemberType.TEXT, MemberType.FILE,
                                 MemberType.BOOLEAN):
            return self._get_primitive_value_str(
                self._member_type,
                self.value,
                import_tree=import_tree,
                namespace=namespace
            )

        elif self._member_type in (MemberType.SET, MemberType.ORDEREDSET):
            return self._get_complex_value_str(
                indent_depth,
                self._member_type,
                self.value,
                import_tree=import_tree,
                namespace=namespace
            )

        elif isinstance(self._member_type, NyanObject):
            if import_tree:
                sfqon = ""."".join(import_tree.get_alias_fqon(
                    self.value.get_fqon(),
                    namespace
                ))

            else:
                sfqon = ""."".join(self.value.get_fqon())

            return sfqon

        else:
            raise Exception(f""{self.__repr__()} has no valid type"")","    def _get_str_representation(self, import_tree=None):
        """"""
        Returns the nyan string representation of the value.
        """"""
        if not self.is_initialized():
            return f""UNINITIALIZED VALUE {self.__repr__()}""

        if self._optional and self.value is MemberSpecialValue.NYAN_NONE:
            return MemberSpecialValue.NYAN_NONE.value

        if self.value is MemberSpecialValue.NYAN_INF:
            return MemberSpecialValue.NYAN_INF.value

        if self._member_type in (MemberType.INT, MemberType.FLOAT,
                                 MemberType.TEXT, MemberType.FILE,
                                 MemberType.BOOLEAN):
            return self._get_primitive_value_str(self._member_type,
                                                 self.value,
                                                 import_tree=import_tree)

        elif self._member_type in (MemberType.SET, MemberType.ORDEREDSET):
            output_str = """"

            if self._member_type is MemberType.ORDEREDSET:
                output_str += ""o""

            output_str += ""{""

            if len(self.value) > 0:
                for val in self.value:
                    output_str += ""%s, "" % self._get_primitive_value_str(
                        self._set_type,
                        val,
                        import_tree=import_tree
                    )

                return output_str[:-2] + ""}""

            return output_str + ""}""

        elif isinstance(self._member_type, NyanObject):
            if import_tree:
                sfqon = ""."".join(import_tree.get_alias_fqon(self.value.get_fqon()))

            else:
                sfqon = ""."".join(self.value.get_fqon())

            return sfqon

        else:
            raise Exception(f""{self.__repr__()} has no valid type"")"
"    def dump(self, indent_depth, import_tree=None, namespace=None):
        """"""
        Returns the string representation of the member.
        """"""
        return self.dump_short(indent_depth, import_tree=import_tree, namespace=namespace)","    def dump(self, import_tree=None):
        """"""
        Returns the string representation of the member.
        """"""
        return self.dump_short(import_tree=import_tree)"
"    def dump_short(self, indent_depth, import_tree=None, namespace=None):
        """"""
        Returns the nyan string representation of the member, but
        without the type definition.
        """"""
        return ""%s %s%s %s"" % (
            self.get_name_with_origin(),
            ""@"" * self._override_depth,
            self._operator.value,
            self._get_str_representation(
                indent_depth,
                import_tree=import_tree,
                namespace=namespace
            )
        )","    def dump_short(self, import_tree=None):
        """"""
        Returns the nyan string representation of the member, but
        without the type definition.
        """"""
        return ""%s %s%s %s"" % (self.get_name_with_origin(),
                               ""@"" * self._override_depth,
                               self._operator.value,
                               self._get_str_representation(import_tree=import_tree))"
"def convert_assets(assets, args, srcdir=None, prev_source_dir_path=None):
    """"""
    Perform asset conversion.

    Requires original assets and stores them in usable and free formats.

    assets must be a filesystem-like object pointing at the game's asset dir.
    srcdir must be None, or point at some source directory.

    If gen_extra_files is True, some more files, mostly for debugging purposes,
    are created.

    This method prepares srcdir and targetdir to allow a pleasant, unified
    conversion experience, then passes them to .driver.convert().
    """"""
    # acquire conversion source directory
    if srcdir is None:
        srcdir = acquire_conversion_source_dir(prev_source_dir_path)

    converted_path = assets / ""converted""
    converted_path.mkdirs()
    targetdir = DirectoryCreator(converted_path).root

    # Set compression level for media output if it was not set
    if ""compression_level"" not in vars(args):
        args.compression_level = 1

    # Set verbosity for debug output
    if ""debug_info"" not in vars(args):
        if args.devmode:
            args.debug_info = 3

        else:
            args.debug_info = 0

    # add a dir for debug info
    debug_log_path = converted_path / ""debug"" / datetime.now().strftime(""%Y-%m-%d-%H-%M-%S"")
    debugdir = DirectoryCreator(debug_log_path).root
    args.debugdir = AccessSynchronizer(debugdir).root

    # Create CLI args info
    debug_cli_args(args.debugdir, args.debug_info, args)

    # Initialize game versions data
    auxiliary_files_dir = args.cfg_dir / ""converter"" / ""games""
    args.avail_game_eds, args.avail_game_exps = create_version_objects(auxiliary_files_dir)

    # Acquire game version info
    args.game_version = get_game_version(srcdir, args.avail_game_eds, args.avail_game_exps)
    debug_game_version(args.debugdir, args.debug_info, args)

    # Mount assets into conversion folder
    data_dir = mount_asset_dirs(srcdir, args.game_version)
    if not data_dir:
        return None

    # make srcdir and targetdir safe for threaded conversion
    args.srcdir = AccessSynchronizer(data_dir).root
    args.targetdir = AccessSynchronizer(targetdir).root

    # Create mountpoint info
    debug_mounts(args.debugdir, args.debug_info, args)

    def flag(name):
        """"""
        Convenience function for accessing boolean flags in args.
        Flags default to False if they don't exist.
        """"""
        return getattr(args, name, False)

    args.flag = flag

    # import here so codegen.py doesn't depend on it.
    from .tool.driver import convert

    converted_count = 0
    total_count = None
    for current_item in convert(args):
        if isinstance(current_item, int):
            # convert is informing us about the estimated number of remaining
            # items.
            total_count = current_item + converted_count
            continue

        # TODO a GUI would be nice here.

        if total_count is None:
            info(""[%s] %s"", converted_count, current_item)
        else:
            info(""[%s] %s"", format_progress(converted_count, total_count), current_item)

        converted_count += 1

    # clean args
    del args.srcdir
    del args.targetdir

    return data_dir.resolve_native_path()","def convert_assets(assets, args, srcdir=None, prev_source_dir_path=None):
    """"""
    Perform asset conversion.

    Requires original assets and stores them in usable and free formats.

    assets must be a filesystem-like object pointing at the game's asset dir.
    srcdir must be None, or point at some source directory.

    If gen_extra_files is True, some more files, mostly for debugging purposes,
    are created.

    This method prepares srcdir and targetdir to allow a pleasant, unified
    conversion experience, then passes them to .driver.convert().
    """"""
    # acquire conversion source directory
    if srcdir is None:
        srcdir = acquire_conversion_source_dir(prev_source_dir_path)

    converted_path = assets / ""converted""
    converted_path.mkdirs()
    targetdir = DirectoryCreator(converted_path).root

    # add a dir for debug info
    debug_log_path = converted_path / ""debug"" / datetime.now().strftime(""%Y-%m-%d-%H-%M-%S"")
    debugdir = DirectoryCreator(debug_log_path).root
    args.debugdir = AccessSynchronizer(debugdir).root

    # Create CLI args info
    debug_cli_args(args.debugdir, args.debug_info, args)

    # Initialize game versions data
    auxiliary_files_dir = args.cfg_dir / ""converter"" / ""games""
    args.avail_game_eds, args.avail_game_exps = create_version_objects(auxiliary_files_dir)

    # Acquire game version info
    args.game_version = get_game_version(srcdir, args.avail_game_eds, args.avail_game_exps)
    debug_game_version(args.debugdir, args.debug_info, args)

    # Mount assets into conversion folder
    data_dir = mount_asset_dirs(srcdir, args.game_version)
    if not data_dir:
        return None

    # make srcdir and targetdir safe for threaded conversion
    args.srcdir = AccessSynchronizer(data_dir).root
    args.targetdir = AccessSynchronizer(targetdir).root

    # Create mountpoint info
    debug_mounts(args.debugdir, args.debug_info, args)

    def flag(name):
        """"""
        Convenience function for accessing boolean flags in args.
        Flags default to False if they don't exist.
        """"""
        return getattr(args, name, False)

    args.flag = flag

    # import here so codegen.py doesn't depend on it.
    from .tool.driver import convert

    converted_count = 0
    total_count = None
    for current_item in convert(args):
        if isinstance(current_item, int):
            # convert is informing us about the estimated number of remaining
            # items.
            total_count = current_item + converted_count
            continue

        # TODO a GUI would be nice here.

        if total_count is None:
            info(""[%s] %s"", converted_count, current_item)
        else:
            info(""[%s] %s"", format_progress(converted_count, total_count), current_item)

        converted_count += 1

    # clean args
    del args.srcdir
    del args.targetdir

    return data_dir.resolve_native_path()"
"def main(args, error):
    """""" CLI entry point """"""
    del error  # unused

    # initialize libopenage
    from ..cppinterface.setup import setup
    setup(args)

    # conversion source
    if args.source_dir is not None:
        srcdir = CaseIgnoringDirectory(args.source_dir).root
    else:
        srcdir = None

    # mount the config folder at ""cfg/""
    from ..cvar.location import get_config_path
    from ..util.fslike.union import Union
    root = Union().root
    root[""cfg""].mount(get_config_path())
    args.cfg_dir = root[""cfg""]

    if args.interactive:
        interactive_browser(root[""cfg""], srcdir)
        return 0

    # conversion target
    from ..assets import get_asset_path
    outdir = get_asset_path(args.output_dir)

    if args.force or wanna_convert() or conversion_required(outdir, args):
        if not convert_assets(outdir, args, srcdir):
            return 1
    else:
        print(""assets are up to date; no conversion is required."")
        print(""override with --force."")

    return 0","def main(args, error):
    """""" CLI entry point """"""
    del error  # unused

    # initialize libopenage
    from ..cppinterface.setup import setup
    setup(args)

    # conversion source
    if args.source_dir is not None:
        srcdir = CaseIgnoringDirectory(args.source_dir).root
    else:
        srcdir = None

    # Set verbosity for debug output
    if not args.debug_info:
        if args.devmode:
            args.debug_info = 3

        else:
            args.debug_info = 0

    # mount the config folder at ""cfg/""
    from ..cvar.location import get_config_path
    from ..util.fslike.union import Union
    root = Union().root
    root[""cfg""].mount(get_config_path())
    args.cfg_dir = root[""cfg""]

    if args.interactive:
        interactive_browser(root[""cfg""], srcdir)
        return 0

    # conversion target
    from ..assets import get_asset_path
    outdir = get_asset_path(args.output_dir)

    if args.force or wanna_convert() or conversion_required(outdir, args):
        if not convert_assets(outdir, args, srcdir):
            return 1
    else:
        print(""assets are up to date; no conversion is required."")
        print(""override with --force."")

    return 0"
"def main(args, error):
    """"""
    Makes sure that the assets have been converted,
    and jumps into the C++ main method.
    """"""
    del error  # unused

    # we have to import stuff inside the function
    # as it depends on generated/compiled code
    from .main_cpp import run_game
    from .. import config
    from ..assets import get_asset_path
    from ..convert.main import conversion_required, convert_assets
    from ..cppinterface.setup import setup as cpp_interface_setup
    from ..cvar.location import get_config_path
    from ..util.fslike.union import Union

    # initialize libopenage
    cpp_interface_setup(args)

    info(""launching openage %s"", config.VERSION)
    info(""compiled by %s"", config.COMPILER)

    if config.DEVMODE:
        info(""running in DEVMODE"")

    # create virtual file system for data paths
    root = Union().root

    # mount the assets folder union at ""assets/""
    root[""assets""].mount(get_asset_path(args.asset_dir))

    # mount the config folder at ""cfg/""
    root[""cfg""].mount(get_config_path(args.cfg_dir))
    args.cfg_dir = root[""cfg""]

    # ensure that the assets have been converted
    if wanna_convert() or conversion_required(root[""assets""], args):
        # try to get previously used source dir
        asset_location_path = root[""cfg""] / ""asset_location""
        try:
            with asset_location_path.open(""r"") as file_obj:
                prev_source_dir_path = file_obj.read().strip()
        except FileNotFoundError:
            prev_source_dir_path = None
        used_asset_path = convert_assets(
            root[""assets""],
            args,
            prev_source_dir_path=prev_source_dir_path
        )

        if used_asset_path:
            # Remember the asset location
            with asset_location_path.open(""wb"") as file_obj:
                file_obj.write(used_asset_path)
        else:
            err(""game asset conversion failed"")
            return 1

    # Exit here with an explanation because the converted assets are incompatible!
    # Remove this when the gamestate works again
    info(""Generated nyan assets are not yet compatible to the engine."")
    info(""Please revert to release v0.4.1 if you want to test the previous working gamestate."")
    info(""Exiting..."")
    import sys
    sys.exit()

    # start the game, continue in main_cpp.pyx!
    return run_game(args, root)","def main(args, error):
    """"""
    Makes sure that the assets have been converted,
    and jumps into the C++ main method.
    """"""
    del error  # unused

    # we have to import stuff inside the function
    # as it depends on generated/compiled code
    from .main_cpp import run_game
    from .. import config
    from ..assets import get_asset_path
    from ..convert.main import conversion_required, convert_assets
    from ..cppinterface.setup import setup as cpp_interface_setup
    from ..cvar.location import get_config_path
    from ..util.fslike.union import Union

    # initialize libopenage
    cpp_interface_setup(args)

    info(""launching openage %s"", config.VERSION)
    info(""compiled by %s"", config.COMPILER)

    if config.DEVMODE:
        info(""running in DEVMODE"")

    # create virtual file system for data paths
    root = Union().root

    # mount the assets folder union at ""assets/""
    root[""assets""].mount(get_asset_path(args.asset_dir))

    # mount the config folder at ""cfg/""
    root[""cfg""].mount(get_config_path(args.cfg_dir))

    # ensure that the assets have been converted
    if wanna_convert() or conversion_required(root[""assets""], args):
        # try to get previously used source dir
        asset_location_path = root[""cfg""] / ""asset_location""
        try:
            with asset_location_path.open(""r"") as file_obj:
                prev_source_dir_path = file_obj.read().strip()
        except FileNotFoundError:
            prev_source_dir_path = None
        used_asset_path = convert_assets(root[""assets""], root[""cfg""], args,
                                         prev_source_dir_path=prev_source_dir_path)
        if used_asset_path:
            # Remember the asset location
            with asset_location_path.open(""wb"") as file_obj:
                file_obj.write(used_asset_path)
        else:
            err(""game asset conversion failed"")
            return 1

    # start the game, continue in main_cpp.pyx!
    return run_game(args, root)"
"def main():
    """""" CLI entry point """"""
    args = parse_args()
    cppname = ""libopenage""
    cppdir = Path(cppname).absolute()
    out_cppdir = Path(args.output_dir) / cppname

    if args.verbose:
        hdr_count = len(args.all_files)
        plural = ""s"" if hdr_count > 1 else """"

        print(""extracting pxd information ""
              ""from {} header{}..."".format(hdr_count, plural))

    for filename in args.all_files:
        filename = Path(filename).resolve()
        if cppdir not in filename.parents:
            print(""pxdgen source file is not in "" + cppdir + "": "" + filename)
            sys.exit(1)

        # join out_cppdir with relative path from cppdir
        pxdfile_relpath = filename.with_suffix('.pxd').relative_to(cppdir)
        pxdfile = out_cppdir / pxdfile_relpath

        if args.verbose:
            print(""creating '{}' for '{}':"".format(pxdfile, filename))

        generator = PXDGenerator(filename)

        result = generator.generate(
            pxdfile,
            ignore_timestamps=args.ignore_timestamps,
            print_warnings=True
        )

        if args.verbose and not result:
            print(""nothing done."")

        # create empty __init__.py in all parent directories.
        # Cython requires this; else it won't find the .pxd files.
        for dirname in pxdfile_relpath.parents:
            template = out_cppdir / dirname / ""__init__""
            for extension in (""py"", ""pxd""):
                initfile = template.with_suffix(""."" + extension)
                if not initfile.exists():
                    print(""\\x1b[36mpxdgen: create package index %s\\x1b[0m"" % (
                        initfile.relative_to(args.output_dir)))

                    initfile.touch()","def main():
    """""" CLI entry point """"""
    args = parse_args()
    cppname = ""libopenage""
    cppdir = Path(cppname).absolute()
    out_cppdir = Path(args.output_dir) / cppname

    if args.verbose:
        hdr_count = len(args.all_files)
        plural = ""s"" if hdr_count > 1 else """"

        print(""extracting pxd information ""
              ""from {} header{}..."".format(hdr_count, plural))

    for filename in args.all_files:
        filename = Path(filename).resolve()
        if cppdir not in filename.parents:
            print(""pxdgen source file is not in "" + cppdir + "": "" + filename)
            sys.exit(1)

        # join out_cppdir with relative path from cppdir
        pxdfile_relpath = filename.with_suffix('.pxd').relative_to(cppdir)
        pxdfile = out_cppdir / pxdfile_relpath

        if args.verbose:
            print(""creating '{}' for '{}':"".format(pxdfile, filename))

        generator = PXDGenerator(filename)

        result = generator.generate(
            pxdfile,
            ignore_timestamps=args.ignore_timestamps,
            print_warnings=True
        )

        if args.verbose and not result:
            print(""nothing done."")

        # create empty __init__.py in all parent directories.
        # Cython requires this; else it won't find the .pxd files.
        for dirname in pxdfile_relpath.parents:
            template = out_cppdir / dirname / ""__init__""
            for extension in (""py"", ""pxd""):
                initfile = template.with_suffix(""."" + extension)
                if not initfile.exists():
                    print(""\\x1b[36mpxdgen: create package index %s\\x1b[0m"" % (
                        initfile.relative_to(CWD)))

                    initfile.touch()"
"def main(argv=None):
    """""" Top-level argparsing; invokes subparser for all submodules. """"""
    cli = argparse.ArgumentParser(
        ""openage"",
        description=(""free age of empires II engine clone"")
    )

    cli.add_argument(""--version"", ""-V"", nargs=0, action=PrintVersion,
                     help=""print version info and exit"")

    # shared arguments for all subcommands
    global_cli = argparse.ArgumentParser(add_help=False)
    global_cli.add_argument(""--verbose"", ""-v"", action='count',
                            default=ENV_VERBOSITY,
                            help=""increase verbosity"")
    global_cli.add_argument(""--quiet"", ""-q"", action='count', default=0,
                            help=""decrease verbosity"")
    global_cli.add_argument(""--devmode"", action=""store_true"",
                            help=""force-enable development mode"")
    global_cli.add_argument(""--no-devmode"", action=""store_true"",
                            help=""force-disable devlopment mode"")
    global_cli.add_argument(""--trap-exceptions"", action=""store_true"",
                            help=(""upon throwing an exception a debug break is ""
                                  ""triggered. this will crash openage if no ""
                                  ""debugger is present""))

    # shared directory arguments for most subcommands
    cfg_cli = argparse.ArgumentParser(add_help=False)

    cfg_cli.add_argument(""--asset-dir"",
                         help=""Use this as an additional asset directory."")
    cfg_cli.add_argument(""--cfg-dir"",
                         help=""Use this as an additional config directory."")

    subparsers = cli.add_subparsers(dest=""subcommand"")

    # enable reimports for ""init_subparser""
    # pylint: disable=reimported

    from .game.main import init_subparser
    game_cli = subparsers.add_parser(
        ""game"",
        parents=[global_cli, cfg_cli])
    init_subparser(game_cli)

    from .testing.main import init_subparser
    init_subparser(subparsers.add_parser(
        ""test"",
        parents=[global_cli, cfg_cli]))

    from .convert.main import init_subparser
    init_subparser(subparsers.add_parser(
        ""convert"",
        parents=[global_cli]))

    from .convert.singlefile import init_subparser
    init_subparser(subparsers.add_parser(
        ""convert-file"",
        parents=[global_cli]))

    from .codegen.main import init_subparser
    init_subparser(subparsers.add_parser(
        ""codegen"",
        parents=[global_cli]))

    args = cli.parse_args(argv)

    if not args.subcommand:
        # the user didn't specify a subcommand. default to 'game'.
        args = game_cli.parse_args(argv)

    # process the shared args
    set_loglevel(verbosity_to_level(args.verbose - args.quiet))

    if args.no_devmode and args.devmode:
        cli.error(""can't force enable and disable devmode at the same time"")

    try:
        from . import config

        if args.no_devmode:
            config.DEVMODE = False
        if args.devmode:
            config.DEVMODE = True
    except ImportError:
        if args.no_devmode or args.devmode:
            print(""code was not yet generated, ignoring devmode activation request"")

    if ""asset_dir"" in args and args.asset_dir:
        if not os.path.exists(args.asset_dir):
            cli.error(""asset directory does not exist: "" + args.asset_dir)

    # call the entry point for the subcommand.
    return args.entrypoint(args, cli.error)","def main(argv=None):
    """""" Top-level argparsing; invokes subparser for all submodules. """"""
    cli = argparse.ArgumentParser(
        ""openage"",
        description=(""free age of empires II engine clone"")
    )

    cli.add_argument(""--version"", ""-V"", nargs=0, action=PrintVersion,
                     help=""print version info and exit"")

    # shared arguments for all subcommands
    global_cli = argparse.ArgumentParser(add_help=False)
    global_cli.add_argument(""--verbose"", ""-v"", action='count',
                            default=ENV_VERBOSITY,
                            help=""increase verbosity"")
    global_cli.add_argument(""--quiet"", ""-q"", action='count', default=0,
                            help=""decrease verbosity"")
    global_cli.add_argument(""--devmode"", action=""store_true"",
                            help=""force-enable development mode"")
    global_cli.add_argument(""--no-devmode"", action=""store_true"",
                            help=""force-disable devlopment mode"")
    global_cli.add_argument(""--trap-exceptions"", action=""store_true"",
                            help=(""upon throwing an exception a debug break is ""
                                  ""triggered. this will crash openage if no ""
                                  ""debugger is present""))

    # shared directory arguments for most subcommands
    cfg_cli = argparse.ArgumentParser(add_help=False)

    cfg_cli.add_argument(""--asset-dir"",
                         help=""Use this as an additional asset directory."")
    cfg_cli.add_argument(""--cfg-dir"",
                         help=""Use this as an additional config directory."")

    subparsers = cli.add_subparsers(dest=""subcommand"")

    # enable reimports for ""init_subparser""
    # pylint: disable=reimported

    from .game.main import init_subparser
    game_cli = subparsers.add_parser(
        ""game"",
        parents=[global_cli, cfg_cli])
    init_subparser(game_cli)

    from .testing.main import init_subparser
    init_subparser(subparsers.add_parser(
        ""test"",
        parents=[global_cli, cfg_cli]))

    from .convert.main import init_subparser
    init_subparser(subparsers.add_parser(
        ""convert"",
        parents=[global_cli, cfg_cli]))

    from .convert.singlefile import init_subparser
    init_subparser(subparsers.add_parser(
        ""convert-file"",
        parents=[global_cli, cfg_cli]))

    from .codegen.main import init_subparser
    init_subparser(subparsers.add_parser(
        ""codegen"",
        parents=[global_cli]))

    args = cli.parse_args(argv)

    if not args.subcommand:
        # the user didn't specify a subcommand. default to 'game'.
        args = game_cli.parse_args(argv)

    # process the shared args
    set_loglevel(verbosity_to_level(args.verbose - args.quiet))

    if args.no_devmode and args.devmode:
        cli.error(""can't force enable and disable devmode at the same time"")

    try:
        from . import config

        if args.no_devmode:
            config.DEVMODE = False
        if args.devmode:
            config.DEVMODE = True
    except ImportError:
        if args.no_devmode or args.devmode:
            print(""code was not yet generated, ignoring devmode activation request"")

    if ""asset_dir"" in args and args.asset_dir:
        if not os.path.exists(args.asset_dir):
            cli.error(""directory does not exist: "" + args.asset_dir)

    # call the entry point for the subcommand.
    return args.entrypoint(args, cli.error)"
"def get_asset_path(custom_asset_dir=None):
    """"""
    Returns a Path object for the game assets.

    `custom_asset_dir` can a custom asset directory, which is mounted at the
    top of the union filesystem (i.e. has highest priority).

    This function is used by the both the conversion process
    and the game startup. The conversion uses it for its output,
    the game as its data source(s).
    """"""

    # if we're in devmode, use only the in-repo asset folder
    if not custom_asset_dir and config.DEVMODE:
        return Directory(os.path.join(config.BUILD_SRC_DIR, ""assets"")).root

    # else, mount the possible locations in an union:
    # overlay the global dir and the user dir.
    result = Union().root

    # the cmake-determined folder for storing assets
    global_data = Path(config.GLOBAL_ASSET_DIR)
    if global_data.is_dir():
        result.mount(WriteBlocker(Directory(global_data).root).root)

    # user-data directory as provided by environment variables
    # and platform standards
    # we always create this!
    home_data = default_dirs.get_dir(""data_home"") / ""openage""
    result.mount(
        Directory(
            home_data,
            create_if_missing=True
        ).root / ""assets""
    )

    # the program argument overrides it all
    if custom_asset_dir:
        result.mount(Directory(custom_asset_dir).root)

    return result","def get_asset_path(args):
    """"""
    Returns a Path object for the game assets.

    args are the arguments, as provided by the CLI's ArgumentParser.
    """"""

    # if we're in devmode, use only the build source asset folder
    if not args.asset_dir and config.DEVMODE:
        return Directory(os.path.join(config.BUILD_SRC_DIR, ""assets"")).root

    # else, mount the possible locations in an union:
    # overlay the global dir and the user dir.
    result = Union().root

    # the cmake-determined folder for storing assets
    global_data = Path(config.GLOBAL_ASSET_DIR)
    if global_data.is_dir():
        result.mount(WriteBlocker(Directory(global_data).root).root)

    # user-data directory as provided by environment variables
    # and platform standards
    # we always create this!
    home_data = default_dirs.get_dir(""data_home"") / ""openage""
    result.mount(
        Directory(
            home_data,
            create_if_missing=True
        ).root / ""assets""
    )

    # the program argument overrides it all
    if args.asset_dir:
        result.mount(Directory(args.asset_dir).root)

    return result"
"def interactive_browser(srcdir=None):
    """"""
    launch an interactive view for browsing the original
    archives.
    """"""

    info(""launching interactive data browser..."")

    # the variables are actually used, in the interactive prompt.
    # pylint: disable=unused-variable
    data, game_versions = mount_input(srcdir)

    if not data:
        warn(""cannot launch browser as no valid input assets were found."")
        return

    def save(path, target):
        """"""
        save a path to a custom target
        """"""
        with path.open(""rb"") as infile:
            with open(target, ""rb"") as outfile:
                outfile.write(infile.read())

    def save_slp(path, target, palette=None):
        """"""
        save a slp as png.
        """"""
        from .texture import Texture
        from .slp import SLP
        from .driver import get_palette

        if not palette:
            palette = get_palette(data)

        with path.open(""rb"") as slpfile:
            tex = Texture(SLP(slpfile.read()), palette)

            out_path, filename = os.path.split(target)
            tex.save(Directory(out_path).root, filename)

    import code
    from pprint import pprint

    import rlcompleter

    completer = rlcompleter.Completer(locals())
    readline.parse_and_bind(""tab: complete"")
    readline.parse_and_bind(""set show-all-if-ambiguous on"")
    readline.set_completer(completer.complete)

    code.interact(
        banner=(""\\nuse `pprint` for beautiful output!\\n""
                ""you can access stuff by the `data` variable!\\n""
                ""`data` is an openage.util.fslike.path.Path!\\n\\n""
                ""* version detection:   pprint(game_versions)\\n""
                ""* list contents:       pprint(list(data['graphics'].list()))\\n""
                ""* dump data:           save(data['file/path'], '/tmp/outputfile')\\n""
                ""* save a slp as png:   save_slp(data['dir/123.slp'], '/tmp/pic.png')\\n""),
        local=locals()
    )","def interactive_browser(srcdir=None):
    """"""
    launch an interactive view for browsing the original
    archives.
    """"""

    info(""launching interactive data browser..."")

    # the variable is actually used, in the interactive prompt.
    # pylint: disable=unused-variable
    data, game_versions = mount_input(srcdir)

    if not data:
        warn(""cannot launch browser as no valid input assets were found."")
        return

    def save(path, target):
        """"""
        save a path to a custom target
        """"""
        with path.open(""rb"") as infile:
            with open(target, ""rb"") as outfile:
                outfile.write(infile.read())

    def save_slp(path, target, palette=None):
        """"""
        save a slp as png.
        """"""
        from .texture import Texture
        from .slp import SLP
        from .driver import get_palette

        if not palette:
            palette = get_palette(data, game_versions)

        with path.open(""rb"") as slpfile:
            tex = Texture(SLP(slpfile.read()), palette)

            out_path, filename = os.path.split(target)
            tex.save(Directory(out_path).root, filename)

    import code
    from pprint import pprint

    import rlcompleter

    completer = rlcompleter.Completer(locals())
    readline.parse_and_bind(""tab: complete"")
    readline.parse_and_bind(""set show-all-if-ambiguous on"")
    readline.set_completer(completer.complete)

    code.interact(
        banner=(""\\nuse `pprint` for beautiful output!\\n""
                ""you can access stuff by the `data` variable!\\n""
                ""`data` is an openage.util.fslike.path.Path!\\n""
                ""* list contents:      `pprint(list(data['graphics'].list()))`\\n""
                ""* dump data:          `save(data['file/path'], '/tmp/outputfile')`.\\n""
                ""* save a slp as png:  `save_slp(data['dir/123.slp'], '/tmp/pic.png')`.\\n""),
        local=locals()
    )"
"    def save_slp(path, target, palette=None):
        """"""
        save a slp as png.
        """"""
        from .texture import Texture
        from .slp import SLP
        from .driver import get_palette

        if not palette:
            palette = get_palette(data)

        with path.open(""rb"") as slpfile:
            tex = Texture(SLP(slpfile.read()), palette)

            out_path, filename = os.path.split(target)
            tex.save(Directory(out_path).root, filename)","    def save_slp(path, target, palette=None):
        """"""
        save a slp as png.
        """"""
        from .texture import Texture
        from .slp import SLP
        from .driver import get_palette

        if not palette:
            palette = get_palette(data, game_versions)

        with path.open(""rb"") as slpfile:
            tex = Texture(SLP(slpfile.read()), palette)

            out_path, filename = os.path.split(target)
            tex.save(Directory(out_path).root, filename)"
"def init_subparser(cli):
    """""" Initializes the parser for convert-specific args. """"""
    cli.set_defaults(entrypoint=main)

    cli.add_argument(
        ""--source-dir"", default=None,
        help=""source data directory"")

    cli.add_argument(
        ""--output-dir"", default=None,
        help=""destination data output directory"")

    cli.add_argument(
        ""--force"", action='store_true',
        help=""force conversion, even if up-to-date assets already exist."")

    cli.add_argument(
        ""--gen-extra-files"", action='store_true',
        help=""generate some extra files, useful for debugging the converter."")

    cli.add_argument(
        ""--no-media"", action='store_true',
        help=""do not convert any media files (slp, wav, ...)"")

    cli.add_argument(
        ""--no-metadata"", action='store_true',
        help=(""do not store any metadata ""
              ""(except for those associated with media files)""))

    cli.add_argument(
        ""--no-sounds"", action='store_true',
        help=""do not convert any sound files"")

    cli.add_argument(
        ""--no-graphics"", action='store_true',
        help=""do not convert game graphics"")

    cli.add_argument(
        ""--no-interface"", action='store_true',
        help=""do not convert interface graphics"")

    cli.add_argument(
        ""--no-scripts"", action='store_true',
        help=""do not convert scripts (AI and Random Maps)"")

    cli.add_argument(
        ""--no-pickle-cache"", action='store_true',
        help=""don't use a pickle file to skip the dat file reading."")

    cli.add_argument(
        ""--jobs"", ""-j"", type=int, default=None)

    cli.add_argument(
        ""--interactive"", ""-i"", action='store_true',
        help=""browse the files interactively"")

    cli.add_argument(
        ""--id"", type=int, default=None,
        help=""only convert files with this id (used for debugging..)"")","def init_subparser(cli):
    """""" Initializes the parser for convert-specific args. """"""
    cli.set_defaults(entrypoint=main)

    cli.add_argument(
        ""--source-dir"", default=None,
        help=""source data directory"")

    cli.add_argument(
        ""--force"", action='store_true',
        help=""force conversion, even if up-to-date assets already exist."")

    cli.add_argument(
        ""--gen-extra-files"", action='store_true',
        help=""generate some extra files, useful for debugging the converter."")

    cli.add_argument(
        ""--no-media"", action='store_true',
        help=""do not convert any media files (slp, wav, ...)"")

    cli.add_argument(
        ""--no-metadata"", action='store_true',
        help=(""do not store any metadata ""
              ""(except for those associated with media files)""))

    cli.add_argument(
        ""--no-sounds"", action='store_true',
        help=""do not convert any sound files"")

    cli.add_argument(
        ""--no-graphics"", action='store_true',
        help=""do not convert game graphics"")

    cli.add_argument(
        ""--no-interface"", action='store_true',
        help=""do not convert interface graphics"")

    cli.add_argument(
        ""--no-scripts"", action='store_true',
        help=""do not convert scripts (AI and Random Maps)"")

    cli.add_argument(
        ""--no-pickle-cache"", action='store_true',
        help=""don't use a pickle file to skip the dat file reading."")

    cli.add_argument(
        ""--jobs"", ""-j"", type=int, default=None)

    cli.add_argument(
        ""--interactive"", ""-i"", action='store_true',
        help=""browse the files interactively"")

    cli.add_argument(
        ""--id"", type=int, default=None,
        help=""only convert files with this id (used for debugging..)"")"
"def main(args, error):
    """""" CLI entry point """"""
    del error  # unused

    # initialize libopenage
    from ..cppinterface.setup import setup
    setup(args)

    # conversion source
    if args.source_dir is not None:
        srcdir = CaseIgnoringDirectory(args.source_dir).root
    else:
        srcdir = None

    if args.interactive:
        interactive_browser(srcdir)
        return 0

    # conversion target
    from ..assets import get_asset_path
    outdir = get_asset_path(args.output_dir)

    if args.force or conversion_required(outdir, args):
        if not convert_assets(outdir, args, srcdir):
            return 1
    else:
        print(""assets are up to date; no conversion is required."")
        print(""override with --force."")","def main(args, error):
    """""" CLI entry point """"""
    del error  # unused

    # initialize libopenage
    from ..cppinterface.setup import setup
    setup(args)

    # conversion source
    if args.source_dir is not None:
        srcdir = CaseIgnoringDirectory(args.source_dir).root
    else:
        srcdir = None

    if args.interactive:
        interactive_browser(srcdir)
        return 0

    # conversion target
    from ..assets import get_asset_path
    assets = get_asset_path(args)

    if args.force or conversion_required(assets, args):
        if not convert_assets(assets, args, srcdir):
            return 1
    else:
        print(""assets are up to date; no conversion is required."")
        print(""override with --force."")"
"def init_subparser(cli):
    """""" Initializes the parser for convert-specific args. """"""
    import argparse

    cli.set_defaults(entrypoint=main)

    cli.add_argument(""--palette"", default=""50500"", help=""palette number"")
    cli.add_argument(""--interfac"", type=argparse.FileType('rb'),
                     help=(""drs archive where palette ""
                           ""is contained (interfac.drs). ""
                           ""If not set, assumed to be in same ""
                           ""directory as the source drs archive""))
    cli.add_argument(""drs"", type=argparse.FileType('rb'),
                     help=(""drs archive filename that contains the slp ""
                           ""e.g. path ~/games/aoe/graphics.drs""))
    cli.add_argument(""slp"", help=(""slp filename inside the drs archive ""
                                  ""e.g. 326.slp""))
    cli.add_argument(""output"", help=""image output path name"")","def init_subparser(cli):
    """""" Initializes the parser for convert-specific args. """"""
    cli.set_defaults(entrypoint=main)

    cli.add_argument(""--palette"", default=""50500"", help=""palette number"")
    cli.add_argument(""--interfac"", help=(""drs archive where palette ""
                                         ""is contained (interfac.drs)""))
    cli.add_argument(""drs"", help=(""drs archive filename that contains the slp ""
                                  ""e.g. path ~/games/aoe/graphics.drs""))
    cli.add_argument(""slp"", help=(""slp filename inside the drs archive ""
                                  ""e.g. 326.slp""))
    cli.add_argument(""output"", help=""image output path name"")"
"def main(args, error):
    """""" CLI entry point for single file conversions """"""
    del error  # unused

    drspath = Path(args.drs.name)
    outputpath = Path(args.output)

    if args.interfac:
        interfacfile = args.interfac
    else:
        # if no interfac was given, assume
        # the same path of the drs archive.

        interfacfile = drspath.with_name(""interfac.drs"").open(""rb"")  # pylint: disable=no-member

    # here, try opening slps from interfac or whereever
    info(""opening slp in drs '%s:%s'..."" % (drspath, args.slp))
    slpfile = DRS(args.drs).root[args.slp].open(""rb"")

    # import here to prevent that the __main__ depends on SLP
    # just by importing this singlefile.py.
    from .slp import SLP

    # parse the slp image
    info(""parsing slp image..."")
    slpimage = SLP(slpfile.read())

    # open color palette
    info(""opening palette in drs '%s:%s.bina'..."" % (interfacfile.name, args.palette))
    palettefile = DRS(interfacfile).root[""%s.bina"" % args.palette].open(""rb"")

    info(""parsing palette data..."")
    palette = ColorTable(palettefile.read())

    # create texture
    info(""packing texture..."")
    tex = Texture(slpimage, palette)

    # to save as png:
    tex.save(Directory(outputpath.parent).root, outputpath.name)","def main(args, error):
    """""" CLI entry point for single file conversions """"""
    del error  # unused

    if args.interfac:
        interfacfile = args.interfac
    else:
        # if no interfac was given, assume
        # the same path of the drs archive.
        drspath = os.path.split(args.drs)[0]
        interfacfile = os.path.join(drspath, ""interfac.drs"")

    # if .png was passed, strip it away.
    if args.output.endswith("".png""):
        args.output = args.output[:-4]

    # here, try opening slps from interfac or whereever
    info(""opening slp in drs '%s:%s'..."" % (args.drs, args.slp))
    slpfile = DRS(open(args.drs, ""rb"")).root[args.slp].open(""rb"")

    # import here to prevent that the __main__ depends on SLP
    # just by importing this singlefile.py.
    from .slp import SLP

    # parse the slp image
    info(""parsing slp image..."")
    slpimage = SLP(slpfile.read())

    # open color palette
    info(""opening palette in drs '%s:%s.bina'..."" % (interfacfile,
                                                     args.palette))
    palettefile = DRS(open(interfacfile, ""rb"")).\\
        root[""%s.bina"" % args.palette].open(""rb"")
    info(""parsing palette data..."")
    palette = ColorTable(palettefile.read())

    # create texture
    info(""packing texture..."")
    tex = Texture(slpimage, palette)

    # to save as png:
    path, filename = os.path.split(args.output)
    tex.save(Directory(path).root, filename)"
"    def save(self, targetdir, filename, meta_formats=None):
        """"""
        Store the image data into the target directory path,
        with given filename=""dir/out.png""
        If metaformats are requested, export e.g. as ""dir/out.docx"".
        """"""
        if not isinstance(targetdir, Path):
            raise ValueError(""util.fslike Path expected as targetdir"")
        if not isinstance(filename, str):
            raise ValueError(""str expected as filename, not %s"" % type(filename))

        basename, ext = os.path.splitext(filename)

        # only allow png, although PIL could of course
        # do other formats.
        if ext != "".png"":
            raise ValueError(""Filename invalid, a texture must be saved""
                             ""as 'filename.png', not '%s'"" % (filename))

        # without the dot
        ext = ext[1:]

        # generate PNG file
        with targetdir[filename].open(""wb"") as imagefile:
            self.image_data.get_pil_image().save(imagefile, ext)

        if meta_formats:
            # generate formatted texture metadata
            formatter = data_formatter.DataFormatter()
            formatter.add_data(self.dump(basename))
            formatter.export(targetdir, meta_formats)","    def save(self, targetdir, filename, meta_formats=None):
        """"""
        Store the image data into the target directory path,
        with given filename=""dir/out.png""
        If metaformats are requested, export e.g. as ""dir/out.docx"".
        """"""
        if not isinstance(targetdir, Path):
            raise ValueError(""util.fslike Path expected as targetdir"")
        if not isinstance(filename, str):
            raise ValueError(""str expected as filename, not %s"" % type(filename))

        basename, ext = os.path.splitext(filename)

        if ext != "".png"":
            raise ValueError(""Texture must be saved as name.png. got: %s"" % filename)

        # without the dot
        ext = ext[1:]

        # generate PNG file
        with targetdir[filename].open(""wb"") as imagefile:
            self.image_data.get_pil_image().save(imagefile, ext)

        if meta_formats:
            # generate formatted texture metadata
            formatter = data_formatter.DataFormatter()
            formatter.add_data(self.dump(basename))
            formatter.export(targetdir, meta_formats)"
"def get_config_path(custom_cfg_dir=None):
    """"""
    Locates the main configuration file by name in some searchpaths.
    Optionally, mount a custom directory with highest priority.
    """"""

    # if we're in devmode, use only the build source config folder
    if config.DEVMODE:
        return Directory(os.path.join(config.BUILD_SRC_DIR, ""cfg"")).root

    # else, mount the possible locations in an union
    # to overlay the global dir and the user dir.
    result = Union().root

    # mount the global config dir
    # we don't use xdg config_dirs because that would be /etc/xdg/openage
    # and nobody would ever look in there.
    global_configs = pathlib.Path(config.GLOBAL_CONFIG_DIR)
    if global_configs.is_dir():
        result.mount(WriteBlocker(Directory(global_configs).root).root)

    # then the per-user config dir (probably ~/.config/openage)
    home_cfg = default_dirs.get_dir(""config_home"") / ""openage""
    result.mount(
        Directory(
            home_cfg,
            create_if_missing=True
        ).root
    )

    # the optional command line argument overrides it all
    if custom_cfg_dir:
        result.mount(Directory(custom_cfg_dir).root)

    return result","def get_config_path(args):
    """"""
    Locates the main configuration file by name in some searchpaths.
    """"""

    # if we're in devmode, use only the build source config folder
    if config.DEVMODE:
        return Directory(os.path.join(config.BUILD_SRC_DIR, ""cfg"")).root

    # else, mount the possible locations in an union
    # to overlay the global dir and the user dir.
    result = Union().root

    # mount the global config dir
    # we don't use xdg config_dirs because that would be /etc/xdg/openage
    # and nobody would ever look in there.
    global_configs = pathlib.Path(config.GLOBAL_CONFIG_DIR)
    if global_configs.is_dir():
        result.mount(WriteBlocker(Directory(global_configs).root).root)

    # then the per-user config dir (probably ~/.config/openage)
    home_cfg = default_dirs.get_dir(""config_home"") / ""openage""
    result.mount(
        Directory(
            home_cfg,
            create_if_missing=True
        ).root
    )

    # the optional command line argument overrides it all
    if args.cfg_dir:
        result.mount(Directory(args.cfg_dir).root)

    return result"
"def main(args, error):
    """"""
    Makes sure that the assets have been converted,
    and jumps into the C++ main method.
    """"""
    del error  # unused

    # we have to import stuff inside the function
    # as it depends on generated/compiled code
    from .main_cpp import run_game
    from .. import config
    from ..assets import get_asset_path
    from ..convert.main import conversion_required, convert_assets
    from ..cppinterface.setup import setup as cpp_interface_setup
    from ..cvar.location import get_config_path
    from ..util.fslike.union import Union

    # initialize libopenage
    cpp_interface_setup(args)

    info(""launching openage {}"".format(config.VERSION))
    info(""compiled by {}"".format(config.COMPILER))

    if config.DEVMODE:
        info(""running in DEVMODE"")

    # create virtual file system for data paths
    root = Union().root

    # mount the assets folder union at ""assets/""
    root[""assets""].mount(get_asset_path(args.asset_dir))

    # mount the config folder at ""cfg/""
    root[""cfg""].mount(get_config_path(args.cfg_dir))

    # ensure that the assets have been converted
    if conversion_required(root[""assets""], args):
        if not convert_assets(root[""assets""], args):
            err(""game asset conversion failed"")
            return 1

    # start the game, continue in main_cpp.pyx!
    return run_game(args, root)","def main(args, error):
    """"""
    Makes sure that the assets have been converted,
    and jumps into the C++ main method.
    """"""
    del error  # unused

    # we have to import stuff inside the function
    # as it depends on generated/compiled code
    from .main_cpp import run_game
    from .. import config
    from ..assets import get_asset_path
    from ..convert.main import conversion_required, convert_assets
    from ..cppinterface.setup import setup as cpp_interface_setup
    from ..cvar.location import get_config_path
    from ..util.fslike.union import Union

    # initialize libopenage
    cpp_interface_setup(args)

    info(""launching openage {}"".format(config.VERSION))
    info(""compiled by {}"".format(config.COMPILER))

    if config.DEVMODE:
        info(""running in DEVMODE"")

    # create virtual file system for data paths
    root = Union().root

    # mount the assets folder union at ""assets/""
    root[""assets""].mount(get_asset_path(args))

    # mount the config folder at ""cfg/""
    root[""cfg""].mount(get_config_path(args))

    # ensure that the assets have been converted
    if conversion_required(root[""assets""], args):
        if not convert_assets(root[""assets""], args):
            err(""game asset conversion failed"")
            return 1

    # start the game, continue in main_cpp.pyx!
    return run_game(args, root)"
"def get_string_resources(args):
    """""" reads the (language) string resources """"""
    from .stringresource import StringResource
    stringres = StringResource()

    srcdir = args.srcdir
    count = 0

    # AoK:TC uses .DLL PE files for its string resources,
    # HD uses plaintext files
    if GameVersion.age2_fe in args.game_versions:
        from .hdlanguagefile import read_hd_language_file

        for lang in srcdir[""resources""].list():
            try:
                if lang == b'_common':
                    continue
                langfilename = [""resources"", lang.decode(), ""strings"", ""key-value"",
                                ""key-value-strings-utf8.txt""]

                with srcdir[langfilename].open('rb') as langfile:
                    stringres.fill_from(read_hd_language_file(langfile, lang))

                count += 1
            except FileNotFoundError:
                # that's fine, there are no language files for every language.
                pass
    elif GameVersion.age2_hd_3x in args.game_versions:
        from .hdlanguagefile import read_hd_language_file

        # HD Edition 3.x and below store language .txt files in the Bin/ folder.
        # Specific language strings are in Bin/$LANG/*.txt.
        for lang in srcdir[""bin""].list():
            dirname = [""bin"", lang.decode()]

            # There are some .txt files immediately in bin/, but they don't seem
            # to contain anything useful. (Everything is overridden by files in
            # Bin/$LANG/.)
            if not srcdir[dirname].is_dir():
                continue

            for basename in srcdir[dirname].list():
                langfilename = [""bin"", lang.decode(), basename]
                with srcdir[langfilename].open('rb') as langfile:
                    # No utf-8 :(
                    stringres.fill_from(read_hd_language_file(langfile, lang, enc='iso-8859-1'))
                count += 1
    elif srcdir[""language.dll""].is_file():
        from .pefile import PEFile
        for name in [""language.dll"", ""language_x1.dll"", ""language_x1_p1.dll""]:
            pefile = PEFile(srcdir[name].open('rb'))
            stringres.fill_from(pefile.resources().strings)
            count += 1

    if not count:
        raise FileNotFoundError(""could not find any language files"")

    # TODO transform and cleanup the read strings:
    #      convert formatting indicators from HTML to something sensible, etc

    return stringres","def get_string_resources(args):
    """""" reads the (language) string resources """"""
    from .stringresource import StringResource
    stringres = StringResource()

    srcdir = args.srcdir
    count = 0

    # AoK:TC uses .DLL PE files for its string resources,
    # HD uses plaintext files
    if srcdir[""language.dll""].is_file():
        from .pefile import PEFile
        for name in [""language.dll"", ""language_x1.dll"", ""language_x1_p1.dll""]:
            pefile = PEFile(srcdir[name].open('rb'))
            stringres.fill_from(pefile.resources().strings)
            count += 1
    elif GameVersion.age2_fe in args.game_versions:
        from .hdlanguagefile import read_hd_language_file

        for lang in srcdir[""resources""].list():
            try:
                if lang == b'_common':
                    continue
                langfilename = [""resources"", lang.decode(), ""strings"", ""key-value"",
                                ""key-value-strings-utf8.txt""]

                with srcdir[langfilename].open('rb') as langfile:
                    stringres.fill_from(read_hd_language_file(langfile, lang))

                count += 1
            except FileNotFoundError:
                # that's fine, there are no language files for every language.
                pass
    elif GameVersion.age2_hd_3x in args.game_versions:
        from .hdlanguagefile import read_hd_language_file

        # HD Edition 3.x and below store language .txt files in the Bin/ folder.
        # Specific language strings are in Bin/$LANG/*.txt.
        for lang in srcdir[""bin""].list():
            dirname = [""bin"", lang.decode()]

            # There are some .txt files immediately in bin/, but they don't seem
            # to contain anything useful. (Everything is overridden by files in
            # Bin/$LANG/.)
            if not srcdir[dirname].is_dir():
                continue

            for basename in srcdir[dirname].list():
                langfilename = [""bin"", lang.decode(), basename]
                with srcdir[langfilename].open('rb') as langfile:
                    # No utf-8 :(
                    stringres.fill_from(read_hd_language_file(langfile, lang, enc='iso-8859-1'))
                count += 1

    if not count:
        raise FileNotFoundError(""could not find any language files"")

    # TODO transform and cleanup the read strings:
    #      convert formatting indicators from HTML to something sensible, etc

    return stringres"
"def get_domain_mx_list(domain):
    """"""Return a list of MX IP address for domain.""""""
    result = []
    logger = logging.getLogger(""modoboa.admin"")
    dns_server = param_tools.get_global_parameter(""custom_dns_server"")
    if dns_server:
        resolver = dns.resolver.Resolver()
        resolver.nameservers = [dns_server]
    else:
        resolver = dns.resolver

    try:
        dns_answers = resolver.query(domain, ""MX"")
    except dns.resolver.NXDOMAIN as e:
        logger.error(_(""No DNS records found for %s"") % domain, exc_info=e)
    except dns.resolver.NoAnswer as e:
        logger.error(_(""No MX record for %s"") % domain, exc_info=e)
    except dns.resolver.NoNameservers as e:
        logger.error(_(""No working name servers found""), exc_info=e)
    except dns.resolver.Timeout as e:
        logger.warning(
            _(""DNS resolution timeout, unable to query %s at the moment"") %
            domain, exc_info=e)
    else:
        for dns_answer in dns_answers:
            for rtype in [""A"", ""AAAA""]:
                try:
                    mx_domain = dns_answer.exchange.to_unicode(
                        omit_final_dot=True, idna_codec=IDNA_2008_UTS_46)
                    ip_answers = resolver.query(mx_domain, rtype)
                except dns.resolver.NXDOMAIN as e:
                    logger.error(
                        _(""No {type} record found for MX {mx}"").format(
                            type=rtype, mx=domain), exc_info=e)
                except dns.resolver.NoAnswer:
                    pass
                else:
                    for ip_answer in ip_answers:
                        try:
                            address_smart = smart_text(ip_answer.address)
                            mx_ip = ipaddress.ip_address(address_smart)
                        except ValueError as e:
                            logger.warning(
                                _(""Invalid IP address format for ""
                                  ""{domain}; {addr}"").format(
                                      domain=mx_domain,
                                      addr=smart_text(ip_answer.address)
                                  ), exc_info=e)
                        else:
                            result.append((mx_domain, mx_ip))
    return result","def get_domain_mx_list(domain):
    """"""Return a list of MX IP address for domain.""""""
    result = []
    logger = logging.getLogger(""modoboa.admin"")
    dns_server = param_tools.get_global_parameter(""custom_dns_server"")
    if dns_server:
        resolver = dns.resolver.Resolver()
        resolver.nameservers = [dns_server]
    else:
        resolver = dns.resolver

    try:
        dns_answers = resolver.query(domain, ""MX"")
    except dns.resolver.NXDOMAIN as e:
        logger.error(_(""No DNS records found for %s"") % domain, exc_info=e)
    except dns.resolver.NoAnswer as e:
        logger.error(_(""No MX record for %s"") % domain, exc_info=e)
    except dns.resolver.NoNameservers as e:
        logger.error(_(""No working name servers found""), exc_info=e)
    except dns.resolver.Timeout as e:
        logger.warning(
            _(""DNS resolution timeout, unable to query %s at the moment"") %
            domain, exc_info=e)
    else:
        for dns_answer in dns_answers:
            for rtype in [""A"", ""AAAA""]:
                try:
                    mx_domain = dns_answer.exchange.to_unicode(
                        omit_final_dot=True, idna_codec=IDNA_2008_UTS_46)
                    ip_answers = resolver.query(mx_domain, rtype)
                except dns.resolver.NXDOMAIN as e:
                    logger.error(
                        _(""No {type} record found for MX {mx}"").format(
                            type=rtype, mx=domain), exc_info=e)
                else:
                    for ip_answer in ip_answers:
                        try:
                            address_smart = smart_text(ip_answer.address)
                            mx_ip = ipaddress.ip_address(address_smart)
                        except ValueError as e:
                            logger.warning(
                                _(""Invalid IP address format for ""
                                  ""{domain}; {addr}"").format(
                                      domain=mx_domain,
                                      addr=smart_text(ip_answer.address)
                                  ), exc_info=e)
                        else:
                            result.append((mx_domain, mx_ip))
    return result"
"    def authenticate(self, request, username=None, password=None):
        """"""Check the username/password and return a User.""""""
        host = getattr(settings, ""AUTH_SMTP_SERVER_ADDRESS"", ""localhost"")
        port = getattr(settings, ""AUTH_SMTP_SERVER_PORT"", 25)
        secured_mode = getattr(settings, ""AUTH_SMTP_SECURED_MODE"", None)
        if secured_mode == ""ssl"":
            smtp = smtplib.SMTP_SSL(host, port)
        else:
            smtp = smtplib.SMTP(host, port)
            if secured_mode == ""starttls"":
                smtp.starttls()
        try:
            smtp.login(username, password)
        except smtplib.SMTPException:
            return None
        return self.get_or_build_user(username)","    def authenticate(self, username=None, password=None):
        """"""Check the username/password and return a User.""""""
        host = getattr(settings, ""AUTH_SMTP_SERVER_ADDRESS"", ""localhost"")
        port = getattr(settings, ""AUTH_SMTP_SERVER_PORT"", 25)
        secured_mode = getattr(settings, ""AUTH_SMTP_SECURED_MODE"", None)
        if secured_mode == ""ssl"":
            smtp = smtplib.SMTP_SSL(host, port)
        else:
            smtp = smtplib.SMTP(host, port)
            if secured_mode == ""starttls"":
                smtp.starttls()
        try:
            smtp.login(username, password)
        except smtplib.SMTPException:
            return None
        return self.get_or_build_user(username)"
"        def authenticate(self, *args, **kwargs):
            if self.global_params[""authentication_type""] == ""ldap"":
                return super(LDAPBackend, self).authenticate(*args, **kwargs)
            return None","        def authenticate(self, username, password):
            if self.global_params[""authentication_type""] == ""ldap"":
                return super(LDAPBackend, self).authenticate(
                    username=username, password=password)
            return None"
"    def handle(self, *args, **options):
        exts_pool.load_all()
        self.csvwriter = csv.writer(
            self.stdout, delimiter=smart_text(options[""sepchar""]))
        getattr(self, ""export_{}"".format(options[""objtype""]))()","    def handle(self, *args, **options):
        exts_pool.load_all()
        self.csvwriter = csv.writer(sys.stdout, delimiter=options[""sepchar""])
        getattr(self, ""export_{}"".format(options[""objtype""]))()"
"    def ready(self):
        load_core_settings()

        # Import these to force registration of checks and signals
        from . import checks  # noqa:F401
        from . import handlers

        signals.post_migrate.connect(handlers.create_local_config, sender=self)","    def ready(self):
        load_core_settings()

        from . import handlers

        signals.post_migrate.connect(handlers.create_local_config, sender=self)"
"    def store_dnsbl_result(self, domain, provider, results, **options):
        """"""Store DNSBL provider results for domain.""""""
        alerts = {}
        to_create = []
        for mx, result in list(results.items()):
            if not result:
                result = """"
            dnsbl_result = models.DNSBLResult.objects.filter(
                domain=domain, provider=provider, mx=mx).first()
            trigger = False
            if dnsbl_result is None:
                to_create.append(
                    models.DNSBLResult(
                        domain=domain, provider=provider, mx=mx,
                        status=result)
                )
                if result:
                    trigger = True
            else:
                dnsbl_result.status = result
                dnsbl_result.save()
                if not dnsbl_result.status and result:
                    trigger = True
            if trigger:
                if domain not in alerts:
                    alerts[domain] = []
                alerts[domain].append((provider, mx))
        models.DNSBLResult.objects.bulk_create(to_create)
        if not alerts:
            return
        emails = list(options[""email""])
        if not options[""skip_admin_emails""]:
            emails.extend(
                domain.admins.exclude(mailbox__isnull=True)
                .values_list(""email"", flat=True)
            )
        if not len(emails):
            return
        with mail.get_connection() as connection:
            for domain, providers in list(alerts.items()):
                content = render_to_string(
                    ""admin/notifications/domain_in_dnsbl.html"", {
                        ""domain"": domain, ""alerts"": providers
                    })
                subject = _(""[modoboa] DNSBL issue(s) for domain {}"").format(
                    domain.name)
                msg = EmailMessage(
                    subject, content.strip(), self.sender, emails,
                    connection=connection
                )
                msg.send()","    def store_dnsbl_result(self, domain, provider, results, **options):
        """"""Store DNSBL provider results for domain.""""""
        alerts = {}
        to_create = []
        for mx in results.keys():
            result = """" if not results[mx] else results[mx]
            dnsbl_result = models.DNSBLResult.objects.filter(
                domain=domain, provider=provider, mx=mx).first()
            trigger = False
            if dnsbl_result is None:
                to_create.append(
                    models.DNSBLResult(
                        domain=domain, provider=provider, mx=mx,
                        status=result)
                )
                if result:
                    trigger = True
            else:
                dnsbl_result.status = result
                dnsbl_result.save()
                if not dnsbl_result.status and result:
                    trigger = True
            if trigger:
                if domain not in alerts:
                    alerts[domain] = []
                alerts[domain].append((provider, mx))
        models.DNSBLResult.objects.bulk_create(to_create)
        if not alerts:
            return
        emails = list(options[""email""])
        if not options[""skip_admin_emails""]:
            emails.extend(
                domain.admins.exclude(mailbox__isnull=True)
                .values_list(""email"", flat=True)
            )
        if not len(emails):
            return
        with mail.get_connection() as connection:
            for domain, providers in list(alerts.items()):
                content = render_to_string(
                    ""admin/notifications/domain_in_dnsbl.html"", {
                        ""domain"": domain, ""alerts"": providers
                    })
                subject = _(""[modoboa] DNSBL issue(s) for domain {}"").format(
                    domain.name)
                msg = EmailMessage(
                    subject, content.strip(), self.sender, emails,
                    connection=connection
                )
                msg.send()"
"def on_mailbox_modified(mailbox):
    """"""Update amavis records if address has changed.""""""
    if parameters.get_admin(""MANUAL_LEARNING"") == ""no"" or \\
       not hasattr(mailbox, ""old_full_address"") or \\
       mailbox.full_address == mailbox.old_full_address:
        return
    user = Users.objects.select_related().get(email=mailbox.old_full_address)
    full_address = mailbox.full_address
    user.email = full_address
    user.policy.policy_name = full_address[:32]
    user.policy.sa_username = full_address
    user.policy.save()
    user.save()","def on_mailbox_modified(mailbox):
    """"""Update amavis records if address has changed.""""""
    if parameters.get_admin(""MANUAL_LEARNING"") == ""no"" or \\
       not hasattr(mailbox, ""old_full_address"") or \\
       mailbox.full_address == mailbox.old_full_address:
        return
    user = Users.objects.select_related.get(email=mailbox.old_full_address)
    full_address = mailbox.full_address
    user.email = full_address
    user.policy.policy_name = full_address[:32]
    user.policy.sa_username = full_address
    user.policy.save()
    user.save()"
"    def get_mail_content(self, mailid):
        """"""Retrieve the content of a message.""""""
        content = """".join([
            str(qmail.mail_text)
            for qmail in Quarantine.objects.filter(mail=mailid)
        ])
        if isinstance(content, unicode):
            content = content.encode(""utf-8"")
        return content","    def get_mail_content(self, mailid):
        """"""Retrieve the content of a message.
        """"""
        return Quarantine.objects.filter(mail=mailid)"
"    def get_mail_content(self, mailid):
        """"""Retrieve the content of a message.""""""
        content = """".join([
            str(qmail.mail_text)
            for qmail in Quarantine.objects.filter(mail=mailid)
        ])
        if isinstance(content, unicode):
            content = content.encode(""utf-8"")
        return content","    def get_mail_content(self, mailid):
        """"""Retrieve the content of a message.""""""
        return Quarantine.objects.filter(mail=mailid).extra(
            select={'mail_text': ""convert_from(mail_text, 'UTF8')""}
        )"
"    def msg(self):
        """"""Get message's content.""""""
        import email

        if self._msg is None:
            mail_text = get_connector().get_mail_content(self.mailid)
            self._msg = email.message_from_string(mail_text)
            self._parse(self._msg)
        return self._msg","    def msg(self):
        """"""Get message's content.""""""
        import email

        if self._msg is None:
            qmails = get_connector().get_mail_content(self.mailid)
            mail_text = """".join([qm.mail_text for qm in qmails])
            if type(mail_text) is unicode:
                mail_text = mail_text.encode(""utf-8"")
            self._msg = email.message_from_string(mail_text)
            self._parse(self._msg)
        return self._msg"
"def viewheaders(request, mail_id):
    """"""Display message headers.""""""
    content = get_connector().get_mail_content(mail_id)
    msg = email.message_from_string(content)
    headers = []
    for name, value in msg.items():
        if value:
            result = chardet.detect(value)
            if result[""encoding""] is not None:
                value = value.decode(result[""encoding""])
        headers += [(name, value)]
    return render(request, 'amavis/viewheader.html', {
        ""headers"": headers
    })","def viewheaders(request, mail_id):
    """"""Display message headers.""""""
    content = """"
    for qm in get_connector().get_mail_content(mail_id):
        content += qm.mail_text
    if type(content) is unicode:
        content = content.encode(""utf-8"")
    msg = email.message_from_string(content)
    headers = []
    for name, value in msg.items():
        if value:
            result = chardet.detect(value)
            if result[""encoding""] is not None:
                value = value.decode(result[""encoding""])
        headers += [(name, value)]
    return render(request, 'amavis/viewheader.html', {
        ""headers"": headers
    })"
"def mark_messages(request, selection, mtype, recipient_db=None):
    """"""Mark a selection of messages as spam.

    :param str selection: message unique identifier
    :param str mtype: type of marking (spam or ham)
    """"""
    if not manual_learning_enabled(request.user):
        return render_to_json_response({""status"": ""ok""})
    if recipient_db is None:
        recipient_db = (
            ""user"" if request.user.group == ""SimpleUsers"" else ""global""
        )
    selection = check_mail_id(request, selection)
    connector = get_connector()
    saclient = SpamassassinClient(request.user, recipient_db)
    for item in selection:
        rcpt, mail_id = item.split()
        content = connector.get_mail_content(mail_id)
        result = saclient.learn_spam(rcpt, content) if mtype == ""spam"" \\
            else saclient.learn_ham(rcpt, content)
        if not result:
            break
        connector.set_msgrcpt_status(rcpt, mail_id, mtype[0].upper())
    if saclient.error is None:
        saclient.done()
        message = ungettext(""%(count)d message processed successfully"",
                            ""%(count)d messages processed successfully"",
                            len(selection)) % {""count"": len(selection)}
    else:
        message = saclient.error
    status = 400 if saclient.error else 200
    return render_to_json_response({
        ""message"": message, ""reload"": True
    }, status=status)","def mark_messages(request, selection, mtype, recipient_db=None):
    """"""Mark a selection of messages as spam.

    :param str selection: message unique identifier
    :param str mtype: type of marking (spam or ham)
    """"""
    if not manual_learning_enabled(request.user):
        return render_to_json_response({""status"": ""ok""})
    if recipient_db is None:
        recipient_db = (
            ""user"" if request.user.group == ""SimpleUsers"" else ""global""
        )
    selection = check_mail_id(request, selection)
    connector = get_connector()
    saclient = SpamassassinClient(request.user, recipient_db)
    for item in selection:
        rcpt, mail_id = item.split()
        content = """".join(
            [msg.mail_text for msg in connector.get_mail_content(mail_id)]
        )
        result = saclient.learn_spam(rcpt, content) if mtype == ""spam"" \\
            else saclient.learn_ham(rcpt, content)
        if not result:
            break
        connector.set_msgrcpt_status(rcpt, mail_id, mtype[0].upper())
    if saclient.error is None:
        saclient.done()
        message = ungettext(""%(count)d message processed successfully"",
                            ""%(count)d messages processed successfully"",
                            len(selection)) % {""count"": len(selection)}
    else:
        message = saclient.error
    status = 400 if saclient.error else 200
    return render_to_json_response({
        ""message"": message, ""reload"": True
    }, status=status)"
"def list_quotas(request):
    from modoboa.lib.dbutils import db_type

    sort_order, sort_dir = get_sort_order(request.GET, ""address"")
    mboxes = Mailbox.objects.get_for_admin(
        request.user, request.GET.get(""searchquery"", None)
    )
    mboxes = mboxes.exclude(quota=0)
    if sort_order in [""address"", ""quota""]:
        mboxes = mboxes.order_by(""%s%s"" % (sort_dir, sort_order))
    elif sort_order == ""quota_value__bytes"":
        where = ""admin_mailbox.address||'@'||admin_domain.name""
        mboxes = mboxes.extra(
            select={""quota_value__bytes"": ""admin_quota.bytes""},
            where=[""admin_quota.username=%s"" % where],
            tables=[""admin_quota"", ""admin_domain""],
            order_by=[""%s%s"" % (sort_dir, sort_order)]
        )
    elif sort_order == ""quota_usage"":
        where = ""admin_mailbox.address||'@'||admin_domain.name""
        db_type = db_type()
        if db_type == ""postgres"":
            select = '(admin_quota.bytes::float / (CAST(admin_mailbox.quota AS BIGINT) * 1048576)) * 100'
        else:
            select = 'admin_quota.bytes / (admin_mailbox.quota * 1048576) * 100'
            if db_type == ""mysql"":
                where = ""CONCAT(admin_mailbox.address,'@',admin_domain.name)""
        mboxes = mboxes.extra(
            select={'quota_usage': select},
            where=[""admin_quota.username=%s"" % where],
            tables=[""admin_quota"", ""admin_domain""],
            order_by=[""%s%s"" % (sort_dir, sort_order)]
        )
    else:
        raise BadRequest(_(""Invalid request""))
    page = get_listing_page(mboxes, request.GET.get(""page"", 1))
    context = {
        ""headers"": _render_to_string(
            request, ""admin/quota_headers.html"", {}
        )
    }
    if page is None:
        context[""length""] = 0
    else:
        context[""rows""] = _render_to_string(
            request, ""admin/quotas.html"", {
                ""mboxes"": page
            }
        )
        context[""pages""] = [page.number]
    return render_to_json_response(context)","def list_quotas(request):
    from modoboa.lib.dbutils import db_type

    sort_order, sort_dir = get_sort_order(request.GET, ""address"")
    mboxes = Mailbox.objects.get_for_admin(
        request.user, request.GET.get(""searchquery"", None)
    )
    mboxes = mboxes.exclude(quota=0)
    if sort_order in [""address"", ""quota"", ""quota_value__bytes""]:
        mboxes = mboxes.order_by(""%s%s"" % (sort_dir, sort_order))
    elif sort_order == ""quota_usage"":
        where = ""admin_mailbox.address||'@'||admin_domain.name""
        db_type = db_type()
        if db_type == ""postgres"":
            select = '(admin_quota.bytes::float / (CAST(admin_mailbox.quota AS BIGINT) * 1048576)) * 100'
        else:
            select = 'admin_quota.bytes / (admin_mailbox.quota * 1048576) * 100'
            if db_type == ""mysql"":
                where = ""CONCAT(admin_mailbox.address,'@',admin_domain.name)""
        mboxes = mboxes.extra(
            select={'quota_usage': select},
            where=[""admin_quota.username=%s"" % where],
            tables=[""admin_quota"", ""admin_domain""],
            order_by=[""%s%s"" % (sort_dir, sort_order)]
        )
    else:
        raise BadRequest(_(""Invalid request""))
    page = get_listing_page(mboxes, request.GET.get(""page"", 1))
    context = {
        ""headers"": _render_to_string(
            request, ""admin/quota_headers.html"", {}
        )
    }
    if page is None:
        context[""length""] = 0
    else:
        context[""rows""] = _render_to_string(
            request, ""admin/quotas.html"", {
                ""mboxes"": page
            }
        )
        context[""pages""] = [page.number]
    return render_to_json_response(context)"
"    def messages_count(self, **kwargs):
        if self.count is None:
            filter = Q(chunk_ind=1)
            if self.mail_ids is not None:
                filter &= Q(mail__in=self.mail_ids)
            if self.filter:
                filter &= self.filter
            self.messages = Quarantine.objects.filter(filter).values(
                ""mail__from_addr"",
                ""mail__msgrcpt__rid__email"",
                ""mail__subject"",
                ""mail__mail_id"",
                ""mail__time_num"",
                ""mail__msgrcpt__content"",
                ""mail__msgrcpt__bspam_level"",
                ""mail__msgrcpt__rs""
            )
            if ""order"" in kwargs:
                order = kwargs[""order""]
                sign = """"
                if order[0] == ""-"":
                    sign = ""-""
                    order = order[1:]
                order = self.order_translation_table[order]
                self.messages = self.messages.order_by(sign + order)

            self.count = self.messages.count()
        return self.count","    def messages_count(self, **kwargs):
        if self.count is None:
            filter = Q(chunk_ind=1)
            if self.mail_ids is not None:
                filter &= Q(mail__in=self.mail_ids)
            if self.filter:
                filter &= self.filter
            self.messages = Quarantine.objects.filter(filter).values(
                ""mail__from_addr"",
                ""mail__msgrcpt__rid__email"",
                ""mail__subject"",
                ""mail__mail_id"",
                ""mail__time_num"",
                ""mail__msgrcpt__content"",
                ""mail__msgrcpt__bspam_level"",
                ""mail__msgrcpt__rs""
            )
            if ""order"" in kwargs:
                order = kwargs[""order""]
                sign = """"
                if order[0] == ""-"":
                    sign = ""-""
                    order = order[1:]
                order = self.order_translation_table[order]
                self.messages = self.messages.order_by(sign + order)

            self.count = len(self.messages)
        return self.count"
"    def fetch(self, start=None, stop=None, **kwargs):
        emails = []
        for qm in self.messages[start - 1:stop]:
            m = {""from"": qm[""mail__from_addr""],
                 ""to"": qm[""mail__msgrcpt__rid__email""],
                 ""subject"": qm[""mail__subject""],
                 ""mailid"": qm[""mail__mail_id""],
                 ""date"": qm[""mail__time_num""],
                 ""type"": qm[""mail__msgrcpt__content""],
                 ""score"": qm[""mail__msgrcpt__bspam_level""]}
            rs = qm[""mail__msgrcpt__rs""]
            if rs == 'D':
                continue
            elif rs == '':
                m[""class""] = ""unseen""
            elif rs == 'R':
                m[""img_rstatus""] = static_url(""pics/release.png"")
            elif rs == 'p':
                m[""class""] = ""pending""
            emails.append(m)
        return emails","    def fetch(self, start=None, stop=None, **kwargs):
        emails = []
        for qm in self.messages[start - 1:stop]:
            m = {""from"": qm[""mail__from_addr""],
                 ""to"": qm[""mail__msgrcpt__rid__email""],
                 ""subject"": qm[""mail__subject""],
                 ""mailid"": qm[""mail__mail_id""],
                 ""date"": qm[""mail__time_num""],
                 ""type"": qm[""mail__msgrcpt__content""],
                 ""score"": qm[""mail__msgrcpt__bspam_level""]}
            rs = qm[""mail__msgrcpt__rs""]
            if rs == '':
                m[""class""] = ""unseen""
            elif rs == 'R':
                m[""img_rstatus""] = static_url(""pics/release.png"")
            elif rs == 'p':
                m[""class""] = ""pending""
            emails.append(m)
        return emails"
"def viewmail(request, mail_id):
    rcpt = request.GET[""rcpt""]
    if request.user.mailbox_set.count():
        mb = Mailbox.objects.get(user=request.user)
        if rcpt in mb.alias_addresses:
            get_wrapper().set_msgrcpt_status(rcpt, mail_id, 'V')

    content = Template(""""""
<iframe src=""{{ url }}"" id=""mailcontent""></iframe>
"""""").render(Context({""url"": reverse(getmailcontent, args=[mail_id])}))
    menu = viewm_menu(mail_id, rcpt)
    ctx = getctx(""ok"", menu=menu, listing=content)
    request.session['location'] = 'viewmail'
    return render_to_json_response(ctx)","def viewmail(request, mail_id):
    rcpt = request.GET[""rcpt""]
    if request.user.mailbox_set.count():
        mb = Mailbox.objects.get(user=request.user)
        if rcpt in mb.alias_addresses:
            msgrcpt = get_wrapper().get_recipient_message(rcpt, mail_id)
            msgrcpt.rs = 'V'
            msgrcpt.save()

    content = Template(""""""
<iframe src=""{{ url }}"" id=""mailcontent""></iframe>
"""""").render(Context({""url"": reverse(getmailcontent, args=[mail_id])}))
    menu = viewm_menu(mail_id, rcpt)
    ctx = getctx(""ok"", menu=menu, listing=content)
    request.session['location'] = 'viewmail'
    return render_to_json_response(ctx)"
"def delete_selfservice(request, mail_id):
    rcpt = request.GET.get(""rcpt"", None)
    if rcpt is None:
        raise BadRequest(_(""Invalid request""))
    try:
        get_wrapper().set_msgrcpt_status(rcpt, mail_id, 'D')
    except Msgrcpt.DoesNotExist:
        raise BadRequest(_(""Invalid request""))
    return render_to_json_response(_(""Message deleted""))","def delete_selfservice(request, mail_id):
    rcpt = request.GET.get(""rcpt"", None)
    if rcpt is None:
        raise BadRequest(_(""Invalid request""))
    try:
        msgrcpt = get_wrapper().get_recipient_message(rcpt, mail_id)
        msgrcpt.rs = 'D'
        msgrcpt.save()
    except Msgrcpt.DoesNotExist:
        raise BadRequest(_(""Invalid request""))
    return render_to_json_response(_(""Message deleted""))"
"def delete(request, mail_id):
    """"""Delete message selection.

    :param str mail_id: message unique identifier
    """"""
    mail_id = check_mail_id(request, mail_id)
    wrapper = get_wrapper()
    mb = Mailbox.objects.get(user=request.user) \\
        if request.user.group == 'SimpleUsers' else None
    for mid in mail_id:
        r, i = mid.split()
        if mb is not None and r != mb.full_address \\
                and not r in mb.alias_addresses:
            continue
        wrapper.set_msgrcpt_status(r, i, 'D')
    message = ungettext(""%(count)d message deleted successfully"",
                        ""%(count)d messages deleted successfully"",
                        len(mail_id)) % {""count"": len(mail_id)}
    return ajax_response(
        request, respmsg=message,
        url=QuarantineNavigationParameters(request).back_to_listing()
    )","def delete(request, mail_id):
    """"""Delete message selection.

    :param str mail_id: message unique identifier
    """"""
    mail_id = check_mail_id(request, mail_id)
    wrapper = get_wrapper()
    mb = Mailbox.objects.get(user=request.user) \\
        if request.user.group == 'SimpleUsers' else None
    for mid in mail_id:
        r, i = mid.split()
        if mb is not None and r != mb.full_address \\
                and not r in mb.alias_addresses:
            continue
        msgrcpt = wrapper.get_recipient_message(r, i)
        msgrcpt.rs = 'D'
        msgrcpt.save()
    message = ungettext(""%(count)d message deleted successfully"",
                        ""%(count)d messages deleted successfully"",
                        len(mail_id)) % {""count"": len(mail_id)}
    return ajax_response(
        request, respmsg=message,
        url=QuarantineNavigationParameters(request).back_to_listing()
    )"
"def release_selfservice(request, mail_id):
    rcpt = request.GET.get(""rcpt"", None)
    secret_id = request.GET.get(""secret_id"", None)
    if rcpt is None or secret_id is None:
        raise BadRequest(_(""Invalid request""))
    wrapper = get_wrapper()
    try:
        msgrcpt = wrapper.get_recipient_message(rcpt, mail_id)
    except Msgrcpt.DoesNotExist:
        raise BadRequest(_(""Invalid request""))
    if secret_id != msgrcpt.mail.secret_id:
        raise BadRequest(_(""Invalid request""))
    if parameters.get_admin(""USER_CAN_RELEASE"") == ""no"":
        wrapper.set_msgrcpt_status(rcpt, mail_id, 'p')
        msg = _(""Request sent"")
    else:
        amr = AMrelease()
        result = amr.sendreq(mail_id, secret_id, rcpt)
        if result:
            wrapper.set_msgrcpt_status(rcpt, mail_id, 'R')
            msg = _(""Message released"")
        else:
            raise BadRequest(result)
    return render_to_json_response(msg)","def release_selfservice(request, mail_id):
    rcpt = request.GET.get(""rcpt"", None)
    secret_id = request.GET.get(""secret_id"", None)
    if rcpt is None or secret_id is None:
        raise BadRequest(_(""Invalid request""))
    try:
        msgrcpt = get_wrapper().get_recipient_message(rcpt, mail_id)
    except Msgrcpt.DoesNotExist:
        raise BadRequest(_(""Invalid request""))
    if secret_id != msgrcpt.mail.secret_id:
        raise BadRequest(_(""Invalid request""))
    if parameters.get_admin(""USER_CAN_RELEASE"") == ""no"":
        msgrcpt.rs = 'p'
        msg = _(""Request sent"")
    else:
        amr = AMrelease()
        result = amr.sendreq(mail_id, secret_id, rcpt)
        if result:
            rcpt.rs = 'R'
            msg = _(""Message released"")
        else:
            raise BadRequest(result)
    msgrcpt.save()
    return render_to_json_response(msg)"
"def release(request, mail_id):
    """"""Release message selection.

    :param str mail_id: message unique identifier
    """"""
    mail_id = check_mail_id(request, mail_id)
    msgrcpts = []
    wrapper = get_wrapper()
    mb = Mailbox.objects.get(user=request.user) \\
        if request.user.group == 'SimpleUsers' else None
    for mid in mail_id:
        r, i = mid.split()
        if mb is not None and r != mb.full_address \\
                and not r in mb.alias_addresses:
            continue
        msgrcpts += [wrapper.get_recipient_message(r, i)]
    if mb is not None and parameters.get_admin(""USER_CAN_RELEASE"") == ""no"":
        for msgrcpt in msgrcpts:
            wrapper.set_msgrcpt_status(
                msgrcpt.rid.email, msgrcpt.mail.mail_id, 'p'
            )
        message = ungettext(""%(count)d request sent"",
                            ""%(count)d requests sent"",
                            len(mail_id)) % {""count"": len(mail_id)}
        return ajax_response(
            request, ""ok"", respmsg=message,
            url=QuarantineNavigationParameters(request).back_to_listing()
        )

    amr = AMrelease()
    error = None
    for rcpt in msgrcpts:
        result = amr.sendreq(
            rcpt.mail.mail_id, rcpt.mail.secret_id, rcpt.rid.email
        )
        if result:
            wrapper.set_msgrcpt_status(rcpt.rid.email, rcpt.mail.mail_id, 'R')
        else:
            error = result
            break

    if not error:
        message = ungettext(""%(count)d message released successfully"",
                            ""%(count)d messages released successfully"",
                            len(mail_id)) % {""count"": len(mail_id)}
    else:
        message = error
    return ajax_response(
        request, ""ko"" if error else ""ok"", respmsg=message,
        url=QuarantineNavigationParameters(request).back_to_listing()
    )","def release(request, mail_id):
    """"""Release message selection.

    :param str mail_id: message unique identifier
    """"""
    mail_id = check_mail_id(request, mail_id)
    msgrcpts = []
    wrapper = get_wrapper()
    mb = Mailbox.objects.get(user=request.user) \\
        if request.user.group == 'SimpleUsers' else None
    for mid in mail_id:
        r, i = mid.split()
        if mb is not None and r != mb.full_address \\
                and not r in mb.alias_addresses:
            continue
        msgrcpts += [wrapper.get_recipient_message(r, i)]
    if mb is not None and parameters.get_admin(""USER_CAN_RELEASE"") == ""no"":
        # FIXME : can't use this syntax because extra SQL (using
        # .extra() for postgres) is not propagated (the 'tables'
        # parameter is lost somewhere...)
        #
        # msgrcpts.update(rs='p')
        for msgrcpt in msgrcpts:
            msgrcpt.rs = 'p'
            msgrcpt.save()
        message = ungettext(""%(count)d request sent"",
                            ""%(count)d requests sent"",
                            len(mail_id)) % {""count"": len(mail_id)}
        return ajax_response(
            request, ""ok"", respmsg=message,
            url=QuarantineNavigationParameters(request).back_to_listing()
        )

    amr = AMrelease()
    error = None
    for rcpt in msgrcpts:
        result = amr.sendreq(rcpt.mail.mail_id, rcpt.mail.secret_id, rcpt.rid.email)
        if result:
            rcpt.rs = 'R'
            rcpt.save()
        else:
            error = result
            break

    if not error:
        message = ungettext(""%(count)d message released successfully"",
                            ""%(count)d messages released successfully"",
                            len(mail_id)) % {""count"": len(mail_id)}
    else:
        message = error
    return ajax_response(
        request, ""ko"" if error else ""ok"", respmsg=message,
        url=QuarantineNavigationParameters(request).back_to_listing()
    )"
"def get_password_hasher(scheme):
    """"""Retrieve the hasher corresponding to :keyword:`scheme`.

    If no class is found, `PLAINHasher` is returned.

    :param str scheme: a valid scheme name
    :rtype: PasswordHasher sub class
    :return: The hasher class
    """"""
    try:
        scheme = scheme.replace('-', '')
        hasher = globals()['%sHasher' % scheme.upper()]
    except KeyError:
        hasher = PLAINHasher
    return hasher","def get_password_hasher(scheme):
    """"""Retrieve the hasher corresponding to :keyword:`scheme`.

    If no class is found, `PLAINHasher` is returned.

    :param str scheme: a valid scheme name
    :rtype: PasswordHasher sub class
    :return: The hasher class
    """"""
    try:
        scheme = scheme.replace('-', '')
        hasher = globals()['%sHasher' % scheme]
    except AttributeError:
        hasher = PLAINHasher
    return hasher"
"def print_summary(samples, prob=0.9, group_by_chain=True):
    """"""
    Prints a summary table displaying diagnostics of ``samples`` from the
    posterior. The diagnostics displayed are mean, standard deviation, median,
    the 90% Credibility Interval, :func:`~pyro.ops.stats.effective_sample_size`,
    :func:`~pyro.ops.stats.split_gelman_rubin`.

    :param dict samples: dictionary of samples keyed by site name.
    :param float prob: the probability mass of samples within the credibility interval.
    :param bool group_by_chain: If True, each variable in `samples`
        will be treated as having shape `num_chains x num_samples x sample_shape`.
        Otherwise, the corresponding shape will be `num_samples x sample_shape`
        (i.e. without chain dimension).
    """"""
    if len(samples) == 0:
        return
    summary_dict = summary(samples, prob, group_by_chain)

    row_names = {k: k + '[' + ','.join(map(lambda x: str(x - 1), v.shape[2:])) + ']'
                 for k, v in samples.items()}
    max_len = max(max(map(lambda x: len(x), row_names.values())), 10)
    name_format = '{:>' + str(max_len) + '}'
    header_format = name_format + ' {:>9}' * 7
    columns = [''] + list(list(summary_dict.values())[0].keys())

    print()
    print(header_format.format(*columns))

    row_format = name_format + ' {:>9.2f}' * 7
    for name, stats_dict in summary_dict.items():
        shape = stats_dict[""mean""].shape
        if len(shape) == 0:
            print(row_format.format(name, *stats_dict.values()))
        else:
            for idx in product(*map(range, shape)):
                idx_str = '[{}]'.format(','.join(map(str, idx)))
                print(row_format.format(name + idx_str, *[v[idx] for v in stats_dict.values()]))
    print()","def print_summary(samples, prob=0.9, group_by_chain=True):
    """"""
    Prints a summary table displaying diagnostics of ``samples`` from the
    posterior. The diagnostics displayed are mean, standard deviation, median,
    the 90% Credibility Interval, :func:`~pyro.ops.stats.effective_sample_size`,
    :func:`~pyro.ops.stats.split_gelman_rubin`.

    :param dict samples: dictionary of samples keyed by site name.
    :param float prob: the probability mass of samples within the credibility interval.
    :param bool group_by_chain: If True, each variable in `samples`
        will be treated as having shape `num_chains x num_samples x sample_shape`.
        Otherwise, the corresponding shape will be `num_samples x sample_shape`
        (i.e. without chain dimension).
    """"""
    summary_dict = summary(samples, prob, group_by_chain)

    row_names = {k: k + '[' + ','.join(map(lambda x: str(x - 1), v.shape[2:])) + ']'
                 for k, v in samples.items()}
    max_len = max(max(map(lambda x: len(x), row_names.values())), 10)
    name_format = '{:>' + str(max_len) + '}'
    header_format = name_format + ' {:>9}' * 7
    columns = [''] + list(list(summary_dict.values())[0].keys())

    print()
    print(header_format.format(*columns))

    row_format = name_format + ' {:>9.2f}' * 7
    for name, stats_dict in summary_dict.items():
        shape = stats_dict[""mean""].shape
        if len(shape) == 0:
            print(row_format.format(name, *stats_dict.values()))
        else:
            for idx in product(*map(range, shape)):
                idx_str = '[{}]'.format(','.join(map(str, idx)))
                print(row_format.format(name + idx_str, *[v[idx] for v in stats_dict.values()]))
    print()"
"def get_dependent_plate_dims(sites):
    """"""
    Return a list of dims for plates that are not common to all sites.
    """"""
    plate_sets = [site[""cond_indep_stack""]
                  for site in sites if site[""type""] == ""sample""]
    all_plates = set().union(*plate_sets)
    common_plates = all_plates.intersection(*plate_sets)
    sum_plates = all_plates - common_plates
    sum_dims = list(sorted(f.dim for f in sum_plates if f.dim is not None))
    return sum_dims","def get_dependent_plate_dims(sites):
    """"""
    Return a list of dims for plates that are not common to all sites.
    """"""
    plate_sets = [site[""cond_indep_stack""]
                  for site in sites if site[""type""] == ""sample""]
    all_plates = set().union(*plate_sets)
    common_plates = all_plates.intersection(*plate_sets)
    sum_plates = all_plates - common_plates
    sum_dims = list(sorted(f.dim for f in sum_plates))
    return sum_dims"
"    def run(self, *args, **kwargs):
        self._args, self._kwargs = args, kwargs
        num_samples = [0] * self.num_chains
        z_flat_acc = [[] for _ in range(self.num_chains)]
        with pyro.validation_enabled(not self.disable_validation):
            for x, chain_id in self.sampler.run(*args, **kwargs):
                if num_samples[chain_id] == 0:
                    num_samples[chain_id] += 1
                    z_structure = x
                elif num_samples[chain_id] == self.num_samples + 1:
                    self._diagnostics[chain_id] = x
                else:
                    num_samples[chain_id] += 1
                    if self.num_chains > 1:
                        x_cloned = x.clone()
                        del x
                    else:
                        x_cloned = x
                    z_flat_acc[chain_id].append(x_cloned)

        z_flat_acc = torch.stack([torch.stack(l) for l in z_flat_acc])

        # unpack latent
        pos = 0
        z_acc = z_structure.copy()
        for k in sorted(z_structure):
            shape = z_structure[k]
            next_pos = pos + shape.numel()
            z_acc[k] = z_flat_acc[:, :, pos:next_pos].reshape(
                (self.num_chains, self.num_samples) + shape)
            pos = next_pos
        assert pos == z_flat_acc.shape[-1]

        # If transforms is not explicitly provided, infer automatically using
        # model args, kwargs.
        if self.transforms is None:
            # Use `kernel.transforms` when available
            if hasattr(self.kernel, 'transforms') and self.kernel.transforms is not None:
                self.transforms = self.kernel.transforms
            # Else, get transforms from model (e.g. in multiprocessing).
            elif self.kernel.model:
                _, _, self.transforms, _ = initialize_model(self.kernel.model,
                                                            model_args=args,
                                                            model_kwargs=kwargs)
            # Assign default value
            else:
                self.transforms = {}

        # transform samples back to constrained space
        for name, transform in self.transforms.items():
            z_acc[name] = transform.inv(z_acc[name])
        self._samples = z_acc

        # terminate the sampler (shut down worker processes)
        self.sampler.terminate(True)","    def run(self, *args, **kwargs):
        self._args, self._kwargs = args, kwargs
        num_samples = [0] * self.num_chains
        z_flat_acc = [[] for _ in range(self.num_chains)]
        with pyro.validation_enabled(not self.disable_validation):
            for x, chain_id in self.sampler.run(*args, **kwargs):
                if num_samples[chain_id] == 0:
                    num_samples[chain_id] += 1
                    z_structure = x
                elif num_samples[chain_id] == self.num_samples + 1:
                    self._diagnostics[chain_id] = x
                else:
                    num_samples[chain_id] += 1
                    if self.num_chains > 1:
                        x_cloned = x.clone()
                        del x
                    else:
                        x_cloned = x
                    z_flat_acc[chain_id].append(x_cloned)

        z_flat_acc = torch.stack([torch.stack(l) for l in z_flat_acc])

        # unpack latent
        pos = 0
        z_acc = z_structure.copy()
        for k in sorted(z_structure):
            shape = z_structure[k]
            next_pos = pos + shape.numel()
            z_acc[k] = z_flat_acc[:, :, pos:next_pos].reshape(
                (self.num_chains, self.num_samples) + shape)
            pos = next_pos
        assert pos == z_flat_acc.shape[-1]

        # If transforms is not explicitly provided, infer automatically using
        # model args, kwargs.
        if self.transforms is None:
            if hasattr(self.kernel, 'transforms'):
                if self.kernel.transforms is not None:
                    self.transforms = self.kernel.transforms
            elif self.kernel.model:
                _, _, self.transforms, _ = initialize_model(self.kernel.model,
                                                            model_args=args,
                                                            model_kwargs=kwargs)
            else:
                self.transforms = {}

        # transform samples back to constrained space
        for name, transform in self.transforms.items():
            z_acc[name] = transform.inv(z_acc[name])
        self._samples = z_acc

        # terminate the sampler (shut down worker processes)
        self.sampler.terminate(True)"
"    def get_posterior(self, *args, **kwargs):
        """"""
        Returns a MultivariateNormal posterior distribution.
        """"""
        loc = pyro.param(""{}_loc"".format(self.prefix), self._init_loc)
        scale_tril = pyro.param(""{}_scale_tril"".format(self.prefix),
                                lambda: eye_like(loc, self.latent_dim),
                                constraint=constraints.lower_cholesky)
        return dist.MultivariateNormal(loc, scale_tril=scale_tril)","    def get_posterior(self, *args, **kwargs):
        """"""
        Returns a MultivariateNormal posterior distribution.
        """"""
        loc = pyro.param(""{}_loc"".format(self.prefix),
                         lambda: torch.zeros(self.latent_dim))
        scale_tril = pyro.param(""{}_scale_tril"".format(self.prefix),
                                lambda: torch.eye(self.latent_dim),
                                constraint=constraints.lower_cholesky)
        return dist.MultivariateNormal(loc, scale_tril=scale_tril)"
"    def get_posterior(self, *args, **kwargs):
        """"""
        Returns a diagonal Normal posterior distribution.
        """"""
        loc = pyro.param(""{}_loc"".format(self.prefix), self._init_loc)
        scale = pyro.param(""{}_scale"".format(self.prefix),
                           lambda: loc.new_ones(self.latent_dim),
                           constraint=constraints.positive)
        return dist.Normal(loc, scale).to_event(1)","    def get_posterior(self, *args, **kwargs):
        """"""
        Returns a diagonal Normal posterior distribution.
        """"""
        loc = pyro.param(""{}_loc"".format(self.prefix),
                         lambda: torch.zeros(self.latent_dim))
        scale = pyro.param(""{}_scale"".format(self.prefix),
                           lambda: torch.ones(self.latent_dim),
                           constraint=constraints.positive)
        return dist.Normal(loc, scale).to_event(1)"
"    def __init__(self, model, prefix=""auto"", init_loc_fn=init_to_median, rank=1):
        if not isinstance(rank, numbers.Number) or not rank > 0:
            raise ValueError(""Expected rank > 0 but got {}"".format(rank))
        self.rank = rank
        super(AutoLowRankMultivariateNormal, self).__init__(
            model, prefix=prefix, init_loc_fn=init_loc_fn)","    def __init__(self, model, prefix=""auto"", rank=1):
        if not isinstance(rank, numbers.Number) or not rank > 0:
            raise ValueError(""Expected rank > 0 but got {}"".format(rank))
        self.rank = rank
        super(AutoLowRankMultivariateNormal, self).__init__(model, prefix)"
"    def get_posterior(self, *args, **kwargs):
        """"""
        Returns a LowRankMultivariateNormal posterior distribution.
        """"""
        loc = pyro.param(""{}_loc"".format(self.prefix), self._init_loc)
        factor = pyro.param(""{}_cov_factor"".format(self.prefix),
                            lambda: loc.new_empty(self.latent_dim, self.rank).normal_(0, (0.5 / self.rank) ** 0.5))
        diagonal = pyro.param(""{}_cov_diag"".format(self.prefix),
                              lambda: loc.new_full((self.latent_dim,), 0.5),
                              constraint=constraints.positive)
        return dist.LowRankMultivariateNormal(loc, factor, diagonal)","    def get_posterior(self, *args, **kwargs):
        """"""
        Returns a LowRankMultivariateNormal posterior distribution.
        """"""
        loc = pyro.param(""{}_loc"".format(self.prefix),
                         lambda: torch.zeros(self.latent_dim))
        factor = pyro.param(""{}_cov_factor"".format(self.prefix),
                            lambda: torch.randn(self.latent_dim, self.rank) * (0.5 / self.rank) ** 0.5)
        diagonal = pyro.param(""{}_cov_diag"".format(self.prefix),
                              lambda: torch.ones(self.latent_dim) * 0.5,
                              constraint=constraints.positive)
        return dist.LowRankMultivariateNormal(loc, factor, diagonal)"
"    def __init__(self, model, hidden_dim=None, prefix=""auto"", init_loc_fn=init_to_median):
        self.hidden_dim = hidden_dim
        self.arn = None
        super(AutoIAFNormal, self).__init__(model, prefix=prefix, init_loc_fn=init_loc_fn)","    def __init__(self, model, hidden_dim=None, prefix=""auto""):
        self.hidden_dim = hidden_dim
        self.arn = None
        super(AutoIAFNormal, self).__init__(model, prefix)"
"    def get_posterior(self, *args, **kwargs):
        """"""
        Returns a Delta posterior distribution for MAP inference.
        """"""
        loc = pyro.param(""{}_loc"".format(self.prefix), self._init_loc)
        return dist.Delta(loc).to_event(1)","    def get_posterior(self, *args, **kwargs):
        """"""
        Returns a Delta posterior distribution for MAP inference.
        """"""
        loc = pyro.param(""{}_loc"".format(self.prefix),
                         lambda: torch.zeros(self.latent_dim))
        return dist.Delta(loc).to_event(1)"
"def main(args):
    baseball_dataset = pd.read_csv(DATA_URL, ""\\t"")
    train, _, player_names = train_test_split(baseball_dataset)
    at_bats, hits = train[:, 0], train[:, 1]
    nuts_kernel = NUTS(conditioned_model)
    logging.info(""Original Dataset:"")
    logging.info(baseball_dataset)

    # (1) Full Pooling Model
    posterior_fully_pooled = MCMC(nuts_kernel, num_samples=args.num_samples, warmup_steps=args.warmup_steps) \\
        .run(fully_pooled, at_bats, hits)
    logging.info(""\\nModel: Fully Pooled"")
    logging.info(""==================="")
    logging.info(""\\nphi:"")
    logging.info(summary(posterior_fully_pooled, sites=[""phi""], player_names=player_names)[""phi""])
    posterior_predictive = TracePredictive(fully_pooled,
                                           posterior_fully_pooled,
                                           num_samples=args.num_samples)
    sample_posterior_predictive(posterior_predictive, baseball_dataset)
    evaluate_log_predictive_density(fully_pooled, posterior_fully_pooled, baseball_dataset)

    # (2) No Pooling Model
    posterior_not_pooled = MCMC(nuts_kernel, num_samples=args.num_samples, warmup_steps=args.warmup_steps) \\
        .run(not_pooled, at_bats, hits)
    logging.info(""\\nModel: Not Pooled"")
    logging.info(""================="")
    logging.info(""\\nphi:"")
    logging.info(summary(posterior_not_pooled, sites=[""phi""], player_names=player_names)[""phi""])
    posterior_predictive = TracePredictive(not_pooled,
                                           posterior_not_pooled,
                                           num_samples=args.num_samples)
    sample_posterior_predictive(posterior_predictive, baseball_dataset)
    evaluate_log_predictive_density(not_pooled, posterior_not_pooled, baseball_dataset)

    # (3) Partially Pooled Model
    # TODO: remove once htps://github.com/uber/pyro/issues/1458 is resolved
    if ""CI"" not in os.environ:
        posterior_partially_pooled = MCMC(nuts_kernel, num_samples=args.num_samples, warmup_steps=args.warmup_steps) \\
            .run(partially_pooled, at_bats, hits)
        logging.info(""\\nModel: Partially Pooled"")
        logging.info(""======================="")
        logging.info(""\\nphi:"")
        logging.info(summary(posterior_partially_pooled, sites=[""phi""],
                             player_names=player_names)[""phi""])
        posterior_predictive = TracePredictive(partially_pooled,
                                               posterior_partially_pooled,
                                               num_samples=args.num_samples)
        sample_posterior_predictive(posterior_predictive, baseball_dataset)
        evaluate_log_predictive_density(partially_pooled, posterior_partially_pooled, baseball_dataset)

    # (4) Partially Pooled with Logit Model
    posterior_partially_pooled_with_logit = MCMC(nuts_kernel, num_samples=args.num_samples,
                                                 warmup_steps=args.warmup_steps) \\
        .run(partially_pooled_with_logit, at_bats, hits)
    logging.info(""\\nModel: Partially Pooled with Logit"")
    logging.info(""=================================="")
    logging.info(""\\nSigmoid(alpha):"")
    logging.info(summary(posterior_partially_pooled_with_logit,
                         sites=[""alpha""],
                         player_names=player_names,
                         transforms={""alpha"": lambda x: 1. / (1 + np.exp(-x))})[""alpha""])
    posterior_predictive = TracePredictive(partially_pooled_with_logit,
                                           posterior_partially_pooled_with_logit,
                                           num_samples=args.num_samples)
    sample_posterior_predictive(posterior_predictive, baseball_dataset)
    evaluate_log_predictive_density(partially_pooled_with_logit,
                                    posterior_partially_pooled_with_logit, baseball_dataset)","def main(args):
    baseball_dataset = pd.read_csv(DATA_URL, ""\\t"")
    train, _, player_names = train_test_split(baseball_dataset)
    at_bats, hits = train[:, 0], train[:, 1]
    nuts_kernel = NUTS(conditioned_model)
    logging.info(""Original Dataset:"")
    logging.info(baseball_dataset)

    # (1) Full Pooling Model
    posterior_fully_pooled = MCMC(nuts_kernel, num_samples=args.num_samples, warmup_steps=args.warmup_steps) \\
        .run(fully_pooled, at_bats, hits)
    logging.info(""\\nModel: Fully Pooled"")
    logging.info(""==================="")
    logging.info(""\\nphi:"")
    logging.info(summary(posterior_fully_pooled, sites=[""phi""], player_names=player_names)[""phi""])
    posterior_predictive = TracePredictive(fully_pooled,
                                           posterior_fully_pooled,
                                           num_samples=args.num_samples)
    sample_posterior_predictive(posterior_predictive, baseball_dataset)
    evaluate_log_predictive_density(fully_pooled, posterior_fully_pooled, baseball_dataset)

    # (2) No Pooling Model
    posterior_not_pooled = MCMC(nuts_kernel, num_samples=args.num_samples, warmup_steps=args.warmup_steps) \\
        .run(not_pooled, at_bats, hits)
    logging.info(""\\nModel: Not Pooled"")
    logging.info(""================="")
    logging.info(""\\nphi:"")
    logging.info(summary(posterior_not_pooled, sites=[""phi""], player_names=player_names)[""phi""])
    posterior_predictive = TracePredictive(not_pooled,
                                           posterior_not_pooled,
                                           num_samples=args.num_samples)
    sample_posterior_predictive(posterior_predictive, baseball_dataset)
    evaluate_log_predictive_density(not_pooled, posterior_not_pooled, baseball_dataset)

    # (3) Partially Pooled Model
    posterior_partially_pooled = MCMC(nuts_kernel, num_samples=args.num_samples, warmup_steps=args.warmup_steps) \\
        .run(partially_pooled, at_bats, hits)
    logging.info(""\\nModel: Partially Pooled"")
    logging.info(""======================="")
    logging.info(""\\nphi:"")
    logging.info(summary(posterior_partially_pooled, sites=[""phi""],
                         player_names=player_names)[""phi""])
    posterior_predictive = TracePredictive(partially_pooled,
                                           posterior_partially_pooled,
                                           num_samples=args.num_samples)
    sample_posterior_predictive(posterior_predictive, baseball_dataset)
    evaluate_log_predictive_density(partially_pooled, posterior_partially_pooled, baseball_dataset)

    # (4) Partially Pooled with Logit Model
    posterior_partially_pooled_with_logit = MCMC(nuts_kernel, num_samples=args.num_samples,
                                                 warmup_steps=args.warmup_steps) \\
        .run(partially_pooled_with_logit, at_bats, hits)
    logging.info(""\\nModel: Partially Pooled with Logit"")
    logging.info(""=================================="")
    logging.info(""\\nSigmoid(alpha):"")
    logging.info(summary(posterior_partially_pooled_with_logit,
                         sites=[""alpha""],
                         player_names=player_names,
                         transforms={""alpha"": lambda x: 1. / (1 + np.exp(-x))})[""alpha""])
    posterior_predictive = TracePredictive(partially_pooled_with_logit,
                                           posterior_partially_pooled_with_logit,
                                           num_samples=args.num_samples)
    sample_posterior_predictive(posterior_predictive, baseball_dataset)
    evaluate_log_predictive_density(partially_pooled_with_logit,
                                    posterior_partially_pooled_with_logit, baseball_dataset)"
"def main(args):
    pyro.set_rng_seed(args.rng_seed)
    baseball_dataset = pd.read_csv(DATA_URL, ""\\t"")
    train, _, player_names = train_test_split(baseball_dataset)
    at_bats, hits = train[:, 0], train[:, 1]
    nuts_kernel = NUTS(conditioned_model)
    logging.info(""Original Dataset:"")
    logging.info(baseball_dataset)

    # (1) Full Pooling Model
    posterior_fully_pooled = MCMC(nuts_kernel, num_samples=args.num_samples, warmup_steps=args.warmup_steps) \\
        .run(fully_pooled, at_bats, hits)
    logging.info(""\\nModel: Fully Pooled"")
    logging.info(""==================="")
    logging.info(""\\nphi:"")
    logging.info(summary(posterior_fully_pooled, sites=[""phi""], player_names=player_names)[""phi""])
    posterior_predictive = TracePredictive(fully_pooled,
                                           posterior_fully_pooled,
                                           num_samples=args.num_samples)
    sample_posterior_predictive(posterior_predictive, baseball_dataset)
    evaluate_log_predictive_density(fully_pooled, posterior_fully_pooled, baseball_dataset)

    # (2) No Pooling Model
    posterior_not_pooled = MCMC(nuts_kernel, num_samples=args.num_samples, warmup_steps=args.warmup_steps) \\
        .run(not_pooled, at_bats, hits)
    logging.info(""\\nModel: Not Pooled"")
    logging.info(""================="")
    logging.info(""\\nphi:"")
    logging.info(summary(posterior_not_pooled, sites=[""phi""], player_names=player_names)[""phi""])
    posterior_predictive = TracePredictive(not_pooled,
                                           posterior_not_pooled,
                                           num_samples=args.num_samples)
    sample_posterior_predictive(posterior_predictive, baseball_dataset)
    evaluate_log_predictive_density(not_pooled, posterior_not_pooled, baseball_dataset)

    # (3) Partially Pooled Model
    # TODO: remove once htps://github.com/uber/pyro/issues/1458 is resolved
    if ""CI"" not in os.environ:
        posterior_partially_pooled = MCMC(nuts_kernel, num_samples=args.num_samples, warmup_steps=args.warmup_steps) \\
            .run(partially_pooled, at_bats, hits)
        logging.info(""\\nModel: Partially Pooled"")
        logging.info(""======================="")
        logging.info(""\\nphi:"")
        logging.info(summary(posterior_partially_pooled, sites=[""phi""],
                             player_names=player_names)[""phi""])
        posterior_predictive = TracePredictive(partially_pooled,
                                               posterior_partially_pooled,
                                               num_samples=args.num_samples)
        sample_posterior_predictive(posterior_predictive, baseball_dataset)
        evaluate_log_predictive_density(partially_pooled, posterior_partially_pooled, baseball_dataset)

    # (4) Partially Pooled with Logit Model
    posterior_partially_pooled_with_logit = MCMC(nuts_kernel, num_samples=args.num_samples,
                                                 warmup_steps=args.warmup_steps) \\
        .run(partially_pooled_with_logit, at_bats, hits)
    logging.info(""\\nModel: Partially Pooled with Logit"")
    logging.info(""=================================="")
    logging.info(""\\nSigmoid(alpha):"")
    logging.info(summary(posterior_partially_pooled_with_logit,
                         sites=[""alpha""],
                         player_names=player_names,
                         transforms={""alpha"": lambda x: 1. / (1 + np.exp(-x))})[""alpha""])
    posterior_predictive = TracePredictive(partially_pooled_with_logit,
                                           posterior_partially_pooled_with_logit,
                                           num_samples=args.num_samples)
    sample_posterior_predictive(posterior_predictive, baseball_dataset)
    evaluate_log_predictive_density(partially_pooled_with_logit,
                                    posterior_partially_pooled_with_logit, baseball_dataset)","def main(args):
    baseball_dataset = pd.read_csv(DATA_URL, ""\\t"")
    train, _, player_names = train_test_split(baseball_dataset)
    at_bats, hits = train[:, 0], train[:, 1]
    nuts_kernel = NUTS(conditioned_model)
    logging.info(""Original Dataset:"")
    logging.info(baseball_dataset)

    # (1) Full Pooling Model
    posterior_fully_pooled = MCMC(nuts_kernel, num_samples=args.num_samples, warmup_steps=args.warmup_steps) \\
        .run(fully_pooled, at_bats, hits)
    logging.info(""\\nModel: Fully Pooled"")
    logging.info(""==================="")
    logging.info(""\\nphi:"")
    logging.info(summary(posterior_fully_pooled, sites=[""phi""], player_names=player_names)[""phi""])
    posterior_predictive = TracePredictive(fully_pooled,
                                           posterior_fully_pooled,
                                           num_samples=args.num_samples)
    sample_posterior_predictive(posterior_predictive, baseball_dataset)
    evaluate_log_predictive_density(fully_pooled, posterior_fully_pooled, baseball_dataset)

    # (2) No Pooling Model
    posterior_not_pooled = MCMC(nuts_kernel, num_samples=args.num_samples, warmup_steps=args.warmup_steps) \\
        .run(not_pooled, at_bats, hits)
    logging.info(""\\nModel: Not Pooled"")
    logging.info(""================="")
    logging.info(""\\nphi:"")
    logging.info(summary(posterior_not_pooled, sites=[""phi""], player_names=player_names)[""phi""])
    posterior_predictive = TracePredictive(not_pooled,
                                           posterior_not_pooled,
                                           num_samples=args.num_samples)
    sample_posterior_predictive(posterior_predictive, baseball_dataset)
    evaluate_log_predictive_density(not_pooled, posterior_not_pooled, baseball_dataset)

    # (3) Partially Pooled Model
    # TODO: remove once htps://github.com/uber/pyro/issues/1458 is resolved
    if ""CI"" not in os.environ:
        posterior_partially_pooled = MCMC(nuts_kernel, num_samples=args.num_samples, warmup_steps=args.warmup_steps) \\
            .run(partially_pooled, at_bats, hits)
        logging.info(""\\nModel: Partially Pooled"")
        logging.info(""======================="")
        logging.info(""\\nphi:"")
        logging.info(summary(posterior_partially_pooled, sites=[""phi""],
                             player_names=player_names)[""phi""])
        posterior_predictive = TracePredictive(partially_pooled,
                                               posterior_partially_pooled,
                                               num_samples=args.num_samples)
        sample_posterior_predictive(posterior_predictive, baseball_dataset)
        evaluate_log_predictive_density(partially_pooled, posterior_partially_pooled, baseball_dataset)

    # (4) Partially Pooled with Logit Model
    posterior_partially_pooled_with_logit = MCMC(nuts_kernel, num_samples=args.num_samples,
                                                 warmup_steps=args.warmup_steps) \\
        .run(partially_pooled_with_logit, at_bats, hits)
    logging.info(""\\nModel: Partially Pooled with Logit"")
    logging.info(""=================================="")
    logging.info(""\\nSigmoid(alpha):"")
    logging.info(summary(posterior_partially_pooled_with_logit,
                         sites=[""alpha""],
                         player_names=player_names,
                         transforms={""alpha"": lambda x: 1. / (1 + np.exp(-x))})[""alpha""])
    posterior_predictive = TracePredictive(partially_pooled_with_logit,
                                           posterior_partially_pooled_with_logit,
                                           num_samples=args.num_samples)
    sample_posterior_predictive(posterior_predictive, baseball_dataset)
    evaluate_log_predictive_density(partially_pooled_with_logit,
                                    posterior_partially_pooled_with_logit, baseball_dataset)"
"    def _kinetic_energy(self, r):
        # TODO: revert to `torch.dot` in pytorch==1.0
        # See: https://github.com/uber/pyro/issues/1458
        r_flat = torch.cat([r[site_name].reshape(-1) for site_name in sorted(r)])
        if self.full_mass:
            return 0.5 * (r_flat * (self._inverse_mass_matrix.matmul(r_flat))).sum()
        else:
            return 0.5 * (self._inverse_mass_matrix * (r_flat ** 2)).sum()","    def _kinetic_energy(self, r):
        r_flat = torch.cat([r[site_name].reshape(-1) for site_name in sorted(r)])
        if self.full_mass:
            return 0.5 * r_flat.dot(self._inverse_mass_matrix.matmul(r_flat))
        else:
            return 0.5 * self._inverse_mass_matrix.dot(r_flat ** 2)"
"    def model(self, xs, ys=None):
        """"""
        The model corresponds to the following generative process:
        p(z) = normal(0,I)              # handwriting style (latent)
        p(y|x) = categorical(I/10.)     # which digit (semi-supervised)
        p(x|y,z) = bernoulli(mu(y,z))   # an image
        mu is given by a neural network  `decoder`

        :param xs: a batch of scaled vectors of pixels from an image
        :param ys: (optional) a batch of the class labels i.e.
                   the digit corresponding to the image(s)
        :return: None
        """"""
        # register this pytorch module and all of its sub-modules with pyro
        pyro.module(""ss_vae"", self)

        batch_size = xs.size(0)
        with pyro.iarange(""independent""):

            # sample the handwriting style from the constant prior distribution
            prior_mu = Variable(torch.zeros([batch_size, self.z_dim]))
            prior_sigma = Variable(torch.ones([batch_size, self.z_dim]))
            zs = pyro.sample(""z"", dist.normal, prior_mu, prior_sigma, extra_event_dims=1)

            # if the label y (which digit to write) is supervised, sample from the
            # constant prior, otherwise, observe the value (i.e. score it against the constant prior)
            alpha_prior = Variable(torch.ones([batch_size, self.output_size]) / (1.0 * self.output_size))
            if ys is None:
                ys = pyro.sample(""y"", dist.one_hot_categorical, alpha_prior)
            else:
                pyro.sample(""y"", dist.one_hot_categorical, alpha_prior, obs=ys)

            # finally, score the image (x) using the handwriting style (z) and
            # the class label y (which digit to write) against the
            # parametrized distribution p(x|y,z) = bernoulli(decoder(y,z))
            # where `decoder` is a neural network
            mu = self.decoder.forward([zs, ys])
            pyro.sample(""x"", dist.bernoulli, mu, extra_event_dims=1, obs=xs)","    def model(self, xs, ys=None):
        """"""
        The model corresponds to the following generative process:
        p(z) = normal(0,I)              # handwriting style (latent)
        p(y|x) = categorical(I/10.)     # which digit (semi-supervised)
        p(x|y,z) = bernoulli(mu(y,z))   # an image
        mu is given by a neural network  `decoder`

        :param xs: a batch of scaled vectors of pixels from an image
        :param ys: (optional) a batch of the class labels i.e.
                   the digit corresponding to the image(s)
        :return: None
        """"""
        # register this pytorch module and all of its sub-modules with pyro
        pyro.module(""ss_vae"", self)

        batch_size = xs.size(0)
        with pyro.iarange(""independent""):

            # sample the handwriting style from the constant prior distribution
            prior_mu = Variable(torch.zeros([batch_size, self.z_dim]))
            prior_sigma = Variable(torch.ones([batch_size, self.z_dim]))
            zs = pyro.sample(""z"", dist.normal, prior_mu, prior_sigma)

            # if the label y (which digit to write) is supervised, sample from the
            # constant prior, otherwise, observe the value (i.e. score it against the constant prior)
            alpha_prior = Variable(torch.ones([batch_size, self.output_size]) / (1.0 * self.output_size))
            if ys is None:
                ys = pyro.sample(""y"", dist.one_hot_categorical, alpha_prior)
            else:
                pyro.sample(""y"", dist.one_hot_categorical, alpha_prior, obs=ys)

            # finally, score the image (x) using the handwriting style (z) and
            # the class label y (which digit to write) against the
            # parametrized distribution p(x|y,z) = bernoulli(decoder(y,z))
            # where `decoder` is a neural network
            mu = self.decoder.forward([zs, ys])
            pyro.sample(""x"", dist.bernoulli, mu, obs=xs)"
"    def guide(self, xs, ys=None):
        """"""
        The guide corresponds to the following:
        q(y|x) = categorical(alpha(x))              # infer digit from an image
        q(z|x,y) = normal(mu(x,y),sigma(x,y))       # infer handwriting style from an image and the digit
        mu, sigma are given by a neural network `encoder_z`
        alpha is given by a neural network `encoder_y`
        :param xs: a batch of scaled vectors of pixels from an image
        :param ys: (optional) a batch of the class labels i.e.
                   the digit corresponding to the image(s)
        :return: None
        """"""
        # inform Pyro that the variables in the batch of xs, ys are conditionally independent
        with pyro.iarange(""independent""):

            # if the class label (the digit) is not supervised, sample
            # (and score) the digit with the variational distribution
            # q(y|x) = categorical(alpha(x))
            if ys is None:
                alpha = self.encoder_y.forward(xs)
                ys = pyro.sample(""y"", dist.one_hot_categorical, alpha)

            # sample (and score) the latent handwriting-style with the variational
            # distribution q(z|x,y) = normal(mu(x,y),sigma(x,y))
            mu, sigma = self.encoder_z.forward([xs, ys])
            pyro.sample(""z"", dist.normal, mu, sigma, extra_event_dims=1)","    def guide(self, xs, ys=None):
        """"""
        The guide corresponds to the following:
        q(y|x) = categorical(alpha(x))              # infer digit from an image
        q(z|x,y) = normal(mu(x,y),sigma(x,y))       # infer handwriting style from an image and the digit
        mu, sigma are given by a neural network `encoder_z`
        alpha is given by a neural network `encoder_y`
        :param xs: a batch of scaled vectors of pixels from an image
        :param ys: (optional) a batch of the class labels i.e.
                   the digit corresponding to the image(s)
        :return: None
        """"""
        # inform Pyro that the variables in the batch of xs, ys are conditionally independent
        with pyro.iarange(""independent""):

            # if the class label (the digit) is not supervised, sample
            # (and score) the digit with the variational distribution
            # q(y|x) = categorical(alpha(x))
            if ys is None:
                alpha = self.encoder_y.forward(xs)
                ys = pyro.sample(""y"", dist.one_hot_categorical, alpha)

            # sample (and score) the latent handwriting-style with the variational
            # distribution q(z|x,y) = normal(mu(x,y),sigma(x,y))
            mu, sigma = self.encoder_z.forward([xs, ys])
            zs = pyro.sample(""z"", dist.normal, mu, sigma)   # noqa: F841"
"    def model_sample(self, ys, batch_size=1):
        # sample the handwriting style from the constant prior distribution
        prior_mu = Variable(torch.zeros([batch_size, self.z_dim]))
        prior_sigma = Variable(torch.ones([batch_size, self.z_dim]))
        zs = pyro.sample(""z"", dist.normal, prior_mu, prior_sigma, extra_event_dims=1)

        # sample an image using the decoder
        mu = self.decoder.forward([zs, ys])
        xs = pyro.sample(""sample"", dist.bernoulli, mu, extra_event_dims=1)
        return xs, mu","    def model_sample(self, ys, batch_size=1):
        # sample the handwriting style from the constant prior distribution
        prior_mu = Variable(torch.zeros([batch_size, self.z_dim]))
        prior_sigma = Variable(torch.ones([batch_size, self.z_dim]))
        zs = pyro.sample(""z"", dist.normal, prior_mu, prior_sigma)

        # sample an image using the decoder
        mu = self.decoder.forward([zs, ys])
        xs = pyro.sample(""sample"", dist.bernoulli, mu)
        return xs, mu"
"def torch_multinomial(input, num_samples, replacement=False):
    """"""
    Like `torch.multinomial()` but works with cuda tensors.
    Does not support keyword argument `out`.
    """"""
    if input.is_cuda:
        return torch.multinomial(input.cpu(), num_samples, replacement).cuda(input.get_device())
    else:
        return torch.multinomial(input, num_samples, replacement)","def torch_multinomial(input, num_samples, replacement=False):
    """"""
    Like `torch.multinomial()` but works with cuda tensors.
    Does not support keyword argument `out`.
    """"""
    if input.is_cuda:
        return torch_multinomial(input.cpu(), num_samples, replacement).cuda()
    else:
        return torch.multinomial(input, num_samples, replacement)"
"def iter_discrete_traces(graph_type, fn, *args, **kwargs):
    """"""
    Iterate over all discrete choices of a stochastic function.

    When sampling continuous random variables, this behaves like `fn`.
    When sampling discrete random variables, this iterates over all choices.

    This yields `(scale, trace)` pairs, where `scale` is the probability of the
    discrete choices made in the `trace`.

    :param str graph_type: The type of the graph, e.g. ""flat"" or ""dense"".
    :param callable fn: A stochastic function.
    :returns: An iterator over (scale, trace) pairs.
    """"""
    queue = LifoQueue()
    queue.put(Trace())
    while not queue.empty():
        partial_trace = queue.get()
        escape_fn = functools.partial(util.discrete_escape, partial_trace)
        traced_fn = poutine.trace(poutine.escape(poutine.replay(fn, partial_trace), escape_fn),
                                  graph_type=graph_type)
        try:
            full_trace = traced_fn.get_trace(*args, **kwargs)
        except util.NonlocalExit as e:
            for extended_trace in util.enum_extend(traced_fn.trace.copy(), e.site):
                queue.put(extended_trace)
            continue

        # Scale trace by probability of discrete choices.
        log_pdf = full_trace.batch_log_pdf(site_filter=site_is_discrete)
        if isinstance(log_pdf, Variable):
            scale = torch.exp(log_pdf.detach())
        else:
            scale = math.exp(log_pdf)
        yield scale, full_trace","def iter_discrete_traces(graph_type, fn, *args, **kwargs):
    """"""
    Iterate over all discrete choices of a stochastic function.

    When sampling continuous random variables, this behaves like `fn`.
    When sampling discrete random variables, this iterates over all choices.

    This yields `(scale, trace)` pairs, where `scale` is the probability of the
    discrete choices made in the `trace`.

    :param str graph_type: The type of the graph, e.g. ""flat"" or ""dense"".
    :param callable fn: A stochastic function.
    :returns: An iterator over (scale, trace) pairs.
    """"""
    queue = LifoQueue()
    queue.put(Trace())
    while not queue.empty():
        partial_trace = queue.get()
        escape_fn = functools.partial(util.discrete_escape, partial_trace)
        traced_fn = poutine.trace(poutine.escape(poutine.replay(fn, partial_trace), escape_fn),
                                  graph_type=graph_type)
        try:
            full_trace = traced_fn.get_trace(*args, **kwargs)
        except util.NonlocalExit as e:
            for extended_trace in util.enum_extend(traced_fn.trace.copy(), e.site):
                queue.put(extended_trace)
            continue

        # Scale trace by probability of discrete choices.
        log_pdf = full_trace.batch_log_pdf(site_filter=site_is_discrete)
        if isinstance(log_pdf, float):
            log_pdf = torch.Tensor([log_pdf])
        if isinstance(log_pdf, torch.Tensor):
            log_pdf = Variable(log_pdf)
        scale = torch.exp(log_pdf.detach())
        yield scale, full_trace"
"    def loss(self, model, guide, *args, **kwargs):
        """"""
        :returns: returns an estimate of the ELBO
        :rtype: float

        Evaluates the ELBO with an estimator that uses num_particles many samples/particles.
        """"""
        elbo = 0.0
        for weight, model_trace, guide_trace, log_r in self._get_traces(model, guide, *args, **kwargs):
            elbo_particle = weight * 0

            if (self.enum_discrete and isinstance(weight, Variable) and weight.size(0) > 1):
                log_pdf = ""batch_log_pdf""
            else:
                log_pdf = ""log_pdf""
            for name in model_trace.nodes.keys():
                if model_trace.nodes[name][""type""] == ""sample"":
                    if model_trace.nodes[name][""is_observed""]:
                        elbo_particle += model_trace.nodes[name][log_pdf]
                    else:
                        elbo_particle += model_trace.nodes[name][log_pdf]
                        elbo_particle -= guide_trace.nodes[name][log_pdf]

            # drop terms of weight zero to avoid nans
            if isinstance(weight, numbers.Number):
                if weight == 0.0:
                    elbo_particle = torch_zeros_like(elbo_particle)
            else:
                elbo_particle[weight == 0] = 0.0

            elbo += torch_data_sum(weight * elbo_particle)

        loss = -elbo
        if np.isnan(loss):
            warnings.warn('Encountered NAN loss')
        return loss","    def loss(self, model, guide, *args, **kwargs):
        """"""
        :returns: returns an estimate of the ELBO
        :rtype: float

        Evaluates the ELBO with an estimator that uses num_particles many samples/particles.
        """"""
        elbo = 0.0
        for weight, model_trace, guide_trace, log_r in self._get_traces(model, guide, *args, **kwargs):
            elbo_particle = weight * 0

            log_pdf = ""batch_log_pdf"" if (self.enum_discrete and weight.size(0) > 1) else ""log_pdf""
            for name in model_trace.nodes.keys():
                if model_trace.nodes[name][""type""] == ""sample"":
                    if model_trace.nodes[name][""is_observed""]:
                        elbo_particle += model_trace.nodes[name][log_pdf]
                    else:
                        elbo_particle += model_trace.nodes[name][log_pdf]
                        elbo_particle -= guide_trace.nodes[name][log_pdf]

            # drop terms of weight zero to avoid nans
            if isinstance(weight, numbers.Number):
                if weight == 0.0:
                    elbo_particle = torch_zeros_like(elbo_particle)
            else:
                elbo_particle[weight == 0] = 0.0

            elbo += torch_data_sum(weight * elbo_particle)

        loss = -elbo
        if np.isnan(loss):
            warnings.warn('Encountered NAN loss')
        return loss"
"    def loss_and_grads(self, model, guide, *args, **kwargs):
        """"""
        :returns: returns an estimate of the ELBO
        :rtype: float

        Computes the ELBO as well as the surrogate ELBO that is used to form the gradient estimator.
        Performs backward on the latter. Num_particle many samples are used to form the estimators.
        """"""
        elbo = 0.0
        # grab a trace from the generator
        for weight, model_trace, guide_trace, log_r in self._get_traces(model, guide, *args, **kwargs):
            elbo_particle = weight * 0
            surrogate_elbo_particle = weight * 0
            # compute elbo and surrogate elbo
            if (self.enum_discrete and isinstance(weight, Variable) and weight.size(0) > 1):
                log_pdf = ""batch_log_pdf""
            else:
                log_pdf = ""log_pdf""
            for name, model_site in model_trace.nodes.items():
                if model_site[""type""] == ""sample"":
                    if model_site[""is_observed""]:
                        elbo_particle += model_site[log_pdf]
                        surrogate_elbo_particle += model_site[log_pdf]
                    else:
                        guide_site = guide_trace.nodes[name]
                        lp_lq = model_site[log_pdf] - guide_site[log_pdf]
                        elbo_particle += lp_lq
                        if guide_site[""fn""].reparameterized:
                            surrogate_elbo_particle += lp_lq
                        else:
                            # XXX should the user be able to control inclusion of the -logq term below?
                            guide_log_pdf = guide_site[log_pdf] / guide_site[""scale""]  # not scaled by subsampling
                            surrogate_elbo_particle += model_site[log_pdf] + log_r.detach() * guide_log_pdf

            # drop terms of weight zero to avoid nans
            if isinstance(weight, numbers.Number):
                if weight == 0.0:
                    elbo_particle = torch_zeros_like(elbo_particle)
                    surrogate_elbo_particle = torch_zeros_like(surrogate_elbo_particle)
            else:
                weight_eq_zero = (weight == 0)
                elbo_particle[weight_eq_zero] = 0.0
                surrogate_elbo_particle[weight_eq_zero] = 0.0

            elbo += torch_data_sum(weight * elbo_particle)
            surrogate_elbo_particle = torch_sum(weight * surrogate_elbo_particle)

            # collect parameters to train from model and guide
            trainable_params = set(site[""value""]
                                   for trace in (model_trace, guide_trace)
                                   for site in trace.nodes.values()
                                   if site[""type""] == ""param"")

            if trainable_params:
                surrogate_loss_particle = -surrogate_elbo_particle
                torch_backward(surrogate_loss_particle)
                pyro.get_param_store().mark_params_active(trainable_params)

        loss = -elbo
        if np.isnan(loss):
            warnings.warn('Encountered NAN loss')
        return loss","    def loss_and_grads(self, model, guide, *args, **kwargs):
        """"""
        :returns: returns an estimate of the ELBO
        :rtype: float

        Computes the ELBO as well as the surrogate ELBO that is used to form the gradient estimator.
        Performs backward on the latter. Num_particle many samples are used to form the estimators.
        """"""
        elbo = 0.0
        # grab a trace from the generator
        for weight, model_trace, guide_trace, log_r in self._get_traces(model, guide, *args, **kwargs):
            elbo_particle = weight * 0
            surrogate_elbo_particle = weight * 0
            # compute elbo and surrogate elbo
            log_pdf = ""batch_log_pdf"" if (self.enum_discrete and weight.size(0) > 1) else ""log_pdf""
            for name, model_site in model_trace.nodes.items():
                if model_site[""type""] == ""sample"":
                    if model_site[""is_observed""]:
                        elbo_particle += model_site[log_pdf]
                        surrogate_elbo_particle += model_site[log_pdf]
                    else:
                        guide_site = guide_trace.nodes[name]
                        lp_lq = model_site[log_pdf] - guide_site[log_pdf]
                        elbo_particle += lp_lq
                        if guide_site[""fn""].reparameterized:
                            surrogate_elbo_particle += lp_lq
                        else:
                            # XXX should the user be able to control inclusion of the -logq term below?
                            guide_log_pdf = guide_site[log_pdf] / guide_site[""scale""]  # not scaled by subsampling
                            surrogate_elbo_particle += model_site[log_pdf] + log_r.detach() * guide_log_pdf

            # drop terms of weight zero to avoid nans
            if isinstance(weight, numbers.Number):
                if weight == 0.0:
                    elbo_particle = torch_zeros_like(elbo_particle)
                    surrogate_elbo_particle = torch_zeros_like(surrogate_elbo_particle)
            else:
                weight_eq_zero = (weight == 0)
                elbo_particle[weight_eq_zero] = 0.0
                surrogate_elbo_particle[weight_eq_zero] = 0.0

            elbo += torch_data_sum(weight * elbo_particle)
            surrogate_elbo_particle = torch_sum(weight * surrogate_elbo_particle)

            # collect parameters to train from model and guide
            trainable_params = set(site[""value""]
                                   for trace in (model_trace, guide_trace)
                                   for site in trace.nodes.values()
                                   if site[""type""] == ""param"")

            if trainable_params:
                surrogate_loss_particle = -surrogate_elbo_particle
                torch_backward(surrogate_loss_particle)
                pyro.get_param_store().mark_params_active(trainable_params)

        loss = -elbo
        if np.isnan(loss):
            warnings.warn('Encountered NAN loss')
        return loss"
"def resolve_is_lower_case(tokenizer):
    if isinstance(tokenizer, transformers.BertTokenizer):
        return tokenizer.basic_tokenizer.do_lower_case
    if isinstance(tokenizer, transformers.AlbertTokenizer):
        return tokenizer.do_lower_case
    else:
        return False","def resolve_is_lower_case(tokenizer):
    if isinstance(tokenizer, (transformers.BertTokenizer, transformers.AlbertTokenizer)):
        return tokenizer.basic_tokenizer.do_lower_case
    else:
        return False"
"    def tokenize(self, tokenizer):
        passage = (
            self.passage.lower()
            if model_resolution.resolve_is_lower_case(tokenizer=tokenizer)
            else self.passage
        )
        passage_tokens = tokenizer.tokenize(passage)
        token_aligner = TokenAligner(source=passage, target=passage_tokens)
        answer_token_span = token_aligner.project_char_to_token_span(
            self.answer_char_span[0], self.answer_char_span[1], inclusive=True
        )

        return TokenizedExample(
            guid=self.guid,
            passage=passage_tokens,
            question=tokenizer.tokenize(self.question),
            answer_str=self.answer,
            passage_str=passage,
            answer_token_span=answer_token_span,
            token_idx_to_char_idx_map=token_aligner.source_char_idx_to_target_token_idx.T,
        )","    def tokenize(self, tokenizer):
        passage = (
            self.passage.lower() if resolve_is_lower_case(tokenizer=tokenizer) else self.passage
        )
        passage_tokens = tokenizer.tokenize(passage)
        token_aligner = TokenAligner(source=passage, target=passage_tokens)
        answer_token_span = token_aligner.project_char_to_token_span(
            self.answer_char_span[0], self.answer_char_span[1], inclusive=True
        )

        return TokenizedExample(
            guid=self.guid,
            passage=passage_tokens,
            question=tokenizer.tokenize(self.question),
            answer_str=self.answer,
            passage_str=passage,
            answer_token_span=answer_token_span,
            token_idx_to_char_idx_map=token_aligner.source_char_idx_to_target_token_idx.T,
        )"
"def get_task_specific_params(args, task_name):
    """""" Search args for parameters specific to task.
    Args:
        args: main-program args, a config.Params object
        task_name: (string)
    Returns:
        AllenNLP Params object of task-specific params.
    """"""

    def _get_task_attr(attr_name, default=None):
        return config.get_task_attr(args, task_name, attr_name, default)

    params = {}
    params[""cls_type""] = _get_task_attr(""classifier"")
    params[""d_hid""] = _get_task_attr(""classifier_hid_dim"")
    params[""d_proj""] = _get_task_attr(""d_proj"")
    params[""shared_pair_attn""] = args.shared_pair_attn
    if args.shared_pair_attn:
        params[""attn""] = args.pair_attn
        params[""d_hid_attn""] = args.d_hid_attn
        params[""dropout""] = args.classifier_dropout
    else:
        params[""attn""] = _get_task_attr(""pair_attn"")
        params[""d_hid_attn""] = _get_task_attr(""d_hid_attn"")
        params[""dropout""] = _get_task_attr(""classifier_dropout"")

    # Used for span/edge classification. Other tasks can safely ignore.
    params[""cls_loss_fn""] = _get_task_attr(""span_classifier_loss_fn"")
    params[""cls_span_pooling""] = _get_task_attr(""classifier_span_pooling"")
    params[""edgeprobe_cnn_context""] = _get_task_attr(""edgeprobe_cnn_context"")

    # For NLI probing tasks, might want to use a classifier trained on
    # something else (typically 'mnli').
    cls_task_name = _get_task_attr(""use_classifier"")
    # default to this task
    params[""use_classifier""] = cls_task_name or task_name

    return Params(params)","def get_task_specific_params(args, task_name):
    """""" Search args for parameters specific to task.
    Args:
        args: main-program args, a config.Params object
        task_name: (string)
    Returns:
        AllenNLP Params object of task-specific params.
    """"""

    def _get_task_attr(attr_name, default=None):
        return config.get_task_attr(args, task_name, attr_name, default)

    params = {}
    params[""cls_type""] = _get_task_attr(""classifier"")
    params[""d_hid""] = _get_task_attr(""classifier_hid_dim"")
    params[""d_proj""] = _get_task_attr(""d_proj"")
    params[""shared_pair_attn""] = args.shared_pair_attn
    if args.shared_pair_attn:
        params[""attn""] = args.pair_attn
        params[""d_hid_attn""] = args.d_hid_attn
        params[""dropout""] = args.classifier_dropout
    else:
        params[""attn""] = _get_task_attr(""pair_attn"")
        params[""d_hid_attn""] = _get_task_attr(""d_hid_attn"")
        params[""dropout""] = _get_task_attr(""classifier_dropout"")

    # Used for edge probing. Other tasks can safely ignore.
    params[""cls_loss_fn""] = _get_task_attr(""classifier_loss_fn"")
    params[""cls_span_pooling""] = _get_task_attr(""classifier_span_pooling"")
    params[""edgeprobe_cnn_context""] = _get_task_attr(""edgeprobe_cnn_context"")

    # For NLI probing tasks, might want to use a classifier trained on
    # something else (typically 'mnli').
    cls_task_name = _get_task_attr(""use_classifier"")
    # default to this task
    params[""use_classifier""] = cls_task_name or task_name

    return Params(params)"
"    def update_metrics(self, logits, labels, tagmask=None):
        logits, labels = logits.detach(), labels.detach()

        def make_one_hot(batch, depth=2):
            """"""
            Creates a one-hot embedding of dimension 2.
            Parameters:
            batch: list of size batch_size of class predictions
            Returns:
            one hot encoding of size [batch_size, 2]
            """"""
            ones = torch.sparse.torch.eye(depth)
            if torch.cuda.is_available():
                ones = ones.cuda()
            return ones.index_select(0, batch)

        binary_preds = make_one_hot(logits, depth=2)
        # Make label_ints a batch_size list of labels
        label_ints = torch.argmax(labels, dim=1)
        self.f1_scorer(binary_preds, label_ints)
        self.acc_scorer(binary_preds.long(), labels.long())","    def update_metrics(self, logits, labels, tagmask=None):
        logits, labels = logits.detach(), labels.detach()

        def make_one_hot(batch, depth=2):
            """"""
            Creates a one-hot embedding of dimension 2.
            Parameters:
            batch: list of size batch_size of class predictions
            Returns:
            one hot encoding of size [batch_size, 2]
            """"""
            ones = torch.sparse.torch.eye(depth).cuda()
            return ones.index_select(0, batch)

        binary_preds = make_one_hot(logits, depth=2)
        # Make label_ints a batch_size list of labels
        label_ints = torch.argmax(labels, dim=1)
        self.f1_scorer(binary_preds, label_ints)
        self.acc_scorer(binary_preds.long(), labels.long())"
"        def make_one_hot(batch, depth=2):
            """"""
            Creates a one-hot embedding of dimension 2.
            Parameters:
            batch: list of size batch_size of class predictions
            Returns:
            one hot encoding of size [batch_size, 2]
            """"""
            ones = torch.sparse.torch.eye(depth)
            if torch.cuda.is_available():
                ones = ones.cuda()
            return ones.index_select(0, batch)","        def make_one_hot(batch, depth=2):
            """"""
            Creates a one-hot embedding of dimension 2.
            Parameters:
            batch: list of size batch_size of class predictions
            Returns:
            one hot encoding of size [batch_size, 2]
            """"""
            ones = torch.sparse.torch.eye(depth).cuda()
            return ones.index_select(0, batch)"
"    def parse(self, file):
        data = []
        file = EncodedIO(file)
        file = io.TextIOWrapper(file, encoding=file.encoding)

        # Add check exception

        field_parsers = {
            ""ne"": lambda line, i: conllu.parser.parse_nullable_value(line[i]),
        }

        gen_parser = conllu.parse_incr(
            file,
            fields=(""form"", ""ne""),
            field_parsers=field_parsers
        )

        try:
            for sentence in gen_parser:
                if not sentence:
                    continue
                if len(data) >= settings.IMPORT_BATCH_SIZE:
                    yield data
                    data = []
                words, labels = [], []
                for item in sentence:
                    word = item.get(""form"")
                    tag = item.get(""ne"")

                    if tag is not None:
                        char_left = sum(map(len, words)) + len(words)
                        char_right = char_left + len(word)
                        span = [char_left, char_right, tag]
                        labels.append(span)

                    words.append(word)

                # Create and add JSONL
                data.append({'text': ' '.join(words), 'labels': labels})

        except conllu.parser.ParseException as e:
            raise FileParseException(line_num=-1, line=str(e))

        if data:
            yield data","    def parse(self, file):
        data = []
        file = io.TextIOWrapper(file, encoding='utf-8')

        # Add check exception

        field_parsers = {
            ""ne"": lambda line, i: conllu.parser.parse_nullable_value(line[i]),
        }

        gen_parser = conllu.parse_incr(
            file,
            fields=(""form"", ""ne""),
            field_parsers=field_parsers
        )

        try:
            for sentence in gen_parser:
                if not sentence:
                    continue
                if len(data) >= settings.IMPORT_BATCH_SIZE:
                    yield data
                    data = []
                words, labels = [], []
                for item in sentence:
                    word = item.get(""form"")
                    tag = item.get(""ne"")

                    if tag is not None:
                        char_left = sum(map(len, words)) + len(words)
                        char_right = char_left + len(word)
                        span = [char_left, char_right, tag]
                        labels.append(span)

                    words.append(word)

                # Create and add JSONL
                data.append({'text': ' '.join(words), 'labels': labels})

        except conllu.parser.ParseException as e:
            raise FileParseException(line_num=-1, line=str(e))

        if data:
            yield data"
"    def parse(self, file):
        file = EncodedIO(file)
        file = io.TextIOWrapper(file, encoding=file.encoding)
        while True:
            batch = list(itertools.islice(file, settings.IMPORT_BATCH_SIZE))
            if not batch:
                break
            yield [{'text': line.strip()} for line in batch]","    def parse(self, file):
        file = io.TextIOWrapper(file, encoding='utf-8')
        while True:
            batch = list(itertools.islice(file, settings.IMPORT_BATCH_SIZE))
            if not batch:
                break
            yield [{'text': line.strip()} for line in batch]"
"    def parse(self, file):
        file = EncodedIO(file)
        file = io.TextIOWrapper(file, encoding=file.encoding)
        reader = csv.reader(file)
        yield from ExcelParser.parse_excel_csv_reader(reader)","    def parse(self, file):
        file = io.TextIOWrapper(file, encoding='utf-8')
        reader = csv.reader(file)
        yield from ExcelParser.parse_excel_csv_reader(reader)"
"    def parse(self, file):
        file = EncodedIO(file)
        file = io.TextIOWrapper(file, encoding=file.encoding)
        data = []
        for i, line in enumerate(file, start=1):
            if len(data) >= settings.IMPORT_BATCH_SIZE:
                yield data
                data = []
            try:
                j = json.loads(line)
                j['meta'] = json.dumps(j.get('meta', {}))
                data.append(j)
            except json.decoder.JSONDecodeError:
                raise FileParseException(line_num=i, line=line)
        if data:
            yield data","    def parse(self, file):
        file = io.TextIOWrapper(file, encoding='utf-8')
        data = []
        for i, line in enumerate(file, start=1):
            if len(data) >= settings.IMPORT_BATCH_SIZE:
                yield data
                data = []
            try:
                j = json.loads(line)
                j['meta'] = json.dumps(j.get('meta', {}))
                data.append(j)
            except json.decoder.JSONDecodeError:
                raise FileParseException(line_num=i, line=line)
        if data:
            yield data"
"    def label_per_data(self, project):
        annotation_class = project.get_annotation_class()
        return annotation_class.objects.get_label_per_data(project=project)","    def label_per_data(self, project):
        label_count = Counter()
        user_count = Counter()
        annotation_class = project.get_annotation_class()
        docs = project.documents.all()
        annotations = annotation_class.objects.filter(document_id__in=docs.all())
        for d in annotations.values('label__text', 'user__username').annotate(Count('label'), Count('user')):
            label_count[d['label__text']] += d['label__count']
            user_count[d['user__username']] += d['user__count']
        return label_count, user_count"
"    def get(cls, language):
        try:
            if PYCOUNTRY:
                lang = (languages.get(alpha_2=language)
                        or languages.get(alpha_3=language)
                        or languages.get(bibliographic=language)
                        or languages.get(name=language))
                if not lang:
                    raise KeyError(language)
                return Language(
                    # some languages don't have an alpha_2 code
                    getattr(lang, ""alpha_2"", """"),
                    lang.alpha_3,
                    lang.name,
                    getattr(lang, ""bibliographic"", """")
                )
            else:
                lang = None
                if len(language) == 2:
                    lang = languages.get(alpha2=language)
                elif len(language) == 3:
                    for code_type in ['part2b', 'part2t', 'part3']:
                        try:
                            lang = languages.get(**{code_type: language})
                            break
                        except KeyError:
                            pass
                    if not lang:
                        raise KeyError(language)
                else:
                    raise KeyError(language)
                return Language(lang.alpha2, lang.part3, lang.name, lang.part2b or lang.part2t)
        except (LookupError, KeyError):
            raise LookupError(""Invalid language code: {0}"".format(language))","    def get(cls, language):
        try:
            if PYCOUNTRY:
                # lookup workaround for alpha_2 language codes
                lang = languages.get(alpha_2=language) if re.match(r""^[a-z]{2}$"", language) else languages.lookup(language)
                return Language(lang.alpha_2, lang.alpha_3, lang.name, getattr(lang, ""bibliographic"", None))
            else:
                lang = None
                if len(language) == 2:
                    lang = languages.get(alpha2=language)
                elif len(language) == 3:
                    for code_type in ['part2b', 'part2t', 'part3']:
                        try:
                            lang = languages.get(**{code_type: language})
                            break
                        except KeyError:
                            pass
                    if not lang:
                        raise KeyError(language)
                else:
                    raise KeyError(language)
                return Language(lang.alpha2, lang.part3, lang.name, lang.part2b or lang.part2t)
        except (LookupError, KeyError):
            raise LookupError(""Invalid language code: {0}"".format(language))"
"    def close(self):
        if self.closed:
            return

        log.debug(""Closing ffmpeg thread"")
        if self.process:
            # kill ffmpeg
            self.process.kill()
            self.process.stdout.close()

            # close the streams
            for stream in self.streams:
                if hasattr(stream, ""close"") and callable(stream.close):
                    stream.close()

            log.debug(""Closed all the substreams"")

        if self.close_errorlog:
            self.errorlog.close()
            self.errorlog = None

        super().close()","    def close(self):
        log.debug(""Closing ffmpeg thread"")
        if self.process:
            # kill ffmpeg
            self.process.kill()
            self.process.stdout.close()

            # close the streams
            for stream in self.streams:
                if hasattr(stream, ""close""):
                    stream.close()

            log.debug(""Closed all the substreams"")
        if self.close_errorlog:
            self.errorlog.close()
            self.errorlog = None"
"    def authenticate(self):
        try:
            data = self._api_call(""authenticate"", {""auth"": self.auth}, schema=_login_schema)
        except CrunchyrollAPIError:
            self.auth = None
            self.cache.set(""auth"", None, expires_at=0)
            log.warning(""Saved credentials have expired"")
            return

        log.debug(""Credentials expire at: {}"".format(data[""expires""]))
        self.cache.set(""auth"", self.auth, expires_at=data[""expires""])
        return data","    def authenticate(self):
        data = self._api_call(""authenticate"", {""auth"": self.auth}, schema=_login_schema)
        self.auth = data[""auth""]
        self.cache.set(""auth"", data[""auth""], expires_at=data[""expires""])
        return data"
"    def _get_streams(self):
        api = self._create_api()
        match = _url_re.match(self.url)
        media_id = int(match.group(""media_id""))

        try:
            # the media.stream_data field is required, no stream data is returned otherwise
            info = api.get_info(media_id, fields=[""media.stream_data""], schema=_media_schema)
        except CrunchyrollAPIError as err:
            raise PluginError(u""Media lookup error: {0}"".format(err.msg))

        if not info:
            return

        streams = {}

        # The adaptive quality stream sometimes a subset of all the other streams listed, ultra is no included
        has_adaptive = any([s[u""quality""] == u""adaptive"" for s in info[u""streams""]])
        if has_adaptive:
            log.debug(u""Loading streams from adaptive playlist"")
            for stream in filter(lambda x: x[u""quality""] == u""adaptive"", info[u""streams""]):
                for q, s in HLSStream.parse_variant_playlist(self.session, stream[u""url""]).items():
                    # rename the bitrates to low, mid, or high. ultra doesn't seem to appear in the adaptive streams
                    name = STREAM_NAMES.get(q, q)
                    streams[name] = s

        # If there is no adaptive quality stream then parse each individual result
        for stream in info[u""streams""]:
            if stream[u""quality""] != u""adaptive"":
                # the video_encode_id indicates that the stream is not a variant playlist
                if u""video_encode_id"" in stream:
                    streams[stream[u""quality""]] = HLSStream(self.session, stream[u""url""])
                else:
                    # otherwise the stream url is actually a list of stream qualities
                    for q, s in HLSStream.parse_variant_playlist(self.session, stream[u""url""]).items():
                        # rename the bitrates to low, mid, or high. ultra doesn't seem to appear in the adaptive streams
                        name = STREAM_NAMES.get(q, q)
                        streams[name] = s

        return streams","    def _get_streams(self):
        api = self._create_api()
        match = _url_re.match(self.url)
        media_id = int(match.group(""media_id""))

        try:
            # the media.stream_data field is required, no stream data is returned otherwise
            info = api.get_info(media_id, fields=[""media.stream_data""], schema=_media_schema)
        except CrunchyrollAPIError as err:
            raise PluginError(u""Media lookup error: {0}"".format(err.msg))

        if not info:
            return

        streams = {}

        # The adaptive quality stream sometimes a subset of all the other streams listed, ultra is no included
        has_adaptive = any([s[u""quality""] == u""adaptive"" for s in info[u""streams""]])
        if has_adaptive:
            self.logger.debug(u""Loading streams from adaptive playlist"")
            for stream in filter(lambda x: x[u""quality""] == u""adaptive"", info[u""streams""]):
                for q, s in HLSStream.parse_variant_playlist(self.session, stream[u""url""]).items():
                    # rename the bitrates to low, mid, or high. ultra doesn't seem to appear in the adaptive streams
                    name = STREAM_NAMES.get(q, q)
                    streams[name] = s

        # If there is no adaptive quality stream then parse each individual result
        for stream in info[u""streams""]:
            if stream[u""quality""] != u""adaptive"":
                # the video_encode_id indicates that the stream is not a variant playlist
                if u""video_encode_id"" in stream:
                    streams[stream[u""quality""]] = HLSStream(self.session, stream[u""url""])
                else:
                    # otherwise the stream url is actually a list of stream qualities
                    for q, s in HLSStream.parse_variant_playlist(self.session, stream[u""url""]).items():
                        # rename the bitrates to low, mid, or high. ultra doesn't seem to appear in the adaptive streams
                        name = STREAM_NAMES.get(q, q)
                        streams[name] = s

        return streams"
"    def _create_api(self):
        """"""Creates a new CrunchyrollAPI object, initiates it's session and
        tries to authenticate it either by using saved credentials or the
        user's username and password.
        """"""
        if self.options.get(""purge_credentials""):
            self.cache.set(""session_id"", None, 0)
            self.cache.set(""auth"", None, 0)
            self.cache.set(""session_id"", None, 0)

        # use the crunchyroll locale as an override, for backwards compatibility
        locale = self.get_option(""locale"") or self.session.localization.language_code
        api = CrunchyrollAPI(self.cache,
                             self.session,
                             session_id=self.get_option(""session_id""),
                             locale=locale)

        if not self.get_option(""session_id""):
            log.debug(""Creating session with locale: {0}"", locale)
            api.start_session()

            if api.auth:
                log.debug(""Using saved credentials"")
                login = api.authenticate()
                if login:
                    log.info(""Successfully logged in as '{0}'"",
                             login[""user""][""username""] or login[""user""][""email""])
            if not api.auth and self.options.get(""username""):
                try:
                    log.debug(""Attempting to login using username and password"")
                    api.login(self.options.get(""username""),
                              self.options.get(""password""))
                    login = api.authenticate()
                    log.info(""Logged in as '{0}'"",
                             login[""user""][""username""] or login[""user""][""email""])

                except CrunchyrollAPIError as err:
                    raise PluginError(u""Authentication error: {0}"".format(err.msg))
            if not api.auth:
                log.warning(
                    ""No authentication provided, you won't be able to access ""
                    ""premium restricted content""
                )

        return api","    def _create_api(self):
        """"""Creates a new CrunchyrollAPI object, initiates it's session and
        tries to authenticate it either by using saved credentials or the
        user's username and password.
        """"""
        if self.options.get(""purge_credentials""):
            self.cache.set(""session_id"", None, 0)
            self.cache.set(""auth"", None, 0)
            self.cache.set(""session_id"", None, 0)

        # use the crunchyroll locale as an override, for backwards compatibility
        locale = self.get_option(""locale"") or self.session.localization.language_code
        api = CrunchyrollAPI(self.cache,
                             self.session,
                             session_id=self.get_option(""session_id""),
                             locale=locale)

        if not self.get_option(""session_id""):
            self.logger.debug(""Creating session with locale: {0}"", locale)
            api.start_session()

            if api.auth:
                self.logger.debug(""Using saved credentials"")
                login = api.authenticate()
                self.logger.info(""Successfully logged in as '{0}'"",
                                 login[""user""][""username""] or login[""user""][""email""])
            elif self.options.get(""username""):
                try:
                    self.logger.debug(""Attempting to login using username and password"")
                    api.login(self.options.get(""username""),
                              self.options.get(""password""))
                    login = api.authenticate()
                    self.logger.info(""Logged in as '{0}'"",
                                     login[""user""][""username""] or login[""user""][""email""])

                except CrunchyrollAPIError as err:
                    raise PluginError(u""Authentication error: {0}"".format(err.msg))
            else:
                self.logger.warning(
                    ""No authentication provided, you won't be able to access ""
                    ""premium restricted content""
                )

        return api"
"def create_title(plugin=None):
    if args.title and plugin:
        title = LazyFormatter.format(
            maybe_decode(args.title, get_filesystem_encoding()),
            title=lambda: plugin.get_title() or DEFAULT_STREAM_METADATA[""title""],
            author=lambda: plugin.get_author() or DEFAULT_STREAM_METADATA[""author""],
            category=lambda: plugin.get_category() or DEFAULT_STREAM_METADATA[""category""],
            game=lambda: plugin.get_category() or DEFAULT_STREAM_METADATA[""game""],
            url=plugin.url
        )
    else:
        title = args.url
    return title","def create_title(plugin=None):
    if args.title and plugin:
        title = LazyFormatter.format(
            maybe_encode(args.title),
            title=lambda: plugin.get_title() or DEFAULT_STREAM_METADATA[""title""],
            author=lambda: plugin.get_author() or DEFAULT_STREAM_METADATA[""author""],
            category=lambda: plugin.get_category() or DEFAULT_STREAM_METADATA[""category""],
            game=lambda: plugin.get_category() or DEFAULT_STREAM_METADATA[""game""],
            url=plugin.url
        )
    else:
        title = args.url
    return title"
"    def _create_arguments(self):
        if self.namedpipe:
            filename = self.namedpipe.path
        elif self.filename:
            filename = self.filename
        elif self.http:
            filename = self.http.url
        else:
            filename = ""-""
        extra_args = []

        if self.title is not None:
            # vlc
            if self.player_name == ""vlc"":
                # see https://wiki.videolan.org/Documentation:Format_String/, allow escaping with \\$
                self.title = self.title.replace(""$"", ""$$"").replace(r'\\$$', ""$"")
                extra_args.extend([u""--input-title-format"", self.title])

            # mpv
            if self.player_name == ""mpv"":
                # see https://mpv.io/manual/stable/#property-expansion, allow escaping with \\$, respect mpv's $>
                self.title = self._mpv_title_escape(self.title)
                extra_args.append(u""--title={}"".format(self.title))

            # potplayer
            if self.player_name == ""potplayer"":
                if filename != ""-"":
                    # PotPlayer - About - Command Line
                    # You can specify titles for URLs by separating them with a backslash (\\) at the end of URLs.
                    # eg. ""http://...\\title of this url""
                    self.title = self.title.replace('""', '')
                    filename = filename[:-1] + '\\\\' + self.title + filename[-1]

        args = self.args.format(filename=filename)
        cmd = self.cmd

        # player command
        if is_win32:
            eargs = maybe_decode(subprocess.list2cmdline(extra_args))
            # do not insert and extra "" "" when there are no extra_args
            return u' '.join([cmd] + ([eargs] if eargs else []) + [args])
        return shlex.split(cmd) + extra_args + shlex.split(args)","    def _create_arguments(self):
        if self.namedpipe:
            filename = self.namedpipe.path
        elif self.filename:
            filename = self.filename
        elif self.http:
            filename = self.http.url
        else:
            filename = ""-""
        extra_args = []

        if self.title is not None:
            # vlc
            if self.player_name == ""vlc"":
                # see https://wiki.videolan.org/Documentation:Format_String/, allow escaping with \\$
                self.title = self.title.replace(""$"", ""$$"").replace(r'\\$$', ""$"")
                extra_args.extend([""--input-title-format"", self.title])

            # mpv
            if self.player_name == ""mpv"":
                # see https://mpv.io/manual/stable/#property-expansion, allow escaping with \\$, respect mpv's $>
                self.title = self._mpv_title_escape(self.title)
                extra_args.append(""--title={}"".format(self.title))

            # potplayer
            if self.player_name == ""potplayer"":
                if filename != ""-"":
                    # PotPlayer - About - Command Line
                    # You can specify titles for URLs by separating them with a backslash (\\) at the end of URLs.
                    # eg. ""http://...\\title of this url""
                    self.title = self.title.replace('""', '')
                    filename = filename[:-1] + '\\\\' + self.title + filename[-1]

        args = self.args.format(filename=filename)
        cmd = self.cmd

        # player command
        if is_win32:
            eargs = maybe_decode(subprocess.list2cmdline(extra_args))
            # do not insert and extra "" "" when there are no extra_args
            return maybe_encode(u' '.join([cmd] + ([eargs] if eargs else []) + [args]),
                                encoding=get_filesystem_encoding())
        return shlex.split(cmd) + extra_args + shlex.split(args)"
"    def _open_call(self):
        args = self._create_arguments()
        if is_win32:
            fargs = args
        else:
            fargs = subprocess.list2cmdline(args)
        log.debug(u""Calling: {0}"".format(fargs))

        subprocess.call(maybe_encode(args, get_filesystem_encoding()),
                        stdout=self.stdout,
                        stderr=self.stderr)","    def _open_call(self):
        args = self._create_arguments()
        if is_win32:
            fargs = args
        else:
            fargs = subprocess.list2cmdline(args)
        log.debug(u""Calling: {0}"".format(maybe_decode(fargs)))
        subprocess.call(args,
                        stdout=self.stdout,
                        stderr=self.stderr)"
"    def _open_subprocess(self):
        # Force bufsize=0 on all Python versions to avoid writing the
        # unflushed buffer when closing a broken input pipe
        args = self._create_arguments()
        if is_win32:
            fargs = args
        else:
            fargs = subprocess.list2cmdline(args)
        log.debug(u""Opening subprocess: {0}"".format(fargs))

        self.player = subprocess.Popen(maybe_encode(args, get_filesystem_encoding()),
                                       stdin=self.stdin, bufsize=0,
                                       stdout=self.stdout,
                                       stderr=self.stderr)
        # Wait 0.5 seconds to see if program exited prematurely
        if not self.running:
            raise OSError(""Process exited prematurely"")

        if self.namedpipe:
            self.namedpipe.open(""wb"")
        elif self.http:
            self.http.open()","    def _open_subprocess(self):
        # Force bufsize=0 on all Python versions to avoid writing the
        # unflushed buffer when closing a broken input pipe
        args = self._create_arguments()
        if is_win32:
            fargs = args
        else:
            fargs = subprocess.list2cmdline(args)

        log.debug(u""Opening subprocess: {0}"".format(maybe_decode(fargs)))
        self.player = subprocess.Popen(args,
                                       stdin=self.stdin, bufsize=0,
                                       stdout=self.stdout,
                                       stderr=self.stderr)
        # Wait 0.5 seconds to see if program exited prematurely
        if not self.running:
            raise OSError(""Process exited prematurely"")

        if self.namedpipe:
            self.namedpipe.open(""wb"")
        elif self.http:
            self.http.open()"
"def maybe_encode(text, encoding=""utf8""):
    if is_py2:
        if isinstance(text, unicode):
            return text.encode(encoding)
        else:
            return text
    else:
        return text","def maybe_encode(text, encoding=""utf8""):
    if is_py2:
        return text.encode(encoding)
    else:
        return text"
"    def _open_call(self):
        args = self._create_arguments()
        if is_win32:
            fargs = args
        else:
            fargs = subprocess.list2cmdline(args)
        log.debug(u""Calling: {0}"".format(maybe_decode(fargs)))
        subprocess.call(args,
                        stdout=self.stdout,
                        stderr=self.stderr)","    def _open_call(self):
        args = self._create_arguments()
        if is_win32:
            fargs = args
        else:
            fargs = subprocess.list2cmdline(args)
        log.debug(u""Calling: {0}"".format(fargs))
        subprocess.call(args,
                        stdout=self.stdout,
                        stderr=self.stderr)"
"    def _open_subprocess(self):
        # Force bufsize=0 on all Python versions to avoid writing the
        # unflushed buffer when closing a broken input pipe
        args = self._create_arguments()
        if is_win32:
            fargs = args
        else:
            fargs = subprocess.list2cmdline(args)

        log.debug(u""Opening subprocess: {0}"".format(maybe_decode(fargs)))
        self.player = subprocess.Popen(args,
                                       stdin=self.stdin, bufsize=0,
                                       stdout=self.stdout,
                                       stderr=self.stderr)
        # Wait 0.5 seconds to see if program exited prematurely
        if not self.running:
            raise OSError(""Process exited prematurely"")

        if self.namedpipe:
            self.namedpipe.open(""wb"")
        elif self.http:
            self.http.open()","    def _open_subprocess(self):
        # Force bufsize=0 on all Python versions to avoid writing the
        # unflushed buffer when closing a broken input pipe
        args = self._create_arguments()
        if is_win32:
            fargs = args
        else:
            fargs = subprocess.list2cmdline(args)
        log.debug(u""Opening subprocess: {0}"".format(fargs))
        self.player = subprocess.Popen(args,
                                       stdin=self.stdin, bufsize=0,
                                       stdout=self.stdout,
                                       stderr=self.stderr)
        # Wait 0.5 seconds to see if program exited prematurely
        if not self.running:
            raise OSError(""Process exited prematurely"")

        if self.namedpipe:
            self.namedpipe.open(""wb"")
        elif self.http:
            self.http.open()"
"    def fetch(self, segment, retries=None):
        if self.closed or not retries:
            return

        try:
            request_args = copy.deepcopy(self.reader.stream.args)
            headers = request_args.pop(""headers"", {})
            now = datetime.datetime.now(tz=utc)
            if segment.available_at > now:
                time_to_wait = (segment.available_at - now).total_seconds()
                fname = os.path.basename(urlparse(segment.url).path)
                log.debug(""Waiting for segment: {fname} ({wait:.01f}s)"".format(fname=fname, wait=time_to_wait))
                sleep_until(segment.available_at)

            if segment.range:
                start, length = segment.range
                if length:
                    end = start + length - 1
                else:
                    end = """"
                headers[""Range""] = ""bytes={0}-{1}"".format(start, end)

            return self.session.http.get(segment.url,
                                         timeout=self.timeout,
                                         exception=StreamError,
                                         headers=headers,
                                         **request_args)
        except StreamError as err:
            log.error(""Failed to open segment {0}: {1}"", segment.url, err)
            return self.fetch(segment, retries - 1)","    def fetch(self, segment, retries=None):
        if self.closed or not retries:
            return

        try:
            headers = {}
            now = datetime.datetime.now(tz=utc)
            if segment.available_at > now:
                time_to_wait = (segment.available_at - now).total_seconds()
                fname = os.path.basename(urlparse(segment.url).path)
                log.debug(""Waiting for segment: {fname} ({wait:.01f}s)"".format(fname=fname, wait=time_to_wait))
                sleep_until(segment.available_at)

            if segment.range:
                start, length = segment.range
                if length:
                    end = start + length - 1
                else:
                    end = """"
                headers[""Range""] = ""bytes={0}-{1}"".format(start, end)

            return self.session.http.get(segment.url,
                                         timeout=self.timeout,
                                         exception=StreamError,
                                         headers=headers)
        except StreamError as err:
            log.error(""Failed to open segment {0}: {1}"", segment.url, err)
            return self.fetch(segment, retries - 1)"
"    def reload(self):
        if self.closed:
            return

        self.reader.buffer.wait_free()
        log.debug(""Reloading manifest ({0}:{1})"".format(self.reader.representation_id, self.reader.mime_type))
        res = self.session.http.get(self.mpd.url, exception=StreamError, **self.stream.args)

        new_mpd = MPD(self.session.http.xml(res, ignore_ns=True),
                      base_url=self.mpd.base_url,
                      url=self.mpd.url,
                      timelines=self.mpd.timelines)

        new_rep = self.get_representation(new_mpd, self.reader.representation_id, self.reader.mime_type)
        with freeze_timeline(new_mpd):
            changed = len(list(itertools.islice(new_rep.segments(), 1))) > 0

        if changed:
            self.mpd = new_mpd

        return changed","    def reload(self):
        if self.closed:
            return

        self.reader.buffer.wait_free()
        log.debug(""Reloading manifest ({0}:{1})"".format(self.reader.representation_id, self.reader.mime_type))
        res = self.session.http.get(self.mpd.url, exception=StreamError)

        new_mpd = MPD(self.session.http.xml(res, ignore_ns=True),
                      base_url=self.mpd.base_url,
                      url=self.mpd.url,
                      timelines=self.mpd.timelines)

        new_rep = self.get_representation(new_mpd, self.reader.representation_id, self.reader.mime_type)
        with freeze_timeline(new_mpd):
            changed = len(list(itertools.islice(new_rep.segments(), 1))) > 0

        if changed:
            self.mpd = new_mpd

        return changed"
"    def _get_streams(self):
        match = _url_re.match(self.url)
        if not match:
            return

        channel, media_id = match.group(""channel"", ""media_id"")
        self.logger.debug(""Matched URL: channel={0}, media_id={1}"".format(channel, media_id))
        if not media_id:
            res = http.get(LIVE_API.format(channel))
            livestream = http.json(res, schema=_live_schema)
            if livestream.get(""media_hosted_media""):
                hosted = _live_schema.validate(livestream[""media_hosted_media""])
                self.logger.info(""{0} is hosting {1}"", livestream[""media_user_name""], hosted[""media_user_name""])
                livestream = hosted

            if not livestream[""media_is_live""]:
                return

            media_id = livestream[""media_id""]
            media_type = ""live""
        else:
            media_type = ""video""

        res = http.get(PLAYER_API.format(media_type, media_id))
        player = http.json(res, schema=_player_schema)

        if media_type == ""live"":
            return self._get_live_streams(player)
        else:
            return self._get_video_streams(player)","    def _get_streams(self):
        match = _url_re.match(self.url)
        if not match:
            return

        channel, media_id = match.group(""channel"", ""media_id"")
        self.logger.debug(""Matched URL: channel={0}, media_id={1}"".format(channel, media_id))
        if not media_id:
            res = http.get(LIVE_API.format(channel))
            livestream = http.json(res, schema=_live_schema)
            if livestream[""media_hosted_media""]:
                hosted = _live_schema.validate(livestream[""media_hosted_media""])
                self.logger.info(""{0} is hosting {1}"", livestream[""media_user_name""], hosted[""media_user_name""])
                livestream = hosted

            if not livestream[""media_is_live""]:
                return

            media_id = livestream[""media_id""]
            media_type = ""live""
        else:
            media_type = ""video""

        res = http.get(PLAYER_API.format(media_type, media_id))
        player = http.json(res, schema=_player_schema)

        if media_type == ""live"":
            return self._get_live_streams(player)
        else:
            return self._get_video_streams(player)"
"    def close(self, client_only=False):
        if self.conn:
            self.conn.close()

        if not client_only:
            try:
                self.socket.shutdown(2)
            except (OSError, socket.error):
                pass
            self.socket.close()","    def close(self, client_only=False):
        if self.conn:
            self.conn.close()

        if not client_only:
            try:
                self.socket.shutdown(2)
            except OSError:
                pass
            self.socket.close()"
"    def close(self, client_only=False):
        if self.conn:
            self.conn.close()

        if not client_only:
            try:
                self.socket.shutdown(2)
            except OSError:
                pass
            self.socket.close()","    def close(self, client_only=False):
        if self.conn:
            self.conn.close()

        if not client_only:
            self.socket.shutdown(2)
            self.socket.close()"
"    def write(self, sequence, res, chunk_size=8192, retries=None):
        retries = retries or self.retries
        if retries == 0:
            self.logger.error(""Failed to open segment {0}"", sequence.num)
            return
        try:
            if sequence.segment.key and sequence.segment.key.method != ""NONE"":
                try:
                    decryptor = self.create_decryptor(sequence.segment.key,
                                                      sequence.num)
                except StreamError as err:
                    self.logger.error(""Failed to create decryptor: {0}"", err)
                    self.close()
                    return

                for chunk in res.iter_content(chunk_size):
                    # If the input data is not a multiple of 16, cut off any garbage
                    garbage_len = len(chunk) % 16
                    if garbage_len:
                        self.logger.debug(""Cutting off {0} bytes of garbage ""
                                          ""before decrypting"", garbage_len)
                        decrypted_chunk = decryptor.decrypt(chunk[:-garbage_len])
                    else:
                        decrypted_chunk = decryptor.decrypt(chunk)
                    self.reader.buffer.write(decrypted_chunk)
            else:
                for chunk in res.iter_content(chunk_size):
                    self.reader.buffer.write(chunk)
        except StreamError as err:
            self.logger.error(""Failed to open segment {0}: {1}"", sequence.num, err)
            return self.write(sequence,
                              self.fetch(sequence, retries=self.retries),
                              chunk_size=chunk_size,
                              retries=retries - 1)

        self.logger.debug(""Download of segment {0} complete"", sequence.num)","    def write(self, sequence, res, chunk_size=8192):
        if sequence.segment.key and sequence.segment.key.method != ""NONE"":
            try:
                decryptor = self.create_decryptor(sequence.segment.key,
                                                  sequence.num)
            except StreamError as err:
                self.logger.error(""Failed to create decryptor: {0}"", err)
                self.close()
                return

            for chunk in res.iter_content(chunk_size):
                # If the input data is not a multiple of 16, cut off any garbage
                garbage_len = len(chunk) % 16
                if garbage_len:
                    self.logger.debug(""Cutting off {0} bytes of garbage ""
                                      ""before decrypting"", garbage_len)
                    decrypted_chunk = decryptor.decrypt(chunk[:-garbage_len])
                else:
                    decrypted_chunk = decryptor.decrypt(chunk)
                self.reader.buffer.write(decrypted_chunk)
        else:
            for chunk in res.iter_content(chunk_size):
                self.reader.buffer.write(chunk)

        self.logger.debug(""Download of segment {0} complete"", sequence.num)"
"    def write(self, vals):
        res = super(OpStudent, self).write(vals)
        if vals.get('parent_ids', False):
            user_ids = []
            if self.parent_ids:
                for parent in self.parent_ids:
                    if parent.user_id:
                        user_ids = [x.user_id.id for x in parent.student_ids
                                    if x.user_id]
                        parent.user_id.child_ids = [(6, 0, user_ids)]
            else:
                user_ids = self.env['res.users'].search([
                    ('child_ids', 'in', self.user_id.id)])
                for user_id in user_ids:
                    child_ids = user_id.child_ids.ids
                    child_ids.remove(self.user_id.id)
                    user_id.child_ids = [(6, 0, child_ids)]
        if vals.get('user_id', False):
            for parent_id in self.parent_ids:
                if parent_id.user_id:
                    child_ids = parent_id.user_id.child_ids.ids
                    child_ids.append(vals['user_id'])
                    parent_id.name.user_id.child_ids = [(6, 0, child_ids)]
        self.clear_caches()
        return res","    def write(self, vals):
        res = super(OpStudent, self).write(vals)
        if vals.get('parent_ids', False):
            user_ids = []
            if self.parent_ids:
                for parent in self.parent_ids:
                    if parent.user_id:
                        user_ids = [x.user_id.id for x in parent.student_ids
                                    if x.user_id]
                        parent.user_id.child_ids = [(6, 0, user_ids)]
            else:
                user_ids = self.env['res.users'].search([
                    ('child_ids', 'in', self.user_id.id)])
                for user_id in user_ids:
                    child_ids = user_id.child_ids.ids
                    child_ids.remove(self.user_id.id)
                    user_id.child_ids = [(6, 0, child_ids)]
        if vals.get('user_id', False):
            for parent_id in self.parent_ids:
                child_ids = parent_id.user_id.child_ids.ids
                child_ids.append(vals['user_id'])
                parent_id.name.user_id.child_ids = [(6, 0, child_ids)]
        self.clear_caches()
        return res"
"    def parse_exif_values(self, _path_file):
        # Disable exifread log
        logging.getLogger('exifread').setLevel(logging.CRITICAL)

        with open(_path_file, 'rb') as f:
            tags = exifread.process_file(f, details=False)
            try:
                if 'Image Make' in tags:
                    try:
                        self.camera_make = tags['Image Make'].values
                    except UnicodeDecodeError:
                        log.ODM_WARNING(""EXIF Image Make might be corrupted"")
                        self.camera_make = ""unknown""
                if 'Image Model' in tags:
                    try:
                        self.camera_model = tags['Image Model'].values
                    except UnicodeDecodeError:
                        log.ODM_WARNING(""EXIF Image Model might be corrupted"")
                        self.camera_model = ""unknown""
                if 'GPS GPSAltitude' in tags:
                    self.altitude = self.float_value(tags['GPS GPSAltitude'])
                    if 'GPS GPSAltitudeRef' in tags and self.int_value(tags['GPS GPSAltitudeRef']) > 0:
                        self.altitude *= -1
                if 'GPS GPSLatitude' in tags and 'GPS GPSLatitudeRef' in tags:
                    self.latitude = self.dms_to_decimal(tags['GPS GPSLatitude'], tags['GPS GPSLatitudeRef'])
                if 'GPS GPSLongitude' in tags and 'GPS GPSLongitudeRef' in tags:
                    self.longitude = self.dms_to_decimal(tags['GPS GPSLongitude'], tags['GPS GPSLongitudeRef'])
            except IndexError as e:
                log.ODM_WARNING(""Cannot read basic EXIF tags for %s: %s"" % (_path_file, str(e)))

            try:
                if 'Image Tag 0xC61A' in tags:
                    self.black_level = self.list_values(tags['Image Tag 0xC61A'])
                elif 'BlackLevel' in tags:
                    self.black_level = self.list_values(tags['BlackLevel'])
                
                if 'EXIF ExposureTime' in tags:
                    self.exposure_time = self.float_value(tags['EXIF ExposureTime'])

                if 'EXIF FNumber' in tags:
                    self.fnumber = self.float_value(tags['EXIF FNumber'])
                
                if 'EXIF ISOSpeed' in tags:
                    self.iso_speed = self.int_value(tags['EXIF ISOSpeed'])
                elif 'EXIF PhotographicSensitivity' in tags:
                    self.iso_speed = self.int_value(tags['EXIF PhotographicSensitivity'])
                elif 'EXIF ISOSpeedRatings' in tags:
                    self.iso_speed = self.int_value(tags['EXIF ISOSpeedRatings'])
                    

                if 'Image BitsPerSample' in tags:
                    self.bits_per_sample = self.int_value(tags['Image BitsPerSample'])
                if 'EXIF DateTimeOriginal' in tags:
                    str_time = tags['EXIF DateTimeOriginal'].values
                    utc_time = datetime.strptime(str_time, ""%Y:%m:%d %H:%M:%S"")
                    subsec = 0
                    if 'EXIF SubSecTime' in tags:
                        subsec = self.int_value(tags['EXIF SubSecTime'])
                    negative = 1.0
                    if subsec < 0:
                        negative = -1.0
                        subsec *= -1.0
                    subsec = float('0.{}'.format(int(subsec)))
                    subsec *= negative
                    ms = subsec * 1e3
                    utc_time += timedelta(milliseconds = ms)
                    timezone = pytz.timezone('UTC')
                    epoch = timezone.localize(datetime.utcfromtimestamp(0))
                    self.utc_time = (timezone.localize(utc_time) - epoch).total_seconds() * 1000.0
            except Exception as e:
                log.ODM_WARNING(""Cannot read extended EXIF tags for %s: %s"" % (_path_file, str(e)))


            # Extract XMP tags
            f.seek(0)
            xmp = self.get_xmp(f)

            for tags in xmp:
                try:
                    band_name = self.get_xmp_tag(tags, ['Camera:BandName', '@Camera:BandName'])
                    if band_name is not None:
                        self.band_name = band_name.replace("" "", """")

                    self.set_attr_from_xmp_tag('band_index', tags, [
                        'DLS:SensorId', # Micasense RedEdge
                        '@Camera:RigCameraIndex', # Parrot Sequoia, Sentera 21244-00_3.2MP-GS-0001
                        'Camera:RigCameraIndex', # MicaSense Altum
                    ])
                    self.set_attr_from_xmp_tag('radiometric_calibration', tags, [
                        'MicaSense:RadiometricCalibration',
                    ])

                    self.set_attr_from_xmp_tag('vignetting_center', tags, [
                        'Camera:VignettingCenter',
                        'Sentera:VignettingCenter',
                    ])

                    self.set_attr_from_xmp_tag('vignetting_polynomial', tags, [
                        'Camera:VignettingPolynomial',
                        'Sentera:VignettingPolynomial',
                    ])
                    
                    self.set_attr_from_xmp_tag('horizontal_irradiance', tags, [
                        'Camera:HorizontalIrradiance'
                    ], float)

                    self.set_attr_from_xmp_tag('irradiance_scale_to_si', tags, [
                        'Camera:IrradianceScaleToSIUnits'
                    ], float)

                    self.set_attr_from_xmp_tag('sun_sensor', tags, [
                        'Camera:SunSensor',
                    ], float)

                    self.set_attr_from_xmp_tag('spectral_irradiance', tags, [
                        'Camera:SpectralIrradiance',
                        'Camera:Irradiance',                    
                    ], float)

                    # Phantom 4 RTK
                    if '@drone-dji:RtkStdLon' in tags:
                        y = float(self.get_xmp_tag(tags, '@drone-dji:RtkStdLon'))
                        x = float(self.get_xmp_tag(tags, '@drone-dji:RtkStdLat'))
                        self.gps_xy_stddev = max(x, y)
                    
                        if '@drone-dji:RtkStdHgt' in tags:
                            self.gps_z_stddev = float(self.get_xmp_tag(tags, '@drone-dji:RtkStdHgt'))
                    else:
                        self.set_attr_from_xmp_tag('gps_xy_stddev', tags, [
                            '@Camera:GPSXYAccuracy',
                            'GPSXYAccuracy'
                        ], float)
                        self.set_attr_from_xmp_tag('gps_z_stddev', tags, [
                            '@Camera:GPSZAccuracy',
                            'GPSZAccuracy'
                        ], float)

                    if 'DLS:Yaw' in tags:
                        self.set_attr_from_xmp_tag('dls_yaw', tags, ['DLS:Yaw'], float)
                        self.set_attr_from_xmp_tag('dls_pitch', tags, ['DLS:Pitch'], float)
                        self.set_attr_from_xmp_tag('dls_roll', tags, ['DLS:Roll'], float)
                except Exception as e:
                    log.ODM_WARNING(""Cannot read XMP tags for %s: %s"" % (_path_file, str(e)))

                # self.set_attr_from_xmp_tag('center_wavelength', tags, [
                #     'Camera:CentralWavelength'
                # ], float)

                # self.set_attr_from_xmp_tag('bandwidth', tags, [
                #     'Camera:WavelengthFWHM'
                # ], float)
            
        self.width, self.height = get_image_size.get_image_size(_path_file)
        # Sanitize band name since we use it in folder paths
        self.band_name = re.sub('[^A-Za-z0-9]+', '', self.band_name)","    def parse_exif_values(self, _path_file):
        # Disable exifread log
        logging.getLogger('exifread').setLevel(logging.CRITICAL)

        with open(_path_file, 'rb') as f:
            tags = exifread.process_file(f, details=False)
            try:
                if 'Image Make' in tags:
                    try:
                        self.camera_make = tags['Image Make'].values
                    except UnicodeDecodeError:
                        log.ODM_WARNING(""EXIF Image Make might be corrupted"")
                        self.camera_make = ""unknown""
                if 'Image Model' in tags:
                    try:
                        self.camera_model = tags['Image Model'].values
                    except UnicodeDecodeError:
                        log.ODM_WARNING(""EXIF Image Model might be corrupted"")
                        self.camera_model = ""unknown""
                if 'GPS GPSAltitude' in tags:
                    self.altitude = self.float_value(tags['GPS GPSAltitude'])
                    if 'GPS GPSAltitudeRef' in tags and self.int_value(tags['GPS GPSAltitudeRef']) > 0:
                        self.altitude *= -1
                if 'GPS GPSLatitude' in tags and 'GPS GPSLatitudeRef' in tags:
                    self.latitude = self.dms_to_decimal(tags['GPS GPSLatitude'], tags['GPS GPSLatitudeRef'])
                if 'GPS GPSLongitude' in tags and 'GPS GPSLongitudeRef' in tags:
                    self.longitude = self.dms_to_decimal(tags['GPS GPSLongitude'], tags['GPS GPSLongitudeRef'])
            except IndexError as e:
                log.ODM_WARNING(""Cannot read basic EXIF tags for %s: %s"" % (_path_file, e.message))

            try:
                if 'Image Tag 0xC61A' in tags:
                    self.black_level = self.list_values(tags['Image Tag 0xC61A'])
                elif 'BlackLevel' in tags:
                    self.black_level = self.list_values(tags['BlackLevel'])
                
                if 'EXIF ExposureTime' in tags:
                    self.exposure_time = self.float_value(tags['EXIF ExposureTime'])

                if 'EXIF FNumber' in tags:
                    self.fnumber = self.float_value(tags['EXIF FNumber'])
                
                if 'EXIF ISOSpeed' in tags:
                    self.iso_speed = self.int_value(tags['EXIF ISOSpeed'])
                elif 'EXIF PhotographicSensitivity' in tags:
                    self.iso_speed = self.int_value(tags['EXIF PhotographicSensitivity'])
                elif 'EXIF ISOSpeedRatings' in tags:
                    self.iso_speed = self.int_value(tags['EXIF ISOSpeedRatings'])
                    

                if 'Image BitsPerSample' in tags:
                    self.bits_per_sample = self.int_value(tags['Image BitsPerSample'])
                if 'EXIF DateTimeOriginal' in tags:
                    str_time = tags['EXIF DateTimeOriginal'].values
                    utc_time = datetime.strptime(str_time, ""%Y:%m:%d %H:%M:%S"")
                    subsec = 0
                    if 'EXIF SubSecTime' in tags:
                        subsec = self.int_value(tags['EXIF SubSecTime'])
                    negative = 1.0
                    if subsec < 0:
                        negative = -1.0
                        subsec *= -1.0
                    subsec = float('0.{}'.format(int(subsec)))
                    subsec *= negative
                    ms = subsec * 1e3
                    utc_time += timedelta(milliseconds = ms)
                    timezone = pytz.timezone('UTC')
                    epoch = timezone.localize(datetime.utcfromtimestamp(0))
                    self.utc_time = (timezone.localize(utc_time) - epoch).total_seconds() * 1000.0
            except Exception as e:
                log.ODM_WARNING(""Cannot read extended EXIF tags for %s: %s"" % (_path_file, str(e)))


            # Extract XMP tags
            f.seek(0)
            xmp = self.get_xmp(f)

            for tags in xmp:
                try:
                    band_name = self.get_xmp_tag(tags, ['Camera:BandName', '@Camera:BandName'])
                    if band_name is not None:
                        self.band_name = band_name.replace("" "", """")

                    self.set_attr_from_xmp_tag('band_index', tags, [
                        'DLS:SensorId', # Micasense RedEdge
                        '@Camera:RigCameraIndex', # Parrot Sequoia, Sentera 21244-00_3.2MP-GS-0001
                        'Camera:RigCameraIndex', # MicaSense Altum
                    ])
                    self.set_attr_from_xmp_tag('radiometric_calibration', tags, [
                        'MicaSense:RadiometricCalibration',
                    ])

                    self.set_attr_from_xmp_tag('vignetting_center', tags, [
                        'Camera:VignettingCenter',
                        'Sentera:VignettingCenter',
                    ])

                    self.set_attr_from_xmp_tag('vignetting_polynomial', tags, [
                        'Camera:VignettingPolynomial',
                        'Sentera:VignettingPolynomial',
                    ])
                    
                    self.set_attr_from_xmp_tag('horizontal_irradiance', tags, [
                        'Camera:HorizontalIrradiance'
                    ], float)

                    self.set_attr_from_xmp_tag('irradiance_scale_to_si', tags, [
                        'Camera:IrradianceScaleToSIUnits'
                    ], float)

                    self.set_attr_from_xmp_tag('sun_sensor', tags, [
                        'Camera:SunSensor',
                    ], float)

                    self.set_attr_from_xmp_tag('spectral_irradiance', tags, [
                        'Camera:SpectralIrradiance',
                        'Camera:Irradiance',                    
                    ], float)

                    # Phantom 4 RTK
                    if '@drone-dji:RtkStdLon' in tags:
                        y = float(self.get_xmp_tag(tags, '@drone-dji:RtkStdLon'))
                        x = float(self.get_xmp_tag(tags, '@drone-dji:RtkStdLat'))
                        self.gps_xy_stddev = max(x, y)
                    
                        if '@drone-dji:RtkStdHgt' in tags:
                            self.gps_z_stddev = float(self.get_xmp_tag(tags, '@drone-dji:RtkStdHgt'))
                    else:
                        self.set_attr_from_xmp_tag('gps_xy_stddev', tags, [
                            '@Camera:GPSXYAccuracy',
                            'GPSXYAccuracy'
                        ], float)
                        self.set_attr_from_xmp_tag('gps_z_stddev', tags, [
                            '@Camera:GPSZAccuracy',
                            'GPSZAccuracy'
                        ], float)

                    if 'DLS:Yaw' in tags:
                        self.set_attr_from_xmp_tag('dls_yaw', tags, ['DLS:Yaw'], float)
                        self.set_attr_from_xmp_tag('dls_pitch', tags, ['DLS:Pitch'], float)
                        self.set_attr_from_xmp_tag('dls_roll', tags, ['DLS:Roll'], float)
                except Exception as e:
                    log.ODM_WARNING(""Cannot read XMP tags for %s: %s"" % (_path_file, e.message))

                # self.set_attr_from_xmp_tag('center_wavelength', tags, [
                #     'Camera:CentralWavelength'
                # ], float)

                # self.set_attr_from_xmp_tag('bandwidth', tags, [
                #     'Camera:WavelengthFWHM'
                # ], float)
            
        self.width, self.height = get_image_size.get_image_size(_path_file)
        # Sanitize band name since we use it in folder paths
        self.band_name = re.sub('[^A-Za-z0-9]+', '', self.band_name)"
"    def set_attr_from_xmp_tag(self, attr, xmp_tags, tags, cast=None):
        v = self.get_xmp_tag(xmp_tags, tags)
        if v is not None:
            if cast is None:
                setattr(self, attr, v)
            else:
                # Handle fractions
                if (cast == float or cast == int) and ""/"" in v:
                    v = self.try_parse_fraction(v)
                setattr(self, attr, cast(v))","    def set_attr_from_xmp_tag(self, attr, xmp_tags, tags, cast=None):
        v = self.get_xmp_tag(xmp_tags, tags)
        if v is not None:
            if cast is None:
                setattr(self, attr, v)
            else:
                setattr(self, attr, cast(v))"
"    def process(self, args, outputs):
        tree = outputs['tree']
        reconstruction = outputs['reconstruction']
        photos = reconstruction.photos

        if not photos:
            log.ODM_ERROR('Not enough photos in photos array to start OpenSfM')
            exit(1)

        octx = OSFMContext(tree.opensfm)
        octx.setup(args, tree.dataset_raw, photos, reconstruction=reconstruction, rerun=self.rerun())
        octx.extract_metadata(self.rerun())
        self.update_progress(20)
        octx.feature_matching(self.rerun())
        self.update_progress(30)
        octx.reconstruct(self.rerun())
        octx.extract_cameras(tree.path(""cameras.json""), self.rerun())
        self.update_progress(70)

        if args.optimize_disk_space:
            for folder in [""features"", ""matches"", ""exif"", ""reports""]:
                folder_path = octx.path(folder)
                if os.path.islink(folder_path):
                    os.unlink(folder_path)
                else:
                    shutil.rmtree(folder_path)

        # If we find a special flag file for split/merge we stop right here
        if os.path.exists(octx.path(""split_merge_stop_at_reconstruction.txt"")):
            log.ODM_INFO(""Stopping OpenSfM early because we found: %s"" % octx.path(""split_merge_stop_at_reconstruction.txt""))
            self.next_stage = None
            return

        if args.fast_orthophoto:
            output_file = octx.path('reconstruction.ply')
        elif args.use_opensfm_dense:
            output_file = tree.opensfm_model
        else:
            output_file = tree.opensfm_reconstruction

        updated_config_flag_file = octx.path('updated_config.txt')

        # Make sure it's capped by the depthmap-resolution arg,
        # since the undistorted images are used for MVS
        outputs['undist_image_max_size'] = max(
            gsd.image_max_size(photos, args.orthophoto_resolution, tree.opensfm_reconstruction, ignore_gsd=args.ignore_gsd, has_gcp=reconstruction.has_gcp()),
            args.depthmap_resolution
        )

        if not io.file_exists(updated_config_flag_file) or self.rerun():
            octx.update_config({'undistorted_image_max_size': outputs['undist_image_max_size']})
            octx.touch(updated_config_flag_file)

        # These will be used for texturing / MVS
        if args.radiometric_calibration == ""none"":
            octx.convert_and_undistort(self.rerun())
        else:
            def radiometric_calibrate(shot_id, image):
                photo = reconstruction.get_photo(shot_id)
                return multispectral.dn_to_reflectance(photo, image, use_sun_sensor=args.radiometric_calibration==""camera+sun"")

            octx.convert_and_undistort(self.rerun(), radiometric_calibrate)

        self.update_progress(80)

        if reconstruction.multi_camera:
            # Dump band image lists
            log.ODM_INFO(""Multiple bands found"")
            for band in reconstruction.multi_camera:
                log.ODM_INFO(""Exporting %s band"" % band['name'])
                image_list_file = octx.path(""image_list_%s.txt"" % band['name'].lower())

                if not io.file_exists(image_list_file) or self.rerun():
                    with open(image_list_file, ""w"") as f:
                        f.write(""\\n"".join([p.filename for p in band['photos']]))
                        log.ODM_INFO(""Wrote %s"" % image_list_file)
                else:
                    log.ODM_WARNING(""Found a valid image list in %s for %s band"" % (image_list_file, band['name']))
                
                nvm_file = octx.path(""undistorted"", ""reconstruction_%s.nvm"" % band['name'].lower())
                if not io.file_exists(nvm_file) or self.rerun():
                    octx.run('export_visualsfm --points --image_list ""%s""' % image_list_file)
                    os.rename(tree.opensfm_reconstruction_nvm, nvm_file)
                else:
                    log.ODM_WARNING(""Found a valid NVM file in %s for %s band"" % (nvm_file, band['name']))

        if not io.file_exists(tree.opensfm_reconstruction_nvm) or self.rerun():
            octx.run('export_visualsfm --points')
        else:
            log.ODM_WARNING('Found a valid OpenSfM NVM reconstruction file in: %s' %
                            tree.opensfm_reconstruction_nvm)

        self.update_progress(85)

        # Skip dense reconstruction if necessary and export
        # sparse reconstruction instead
        if args.fast_orthophoto:
            if not io.file_exists(output_file) or self.rerun():
                octx.run('export_ply --no-cameras')
            else:
                log.ODM_WARNING(""Found a valid PLY reconstruction in %s"" % output_file)

        elif args.use_opensfm_dense:
            if not io.file_exists(output_file) or self.rerun():
                octx.run('compute_depthmaps')
            else:
                log.ODM_WARNING(""Found a valid dense reconstruction in %s"" % output_file)

        self.update_progress(90)

        if reconstruction.is_georeferenced() and (not io.file_exists(tree.opensfm_transformation) or self.rerun()):
            octx.run('export_geocoords --transformation --proj \\'%s\\'' % reconstruction.georef.proj4())
        else:
            log.ODM_WARNING(""Will skip exporting %s"" % tree.opensfm_transformation)
        
        if args.optimize_disk_space:
            os.remove(octx.path(""tracks.csv""))
            os.remove(octx.path(""undistorted"", ""tracks.csv""))
            os.remove(octx.path(""undistorted"", ""reconstruction.json""))
            if io.dir_exists(octx.path(""undistorted"", ""depthmaps"")):
                files = glob.glob(octx.path(""undistorted"", ""depthmaps"", ""*.npz""))
                for f in files:
                    os.remove(f)","    def process(self, args, outputs):
        tree = outputs['tree']
        reconstruction = outputs['reconstruction']
        photos = reconstruction.photos

        if not photos:
            log.ODM_ERROR('Not enough photos in photos array to start OpenSfM')
            exit(1)

        octx = OSFMContext(tree.opensfm)
        octx.setup(args, tree.dataset_raw, photos, reconstruction=reconstruction, rerun=self.rerun())
        octx.extract_metadata(self.rerun())
        self.update_progress(20)
        octx.feature_matching(self.rerun())
        self.update_progress(30)
        octx.reconstruct(self.rerun())
        octx.extract_cameras(tree.path(""cameras.json""), self.rerun())
        self.update_progress(70)

        if args.optimize_disk_space:
            shutil.rmtree(octx.path(""features""))
            shutil.rmtree(octx.path(""matches""))
            shutil.rmtree(octx.path(""exif""))
            shutil.rmtree(octx.path(""reports""))

        # If we find a special flag file for split/merge we stop right here
        if os.path.exists(octx.path(""split_merge_stop_at_reconstruction.txt"")):
            log.ODM_INFO(""Stopping OpenSfM early because we found: %s"" % octx.path(""split_merge_stop_at_reconstruction.txt""))
            self.next_stage = None
            return

        if args.fast_orthophoto:
            output_file = octx.path('reconstruction.ply')
        elif args.use_opensfm_dense:
            output_file = tree.opensfm_model
        else:
            output_file = tree.opensfm_reconstruction

        updated_config_flag_file = octx.path('updated_config.txt')

        # Make sure it's capped by the depthmap-resolution arg,
        # since the undistorted images are used for MVS
        outputs['undist_image_max_size'] = max(
            gsd.image_max_size(photos, args.orthophoto_resolution, tree.opensfm_reconstruction, ignore_gsd=args.ignore_gsd, has_gcp=reconstruction.has_gcp()),
            args.depthmap_resolution
        )

        if not io.file_exists(updated_config_flag_file) or self.rerun():
            octx.update_config({'undistorted_image_max_size': outputs['undist_image_max_size']})
            octx.touch(updated_config_flag_file)

        # These will be used for texturing / MVS
        if args.radiometric_calibration == ""none"":
            octx.convert_and_undistort(self.rerun())
        else:
            def radiometric_calibrate(shot_id, image):
                photo = reconstruction.get_photo(shot_id)
                return multispectral.dn_to_reflectance(photo, image, use_sun_sensor=args.radiometric_calibration==""camera+sun"")

            octx.convert_and_undistort(self.rerun(), radiometric_calibrate)

        self.update_progress(80)

        if reconstruction.multi_camera:
            # Dump band image lists
            log.ODM_INFO(""Multiple bands found"")
            for band in reconstruction.multi_camera:
                log.ODM_INFO(""Exporting %s band"" % band['name'])
                image_list_file = octx.path(""image_list_%s.txt"" % band['name'].lower())

                if not io.file_exists(image_list_file) or self.rerun():
                    with open(image_list_file, ""w"") as f:
                        f.write(""\\n"".join([p.filename for p in band['photos']]))
                        log.ODM_INFO(""Wrote %s"" % image_list_file)
                else:
                    log.ODM_WARNING(""Found a valid image list in %s for %s band"" % (image_list_file, band['name']))
                
                nvm_file = octx.path(""undistorted"", ""reconstruction_%s.nvm"" % band['name'].lower())
                if not io.file_exists(nvm_file) or self.rerun():
                    octx.run('export_visualsfm --points --image_list ""%s""' % image_list_file)
                    os.rename(tree.opensfm_reconstruction_nvm, nvm_file)
                else:
                    log.ODM_WARNING(""Found a valid NVM file in %s for %s band"" % (nvm_file, band['name']))

        if not io.file_exists(tree.opensfm_reconstruction_nvm) or self.rerun():
            octx.run('export_visualsfm --points')
        else:
            log.ODM_WARNING('Found a valid OpenSfM NVM reconstruction file in: %s' %
                            tree.opensfm_reconstruction_nvm)

        self.update_progress(85)

        # Skip dense reconstruction if necessary and export
        # sparse reconstruction instead
        if args.fast_orthophoto:
            if not io.file_exists(output_file) or self.rerun():
                octx.run('export_ply --no-cameras')
            else:
                log.ODM_WARNING(""Found a valid PLY reconstruction in %s"" % output_file)

        elif args.use_opensfm_dense:
            if not io.file_exists(output_file) or self.rerun():
                octx.run('compute_depthmaps')
            else:
                log.ODM_WARNING(""Found a valid dense reconstruction in %s"" % output_file)

        self.update_progress(90)

        if reconstruction.is_georeferenced() and (not io.file_exists(tree.opensfm_transformation) or self.rerun()):
            octx.run('export_geocoords --transformation --proj \\'%s\\'' % reconstruction.georef.proj4())
        else:
            log.ODM_WARNING(""Will skip exporting %s"" % tree.opensfm_transformation)
        
        if args.optimize_disk_space:
            os.remove(octx.path(""tracks.csv""))
            os.remove(octx.path(""undistorted"", ""tracks.csv""))
            os.remove(octx.path(""undistorted"", ""reconstruction.json""))
            if io.dir_exists(octx.path(""undistorted"", ""depthmaps"")):
                files = glob.glob(octx.path(""undistorted"", ""depthmaps"", ""*.npz""))
                for f in files:
                    os.remove(f)"
"def build_block_parser(md, **kwargs):
    """""" Build the default block parser used by Markdown. """"""
    parser = BlockParser(md)
    parser.blockprocessors.register(EmptyBlockProcessor(parser), 'empty', 100)
    parser.blockprocessors.register(ListIndentProcessor(parser), 'indent', 90)
    parser.blockprocessors.register(CodeBlockProcessor(parser), 'code', 80)
    parser.blockprocessors.register(HashHeaderProcessor(parser), 'hashheader', 70)
    parser.blockprocessors.register(SetextHeaderProcessor(parser), 'setextheader', 60)
    parser.blockprocessors.register(HRProcessor(parser), 'hr', 50)
    parser.blockprocessors.register(OListProcessor(parser), 'olist', 40)
    parser.blockprocessors.register(UListProcessor(parser), 'ulist', 30)
    parser.blockprocessors.register(BlockQuoteProcessor(parser), 'quote', 20)
    parser.blockprocessors.register(ReferenceProcessor(parser), 'reference', 15)
    parser.blockprocessors.register(ParagraphProcessor(parser), 'paragraph', 10)
    return parser","def build_block_parser(md, **kwargs):
    """""" Build the default block parser used by Markdown. """"""
    parser = BlockParser(md)
    parser.blockprocessors.register(EmptyBlockProcessor(parser), 'empty', 100)
    parser.blockprocessors.register(ListIndentProcessor(parser), 'indent', 90)
    parser.blockprocessors.register(CodeBlockProcessor(parser), 'code', 80)
    parser.blockprocessors.register(HashHeaderProcessor(parser), 'hashheader', 70)
    parser.blockprocessors.register(SetextHeaderProcessor(parser), 'setextheader', 60)
    parser.blockprocessors.register(HRProcessor(parser), 'hr', 50)
    parser.blockprocessors.register(OListProcessor(parser), 'olist', 40)
    parser.blockprocessors.register(UListProcessor(parser), 'ulist', 30)
    parser.blockprocessors.register(BlockQuoteProcessor(parser), 'quote', 20)
    parser.blockprocessors.register(ParagraphProcessor(parser), 'paragraph', 10)
    return parser"
"    def extendMarkdown(self, md):
        """""" Insert AbbrPreprocessor before ReferencePreprocessor. """"""
        md.parser.blockprocessors.register(AbbrPreprocessor(md.parser), 'abbr', 16)","    def extendMarkdown(self, md):
        """""" Insert AbbrPreprocessor before ReferencePreprocessor. """"""
        md.preprocessors.register(AbbrPreprocessor(md), 'abbr', 12)"
"    def run(self, parent, blocks):
        '''
        Find and remove all Abbreviation references from the text.
        Each reference is set as a new AbbrPattern in the markdown instance.

        '''
        block = blocks.pop(0)
        m = self.RE.search(block)
        if m:
            abbr = m.group('abbr').strip()
            title = m.group('title').strip()
            self.parser.md.inlinePatterns.register(
                AbbrInlineProcessor(self._generate_pattern(abbr), title), 'abbr-%s' % abbr, 2
            )
            if block[m.end():].strip():
                # Add any content after match back to blocks as separate block
                blocks.insert(0, block[m.end():].lstrip('\\n'))
            if block[:m.start()].strip():
                # Add any content before match back to blocks as separate block
                blocks.insert(0, block[:m.start()].rstrip('\\n'))
            return True
        # No match. Restore block.
        blocks.insert(0, block)
        return False","    def run(self, lines):
        '''
        Find and remove all Abbreviation references from the text.
        Each reference is set as a new AbbrPattern in the markdown instance.

        '''
        new_text = []
        for line in lines:
            m = ABBR_REF_RE.match(line)
            if m:
                abbr = m.group('abbr').strip()
                title = m.group('title').strip()
                self.md.inlinePatterns.register(
                    AbbrInlineProcessor(self._generate_pattern(abbr), title), 'abbr-%s' % abbr, 2
                )
                # Preserve the line to prevent raw HTML indexing issue.
                # https://github.com/Python-Markdown/markdown/issues/584
                new_text.append('')
            else:
                new_text.append(line)
        return new_text"
"    def extendMarkdown(self, md):
        """""" Add pieces to Markdown. """"""
        md.registerExtension(self)
        self.parser = md.parser
        self.md = md
        # Insert a blockprocessor before ReferencePreprocessor
        md.parser.blockprocessors.register(FootnoteBlockProcessor(self), 'footnote', 17)

        # Insert an inline pattern before ImageReferencePattern
        FOOTNOTE_RE = r'\\[\\^([^\\]]*)\\]'  # blah blah [^1] blah
        md.inlinePatterns.register(FootnoteInlineProcessor(FOOTNOTE_RE, self), 'footnote', 175)
        # Insert a tree-processor that would actually add the footnote div
        # This must be before all other treeprocessors (i.e., inline and
        # codehilite) so they can run on the the contents of the div.
        md.treeprocessors.register(FootnoteTreeprocessor(self), 'footnote', 50)

        # Insert a tree-processor that will run after inline is done.
        # In this tree-processor we want to check our duplicate footnote tracker
        # And add additional backrefs to the footnote pointing back to the
        # duplicated references.
        md.treeprocessors.register(FootnotePostTreeprocessor(self), 'footnote-duplicate', 15)

        # Insert a postprocessor after amp_substitute processor
        md.postprocessors.register(FootnotePostprocessor(self), 'footnote', 25)","    def extendMarkdown(self, md):
        """""" Add pieces to Markdown. """"""
        md.registerExtension(self)
        self.parser = md.parser
        self.md = md
        # Insert a preprocessor before ReferencePreprocessor
        md.preprocessors.register(FootnotePreprocessor(self), 'footnote', 15)

        # Insert an inline pattern before ImageReferencePattern
        FOOTNOTE_RE = r'\\[\\^([^\\]]*)\\]'  # blah blah [^1] blah
        md.inlinePatterns.register(FootnoteInlineProcessor(FOOTNOTE_RE, self), 'footnote', 175)
        # Insert a tree-processor that would actually add the footnote div
        # This must be before all other treeprocessors (i.e., inline and
        # codehilite) so they can run on the the contents of the div.
        md.treeprocessors.register(FootnoteTreeprocessor(self), 'footnote', 50)

        # Insert a tree-processor that will run after inline is done.
        # In this tree-processor we want to check our duplicate footnote tracker
        # And add additional backrefs to the footnote pointing back to the
        # duplicated references.
        md.treeprocessors.register(FootnotePostTreeprocessor(self), 'footnote-duplicate', 15)

        # Insert a postprocessor after amp_substitute processor
        md.postprocessors.register(FootnotePostprocessor(self), 'footnote', 25)"
"    def detectTabbed(self, blocks):
        """""" Find indented text and remove indent before further proccesing.

        Returns: a list of blocks with indentation removed.
        """"""
        fn_blocks = []
        while blocks:
            if blocks[0].startswith(' '*4):
                block = blocks.pop(0)
                # Check for new footnotes within this block and split at new footnote.
                m = self.RE.search(block)
                if m:
                    # Another footnote exists in this block.
                    # Any content before match is continuation of this footnote, which may be lazily indented.
                    before = block[:m.start()].rstrip('\\n')
                    fn_blocks.append(self.detab(before))
                    # Add back to blocks everything from begining of match forward for next iteration.
                    blocks.insert(0, block[m.start():])
                    # End of this footnote.
                    break
                else:
                    # Entire block is part of this footnote.
                    fn_blocks.append(self.detab(block))
            else:
                # End of this footnote.
                break
        return fn_blocks","    def detectTabbed(self, lines):
        """""" Find indented text and remove indent before further proccesing.

        Keyword arguments:

        * lines: an array of strings

        Returns: a list of post processed items and the index of last line.

        """"""
        items = []
        blank_line = False  # have we encountered a blank line yet?
        i = 0  # to keep track of where we are

        def detab(line):
            match = TABBED_RE.match(line)
            if match:
                return match.group(4)

        for line in lines:
            if line.strip():  # Non-blank line
                detabbed_line = detab(line)
                if detabbed_line:
                    items.append(detabbed_line)
                    i += 1
                    continue
                elif not blank_line and not DEF_RE.match(line):
                    # not tabbed but still part of first par.
                    items.append(line)
                    i += 1
                    continue
                else:
                    return items, i+1

            else:  # Blank line: _maybe_ we are done.
                blank_line = True
                i += 1  # advance

                # Find the next non-blank line
                for j in range(i, len(lines)):
                    if lines[j].strip():
                        next_line = lines[j]
                        break
                    else:
                        # Include extreaneous padding to prevent raw HTML
                        # parsing issue: https://github.com/Python-Markdown/markdown/issues/584
                        items.append("""")
                        i += 1
                else:
                    break  # There is no more text; we are done.

                # Check if the next non-blank line is tabbed
                if detab(next_line):  # Yes, more work to do.
                    items.append("""")
                    continue
                else:
                    break  # No, we are done.
        else:
            i += 1

        return items, i"
"    def detab(self, block):
        """""" Remove one level of indent from a block.

        Preserve lazily indented blocks by only removing indent from indented lines.
        """"""
        lines = block.split('\\n')
        for i, line in enumerate(lines):
            if line.startswith(' '*4):
                lines[i] = line[4:]
        return '\\n'.join(lines)","        def detab(line):
            match = TABBED_RE.match(line)
            if match:
                return match.group(4)"
"    def run(self, parent, blocks):
        m = util.HTML_PLACEHOLDER_RE.match(blocks[0])
        if m:
            index = int(m.group(1))
            element = self.parser.md.htmlStash.rawHtmlBlocks[index]
            if isinstance(element, etree.Element):
                # We have a matched element. Process it.
                blocks.pop(0)
                self.parse_element_content(element)
                parent.append(element)
                # Cleanup stash. Replace element with empty string to avoid confusing postprocessor.
                self.parser.md.htmlStash.rawHtmlBlocks.pop(index)
                self.parser.md.htmlStash.rawHtmlBlocks.insert(index, '')
                # Comfirm the match to the blockparser.
                return True
        # No match found.
        return False","    def run(self, parent, blocks, tail=None, nest=False):
        self._tag_data = self.parser.md.htmlStash.tag_data

        self.parser.blockprocessors.tag_counter += 1
        tag = self._tag_data[self.parser.blockprocessors.tag_counter]

        # Create Element
        markdown_value = tag['attrs'].pop('markdown')
        element = etree.SubElement(parent, tag['tag'], tag['attrs'])

        # Slice Off Block
        if nest:
            self.parser.parseBlocks(parent, tail)  # Process Tail
            block = blocks[1:]
        else:  # includes nests since a third level of nesting isn't supported
            block = blocks[tag['left_index'] + 1: tag['right_index']]
            del blocks[:tag['right_index']]

        # Process Text
        if (self.parser.blockprocessors.contain_span_tags.match(  # Span Mode
                tag['tag']) and markdown_value != 'block') or \\
                markdown_value == 'span':
            element.text = '\\n'.join(block)
        else:                                                     # Block Mode
            i = self.parser.blockprocessors.tag_counter + 1
            if len(self._tag_data) > i and self._tag_data[i]['left_index']:
                first_subelement_index = self._tag_data[i]['left_index'] - 1
                self.parser.parseBlocks(
                    element, block[:first_subelement_index])
                if not nest:
                    block = self._process_nests(element, block)
            else:
                self.parser.parseBlocks(element, block)"
"    def extendMarkdown(self, md):
        """""" Register extension instances. """"""

        # Replace raw HTML preprocessor
        md.preprocessors.register(HtmlBlockPreprocessor(md), 'html_block', 20)
        # Add blockprocessor which handles the placeholders for etree elements
        md.parser.blockprocessors.register(
            MarkdownInHtmlProcessor(md.parser), 'markdown_block', 105
        )","    def extendMarkdown(self, md):
        """""" Register extension instances. """"""

        # Turn on processing of markdown text within raw html
        md.preprocessors['html_block'].markdown_in_raw = True
        md.parser.blockprocessors.register(
            MarkdownInHtmlProcessor(md.parser), 'markdown_block', 105
        )
        md.parser.blockprocessors.tag_counter = -1
        md.parser.blockprocessors.contain_span_tags = re.compile(
            r'^(p|h[1-6]|li|dd|dt|td|th|legend|address)$', re.IGNORECASE)"
"    def run(self, text):
        """""" Iterate over html stash and restore html. """"""
        replacements = OrderedDict()
        for i in range(self.md.htmlStash.html_counter):
            html = self.md.htmlStash.rawHtmlBlocks[i]
            if self.isblocklevel(html):
                replacements[""<p>{}</p>"".format(
                    self.md.htmlStash.get_placeholder(i))] = html
            replacements[self.md.htmlStash.get_placeholder(i)] = html

        if replacements:
            pattern = re.compile(""|"".join(re.escape(k) for k in replacements))
            processed_text = pattern.sub(lambda m: replacements[m.group(0)], text)
        else:
            return text

        if processed_text == text:
            return processed_text
        else:
            return self.run(processed_text)","    def run(self, text):
        """""" Iterate over html stash and restore html. """"""
        replacements = OrderedDict()
        for i in range(self.md.htmlStash.html_counter):
            html = self.md.htmlStash.rawHtmlBlocks[i]
            if self.isblocklevel(html):
                replacements[""<p>%s</p>"" %
                             (self.md.htmlStash.get_placeholder(i))] = \\
                    html + ""\\n""
            replacements[self.md.htmlStash.get_placeholder(i)] = html

        if replacements:
            pattern = re.compile(""|"".join(re.escape(k) for k in replacements))
            processed_text = pattern.sub(lambda m: replacements[m.group(0)], text)
        else:
            return text

        if processed_text == text:
            return processed_text
        else:
            return self.run(processed_text)"
"def build_preprocessors(md, **kwargs):
    """""" Build the default set of preprocessors used by Markdown. """"""
    preprocessors = util.Registry()
    preprocessors.register(NormalizeWhitespace(md), 'normalize_whitespace', 30)
    preprocessors.register(HtmlBlockPreprocessor(md), 'html_block', 20)
    return preprocessors","def build_preprocessors(md, **kwargs):
    """""" Build the default set of preprocessors used by Markdown. """"""
    preprocessors = util.Registry()
    preprocessors.register(NormalizeWhitespace(md), 'normalize_whitespace', 30)
    preprocessors.register(HtmlBlockPreprocessor(md), 'html_block', 20)
    preprocessors.register(ReferencePreprocessor(md), 'reference', 10)
    return preprocessors"
"    def run(self, lines):
        source = '\\n'.join(lines)
        parser = HTMLExtractor(self.md)
        parser.feed(source)
        parser.close()
        return ''.join(parser.cleandoc).split('\\n')","    def run(self, lines):
        text = ""\\n"".join(lines)
        new_blocks = []
        text = text.rsplit(""\\n\\n"")
        items = []
        left_tag = ''
        right_tag = ''
        in_tag = False  # flag

        while text:
            block = text[0]
            if block.startswith(""\\n""):
                block = block[1:]
            text = text[1:]

            if block.startswith(""\\n""):
                block = block[1:]

            if not in_tag:
                if block.startswith(""<"") and len(block.strip()) > 1:

                    if block[1:4] == ""!--"":
                        # is a comment block
                        left_tag, left_index, attrs = ""--"", 2, {}
                    else:
                        left_tag, left_index, attrs = self._get_left_tag(block)
                    right_tag, data_index = self._get_right_tag(left_tag,
                                                                left_index,
                                                                block)
                    # keep checking conditions below and maybe just append

                    if data_index < len(block) and (self.md.is_block_level(left_tag) or left_tag == '--'):
                        text.insert(0, block[data_index:])
                        block = block[:data_index]

                    if not (self.md.is_block_level(left_tag) or block[1] in [""!"", ""?"", ""@"", ""%""]):
                        new_blocks.append(block)
                        continue

                    if self._is_oneliner(left_tag):
                        new_blocks.append(block.strip())
                        continue

                    if block.rstrip().endswith("">"") \\
                            and self._equal_tags(left_tag, right_tag):
                        if self.markdown_in_raw and 'markdown' in attrs.keys():
                            block = block[left_index:-len(right_tag) - 2]
                            new_blocks.append(self.md.htmlStash.
                                              store_tag(left_tag, attrs, 0, 2))
                            new_blocks.extend([block])
                        else:
                            new_blocks.append(
                                self.md.htmlStash.store(block.strip()))
                        continue
                    else:
                        # if is block level tag and is not complete
                        if (not self._equal_tags(left_tag, right_tag)) and \\
                           (self.md.is_block_level(left_tag) or left_tag == ""--""):
                            items.append(block.strip())
                            in_tag = True
                        else:
                            new_blocks.append(
                                self.md.htmlStash.store(block.strip())
                            )
                        continue

                else:
                    new_blocks.append(block)

            else:
                items.append(block)

                # Need to evaluate all items so we can calculate relative to the left index.
                right_tag, data_index = self._get_right_tag(left_tag, left_index, ''.join(items))
                # Adjust data_index: relative to items -> relative to last block
                prev_block_length = 0
                for item in items[:-1]:
                    prev_block_length += len(item)
                data_index -= prev_block_length

                if self._equal_tags(left_tag, right_tag):
                    # if find closing tag

                    if data_index < len(block):
                        # we have more text after right_tag
                        items[-1] = block[:data_index]
                        text.insert(0, block[data_index:])

                    in_tag = False
                    if self.markdown_in_raw and 'markdown' in attrs.keys():
                        items[0] = items[0][left_index:]
                        items[-1] = items[-1][:-len(right_tag) - 2]
                        if items[len(items) - 1]:  # not a newline/empty string
                            right_index = len(items) + 3
                        else:
                            right_index = len(items) + 2
                        new_blocks.append(self.md.htmlStash.store_tag(
                            left_tag, attrs, 0, right_index))
                        placeholderslen = len(self.md.htmlStash.tag_data)
                        new_blocks.extend(
                            self._nested_markdown_in_html(items))
                        nests = len(self.md.htmlStash.tag_data) - \\
                            placeholderslen
                        self.md.htmlStash.tag_data[-1 - nests][
                            'right_index'] += nests - 2
                    else:
                        new_blocks.append(
                            self.md.htmlStash.store('\\n\\n'.join(items)))
                    items = []

        if items:
            if self.markdown_in_raw and 'markdown' in attrs.keys():
                items[0] = items[0][left_index:]
                items[-1] = items[-1][:-len(right_tag) - 2]
                if items[len(items) - 1]:  # not a newline/empty string
                    right_index = len(items) + 3
                else:
                    right_index = len(items) + 2
                new_blocks.append(
                    self.md.htmlStash.store_tag(
                        left_tag, attrs, 0, right_index))
                placeholderslen = len(self.md.htmlStash.tag_data)
                new_blocks.extend(self._nested_markdown_in_html(items))
                nests = len(self.md.htmlStash.tag_data) - placeholderslen
                self.md.htmlStash.tag_data[-1 - nests][
                    'right_index'] += nests - 2
            else:
                new_blocks.append(
                    self.md.htmlStash.store('\\n\\n'.join(items)))
            new_blocks.append('\\n')

        new_text = ""\\n\\n"".join(new_blocks)
        return new_text.split(""\\n"")"
"    def run(self, lines):
        source = '\\n'.join(lines)
        parser = HTMLExtractor(self.md)
        parser.feed(source)
        parser.close()
        return ''.join(parser.cleandoc).split('\\n')","    def run(self, lines):
        new_text = []
        while lines:
            line = lines.pop(0)
            m = self.RE.match(line)
            if m:
                id = m.group(1).strip().lower()
                link = m.group(2).lstrip('<').rstrip('>')
                t = m.group(5) or m.group(6) or m.group(7)
                if not t:
                    # Check next line for title
                    tm = self.TITLE_RE.match(lines[0])
                    if tm:
                        lines.pop(0)
                        t = tm.group(2) or tm.group(3) or tm.group(4)
                self.md.references[id] = (link, t)
                # Preserve the line to prevent raw HTML indexing issue.
                # https://github.com/Python-Markdown/markdown/issues/584
                new_text.append('')
            else:
                new_text.append(line)

        return new_text  # + ""\\n"""
"    def run(self, lines):
        '''
        Find and remove all Abbreviation references from the text.
        Each reference is set as a new AbbrPattern in the markdown instance.

        '''
        new_text = []
        for line in lines:
            m = ABBR_REF_RE.match(line)
            if m:
                abbr = m.group('abbr').strip()
                title = m.group('title').strip()
                self.markdown.inlinePatterns['abbr-%s' % abbr] = \\
                    AbbrPattern(self._generate_pattern(abbr), title)
                # Preserve the line to prevent raw HTML indexing issue.
                # https://github.com/Python-Markdown/markdown/issues/584
                new_text.append('')
            else:
                new_text.append(line)
        return new_text","    def run(self, lines):
        '''
        Find and remove all Abbreviation references from the text.
        Each reference is set as a new AbbrPattern in the markdown instance.

        '''
        new_text = []
        for line in lines:
            m = ABBR_REF_RE.match(line)
            if m:
                abbr = m.group('abbr').strip()
                title = m.group('title').strip()
                self.markdown.inlinePatterns['abbr-%s' % abbr] = \\
                    AbbrPattern(self._generate_pattern(abbr), title)
            else:
                new_text.append(line)
        return new_text"
"    def run(self, lines):
        """"""
        Loop through lines and find, set, and remove footnote definitions.

        Keywords:

        * lines: A list of lines of text

        Return: A list of lines of text with footnote definitions removed.

        """"""
        newlines = []
        i = 0
        while True:
            m = DEF_RE.match(lines[i])
            if m:
                fn, _i = self.detectTabbed(lines[i+1:])
                fn.insert(0, m.group(2))
                i += _i-1  # skip past footnote
                footnote = ""\\n"".join(fn)
                self.footnotes.setFootnote(m.group(1), footnote.rstrip())
                # Preserve a line for each block to prevent raw HTML indexing issue.
                # https://github.com/Python-Markdown/markdown/issues/584
                num_blocks = (len(footnote.split('\\n\\n')) * 2)
                newlines.extend([''] * (num_blocks))
            else:
                newlines.append(lines[i])
            if len(lines) > i+1:
                i += 1
            else:
                break
        return newlines","    def run(self, lines):
        """"""
        Loop through lines and find, set, and remove footnote definitions.

        Keywords:

        * lines: A list of lines of text

        Return: A list of lines of text with footnote definitions removed.

        """"""
        newlines = []
        i = 0
        while True:
            m = DEF_RE.match(lines[i])
            if m:
                fn, _i = self.detectTabbed(lines[i+1:])
                fn.insert(0, m.group(2))
                i += _i-1  # skip past footnote
                self.footnotes.setFootnote(m.group(1), ""\\n"".join(fn))
            else:
                newlines.append(lines[i])
            if len(lines) > i+1:
                i += 1
            else:
                break
        return newlines"
"    def detectTabbed(self, lines):
        """""" Find indented text and remove indent before further proccesing.

        Keyword arguments:

        * lines: an array of strings

        Returns: a list of post processed items and the index of last line.

        """"""
        items = []
        blank_line = False  # have we encountered a blank line yet?
        i = 0  # to keep track of where we are

        def detab(line):
            match = TABBED_RE.match(line)
            if match:
                return match.group(4)

        for line in lines:
            if line.strip():  # Non-blank line
                detabbed_line = detab(line)
                if detabbed_line:
                    items.append(detabbed_line)
                    i += 1
                    continue
                elif not blank_line and not DEF_RE.match(line):
                    # not tabbed but still part of first par.
                    items.append(line)
                    i += 1
                    continue
                else:
                    return items, i+1

            else:  # Blank line: _maybe_ we are done.
                blank_line = True
                i += 1  # advance

                # Find the next non-blank line
                for j in range(i, len(lines)):
                    if lines[j].strip():
                        next_line = lines[j]
                        break
                    else:
                        # Include extreaneous padding to prevent raw HTML
                        # parsing issue: https://github.com/Python-Markdown/markdown/issues/584
                        items.append("""")
                        i += 1
                else:
                    break  # There is no more text; we are done.

                # Check if the next non-blank line is tabbed
                if detab(next_line):  # Yes, more work to do.
                    items.append("""")
                    continue
                else:
                    break  # No, we are done.
        else:
            i += 1

        return items, i","    def detectTabbed(self, lines):
        """""" Find indented text and remove indent before further proccesing.

        Keyword arguments:

        * lines: an array of strings

        Returns: a list of post processed items and the index of last line.

        """"""
        items = []
        blank_line = False  # have we encountered a blank line yet?
        i = 0  # to keep track of where we are

        def detab(line):
            match = TABBED_RE.match(line)
            if match:
                return match.group(4)

        for line in lines:
            if line.strip():  # Non-blank line
                detabbed_line = detab(line)
                if detabbed_line:
                    items.append(detabbed_line)
                    i += 1
                    continue
                elif not blank_line and not DEF_RE.match(line):
                    # not tabbed but still part of first par.
                    items.append(line)
                    i += 1
                    continue
                else:
                    return items, i+1

            else:  # Blank line: _maybe_ we are done.
                blank_line = True
                i += 1  # advance

                # Find the next non-blank line
                for j in range(i, len(lines)):
                    if lines[j].strip():
                        next_line = lines[j]
                        break
                else:
                    break  # There is no more text; we are done.

                # Check if the next non-blank line is tabbed
                if detab(next_line):  # Yes, more work to do.
                    items.append("""")
                    continue
                else:
                    break  # No, we are done.
        else:
            i += 1

        return items, i"
"    def run(self, lines):
        new_text = []
        while lines:
            line = lines.pop(0)
            m = self.RE.match(line)
            if m:
                id = m.group(1).strip().lower()
                link = m.group(2).lstrip('<').rstrip('>')
                t = m.group(5) or m.group(6) or m.group(7)
                if not t:
                    # Check next line for title
                    tm = self.TITLE_RE.match(lines[0])
                    if tm:
                        lines.pop(0)
                        t = tm.group(2) or tm.group(3) or tm.group(4)
                self.markdown.references[id] = (link, t)
                # Preserve the line to prevent raw HTML indexing issue.
                # https://github.com/Python-Markdown/markdown/issues/584
                new_text.append('')
            else:
                new_text.append(line)

        return new_text  # + ""\\n""","    def run(self, lines):
        new_text = []
        while lines:
            line = lines.pop(0)
            m = self.RE.match(line)
            if m:
                id = m.group(1).strip().lower()
                link = m.group(2).lstrip('<').rstrip('>')
                t = m.group(5) or m.group(6) or m.group(7)
                if not t:
                    # Check next line for title
                    tm = self.TITLE_RE.match(lines[0])
                    if tm:
                        lines.pop(0)
                        t = tm.group(2) or tm.group(3) or tm.group(4)
                self.markdown.references[id] = (link, t)
            else:
                new_text.append(line)

        return new_text  # + ""\\n"""
"    def run(self, root):
        """""" Add linebreaks to ElementTree root object. """"""

        self._prettifyETree(root)
        # Do <br />'s seperately as they are often in the middle of
        # inline content and missed by _prettifyETree.
        brs = root.iter('br')
        for br in brs:
            if not br.tail or not br.tail.strip():
                br.tail = '\\n'
            else:
                br.tail = '\\n%s' % br.tail
        # Clean up extra empty lines at end of code blocks.
        pres = root.iter('pre')
        for pre in pres:
            if len(pre) and pre[0].tag == 'code':
                pre[0].text = util.AtomicString(pre[0].text.rstrip() + '\\n')","    def run(self, root):
        """""" Add linebreaks to ElementTree root object. """"""

        self._prettifyETree(root)
        # Do <br />'s seperately as they are often in the middle of
        # inline content and missed by _prettifyETree.
        brs = root.getiterator('br')
        for br in brs:
            if not br.tail or not br.tail.strip():
                br.tail = '\\n'
            else:
                br.tail = '\\n%s' % br.tail
        # Clean up extra empty lines at end of code blocks.
        pres = root.getiterator('pre')
        for pre in pres:
            if len(pre) and pre[0].tag == 'code':
                pre[0].text = util.AtomicString(pre[0].text.rstrip() + '\\n')"
"    def unescape(self, text):
        """""" Return unescaped text given text with an inline placeholder. """"""
        try:
            stash = self.markdown.treeprocessors['inline'].stashed_nodes
        except KeyError:
            return text
        def get_stash(m):
            id = m.group(1)
            if id in stash:
                text = stash.get(id)
                if isinstance(text, basestring):
                    return text
                else:
                    return self.markdown.serializer(text)
        return util.INLINE_PLACEHOLDER_RE.sub(get_stash, text)","    def unescape(self, text):
        """""" Return unescaped text given text with an inline placeholder. """"""
        try:
            stash = self.markdown.treeprocessors['inline'].stashed_nodes
        except KeyError:
            return text
        def get_stash(m):
            id = m.group(1)
            if id in stash:
                return stash.get(id)
        return util.INLINE_PLACEHOLDER_RE.sub(get_stash, text)"
"        def get_stash(m):
            id = m.group(1)
            if id in stash:
                text = stash.get(id)
                if isinstance(text, basestring):
                    return text
                else:
                    return self.markdown.serializer(text)","        def get_stash(m):
            id = m.group(1)
            if id in stash:
                return stash.get(id)"
"    def unescape(self, text):
        """""" Return unescaped text given text with an inline placeholder. """"""
        try:
            stash = self.markdown.treeprocessors['inline'].stashed_nodes
        except KeyError:
            return text
        def itertext(el):
            ' Reimplement Element.itertext for older python versions '
            tag = el.tag
            if not isinstance(tag, basestring) and tag is not None:
                return
            if el.text:
                yield el.text
            for e in el:
                for s in itertext(e):
                    yield s
                if e.tail:
                    yield e.tail
        def get_stash(m):
            id = m.group(1)
            if id in stash:
                value = stash.get(id)
                if isinstance(value, basestring):
                    return value
                else:
                    # An etree Element - return text content only
                    return ''.join(itertext(value)) 
        return util.INLINE_PLACEHOLDER_RE.sub(get_stash, text)","    def unescape(self, text):
        """""" Return unescaped text given text with an inline placeholder. """"""
        try:
            stash = self.markdown.treeprocessors['inline'].stashed_nodes
        except KeyError:
            return text
        def get_stash(m):
            id = m.group(1)
            if id in stash:
                text = stash.get(id)
                if isinstance(text, basestring):
                    return text
                else:
                    return self.markdown.serializer(text)
        return util.INLINE_PLACEHOLDER_RE.sub(get_stash, text)"
"        def get_stash(m):
            id = m.group(1)
            value = stash.get(id)
            if value is not None:
                try:
                    return self.markdown.serializer(text)
                except:
                    return '\\%s' % value","        def get_stash(m):
            id = m.group(1)
            value = stash.get(id)
            if value is not None:
                try:
                    return self.markdown.serializer(value)
                except:
                    return '\\%s' % value"
"        def get_stash(m):
            id = m.group(1)
            if id in stash:
                value = stash.get(id)
                if isinstance(value, basestring):
                    return value
                else:
                    # An etree Element - return text content only
                    return ''.join(itertext(value)) ","        def get_stash(m):
            id = m.group(1)
            if id in stash:
                text = stash.get(id)
                if isinstance(text, basestring):
                    return text
                else:
                    return self.markdown.serializer(text)"
"    def unescape(self, text):
        """""" Return unescaped text given text with an inline placeholder. """"""
        try:
            stash = self.markdown.treeprocessors['inline'].stashed_nodes
        except KeyError:
            return text
        def get_stash(m):
            id = m.group(1)
            value = stash.get(id)
            if value is not None:
                try:
                    return self.markdown.serializer(text)
                except:
                    return '\\%s' % value
            
        return util.INLINE_PLACEHOLDER_RE.sub(get_stash, text)","    def unescape(self, text):
        """""" Return unescaped text given text with an inline placeholder. """"""
        try:
            stash = self.markdown.treeprocessors['inline'].stashed_nodes
        except KeyError:
            return text
        def get_stash(m):
            id = m.group(1)
            value = stash.get(id)
            if value is not None:
                try:
                    return self.markdown.serializer(value)
                except:
                    return '\\%s' % value
            
        return util.INLINE_PLACEHOLDER_RE.sub(get_stash, text)"
"    def makeTag(self, href, title, text):
        el = util.etree.Element(""img"")
        el.set(""src"", self.sanitize_url(href))
        if title:
            el.set(""title"", title)
        el.set(""alt"", self.unescape(text))
        return el","    def makeTag(self, href, title, text):
        el = util.etree.Element(""img"")
        el.set(""src"", self.sanitize_url(href))
        if title:
            el.set(""title"", title)
        el.set(""alt"", text)
        return el"
"    def _get_left_tag(self, block):
        m = self.left_tag_re.match(block)
        if m:
            tag = m.group('tag')
            raw_attrs = m.group('attrs')
            attrs = {}
            if raw_attrs:
                for ma in self.attrs_re.finditer(raw_attrs):
                    if ma.group('attr'):
                        if ma.group('value'):
                            attrs[ma.group('attr').strip()] = ma.group('value')
                        else:
                            attrs[ma.group('attr').strip()] = """"
                    elif ma.group('attr1'):
                        if ma.group('value1'):
                            attrs[ma.group('attr1').strip()] = ma.group('value1')
                        else:
                            attrs[ma.group('attr1').strip()] = """"
                    elif ma.group('attr2'):
                        attrs[ma.group('attr2').strip()] = """"
            return tag, len(m.group(0)), attrs
        else:
            tag = block[1:].split("">"", 1)[0].lower()
            return tag, len(tag)+2, {}","    def _get_left_tag(self, block):
        m = self.left_tag_re.match(block)
        if m:
            tag = m.group('tag')
            raw_attrs = m.group('attrs')
            attrs = {}
            if raw_attrs:
                for ma in self.attrs_re.finditer(raw_attrs):
                    if ma.group('attr'):
                        if ma.group('value'):
                            attrs[ma.group('attr').strip()] = ma.group('value')
                        else:
                            attrs[ma.group('attr').strip()] = """"
                    elif ma.group('attr1'):
                        if ma.group('value1'):
                            attrs[ma.group('attr1').strip()] = ma.group('value1')
                        else:
                            attrs[ma.group('attr1').strip()] = """"
                    elif ma.group('attr2'):
                        attrs[ma.group('attr2').strip()] = """"
            return tag, len(m.group(0)), attrs
        else:
            tag = block[1:].replace("">"", "" "", 1).split()[0].lower()
            return tag, len(tag)+2, {}"
"def sort_imports(
    file_name: str,
    config: Config,
    check: bool = False,
    ask_to_apply: bool = False,
    write_to_stdout: bool = False,
    **kwargs: Any,
) -> Optional[SortAttempt]:
    incorrectly_sorted: bool = False
    skipped: bool = False
    try:
        if check:
            try:
                incorrectly_sorted = not api.check_file(file_name, config=config, **kwargs)
            except FileSkipped:
                skipped = True
            return SortAttempt(incorrectly_sorted, skipped, True)

        try:
            incorrectly_sorted = not api.sort_file(
                file_name,
                config=config,
                ask_to_apply=ask_to_apply,
                write_to_stdout=write_to_stdout,
                **kwargs,
            )
        except FileSkipped:
            skipped = True
        return SortAttempt(incorrectly_sorted, skipped, True)
    except (OSError, ValueError) as error:
        warn(f""Unable to parse file {file_name} due to {error}"")
        return None
    except UnsupportedEncoding:
        if config.verbose:
            warn(f""Encoding not supported for {file_name}"")
        return SortAttempt(incorrectly_sorted, skipped, False)
    except KeyError as error:
        if error.args[0] not in DEFAULT_CONFIG.sections:
            _print_hard_fail(config, offending_file=file_name)
            raise
        msg = (
            f""Found {error} imports while parsing, but {error} was not included ""
            ""in the `sections` setting of your config. Please add it before continuing\\n""
            ""See https://pycqa.github.io/isort/#custom-sections-and-ordering ""
            ""for more info.""
        )
        _print_hard_fail(config, message=msg)
        sys.exit(os.EX_CONFIG)
    except Exception:
        _print_hard_fail(config, offending_file=file_name)
        raise","def sort_imports(
    file_name: str,
    config: Config,
    check: bool = False,
    ask_to_apply: bool = False,
    write_to_stdout: bool = False,
    **kwargs: Any,
) -> Optional[SortAttempt]:
    incorrectly_sorted: bool = False
    skipped: bool = False
    try:
        if check:
            try:
                incorrectly_sorted = not api.check_file(file_name, config=config, **kwargs)
            except FileSkipped:
                skipped = True
            return SortAttempt(incorrectly_sorted, skipped, True)

        try:
            incorrectly_sorted = not api.sort_file(
                file_name,
                config=config,
                ask_to_apply=ask_to_apply,
                write_to_stdout=write_to_stdout,
                **kwargs,
            )
        except FileSkipped:
            skipped = True
        return SortAttempt(incorrectly_sorted, skipped, True)
    except (OSError, ValueError) as error:
        warn(f""Unable to parse file {file_name} due to {error}"")
        return None
    except UnsupportedEncoding:
        if config.verbose:
            warn(f""Encoding not supported for {file_name}"")
        return SortAttempt(incorrectly_sorted, skipped, False)
    except Exception:
        printer = create_terminal_printer(color=config.color_output)
        printer.error(
            f""Unrecoverable exception thrown when parsing {file_name}! ""
            ""This should NEVER happen.\\n""
            ""If encountered, please open an issue: https://github.com/PyCQA/isort/issues/new""
        )
        raise"
"    def __init__(
        self,
        filename: Union[str, Path],
    ):
        super().__init__(f""Unknown or unsupported encoding in {filename}"")
        self.filename = filename","    def __init__(self, unsupported_settings: Dict[str, Dict[str, str]]):
        errors = ""\\n"".join(
            self._format_option(name, **option) for name, option in unsupported_settings.items()
        )

        super().__init__(
            ""isort was provided settings that it doesn't support:\\n\\n""
            f""{errors}\\n\\n""
            ""For a complete and up-to-date listing of supported settings see: ""
            ""https://pycqa.github.io/isort/docs/configuration/options/.\\n""
        )
        self.unsupported_settings = unsupported_settings"
"    def from_contents(contents: str, filename: str) -> ""File"":
        encoding = File.detect_encoding(filename, BytesIO(contents.encode(""utf-8"")).readline)
        return File(StringIO(contents), path=Path(filename).resolve(), encoding=encoding)","    def from_contents(contents: str, filename: str) -> ""File"":
        encoding, _ = tokenize.detect_encoding(BytesIO(contents.encode(""utf-8"")).readline)
        return File(StringIO(contents), path=Path(filename).resolve(), encoding=encoding)"
"    def _open(filename):
        """"""Open a file in read only mode using the encoding detected by
        detect_encoding().
        """"""
        buffer = open(filename, ""rb"")
        try:
            encoding = File.detect_encoding(filename, buffer.readline)
            buffer.seek(0)
            text = TextIOWrapper(buffer, encoding, line_buffering=True, newline="""")
            text.mode = ""r""  # type: ignore
            return text
        except Exception:
            buffer.close()
            raise","    def _open(filename):
        """"""Open a file in read only mode using the encoding detected by
        detect_encoding().
        """"""
        buffer = open(filename, ""rb"")
        try:
            encoding, _ = tokenize.detect_encoding(buffer.readline)
            buffer.seek(0)
            text = TextIOWrapper(buffer, encoding, line_buffering=True, newline="""")
            text.mode = ""r""  # type: ignore
            return text
        except Exception:
            buffer.close()
            raise"
"    def __init__(self, incorrectly_sorted: bool, skipped: bool, supported_encoding: bool) -> None:
        self.incorrectly_sorted = incorrectly_sorted
        self.skipped = skipped
        self.supported_encoding = supported_encoding","    def __init__(self, incorrectly_sorted: bool, skipped: bool) -> None:
        self.incorrectly_sorted = incorrectly_sorted
        self.skipped = skipped"
"def sort_imports(
    file_name: str,
    config: Config,
    check: bool = False,
    ask_to_apply: bool = False,
    write_to_stdout: bool = False,
    **kwargs: Any,
) -> Optional[SortAttempt]:
    try:
        incorrectly_sorted: bool = False
        skipped: bool = False
        if check:
            try:
                incorrectly_sorted = not api.check_file(file_name, config=config, **kwargs)
            except FileSkipped:
                skipped = True
            return SortAttempt(incorrectly_sorted, skipped, True)
        else:
            try:
                incorrectly_sorted = not api.sort_file(
                    file_name,
                    config=config,
                    ask_to_apply=ask_to_apply,
                    write_to_stdout=write_to_stdout,
                    **kwargs,
                )
            except FileSkipped:
                skipped = True
            return SortAttempt(incorrectly_sorted, skipped, True)
    except (OSError, ValueError) as error:
        warn(f""Unable to parse file {file_name} due to {error}"")
        return None
    except UnsupportedEncoding:
        if config.verbose:
            warn(f""Encoding not supported for {file_name}"")
        return SortAttempt(incorrectly_sorted, skipped, False)
    except Exception:
        printer = create_terminal_printer(color=config.color_output)
        printer.error(
            f""Unrecoverable exception thrown when parsing {file_name}! ""
            ""This should NEVER happen.\\n""
            ""If encountered, please open an issue: https://github.com/PyCQA/isort/issues/new""
        )
        raise","def sort_imports(
    file_name: str,
    config: Config,
    check: bool = False,
    ask_to_apply: bool = False,
    write_to_stdout: bool = False,
    **kwargs: Any,
) -> Optional[SortAttempt]:
    try:
        incorrectly_sorted: bool = False
        skipped: bool = False
        if check:
            try:
                incorrectly_sorted = not api.check_file(file_name, config=config, **kwargs)
            except FileSkipped:
                skipped = True
            return SortAttempt(incorrectly_sorted, skipped)
        else:
            try:
                incorrectly_sorted = not api.sort_file(
                    file_name,
                    config=config,
                    ask_to_apply=ask_to_apply,
                    write_to_stdout=write_to_stdout,
                    **kwargs,
                )
            except FileSkipped:
                skipped = True
            return SortAttempt(incorrectly_sorted, skipped)
    except (OSError, ValueError) as error:
        warn(f""Unable to parse file {file_name} due to {error}"")
        return None
    except Exception:
        printer = create_terminal_printer(color=config.color_output)
        printer.error(
            f""Unrecoverable exception thrown when parsing {file_name}! ""
            ""This should NEVER happen.\\n""
            ""If encountered, please open an issue: https://github.com/PyCQA/isort/issues/new""
        )
        raise"
"def main(argv: Optional[Sequence[str]] = None, stdin: Optional[TextIOWrapper] = None) -> None:
    arguments = parse_args(argv)
    if arguments.get(""show_version""):
        print(ASCII_ART)
        return

    show_config: bool = arguments.pop(""show_config"", False)
    show_files: bool = arguments.pop(""show_files"", False)
    if show_config and show_files:
        sys.exit(""Error: either specify show-config or show-files not both."")

    if ""settings_path"" in arguments:
        if os.path.isfile(arguments[""settings_path""]):
            arguments[""settings_file""] = os.path.abspath(arguments[""settings_path""])
            arguments[""settings_path""] = os.path.dirname(arguments[""settings_file""])
        else:
            arguments[""settings_path""] = os.path.abspath(arguments[""settings_path""])

    if ""virtual_env"" in arguments:
        venv = arguments[""virtual_env""]
        arguments[""virtual_env""] = os.path.abspath(venv)
        if not os.path.isdir(arguments[""virtual_env""]):
            warn(f""virtual_env dir does not exist: {arguments['virtual_env']}"")

    file_names = arguments.pop(""files"", [])
    if not file_names and not show_config:
        print(QUICK_GUIDE)
        if arguments:
            sys.exit(""Error: arguments passed in without any paths or content."")
        else:
            return
    if ""settings_path"" not in arguments:
        arguments[""settings_path""] = (
            os.path.abspath(file_names[0] if file_names else ""."") or os.getcwd()
        )
        if not os.path.isdir(arguments[""settings_path""]):
            arguments[""settings_path""] = os.path.dirname(arguments[""settings_path""])

    config_dict = arguments.copy()
    ask_to_apply = config_dict.pop(""ask_to_apply"", False)
    jobs = config_dict.pop(""jobs"", ())
    check = config_dict.pop(""check"", False)
    show_diff = config_dict.pop(""show_diff"", False)
    write_to_stdout = config_dict.pop(""write_to_stdout"", False)
    deprecated_flags = config_dict.pop(""deprecated_flags"", False)
    remapped_deprecated_args = config_dict.pop(""remapped_deprecated_args"", False)
    wrong_sorted_files = False
    all_attempt_broken = False
    no_valid_encodings = False

    if ""src_paths"" in config_dict:
        config_dict[""src_paths""] = {
            Path(src_path).resolve() for src_path in config_dict.get(""src_paths"", ())
        }

    config = Config(**config_dict)
    if show_config:
        print(json.dumps(config.__dict__, indent=4, separators=("","", "": ""), default=_preconvert))
        return
    elif file_names == [""-""]:
        if show_files:
            sys.exit(""Error: can't show files for streaming input."")

        if check:
            incorrectly_sorted = not api.check_stream(
                input_stream=sys.stdin if stdin is None else stdin,
                config=config,
                show_diff=show_diff,
            )

            wrong_sorted_files = incorrectly_sorted
        else:
            api.sort_stream(
                input_stream=sys.stdin if stdin is None else stdin,
                output_stream=sys.stdout,
                config=config,
                show_diff=show_diff,
            )
    else:
        skipped: List[str] = []
        broken: List[str] = []

        if config.filter_files:
            filtered_files = []
            for file_name in file_names:
                if config.is_skipped(Path(file_name)):
                    skipped.append(file_name)
                else:
                    filtered_files.append(file_name)
            file_names = filtered_files

        file_names = iter_source_code(file_names, config, skipped, broken)
        if show_files:
            for file_name in file_names:
                print(file_name)
            return
        num_skipped = 0
        num_broken = 0
        num_invalid_encoding = 0
        if config.verbose:
            print(ASCII_ART)

        if jobs:
            import multiprocessing

            executor = multiprocessing.Pool(jobs)
            attempt_iterator = executor.imap(
                functools.partial(
                    sort_imports,
                    config=config,
                    check=check,
                    ask_to_apply=ask_to_apply,
                    write_to_stdout=write_to_stdout,
                ),
                file_names,
            )
        else:
            # https://github.com/python/typeshed/pull/2814
            attempt_iterator = (
                sort_imports(  # type: ignore
                    file_name,
                    config=config,
                    check=check,
                    ask_to_apply=ask_to_apply,
                    show_diff=show_diff,
                    write_to_stdout=write_to_stdout,
                )
                for file_name in file_names
            )

        # If any files passed in are missing considered as error, should be removed
        is_no_attempt = True
        any_encoding_valid = False
        for sort_attempt in attempt_iterator:
            if not sort_attempt:
                continue  # pragma: no cover - shouldn't happen, satisfies type constraint
            incorrectly_sorted = sort_attempt.incorrectly_sorted
            if arguments.get(""check"", False) and incorrectly_sorted:
                wrong_sorted_files = True
            if sort_attempt.skipped:
                num_skipped += (
                    1  # pragma: no cover - shouldn't happen, due to skip in iter_source_code
                )

            if not sort_attempt.supported_encoding:
                num_invalid_encoding += 1
            else:
                any_encoding_valid = True

            is_no_attempt = False

        num_skipped += len(skipped)
        if num_skipped and not arguments.get(""quiet"", False):
            if config.verbose:
                for was_skipped in skipped:
                    warn(
                        f""{was_skipped} was skipped as it's listed in 'skip' setting""
                        "" or matches a glob in 'skip_glob' setting""
                    )
            print(f""Skipped {num_skipped} files"")

        num_broken += len(broken)
        if num_broken and not arguments.get(""quite"", False):
            if config.verbose:
                for was_broken in broken:
                    warn(f""{was_broken} was broken path, make sure it exists correctly"")
            print(f""Broken {num_broken} paths"")

        if num_broken > 0 and is_no_attempt:
            all_attempt_broken = True
        if num_invalid_encoding > 0 and not any_encoding_valid:
            no_valid_encodings = True

    if not config.quiet and (remapped_deprecated_args or deprecated_flags):
        if remapped_deprecated_args:
            warn(
                ""W0502: The following deprecated single dash CLI flags were used and translated: ""
                f""{', '.join(remapped_deprecated_args)}!""
            )
        if deprecated_flags:
            warn(
                ""W0501: The following deprecated CLI flags were used and ignored: ""
                f""{', '.join(deprecated_flags)}!""
            )
        warn(
            ""W0500: Please see the 5.0.0 Upgrade guide: ""
            ""https://pycqa.github.io/isort/docs/upgrade_guides/5.0.0/""
        )

    if wrong_sorted_files:
        sys.exit(1)

    if all_attempt_broken:
        sys.exit(1)

    if no_valid_encodings:
        printer = create_terminal_printer(color=config.color_output)
        printer.error(""No valid encodings."")
        sys.exit(1)","def main(argv: Optional[Sequence[str]] = None, stdin: Optional[TextIOWrapper] = None) -> None:
    arguments = parse_args(argv)
    if arguments.get(""show_version""):
        print(ASCII_ART)
        return

    show_config: bool = arguments.pop(""show_config"", False)
    show_files: bool = arguments.pop(""show_files"", False)
    if show_config and show_files:
        sys.exit(""Error: either specify show-config or show-files not both."")

    if ""settings_path"" in arguments:
        if os.path.isfile(arguments[""settings_path""]):
            arguments[""settings_file""] = os.path.abspath(arguments[""settings_path""])
            arguments[""settings_path""] = os.path.dirname(arguments[""settings_file""])
        else:
            arguments[""settings_path""] = os.path.abspath(arguments[""settings_path""])

    if ""virtual_env"" in arguments:
        venv = arguments[""virtual_env""]
        arguments[""virtual_env""] = os.path.abspath(venv)
        if not os.path.isdir(arguments[""virtual_env""]):
            warn(f""virtual_env dir does not exist: {arguments['virtual_env']}"")

    file_names = arguments.pop(""files"", [])
    if not file_names and not show_config:
        print(QUICK_GUIDE)
        if arguments:
            sys.exit(""Error: arguments passed in without any paths or content."")
        else:
            return
    if ""settings_path"" not in arguments:
        arguments[""settings_path""] = (
            os.path.abspath(file_names[0] if file_names else ""."") or os.getcwd()
        )
        if not os.path.isdir(arguments[""settings_path""]):
            arguments[""settings_path""] = os.path.dirname(arguments[""settings_path""])

    config_dict = arguments.copy()
    ask_to_apply = config_dict.pop(""ask_to_apply"", False)
    jobs = config_dict.pop(""jobs"", ())
    check = config_dict.pop(""check"", False)
    show_diff = config_dict.pop(""show_diff"", False)
    write_to_stdout = config_dict.pop(""write_to_stdout"", False)
    deprecated_flags = config_dict.pop(""deprecated_flags"", False)
    remapped_deprecated_args = config_dict.pop(""remapped_deprecated_args"", False)
    wrong_sorted_files = False
    all_attempt_broken = False

    if ""src_paths"" in config_dict:
        config_dict[""src_paths""] = {
            Path(src_path).resolve() for src_path in config_dict.get(""src_paths"", ())
        }

    config = Config(**config_dict)
    if show_config:
        print(json.dumps(config.__dict__, indent=4, separators=("","", "": ""), default=_preconvert))
        return
    elif file_names == [""-""]:
        if show_files:
            sys.exit(""Error: can't show files for streaming input."")

        if check:
            incorrectly_sorted = not api.check_stream(
                input_stream=sys.stdin if stdin is None else stdin,
                config=config,
                show_diff=show_diff,
            )

            wrong_sorted_files = incorrectly_sorted
        else:
            api.sort_stream(
                input_stream=sys.stdin if stdin is None else stdin,
                output_stream=sys.stdout,
                config=config,
                show_diff=show_diff,
            )
    else:
        skipped: List[str] = []
        broken: List[str] = []

        if config.filter_files:
            filtered_files = []
            for file_name in file_names:
                if config.is_skipped(Path(file_name)):
                    skipped.append(file_name)
                else:
                    filtered_files.append(file_name)
            file_names = filtered_files

        file_names = iter_source_code(file_names, config, skipped, broken)
        if show_files:
            for file_name in file_names:
                print(file_name)
            return
        num_skipped = 0
        num_broken = 0
        if config.verbose:
            print(ASCII_ART)

        if jobs:
            import multiprocessing

            executor = multiprocessing.Pool(jobs)
            attempt_iterator = executor.imap(
                functools.partial(
                    sort_imports,
                    config=config,
                    check=check,
                    ask_to_apply=ask_to_apply,
                    write_to_stdout=write_to_stdout,
                ),
                file_names,
            )
        else:
            # https://github.com/python/typeshed/pull/2814
            attempt_iterator = (
                sort_imports(  # type: ignore
                    file_name,
                    config=config,
                    check=check,
                    ask_to_apply=ask_to_apply,
                    show_diff=show_diff,
                    write_to_stdout=write_to_stdout,
                )
                for file_name in file_names
            )

        # If any files passed in are missing considered as error, should be removed
        is_no_attempt = True
        for sort_attempt in attempt_iterator:
            if not sort_attempt:
                continue  # pragma: no cover - shouldn't happen, satisfies type constraint
            incorrectly_sorted = sort_attempt.incorrectly_sorted
            if arguments.get(""check"", False) and incorrectly_sorted:
                wrong_sorted_files = True
            if sort_attempt.skipped:
                num_skipped += (
                    1  # pragma: no cover - shouldn't happen, due to skip in iter_source_code
                )
            is_no_attempt = False

        num_skipped += len(skipped)
        if num_skipped and not arguments.get(""quiet"", False):
            if config.verbose:
                for was_skipped in skipped:
                    warn(
                        f""{was_skipped} was skipped as it's listed in 'skip' setting""
                        "" or matches a glob in 'skip_glob' setting""
                    )
            print(f""Skipped {num_skipped} files"")

        num_broken += len(broken)
        if num_broken and not arguments.get(""quite"", False):
            if config.verbose:
                for was_broken in broken:
                    warn(f""{was_broken} was broken path, make sure it exists correctly"")
            print(f""Broken {num_broken} paths"")

        if num_broken > 0 and is_no_attempt:
            all_attempt_broken = True

    if not config.quiet and (remapped_deprecated_args or deprecated_flags):
        if remapped_deprecated_args:
            warn(
                ""W0502: The following deprecated single dash CLI flags were used and translated: ""
                f""{', '.join(remapped_deprecated_args)}!""
            )
        if deprecated_flags:
            warn(
                ""W0501: The following deprecated CLI flags were used and ignored: ""
                f""{', '.join(deprecated_flags)}!""
            )
        warn(
            ""W0500: Please see the 5.0.0 Upgrade guide: ""
            ""https://pycqa.github.io/isort/docs/upgrade_guides/5.0.0/""
        )

    if wrong_sorted_files:
        sys.exit(1)

    if all_attempt_broken:
        sys.exit(1)"
"def _known_pattern(name: str, config: Config) -> Optional[Tuple[str, str]]:
    parts = name.split(""."")
    module_names_to_check = (""."".join(parts[:first_k]) for first_k in range(len(parts), 0, -1))
    for module_name_to_check in module_names_to_check:
        for pattern, placement in config.known_patterns:
            if placement in config.sections and pattern.match(module_name_to_check):
                return (placement, f""Matched configured known pattern {pattern}"")

    return None","def _known_pattern(name: str, config: Config) -> Optional[Tuple[str, str]]:
    parts = name.split(""."")
    module_names_to_check = (""."".join(parts[:first_k]) for first_k in range(len(parts), 0, -1))
    for module_name_to_check in module_names_to_check:
        for pattern, placement in config.known_patterns:
            if pattern.match(module_name_to_check):
                return (placement, f""Matched configured known pattern {pattern}"")

    return None"
"def process(
    input_stream: TextIO,
    output_stream: TextIO,
    extension: str = ""py"",
    config: Config = DEFAULT_CONFIG,
) -> bool:
    """"""Parses stream identifying sections of contiguous imports and sorting them

    Code with unsorted imports is read from the provided `input_stream`, sorted and then
    outputted to the specified `output_stream`.

    - `input_stream`: Text stream with unsorted import sections.
    - `output_stream`: Text stream to output sorted inputs into.
    - `config`: Config settings to use when sorting imports. Defaults settings.
        - *Default*: `isort.settings.DEFAULT_CONFIG`.
    - `extension`: The file extension or file extension rules that should be used.
        - *Default*: `""py""`.
        - *Choices*: `[""py"", ""pyi"", ""pyx""]`.

    Returns `True` if there were changes that needed to be made (errors present) from what
    was provided in the input_stream, otherwise `False`.
    """"""
    line_separator: str = config.line_ending
    add_imports: List[str] = [format_natural(addition) for addition in config.add_imports]
    import_section: str = """"
    next_import_section: str = """"
    next_cimports: bool = False
    in_quote: str = """"
    first_comment_index_start: int = -1
    first_comment_index_end: int = -1
    contains_imports: bool = False
    in_top_comment: bool = False
    first_import_section: bool = True
    section_comments = [f""# {heading}"" for heading in config.import_headings.values()]
    indent: str = """"
    isort_off: bool = False
    code_sorting: Union[bool, str] = False
    code_sorting_section: str = """"
    code_sorting_indent: str = """"
    cimports: bool = False
    made_changes: bool = False
    stripped_line: str = """"
    end_of_file: bool = False

    if config.float_to_top:
        new_input = """"
        current = """"
        isort_off = False
        for line in chain(input_stream, (None,)):
            if isort_off and line is not None:
                if line == ""# isort: on\\n"":
                    isort_off = False
                new_input += line
            elif line in (""# isort: split\\n"", ""# isort: off\\n"", None) or str(line).endswith(
                ""# isort: split\\n""
            ):
                if line == ""# isort: off\\n"":
                    isort_off = True
                if current:
                    parsed = parse.file_contents(current, config=config)
                    extra_space = """"
                    while current and current[-1] == ""\\n"":
                        extra_space += ""\\n""
                        current = current[:-1]
                    extra_space = extra_space.replace(""\\n"", """", 1)
                    sorted_output = output.sorted_imports(
                        parsed, config, extension, import_type=""import""
                    )
                    made_changes = made_changes or _has_changed(
                        before=current,
                        after=sorted_output,
                        line_separator=parsed.line_separator,
                        ignore_whitespace=config.ignore_whitespace,
                    )
                    new_input += sorted_output
                    new_input += extra_space
                    current = """"
                new_input += line or """"
            else:
                current += line or """"

        input_stream = StringIO(new_input)

    for index, line in enumerate(chain(input_stream, (None,))):
        if line is None:
            if index == 0 and not config.force_adds:
                return False

            not_imports = True
            end_of_file = True
            line = """"
            if not line_separator:
                line_separator = ""\\n""

            if code_sorting and code_sorting_section:
                output_stream.write(
                    textwrap.indent(
                        isort.literal.assignment(
                            code_sorting_section,
                            str(code_sorting),
                            extension,
                            config=_indented_config(config, indent),
                        ),
                        code_sorting_indent,
                    )
                )
        else:
            stripped_line = line.strip()
            if stripped_line and not line_separator:
                line_separator = line[len(line.rstrip()) :].replace("" "", """").replace(""\\t"", """")

            for file_skip_comment in FILE_SKIP_COMMENTS:
                if file_skip_comment in line:
                    raise FileSkipComment(""Passed in content"")

            if not in_quote and stripped_line == ""# isort: off"":
                isort_off = True

            if (
                (index == 0 or (index in (1, 2) and not contains_imports))
                and stripped_line.startswith(""#"")
                and stripped_line not in section_comments
            ):
                in_top_comment = True
            elif in_top_comment:
                if not line.startswith(""#"") or stripped_line in section_comments:
                    in_top_comment = False
                    first_comment_index_end = index - 1

            if (not stripped_line.startswith(""#"") or in_quote) and '""' in line or ""'"" in line:
                char_index = 0
                if first_comment_index_start == -1 and (
                    line.startswith('""') or line.startswith(""'"")
                ):
                    first_comment_index_start = index
                while char_index < len(line):
                    if line[char_index] == ""\\\\"":
                        char_index += 1
                    elif in_quote:
                        if line[char_index : char_index + len(in_quote)] == in_quote:
                            in_quote = """"
                            if first_comment_index_end < first_comment_index_start:
                                first_comment_index_end = index
                    elif line[char_index] in (""'"", '""'):
                        long_quote = line[char_index : char_index + 3]
                        if long_quote in ('""""""', ""'''""):
                            in_quote = long_quote
                            char_index += 2
                        else:
                            in_quote = line[char_index]
                    elif line[char_index] == ""#"":
                        break
                    char_index += 1

            not_imports = bool(in_quote) or in_top_comment or isort_off
            if not (in_quote or in_top_comment):
                if isort_off:
                    if stripped_line == ""# isort: on"":
                        isort_off = False
                elif stripped_line.endswith(""# isort: split""):
                    not_imports = True
                elif stripped_line in CODE_SORT_COMMENTS:
                    code_sorting = stripped_line.split(""isort: "")[1].strip()
                    code_sorting_indent = line[: -len(line.lstrip())]
                    not_imports = True
                elif code_sorting:
                    if not stripped_line:
                        output_stream.write(
                            textwrap.indent(
                                isort.literal.assignment(
                                    code_sorting_section,
                                    str(code_sorting),
                                    extension,
                                    config=_indented_config(config, indent),
                                ),
                                code_sorting_indent,
                            )
                        )
                        not_imports = True
                        code_sorting = False
                        code_sorting_section = """"
                        code_sorting_indent = """"
                    else:
                        code_sorting_section += line
                        line = """"
                elif stripped_line in config.section_comments and not import_section:
                    import_section += line
                    indent = line[: -len(line.lstrip())]
                elif not (stripped_line or contains_imports):
                    not_imports = True
                elif (
                    not stripped_line
                    or stripped_line.startswith(""#"")
                    and (not indent or indent + line.lstrip() == line)
                    and not config.treat_all_comments_as_code
                    and stripped_line not in config.treat_comments_as_code
                ):
                    import_section += line
                elif stripped_line.startswith(IMPORT_START_IDENTIFIERS):
                    contains_imports = True

                    new_indent = line[: -len(line.lstrip())]
                    import_statement = line
                    stripped_line = line.strip().split(""#"")[0]
                    while stripped_line.endswith(""\\\\"") or (
                        ""("" in stripped_line and "")"" not in stripped_line
                    ):
                        if stripped_line.endswith(""\\\\""):
                            while stripped_line and stripped_line.endswith(""\\\\""):
                                line = input_stream.readline()
                                stripped_line = line.strip().split(""#"")[0]
                                import_statement += line
                        else:
                            while "")"" not in stripped_line:
                                line = input_stream.readline()
                                stripped_line = line.strip().split(""#"")[0]
                                import_statement += line

                    cimport_statement: bool = False
                    if (
                        import_statement.lstrip().startswith(CIMPORT_IDENTIFIERS)
                        or "" cimport "" in import_statement
                        or "" cimport*"" in import_statement
                        or "" cimport("" in import_statement
                        or "".cimport"" in import_statement
                    ):
                        cimport_statement = True

                    if cimport_statement != cimports or (new_indent != indent and import_section):
                        if import_section:
                            next_cimports = cimport_statement
                            next_import_section = import_statement
                            import_statement = """"
                            not_imports = True
                            line = """"
                        else:
                            cimports = cimport_statement

                    indent = new_indent
                    import_section += import_statement
                else:
                    not_imports = True

        if not_imports:
            raw_import_section: str = import_section
            if (
                add_imports
                and (stripped_line or end_of_file)
                and not config.append_only
                and not in_top_comment
                and not in_quote
                and not import_section
                and not line.lstrip().startswith(COMMENT_INDICATORS)
            ):
                import_section = line_separator.join(add_imports) + line_separator
                if end_of_file and index != 0:
                    output_stream.write(line_separator)
                contains_imports = True
                add_imports = []

            if next_import_section and not import_section:  # pragma: no cover
                raw_import_section = import_section = next_import_section
                next_import_section = """"

            if import_section:
                if add_imports and not indent:
                    import_section = (
                        line_separator.join(add_imports) + line_separator + import_section
                    )
                    contains_imports = True
                    add_imports = []

                if not indent:
                    import_section += line
                    raw_import_section += line
                if not contains_imports:
                    output_stream.write(import_section)
                else:
                    leading_whitespace = import_section[: -len(import_section.lstrip())]
                    trailing_whitespace = import_section[len(import_section.rstrip()) :]
                    if first_import_section and not import_section.lstrip(
                        line_separator
                    ).startswith(COMMENT_INDICATORS):
                        import_section = import_section.lstrip(line_separator)
                        raw_import_section = raw_import_section.lstrip(line_separator)
                        first_import_section = False

                    if indent:
                        import_section = """".join(
                            line[len(indent) :] for line in import_section.splitlines(keepends=True)
                        )

                    sorted_import_section = output.sorted_imports(
                        parse.file_contents(import_section, config=config),
                        _indented_config(config, indent),
                        extension,
                        import_type=""cimport"" if cimports else ""import"",
                    )
                    if not (import_section.strip() and not sorted_import_section):
                        if indent:
                            sorted_import_section = (
                                leading_whitespace
                                + textwrap.indent(sorted_import_section, indent).strip()
                                + trailing_whitespace
                            )

                        made_changes = made_changes or _has_changed(
                            before=raw_import_section,
                            after=sorted_import_section,
                            line_separator=line_separator,
                            ignore_whitespace=config.ignore_whitespace,
                        )

                        output_stream.write(sorted_import_section)
                        if not line and not indent and next_import_section:
                            output_stream.write(line_separator)

                if indent:
                    output_stream.write(line)
                    if not next_import_section:
                        indent = """"

                if next_import_section:
                    cimports = next_cimports
                    contains_imports = True
                else:
                    contains_imports = False
                import_section = next_import_section
                next_import_section = """"
            else:
                output_stream.write(line)
                not_imports = False

    return made_changes","def process(
    input_stream: TextIO,
    output_stream: TextIO,
    extension: str = ""py"",
    config: Config = DEFAULT_CONFIG,
) -> bool:
    """"""Parses stream identifying sections of contiguous imports and sorting them

    Code with unsorted imports is read from the provided `input_stream`, sorted and then
    outputted to the specified `output_stream`.

    - `input_stream`: Text stream with unsorted import sections.
    - `output_stream`: Text stream to output sorted inputs into.
    - `config`: Config settings to use when sorting imports. Defaults settings.
        - *Default*: `isort.settings.DEFAULT_CONFIG`.
    - `extension`: The file extension or file extension rules that should be used.
        - *Default*: `""py""`.
        - *Choices*: `[""py"", ""pyi"", ""pyx""]`.

    Returns `True` if there were changes that needed to be made (errors present) from what
    was provided in the input_stream, otherwise `False`.
    """"""
    line_separator: str = config.line_ending
    add_imports: List[str] = [format_natural(addition) for addition in config.add_imports]
    import_section: str = """"
    next_import_section: str = """"
    next_cimports: bool = False
    in_quote: str = """"
    first_comment_index_start: int = -1
    first_comment_index_end: int = -1
    contains_imports: bool = False
    in_top_comment: bool = False
    first_import_section: bool = True
    section_comments = [f""# {heading}"" for heading in config.import_headings.values()]
    indent: str = """"
    isort_off: bool = False
    code_sorting: Union[bool, str] = False
    code_sorting_section: str = """"
    code_sorting_indent: str = """"
    cimports: bool = False
    made_changes: bool = False
    stripped_line: str = """"
    end_of_file: bool = False

    if config.float_to_top:
        new_input = """"
        current = """"
        isort_off = False
        for line in chain(input_stream, (None,)):
            if isort_off and line is not None:
                if line == ""# isort: on\\n"":
                    isort_off = False
                new_input += line
            elif line in (""# isort: split\\n"", ""# isort: off\\n"", None) or str(line).endswith(
                ""# isort: split\\n""
            ):
                if line == ""# isort: off\\n"":
                    isort_off = True
                if current:
                    parsed = parse.file_contents(current, config=config)
                    extra_space = """"
                    while current[-1] == ""\\n"":
                        extra_space += ""\\n""
                        current = current[:-1]
                    extra_space = extra_space.replace(""\\n"", """", 1)
                    sorted_output = output.sorted_imports(
                        parsed, config, extension, import_type=""import""
                    )
                    made_changes = made_changes or _has_changed(
                        before=current,
                        after=sorted_output,
                        line_separator=parsed.line_separator,
                        ignore_whitespace=config.ignore_whitespace,
                    )
                    new_input += sorted_output
                    new_input += extra_space
                    current = """"
                new_input += line or """"
            else:
                current += line or """"

        input_stream = StringIO(new_input)

    for index, line in enumerate(chain(input_stream, (None,))):
        if line is None:
            if index == 0 and not config.force_adds:
                return False

            not_imports = True
            end_of_file = True
            line = """"
            if not line_separator:
                line_separator = ""\\n""

            if code_sorting and code_sorting_section:
                output_stream.write(
                    textwrap.indent(
                        isort.literal.assignment(
                            code_sorting_section,
                            str(code_sorting),
                            extension,
                            config=_indented_config(config, indent),
                        ),
                        code_sorting_indent,
                    )
                )
        else:
            stripped_line = line.strip()
            if stripped_line and not line_separator:
                line_separator = line[len(line.rstrip()) :].replace("" "", """").replace(""\\t"", """")

            for file_skip_comment in FILE_SKIP_COMMENTS:
                if file_skip_comment in line:
                    raise FileSkipComment(""Passed in content"")

            if not in_quote and stripped_line == ""# isort: off"":
                isort_off = True

            if (
                (index == 0 or (index in (1, 2) and not contains_imports))
                and stripped_line.startswith(""#"")
                and stripped_line not in section_comments
            ):
                in_top_comment = True
            elif in_top_comment:
                if not line.startswith(""#"") or stripped_line in section_comments:
                    in_top_comment = False
                    first_comment_index_end = index - 1

            if (not stripped_line.startswith(""#"") or in_quote) and '""' in line or ""'"" in line:
                char_index = 0
                if first_comment_index_start == -1 and (
                    line.startswith('""') or line.startswith(""'"")
                ):
                    first_comment_index_start = index
                while char_index < len(line):
                    if line[char_index] == ""\\\\"":
                        char_index += 1
                    elif in_quote:
                        if line[char_index : char_index + len(in_quote)] == in_quote:
                            in_quote = """"
                            if first_comment_index_end < first_comment_index_start:
                                first_comment_index_end = index
                    elif line[char_index] in (""'"", '""'):
                        long_quote = line[char_index : char_index + 3]
                        if long_quote in ('""""""', ""'''""):
                            in_quote = long_quote
                            char_index += 2
                        else:
                            in_quote = line[char_index]
                    elif line[char_index] == ""#"":
                        break
                    char_index += 1

            not_imports = bool(in_quote) or in_top_comment or isort_off
            if not (in_quote or in_top_comment):
                if isort_off:
                    if stripped_line == ""# isort: on"":
                        isort_off = False
                elif stripped_line.endswith(""# isort: split""):
                    not_imports = True
                elif stripped_line in CODE_SORT_COMMENTS:
                    code_sorting = stripped_line.split(""isort: "")[1].strip()
                    code_sorting_indent = line[: -len(line.lstrip())]
                    not_imports = True
                elif code_sorting:
                    if not stripped_line:
                        output_stream.write(
                            textwrap.indent(
                                isort.literal.assignment(
                                    code_sorting_section,
                                    str(code_sorting),
                                    extension,
                                    config=_indented_config(config, indent),
                                ),
                                code_sorting_indent,
                            )
                        )
                        not_imports = True
                        code_sorting = False
                        code_sorting_section = """"
                        code_sorting_indent = """"
                    else:
                        code_sorting_section += line
                        line = """"
                elif stripped_line in config.section_comments and not import_section:
                    import_section += line
                    indent = line[: -len(line.lstrip())]
                elif not (stripped_line or contains_imports):
                    not_imports = True
                elif (
                    not stripped_line
                    or stripped_line.startswith(""#"")
                    and (not indent or indent + line.lstrip() == line)
                    and not config.treat_all_comments_as_code
                    and stripped_line not in config.treat_comments_as_code
                ):
                    import_section += line
                elif stripped_line.startswith(IMPORT_START_IDENTIFIERS):
                    contains_imports = True

                    new_indent = line[: -len(line.lstrip())]
                    import_statement = line
                    stripped_line = line.strip().split(""#"")[0]
                    while stripped_line.endswith(""\\\\"") or (
                        ""("" in stripped_line and "")"" not in stripped_line
                    ):
                        if stripped_line.endswith(""\\\\""):
                            while stripped_line and stripped_line.endswith(""\\\\""):
                                line = input_stream.readline()
                                stripped_line = line.strip().split(""#"")[0]
                                import_statement += line
                        else:
                            while "")"" not in stripped_line:
                                line = input_stream.readline()
                                stripped_line = line.strip().split(""#"")[0]
                                import_statement += line

                    cimport_statement: bool = False
                    if (
                        import_statement.lstrip().startswith(CIMPORT_IDENTIFIERS)
                        or "" cimport "" in import_statement
                        or "" cimport*"" in import_statement
                        or "" cimport("" in import_statement
                        or "".cimport"" in import_statement
                    ):
                        cimport_statement = True

                    if cimport_statement != cimports or (new_indent != indent and import_section):
                        if import_section:
                            next_cimports = cimport_statement
                            next_import_section = import_statement
                            import_statement = """"
                            not_imports = True
                            line = """"
                        else:
                            cimports = cimport_statement

                    indent = new_indent
                    import_section += import_statement
                else:
                    not_imports = True

        if not_imports:
            raw_import_section: str = import_section
            if (
                add_imports
                and (stripped_line or end_of_file)
                and not config.append_only
                and not in_top_comment
                and not in_quote
                and not import_section
                and not line.lstrip().startswith(COMMENT_INDICATORS)
            ):
                import_section = line_separator.join(add_imports) + line_separator
                if end_of_file and index != 0:
                    output_stream.write(line_separator)
                contains_imports = True
                add_imports = []

            if next_import_section and not import_section:  # pragma: no cover
                raw_import_section = import_section = next_import_section
                next_import_section = """"

            if import_section:
                if add_imports and not indent:
                    import_section = (
                        line_separator.join(add_imports) + line_separator + import_section
                    )
                    contains_imports = True
                    add_imports = []

                if not indent:
                    import_section += line
                    raw_import_section += line
                if not contains_imports:
                    output_stream.write(import_section)
                else:
                    leading_whitespace = import_section[: -len(import_section.lstrip())]
                    trailing_whitespace = import_section[len(import_section.rstrip()) :]
                    if first_import_section and not import_section.lstrip(
                        line_separator
                    ).startswith(COMMENT_INDICATORS):
                        import_section = import_section.lstrip(line_separator)
                        raw_import_section = raw_import_section.lstrip(line_separator)
                        first_import_section = False

                    if indent:
                        import_section = """".join(
                            line[len(indent) :] for line in import_section.splitlines(keepends=True)
                        )

                    sorted_import_section = output.sorted_imports(
                        parse.file_contents(import_section, config=config),
                        _indented_config(config, indent),
                        extension,
                        import_type=""cimport"" if cimports else ""import"",
                    )
                    if not (import_section.strip() and not sorted_import_section):
                        if indent:
                            sorted_import_section = (
                                leading_whitespace
                                + textwrap.indent(sorted_import_section, indent).strip()
                                + trailing_whitespace
                            )

                        made_changes = made_changes or _has_changed(
                            before=raw_import_section,
                            after=sorted_import_section,
                            line_separator=line_separator,
                            ignore_whitespace=config.ignore_whitespace,
                        )

                        output_stream.write(sorted_import_section)
                        if not line and not indent and next_import_section:
                            output_stream.write(line_separator)

                if indent:
                    output_stream.write(line)
                    if not next_import_section:
                        indent = """"

                if next_import_section:
                    cimports = next_cimports
                    contains_imports = True
                else:
                    contains_imports = False
                import_section = next_import_section
                next_import_section = """"
            else:
                output_stream.write(line)
                not_imports = False

    return made_changes"
"def sort_imports(
    file_name: str,
    config: Config,
    check: bool = False,
    ask_to_apply: bool = False,
    write_to_stdout: bool = False,
    **kwargs: Any,
) -> Optional[SortAttempt]:
    try:
        incorrectly_sorted: bool = False
        skipped: bool = False
        if check:
            try:
                incorrectly_sorted = not api.check_file(file_name, config=config, **kwargs)
            except FileSkipped:
                skipped = True
            return SortAttempt(incorrectly_sorted, skipped)
        else:
            try:
                incorrectly_sorted = not api.sort_file(
                    file_name,
                    config=config,
                    ask_to_apply=ask_to_apply,
                    write_to_stdout=write_to_stdout,
                    **kwargs,
                )
            except FileSkipped:
                skipped = True
            return SortAttempt(incorrectly_sorted, skipped)
    except (OSError, ValueError) as error:
        warn(f""Unable to parse file {file_name} due to {error}"")
        return None
    except Exception:
        printer = create_terminal_printer(color=config.color_output)
        printer.error(
            f""Unrecoverable exception thrown when parsing {file_name}! ""
            ""This should NEVER happen.\\n""
            ""If encountered, please open an issue: https://github.com/PyCQA/isort/issues/new""
        )
        raise","def sort_imports(
    file_name: str,
    config: Config,
    check: bool = False,
    ask_to_apply: bool = False,
    write_to_stdout: bool = False,
    **kwargs: Any,
) -> Optional[SortAttempt]:
    try:
        incorrectly_sorted: bool = False
        skipped: bool = False
        if check:
            try:
                incorrectly_sorted = not api.check_file(file_name, config=config, **kwargs)
            except FileSkipped:
                skipped = True
            return SortAttempt(incorrectly_sorted, skipped)
        else:
            try:
                incorrectly_sorted = not api.sort_file(
                    file_name,
                    config=config,
                    ask_to_apply=ask_to_apply,
                    write_to_stdout=write_to_stdout,
                    **kwargs,
                )
            except FileSkipped:
                skipped = True
            return SortAttempt(incorrectly_sorted, skipped)
    except (OSError, ValueError) as error:
        warn(f""Unable to parse file {file_name} due to {error}"")
        return None"
"def main(argv: Optional[Sequence[str]] = None, stdin: Optional[TextIOWrapper] = None) -> None:
    arguments = parse_args(argv)
    if arguments.get(""show_version""):
        print(ASCII_ART)
        return

    show_config: bool = arguments.pop(""show_config"", False)

    if ""settings_path"" in arguments:
        if os.path.isfile(arguments[""settings_path""]):
            arguments[""settings_file""] = os.path.abspath(arguments[""settings_path""])
            arguments[""settings_path""] = os.path.dirname(arguments[""settings_file""])
        else:
            arguments[""settings_path""] = os.path.abspath(arguments[""settings_path""])

    if ""virtual_env"" in arguments:
        venv = arguments[""virtual_env""]
        arguments[""virtual_env""] = os.path.abspath(venv)
        if not os.path.isdir(arguments[""virtual_env""]):
            warn(f""virtual_env dir does not exist: {arguments['virtual_env']}"")

    file_names = arguments.pop(""files"", [])
    if not file_names and not show_config:
        print(QUICK_GUIDE)
        if arguments:
            sys.exit(""Error: arguments passed in without any paths or content."")
        else:
            return
    if ""settings_path"" not in arguments:
        arguments[""settings_path""] = (
            os.path.abspath(file_names[0] if file_names else ""."") or os.getcwd()
        )
        if not os.path.isdir(arguments[""settings_path""]):
            arguments[""settings_path""] = os.path.dirname(arguments[""settings_path""])

    config_dict = arguments.copy()
    ask_to_apply = config_dict.pop(""ask_to_apply"", False)
    jobs = config_dict.pop(""jobs"", ())
    check = config_dict.pop(""check"", False)
    show_diff = config_dict.pop(""show_diff"", False)
    write_to_stdout = config_dict.pop(""write_to_stdout"", False)
    deprecated_flags = config_dict.pop(""deprecated_flags"", False)
    remapped_deprecated_args = config_dict.pop(""remapped_deprecated_args"", False)
    wrong_sorted_files = False

    if ""src_paths"" in config_dict:
        config_dict[""src_paths""] = {
            Path(src_path).resolve() for src_path in config_dict.get(""src_paths"", ())
        }

    config = Config(**config_dict)
    if show_config:
        print(json.dumps(config.__dict__, indent=4, separators=("","", "": ""), default=_preconvert))
        return
    elif file_names == [""-""]:
        api.sort_stream(
            input_stream=sys.stdin if stdin is None else stdin,
            output_stream=sys.stdout,
            config=config,
        )
    else:
        skipped: List[str] = []

        if config.filter_files:
            filtered_files = []
            for file_name in file_names:
                if config.is_skipped(Path(file_name)):
                    skipped.append(file_name)
                else:
                    filtered_files.append(file_name)
            file_names = filtered_files

        file_names = iter_source_code(file_names, config, skipped)
        num_skipped = 0
        if config.verbose:
            print(ASCII_ART)

        if jobs:
            import multiprocessing

            executor = multiprocessing.Pool(jobs)
            attempt_iterator = executor.imap(
                functools.partial(
                    sort_imports,
                    config=config,
                    check=check,
                    ask_to_apply=ask_to_apply,
                    write_to_stdout=write_to_stdout,
                ),
                file_names,
            )
        else:
            # https://github.com/python/typeshed/pull/2814
            attempt_iterator = (
                sort_imports(  # type: ignore
                    file_name,
                    config=config,
                    check=check,
                    ask_to_apply=ask_to_apply,
                    show_diff=show_diff,
                    write_to_stdout=write_to_stdout,
                )
                for file_name in file_names
            )

        for sort_attempt in attempt_iterator:
            if not sort_attempt:
                continue  # pragma: no cover - shouldn't happen, satisfies type constraint
            incorrectly_sorted = sort_attempt.incorrectly_sorted
            if arguments.get(""check"", False) and incorrectly_sorted:
                wrong_sorted_files = True
            if sort_attempt.skipped:
                num_skipped += (
                    1  # pragma: no cover - shouldn't happen, due to skip in iter_source_code
                )

        num_skipped += len(skipped)
        if num_skipped and not arguments.get(""quiet"", False):
            if config.verbose:
                for was_skipped in skipped:
                    warn(
                        f""{was_skipped} was skipped as it's listed in 'skip' setting""
                        "" or matches a glob in 'skip_glob' setting""
                    )
            print(f""Skipped {num_skipped} files"")

    if not config.quiet and (remapped_deprecated_args or deprecated_flags):
        if remapped_deprecated_args:
            warn(
                ""W0502: The following deprecated single dash CLI flags were used and translated: ""
                f""{', '.join(remapped_deprecated_args)}!""
            )
        if deprecated_flags:
            warn(
                ""W0501: The following deprecated CLI flags were used and ignored: ""
                f""{', '.join(deprecated_flags)}!""
            )
        warn(
            ""W0500: Please see the 5.0.0 Upgrade guide: ""
            ""https://pycqa.github.io/isort/docs/upgrade_guides/5.0.0/""
        )

    if wrong_sorted_files:
        sys.exit(1)","def main(argv: Optional[Sequence[str]] = None, stdin: Optional[TextIOWrapper] = None) -> None:
    arguments = parse_args(argv)
    if arguments.get(""show_version""):
        print(ASCII_ART)
        return

    show_config: bool = arguments.pop(""show_config"", False)

    if ""settings_path"" in arguments:
        if os.path.isfile(arguments[""settings_path""]):
            arguments[""settings_file""] = os.path.abspath(arguments[""settings_path""])
            arguments[""settings_path""] = os.path.dirname(arguments[""settings_file""])
        else:
            arguments[""settings_path""] = os.path.abspath(arguments[""settings_path""])

    if ""virtual_env"" in arguments:
        venv = arguments[""virtual_env""]
        arguments[""virtual_env""] = os.path.abspath(venv)
        if not os.path.isdir(arguments[""virtual_env""]):
            warn(f""virtual_env dir does not exist: {arguments['virtual_env']}"")

    file_names = arguments.pop(""files"", [])
    if not file_names and not show_config:
        print(QUICK_GUIDE)
        if arguments:
            sys.exit(""Error: arguments passed in without any paths or content."")
        else:
            return
    if ""settings_path"" not in arguments:
        arguments[""settings_path""] = (
            os.path.abspath(file_names[0] if file_names else ""."") or os.getcwd()
        )
        if not os.path.isdir(arguments[""settings_path""]):
            arguments[""settings_path""] = os.path.dirname(arguments[""settings_path""])

    config_dict = arguments.copy()
    ask_to_apply = config_dict.pop(""ask_to_apply"", False)
    jobs = config_dict.pop(""jobs"", ())
    check = config_dict.pop(""check"", False)
    show_diff = config_dict.pop(""show_diff"", False)
    write_to_stdout = config_dict.pop(""write_to_stdout"", False)
    deprecated_flags = config_dict.pop(""deprecated_flags"", False)
    remapped_deprecated_args = config_dict.pop(""remapped_deprecated_args"", False)
    wrong_sorted_files = False

    if ""src_paths"" in config_dict:
        config_dict[""src_paths""] = {
            Path(src_path).resolve() for src_path in config_dict.get(""src_paths"", ())
        }

    config = Config(**config_dict)
    if show_config:
        print(json.dumps(config.__dict__, indent=4, separators=("","", "": ""), default=_preconvert))
        return
    elif file_names == [""-""]:
        arguments.setdefault(""settings_path"", os.getcwd())
        api.sort_stream(
            input_stream=sys.stdin if stdin is None else stdin,
            output_stream=sys.stdout,
            **arguments,
        )
    else:
        skipped: List[str] = []

        if config.filter_files:
            filtered_files = []
            for file_name in file_names:
                if config.is_skipped(Path(file_name)):
                    skipped.append(file_name)
                else:
                    filtered_files.append(file_name)
            file_names = filtered_files

        file_names = iter_source_code(file_names, config, skipped)
        num_skipped = 0
        if config.verbose:
            print(ASCII_ART)

        if jobs:
            import multiprocessing

            executor = multiprocessing.Pool(jobs)
            attempt_iterator = executor.imap(
                functools.partial(
                    sort_imports,
                    config=config,
                    check=check,
                    ask_to_apply=ask_to_apply,
                    write_to_stdout=write_to_stdout,
                ),
                file_names,
            )
        else:
            # https://github.com/python/typeshed/pull/2814
            attempt_iterator = (
                sort_imports(  # type: ignore
                    file_name,
                    config=config,
                    check=check,
                    ask_to_apply=ask_to_apply,
                    show_diff=show_diff,
                    write_to_stdout=write_to_stdout,
                )
                for file_name in file_names
            )

        for sort_attempt in attempt_iterator:
            if not sort_attempt:
                continue  # pragma: no cover - shouldn't happen, satisfies type constraint
            incorrectly_sorted = sort_attempt.incorrectly_sorted
            if arguments.get(""check"", False) and incorrectly_sorted:
                wrong_sorted_files = True
            if sort_attempt.skipped:
                num_skipped += (
                    1  # pragma: no cover - shouldn't happen, due to skip in iter_source_code
                )

        num_skipped += len(skipped)
        if num_skipped and not arguments.get(""quiet"", False):
            if config.verbose:
                for was_skipped in skipped:
                    warn(
                        f""{was_skipped} was skipped as it's listed in 'skip' setting""
                        "" or matches a glob in 'skip_glob' setting""
                    )
            print(f""Skipped {num_skipped} files"")

    if not config.quiet and (remapped_deprecated_args or deprecated_flags):
        if remapped_deprecated_args:
            warn(
                ""W0502: The following deprecated single dash CLI flags were used and translated: ""
                f""{', '.join(remapped_deprecated_args)}!""
            )
        if deprecated_flags:
            warn(
                ""W0501: The following deprecated CLI flags were used and ignored: ""
                f""{', '.join(deprecated_flags)}!""
            )
        warn(
            ""W0500: Please see the 5.0.0 Upgrade guide: ""
            ""https://pycqa.github.io/isort/docs/upgrade_guides/5.0.0/""
        )

    if wrong_sorted_files:
        sys.exit(1)"
"def parse_args(argv: Optional[Sequence[str]] = None) -> Dict[str, Any]:
    argv = sys.argv[1:] if argv is None else list(argv)
    remapped_deprecated_args = []
    for index, arg in enumerate(argv):
        if arg in DEPRECATED_SINGLE_DASH_ARGS:
            remapped_deprecated_args.append(arg)
            argv[index] = f""-{arg}""

    parser = _build_arg_parser()
    arguments = {key: value for key, value in vars(parser.parse_args(argv)).items() if value}
    if remapped_deprecated_args:
        arguments[""remapped_deprecated_args""] = remapped_deprecated_args
    if ""dont_order_by_type"" in arguments:
        arguments[""order_by_type""] = False
        del arguments[""dont_order_by_type""]
    multi_line_output = arguments.get(""multi_line_output"", None)
    if multi_line_output:
        if multi_line_output.isdigit():
            arguments[""multi_line_output""] = WrapModes(int(multi_line_output))
        else:
            arguments[""multi_line_output""] = WrapModes[multi_line_output]
    return arguments","def parse_args(argv: Optional[Sequence[str]] = None) -> Dict[str, Any]:
    argv = sys.argv[1:] if argv is None else list(argv)
    remapped_deprecated_args = []
    for index, arg in enumerate(argv):
        if arg in DEPRECATED_SINGLE_DASH_ARGS:
            remapped_deprecated_args.append(arg)
            argv[index] = f""-{arg}""

    parser = _build_arg_parser()
    arguments = {key: value for key, value in vars(parser.parse_args(argv)).items() if value}
    if remapped_deprecated_args:
        arguments[""remapped_deprecated_args""] = remapped_deprecated_args
    if ""dont_order_by_type"" in arguments:
        arguments[""order_by_type""] = False
    multi_line_output = arguments.get(""multi_line_output"", None)
    if multi_line_output:
        if multi_line_output.isdigit():
            arguments[""multi_line_output""] = WrapModes(int(multi_line_output))
        else:
            arguments[""multi_line_output""] = WrapModes[multi_line_output]
    return arguments"
"def sort_stream(
    input_stream: TextIO,
    output_stream: TextIO,
    extension: Optional[str] = None,
    config: Config = DEFAULT_CONFIG,
    file_path: Optional[Path] = None,
    disregard_skip: bool = False,
    show_diff: bool = False,
    **config_kwargs,
):
    """"""Sorts any imports within the provided code stream, outputs to the provided output stream.
    Directly returns nothing.

    - **input_stream**: The stream of code with imports that need to be sorted.
    - **output_stream**: The stream where sorted imports should be written to.
    - **extension**: The file extension that contains imports. Defaults to filename extension or py.
    - **config**: The config object to use when sorting imports.
    - **file_path**: The disk location where the code string was pulled from.
    - **disregard_skip**: set to `True` if you want to ignore a skip set in config for this file.
    - ****config_kwargs**: Any config modifications.
    """"""
    if show_diff:
        _output_stream = StringIO()
        _input_stream = StringIO(input_stream.read())
        changed = sort_stream(
            input_stream=_input_stream,
            output_stream=_output_stream,
            extension=extension,
            config=config,
            file_path=file_path,
            disregard_skip=disregard_skip,
            **config_kwargs,
        )
        _output_stream.seek(0)
        _input_stream.seek(0)
        show_unified_diff(
            file_input=_input_stream.read(),
            file_output=_output_stream.read(),
            file_path=file_path,
            output=output_stream,
        )
        return changed

    config = _config(path=file_path, config=config, **config_kwargs)
    content_source = str(file_path or ""Passed in content"")
    if not disregard_skip:
        if file_path and config.is_skipped(file_path):
            raise FileSkipSetting(content_source)

    _internal_output = output_stream

    if config.atomic:
        try:
            file_content = input_stream.read()
            compile(file_content, content_source, ""exec"", 0, 1)
            input_stream = StringIO(file_content)
        except SyntaxError:
            raise ExistingSyntaxErrors(content_source)

        if not output_stream.readable():
            _internal_output = StringIO()

    try:
        changed = _sort_imports(
            input_stream,
            _internal_output,
            extension=extension or (file_path and file_path.suffix.lstrip(""."")) or ""py"",
            config=config,
        )
    except FileSkipComment:
        raise FileSkipComment(content_source)

    if config.atomic:
        _internal_output.seek(0)
        try:
            compile(_internal_output.read(), content_source, ""exec"", 0, 1)
            _internal_output.seek(0)
            if _internal_output != output_stream:
                output_stream.write(_internal_output.read())
        except SyntaxError:  # pragma: no cover
            raise IntroducedSyntaxErrors(content_source)

    return changed","def sort_stream(
    input_stream: TextIO,
    output_stream: TextIO,
    extension: Optional[str] = None,
    config: Config = DEFAULT_CONFIG,
    file_path: Optional[Path] = None,
    disregard_skip: bool = False,
    **config_kwargs,
):
    """"""Sorts any imports within the provided code stream, outputs to the provided output stream.
    Directly returns nothing.

    - **input_stream**: The stream of code with imports that need to be sorted.
    - **output_stream**: The stream where sorted imports should be written to.
    - **extension**: The file extension that contains imports. Defaults to filename extension or py.
    - **config**: The config object to use when sorting imports.
    - **file_path**: The disk location where the code string was pulled from.
    - **disregard_skip**: set to `True` if you want to ignore a skip set in config for this file.
    - ****config_kwargs**: Any config modifications.
    """"""
    config = _config(path=file_path, config=config, **config_kwargs)
    content_source = str(file_path or ""Passed in content"")
    if not disregard_skip:
        if file_path and config.is_skipped(file_path):
            raise FileSkipSetting(content_source)

    _internal_output = output_stream

    if config.atomic:
        try:
            file_content = input_stream.read()
            compile(file_content, content_source, ""exec"", 0, 1)
            input_stream = StringIO(file_content)
        except SyntaxError:
            raise ExistingSyntaxErrors(content_source)

        if not output_stream.readable():
            _internal_output = StringIO()

    try:
        changed = _sort_imports(
            input_stream,
            _internal_output,
            extension=extension or (file_path and file_path.suffix.lstrip(""."")) or ""py"",
            config=config,
        )
    except FileSkipComment:
        raise FileSkipComment(content_source)

    if config.atomic:
        _internal_output.seek(0)
        try:
            compile(_internal_output.read(), content_source, ""exec"", 0, 1)
            _internal_output.seek(0)
            if _internal_output != output_stream:
                output_stream.write(_internal_output.read())
        except SyntaxError:  # pragma: no cover
            raise IntroducedSyntaxErrors(content_source)

    return changed"
"def show_unified_diff(
    *, file_input: str, file_output: str, file_path: Optional[Path], output=sys.stdout
):
    file_name = """" if file_path is None else str(file_path)
    file_mtime = str(
        datetime.now() if file_path is None else datetime.fromtimestamp(file_path.stat().st_mtime)
    )

    unified_diff_lines = unified_diff(
        file_input.splitlines(keepends=True),
        file_output.splitlines(keepends=True),
        fromfile=file_name + "":before"",
        tofile=file_name + "":after"",
        fromfiledate=file_mtime,
        tofiledate=str(datetime.now()),
    )
    for line in unified_diff_lines:
        output.write(line)","def show_unified_diff(*, file_input: str, file_output: str, file_path: Optional[Path]):
    file_name = """" if file_path is None else str(file_path)
    file_mtime = str(
        datetime.now() if file_path is None else datetime.fromtimestamp(file_path.stat().st_mtime)
    )

    unified_diff_lines = unified_diff(
        file_input.splitlines(keepends=True),
        file_output.splitlines(keepends=True),
        fromfile=file_name + "":before"",
        tofile=file_name + "":after"",
        fromfiledate=file_mtime,
        tofiledate=str(datetime.now()),
    )
    for line in unified_diff_lines:
        sys.stdout.write(line)"
"def _build_arg_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(
        description=""Sort Python import definitions alphabetically ""
        ""within logical sections. Run with no arguments to run ""
        ""interactively. Run with `-` as the first argument to read from ""
        ""stdin. Otherwise provide a list of files to sort.""
    )
    inline_args_group = parser.add_mutually_exclusive_group()
    parser.add_argument(
        ""--src"",
        ""--src-path"",
        dest=""src_paths"",
        action=""append"",
        help=""Add an explicitly defined source path ""
        ""(modules within src paths have their imports automatically catorgorized as first_party)."",
    )
    parser.add_argument(
        ""-a"",
        ""--add-import"",
        dest=""add_imports"",
        action=""append"",
        help=""Adds the specified import line to all files, ""
        ""automatically determining correct placement."",
    )
    parser.add_argument(
        ""--ac"",
        ""--atomic"",
        dest=""atomic"",
        action=""store_true"",
        help=""Ensures the output doesn't save if the resulting file contains syntax errors."",
    )
    parser.add_argument(
        ""--af"",
        ""--force-adds"",
        dest=""force_adds"",
        action=""store_true"",
        help=""Forces import adds even if the original file is empty."",
    )
    parser.add_argument(
        ""-b"",
        ""--builtin"",
        dest=""known_standard_library"",
        action=""append"",
        help=""Force isort to recognize a module as part of the python standard library."",
    )
    parser.add_argument(
        ""-c"",
        ""--check-only"",
        ""--check"",
        action=""store_true"",
        dest=""check"",
        help=""Checks the file for unsorted / unformatted imports and prints them to the ""
        ""command line without modifying the file."",
    )
    parser.add_argument(
        ""--ca"",
        ""--combine-as"",
        dest=""combine_as_imports"",
        action=""store_true"",
        help=""Combines as imports on the same line."",
    )
    parser.add_argument(
        ""--cs"",
        ""--combine-star"",
        dest=""combine_star"",
        action=""store_true"",
        help=""Ensures that if a star import is present, ""
        ""nothing else is imported from that namespace."",
    )
    parser.add_argument(
        ""-d"",
        ""--stdout"",
        help=""Force resulting output to stdout, instead of in-place."",
        dest=""write_to_stdout"",
        action=""store_true"",
    )
    parser.add_argument(
        ""--df"",
        ""--diff"",
        dest=""show_diff"",
        action=""store_true"",
        help=""Prints a diff of all the changes isort would make to a file, instead of ""
        ""changing it in place"",
    )
    parser.add_argument(
        ""--ds"",
        ""--no-sections"",
        help=""Put all imports into the same section bucket"",
        dest=""no_sections"",
        action=""store_true"",
    )
    parser.add_argument(
        ""-e"",
        ""--balanced"",
        dest=""balanced_wrapping"",
        action=""store_true"",
        help=""Balances wrapping to produce the most consistent line length possible"",
    )
    parser.add_argument(
        ""-f"",
        ""--future"",
        dest=""known_future_library"",
        action=""append"",
        help=""Force isort to recognize a module as part of the future compatibility libraries."",
    )
    parser.add_argument(
        ""--fas"",
        ""--force-alphabetical-sort"",
        action=""store_true"",
        dest=""force_alphabetical_sort"",
        help=""Force all imports to be sorted as a single section"",
    )
    parser.add_argument(
        ""--fass"",
        ""--force-alphabetical-sort-within-sections"",
        action=""store_true"",
        dest=""force_alphabetical_sort"",
        help=""Force all imports to be sorted alphabetically within a section"",
    )
    parser.add_argument(
        ""--ff"",
        ""--from-first"",
        dest=""from_first"",
        help=""Switches the typical ordering preference, ""
        ""showing from imports first then straight ones."",
    )
    parser.add_argument(
        ""--fgw"",
        ""--force-grid-wrap"",
        nargs=""?"",
        const=2,
        type=int,
        dest=""force_grid_wrap"",
        help=""Force number of from imports (defaults to 2) to be grid wrapped regardless of line ""
        ""length"",
    )
    parser.add_argument(
        ""--fss"",
        ""--force-sort-within-sections"",
        action=""store_true"",
        dest=""force_sort_within_sections"",
        help=""Force imports to be sorted by module, independent of import_type"",
    )
    parser.add_argument(
        ""-i"",
        ""--indent"",
        help='String to place for indents defaults to ""    "" (4 spaces).',
        dest=""indent"",
        type=str,
    )
    parser.add_argument(
        ""-j"", ""--jobs"", help=""Number of files to process in parallel."", dest=""jobs"", type=int
    )
    parser.add_argument(
        ""-k"",
        ""--keep-direct-and-as"",
        dest=""keep_direct_and_as_imports"",
        action=""store_true"",
        help=""Turns off default behavior that removes direct imports when as imports exist."",
    )
    parser.add_argument(""--lai"", ""--lines-after-imports"", dest=""lines_after_imports"", type=int)
    parser.add_argument(""--lbt"", ""--lines-between-types"", dest=""lines_between_types"", type=int)
    parser.add_argument(
        ""--le"",
        ""--line-ending"",
        dest=""line_ending"",
        help=""Forces line endings to the specified value. ""
        ""If not set, values will be guessed per-file."",
    )
    parser.add_argument(
        ""--ls"",
        ""--length-sort"",
        help=""Sort imports by their string length."",
        dest=""length_sort"",
        action=""store_true"",
    )
    parser.add_argument(
        ""-m"",
        ""--multi-line"",
        dest=""multi_line_output"",
        choices=list(WrapModes.__members__.keys())
        + [str(mode.value) for mode in WrapModes.__members__.values()],
        type=str,
        help=""Multi line output (0-grid, 1-vertical, 2-hanging, 3-vert-hanging, 4-vert-grid, ""
        ""5-vert-grid-grouped, 6-vert-grid-grouped-no-comma)."",
    )
    parser.add_argument(
        ""-n"",
        ""--ensure-newline-before-comments"",
        dest=""ensure_newline_before_comments"",
        action=""store_true"",
        help=""Inserts a blank line before a comment following an import."",
    )
    inline_args_group.add_argument(
        ""--nis"",
        ""--no-inline-sort"",
        dest=""no_inline_sort"",
        action=""store_true"",
        help=""Leaves `from` imports with multiple imports 'as-is' ""
        ""(e.g. `from foo import a, c ,b`)."",
    )
    parser.add_argument(
        ""--nlb"",
        ""--no-lines-before"",
        help=""Sections which should not be split with previous by empty lines"",
        dest=""no_lines_before"",
        action=""append"",
    )
    parser.add_argument(
        ""-o"",
        ""--thirdparty"",
        dest=""known_third_party"",
        action=""append"",
        help=""Force isort to recognize a module as being part of a third party library."",
    )
    parser.add_argument(
        ""--ot"",
        ""--order-by-type"",
        dest=""order_by_type"",
        action=""store_true"",
        help=""Order imports by type in addition to alphabetically"",
    )
    parser.add_argument(
        ""--dt"",
        ""--dont-order-by-type"",
        dest=""dont_order_by_type"",
        action=""store_true"",
        help=""Don't order imports by type in addition to alphabetically"",
    )
    parser.add_argument(
        ""-p"",
        ""--project"",
        dest=""known_first_party"",
        action=""append"",
        help=""Force isort to recognize a module as being part of the current python project."",
    )
    parser.add_argument(
        ""-q"",
        ""--quiet"",
        action=""store_true"",
        dest=""quiet"",
        help=""Shows extra quiet output, only errors are outputted."",
    )
    parser.add_argument(
        ""--rm"",
        ""--remove-import"",
        dest=""remove_imports"",
        action=""append"",
        help=""Removes the specified import from all files."",
    )
    parser.add_argument(
        ""--rr"",
        ""--reverse-relative"",
        dest=""reverse_relative"",
        action=""store_true"",
        help=""Reverse order of relative imports."",
    )
    parser.add_argument(
        ""-s"",
        ""--skip"",
        help=""Files that sort imports should skip over. If you want to skip multiple ""
        ""files you should specify twice: --skip file1 --skip file2."",
        dest=""skip"",
        action=""append"",
    )
    parser.add_argument(
        ""--sd"",
        ""--section-default"",
        dest=""default_section"",
        help=""Sets the default section for imports (by default FIRSTPARTY) options: ""
        + str(sections.DEFAULT),
    )
    parser.add_argument(
        ""--sg"",
        ""--skip-glob"",
        help=""Files that sort imports should skip over."",
        dest=""skip_glob"",
        action=""append"",
    )
    inline_args_group.add_argument(
        ""--sl"",
        ""--force-single-line-imports"",
        dest=""force_single_line"",
        action=""store_true"",
        help=""Forces all from imports to appear on their own line"",
    )
    parser.add_argument(
        ""--nsl"",
        ""--single-line-exclusions"",
        help=""One or more modules to exclude from the single line rule."",
        dest=""single_line_exclusions"",
        action=""append"",
    )
    parser.add_argument(
        ""--sp"",
        ""--settings-path"",
        ""--settings-file"",
        ""--settings"",
        dest=""settings_path"",
        help=""Explicitly set the settings path or file instead of auto determining ""
        ""based on file location."",
    )
    parser.add_argument(
        ""-t"",
        ""--top"",
        help=""Force specific imports to the top of their appropriate section."",
        dest=""force_to_top"",
        action=""append"",
    )
    parser.add_argument(
        ""--tc"",
        ""--trailing-comma"",
        dest=""include_trailing_comma"",
        action=""store_true"",
        help=""Includes a trailing comma on multi line imports that include parentheses."",
    )
    parser.add_argument(
        ""--up"",
        ""--use-parentheses"",
        dest=""use_parentheses"",
        action=""store_true"",
        help=""Use parenthesis for line continuation on length limit instead of slashes."",
    )
    parser.add_argument(
        ""-V"",
        ""--version"",
        action=""store_true"",
        dest=""show_version"",
        help=""Displays the currently installed version of isort."",
    )
    parser.add_argument(
        ""-v"",
        ""--verbose"",
        action=""store_true"",
        dest=""verbose"",
        help=""Shows verbose output, such as when files are skipped or when a check is successful."",
    )
    parser.add_argument(
        ""--virtual-env"",
        dest=""virtual_env"",
        help=""Virtual environment to use for determining whether a package is third-party"",
    )
    parser.add_argument(
        ""--conda-env"",
        dest=""conda_env"",
        help=""Conda environment to use for determining whether a package is third-party"",
    )
    parser.add_argument(
        ""--vn"",
        ""--version-number"",
        action=""version"",
        version=__version__,
        help=""Returns just the current version number without the logo"",
    )
    parser.add_argument(
        ""-l"",
        ""-w"",
        ""--line-length"",
        ""--line-width"",
        help=""The max length of an import line (used for wrapping long imports)."",
        dest=""line_length"",
        type=int,
    )
    parser.add_argument(
        ""--wl"",
        ""--wrap-length"",
        dest=""wrap_length"",
        type=int,
        help=""Specifies how long lines that are wrapped should be, if not set line_length is used.""
        ""\\nNOTE: wrap_length must be LOWER than or equal to line_length."",
    )
    parser.add_argument(
        ""--ws"",
        ""--ignore-whitespace"",
        action=""store_true"",
        dest=""ignore_whitespace"",
        help=""Tells isort to ignore whitespace differences when --check-only is being used."",
    )
    parser.add_argument(
        ""--case-sensitive"",
        dest=""case_sensitive"",
        action=""store_true"",
        help=""Tells isort to include casing when sorting module names"",
    )
    parser.add_argument(
        ""--filter-files"",
        dest=""filter_files"",
        action=""store_true"",
        help=""Tells isort to filter files even when they are explicitly passed in as ""
        ""part of the command"",
    )
    parser.add_argument(
        ""files"", nargs=""*"", help=""One or more Python source files that need their imports sorted.""
    )
    parser.add_argument(
        ""--py"",
        ""--python-version"",
        action=""store"",
        dest=""py_version"",
        choices=tuple(VALID_PY_TARGETS) + (""auto"",),
        help=""Tells isort to set the known standard library based on the the specified Python ""
        ""version. Default is to assume any Python 3 version could be the target, and use a union ""
        ""off all stdlib modules across versions. If auto is specified, the version of the ""
        ""interpreter used to run isort ""
        f""(currently: {sys.version_info.major}{sys.version_info.minor}) will be used."",
    )
    parser.add_argument(
        ""--profile"",
        dest=""profile"",
        choices=list(profiles.keys()),
        type=str,
        help=""Base profile type to use for configuration."",
    )
    parser.add_argument(
        ""--interactive"",
        dest=""ask_to_apply"",
        action=""store_true"",
        help=""Tells isort to apply changes interactively."",
    )
    parser.add_argument(
        ""--old-finders"",
        ""--magic"",
        dest=""old_finders"",
        action=""store_true"",
        help=""Use the old deprecated finder logic that relies on environment introspection magic."",
    )
    parser.add_argument(
        ""--show-config"",
        dest=""show_config"",
        action=""store_true"",
        help=""See isort's determined config, as well as sources of config options."",
    )
    return parser","def _build_arg_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(
        description=""Sort Python import definitions alphabetically ""
        ""within logical sections. Run with no arguments to run ""
        ""interactively. Run with `-` as the first argument to read from ""
        ""stdin. Otherwise provide a list of files to sort.""
    )
    inline_args_group = parser.add_mutually_exclusive_group()
    parser.add_argument(
        ""--src"",
        ""--src-path"",
        dest=""source_paths"",
        action=""append"",
        help=""Add an explicitly defined source path ""
        ""(modules within src paths have their imports automatically catorgorized as first_party)."",
    )
    parser.add_argument(
        ""-a"",
        ""--add-import"",
        dest=""add_imports"",
        action=""append"",
        help=""Adds the specified import line to all files, ""
        ""automatically determining correct placement."",
    )
    parser.add_argument(
        ""--ac"",
        ""--atomic"",
        dest=""atomic"",
        action=""store_true"",
        help=""Ensures the output doesn't save if the resulting file contains syntax errors."",
    )
    parser.add_argument(
        ""--af"",
        ""--force-adds"",
        dest=""force_adds"",
        action=""store_true"",
        help=""Forces import adds even if the original file is empty."",
    )
    parser.add_argument(
        ""-b"",
        ""--builtin"",
        dest=""known_standard_library"",
        action=""append"",
        help=""Force isort to recognize a module as part of the python standard library."",
    )
    parser.add_argument(
        ""-c"",
        ""--check-only"",
        ""--check"",
        action=""store_true"",
        dest=""check"",
        help=""Checks the file for unsorted / unformatted imports and prints them to the ""
        ""command line without modifying the file."",
    )
    parser.add_argument(
        ""--ca"",
        ""--combine-as"",
        dest=""combine_as_imports"",
        action=""store_true"",
        help=""Combines as imports on the same line."",
    )
    parser.add_argument(
        ""--cs"",
        ""--combine-star"",
        dest=""combine_star"",
        action=""store_true"",
        help=""Ensures that if a star import is present, ""
        ""nothing else is imported from that namespace."",
    )
    parser.add_argument(
        ""-d"",
        ""--stdout"",
        help=""Force resulting output to stdout, instead of in-place."",
        dest=""write_to_stdout"",
        action=""store_true"",
    )
    parser.add_argument(
        ""--df"",
        ""--diff"",
        dest=""show_diff"",
        action=""store_true"",
        help=""Prints a diff of all the changes isort would make to a file, instead of ""
        ""changing it in place"",
    )
    parser.add_argument(
        ""--ds"",
        ""--no-sections"",
        help=""Put all imports into the same section bucket"",
        dest=""no_sections"",
        action=""store_true"",
    )
    parser.add_argument(
        ""-e"",
        ""--balanced"",
        dest=""balanced_wrapping"",
        action=""store_true"",
        help=""Balances wrapping to produce the most consistent line length possible"",
    )
    parser.add_argument(
        ""-f"",
        ""--future"",
        dest=""known_future_library"",
        action=""append"",
        help=""Force isort to recognize a module as part of the future compatibility libraries."",
    )
    parser.add_argument(
        ""--fas"",
        ""--force-alphabetical-sort"",
        action=""store_true"",
        dest=""force_alphabetical_sort"",
        help=""Force all imports to be sorted as a single section"",
    )
    parser.add_argument(
        ""--fass"",
        ""--force-alphabetical-sort-within-sections"",
        action=""store_true"",
        dest=""force_alphabetical_sort"",
        help=""Force all imports to be sorted alphabetically within a section"",
    )
    parser.add_argument(
        ""--ff"",
        ""--from-first"",
        dest=""from_first"",
        help=""Switches the typical ordering preference, ""
        ""showing from imports first then straight ones."",
    )
    parser.add_argument(
        ""--fgw"",
        ""--force-grid-wrap"",
        nargs=""?"",
        const=2,
        type=int,
        dest=""force_grid_wrap"",
        help=""Force number of from imports (defaults to 2) to be grid wrapped regardless of line ""
        ""length"",
    )
    parser.add_argument(
        ""--fss"",
        ""--force-sort-within-sections"",
        action=""store_true"",
        dest=""force_sort_within_sections"",
        help=""Force imports to be sorted by module, independent of import_type"",
    )
    parser.add_argument(
        ""-i"",
        ""--indent"",
        help='String to place for indents defaults to ""    "" (4 spaces).',
        dest=""indent"",
        type=str,
    )
    parser.add_argument(
        ""-j"", ""--jobs"", help=""Number of files to process in parallel."", dest=""jobs"", type=int
    )
    parser.add_argument(
        ""-k"",
        ""--keep-direct-and-as"",
        dest=""keep_direct_and_as_imports"",
        action=""store_true"",
        help=""Turns off default behavior that removes direct imports when as imports exist."",
    )
    parser.add_argument(""--lai"", ""--lines-after-imports"", dest=""lines_after_imports"", type=int)
    parser.add_argument(""--lbt"", ""--lines-between-types"", dest=""lines_between_types"", type=int)
    parser.add_argument(
        ""--le"",
        ""--line-ending"",
        dest=""line_ending"",
        help=""Forces line endings to the specified value. ""
        ""If not set, values will be guessed per-file."",
    )
    parser.add_argument(
        ""--ls"",
        ""--length-sort"",
        help=""Sort imports by their string length."",
        dest=""length_sort"",
        action=""store_true"",
    )
    parser.add_argument(
        ""-m"",
        ""--multi-line"",
        dest=""multi_line_output"",
        choices=list(WrapModes.__members__.keys())
        + [str(mode.value) for mode in WrapModes.__members__.values()],
        type=str,
        help=""Multi line output (0-grid, 1-vertical, 2-hanging, 3-vert-hanging, 4-vert-grid, ""
        ""5-vert-grid-grouped, 6-vert-grid-grouped-no-comma)."",
    )
    parser.add_argument(
        ""-n"",
        ""--ensure-newline-before-comments"",
        dest=""ensure_newline_before_comments"",
        action=""store_true"",
        help=""Inserts a blank line before a comment following an import."",
    )
    inline_args_group.add_argument(
        ""--nis"",
        ""--no-inline-sort"",
        dest=""no_inline_sort"",
        action=""store_true"",
        help=""Leaves `from` imports with multiple imports 'as-is' ""
        ""(e.g. `from foo import a, c ,b`)."",
    )
    parser.add_argument(
        ""--nlb"",
        ""--no-lines-before"",
        help=""Sections which should not be split with previous by empty lines"",
        dest=""no_lines_before"",
        action=""append"",
    )
    parser.add_argument(
        ""-o"",
        ""--thirdparty"",
        dest=""known_third_party"",
        action=""append"",
        help=""Force isort to recognize a module as being part of a third party library."",
    )
    parser.add_argument(
        ""--ot"",
        ""--order-by-type"",
        dest=""order_by_type"",
        action=""store_true"",
        help=""Order imports by type in addition to alphabetically"",
    )
    parser.add_argument(
        ""--dt"",
        ""--dont-order-by-type"",
        dest=""dont_order_by_type"",
        action=""store_true"",
        help=""Don't order imports by type in addition to alphabetically"",
    )
    parser.add_argument(
        ""-p"",
        ""--project"",
        dest=""known_first_party"",
        action=""append"",
        help=""Force isort to recognize a module as being part of the current python project."",
    )
    parser.add_argument(
        ""-q"",
        ""--quiet"",
        action=""store_true"",
        dest=""quiet"",
        help=""Shows extra quiet output, only errors are outputted."",
    )
    parser.add_argument(
        ""--rm"",
        ""--remove-import"",
        dest=""remove_imports"",
        action=""append"",
        help=""Removes the specified import from all files."",
    )
    parser.add_argument(
        ""--rr"",
        ""--reverse-relative"",
        dest=""reverse_relative"",
        action=""store_true"",
        help=""Reverse order of relative imports."",
    )
    parser.add_argument(
        ""-s"",
        ""--skip"",
        help=""Files that sort imports should skip over. If you want to skip multiple ""
        ""files you should specify twice: --skip file1 --skip file2."",
        dest=""skip"",
        action=""append"",
    )
    parser.add_argument(
        ""--sd"",
        ""--section-default"",
        dest=""default_section"",
        help=""Sets the default section for imports (by default FIRSTPARTY) options: ""
        + str(sections.DEFAULT),
    )
    parser.add_argument(
        ""--sg"",
        ""--skip-glob"",
        help=""Files that sort imports should skip over."",
        dest=""skip_glob"",
        action=""append"",
    )
    inline_args_group.add_argument(
        ""--sl"",
        ""--force-single-line-imports"",
        dest=""force_single_line"",
        action=""store_true"",
        help=""Forces all from imports to appear on their own line"",
    )
    parser.add_argument(
        ""--nsl"",
        ""--single-line-exclusions"",
        help=""One or more modules to exclude from the single line rule."",
        dest=""single_line_exclusions"",
        action=""append"",
    )
    parser.add_argument(
        ""--sp"",
        ""--settings-path"",
        ""--settings-file"",
        ""--settings"",
        dest=""settings_path"",
        help=""Explicitly set the settings path or file instead of auto determining ""
        ""based on file location."",
    )
    parser.add_argument(
        ""-t"",
        ""--top"",
        help=""Force specific imports to the top of their appropriate section."",
        dest=""force_to_top"",
        action=""append"",
    )
    parser.add_argument(
        ""--tc"",
        ""--trailing-comma"",
        dest=""include_trailing_comma"",
        action=""store_true"",
        help=""Includes a trailing comma on multi line imports that include parentheses."",
    )
    parser.add_argument(
        ""--up"",
        ""--use-parentheses"",
        dest=""use_parentheses"",
        action=""store_true"",
        help=""Use parenthesis for line continuation on length limit instead of slashes."",
    )
    parser.add_argument(
        ""-V"",
        ""--version"",
        action=""store_true"",
        dest=""show_version"",
        help=""Displays the currently installed version of isort."",
    )
    parser.add_argument(
        ""-v"",
        ""--verbose"",
        action=""store_true"",
        dest=""verbose"",
        help=""Shows verbose output, such as when files are skipped or when a check is successful."",
    )
    parser.add_argument(
        ""--virtual-env"",
        dest=""virtual_env"",
        help=""Virtual environment to use for determining whether a package is third-party"",
    )
    parser.add_argument(
        ""--conda-env"",
        dest=""conda_env"",
        help=""Conda environment to use for determining whether a package is third-party"",
    )
    parser.add_argument(
        ""--vn"",
        ""--version-number"",
        action=""version"",
        version=__version__,
        help=""Returns just the current version number without the logo"",
    )
    parser.add_argument(
        ""-l"",
        ""-w"",
        ""--line-length"",
        ""--line-width"",
        help=""The max length of an import line (used for wrapping long imports)."",
        dest=""line_length"",
        type=int,
    )
    parser.add_argument(
        ""--wl"",
        ""--wrap-length"",
        dest=""wrap_length"",
        type=int,
        help=""Specifies how long lines that are wrapped should be, if not set line_length is used.""
        ""\\nNOTE: wrap_length must be LOWER than or equal to line_length."",
    )
    parser.add_argument(
        ""--ws"",
        ""--ignore-whitespace"",
        action=""store_true"",
        dest=""ignore_whitespace"",
        help=""Tells isort to ignore whitespace differences when --check-only is being used."",
    )
    parser.add_argument(
        ""--case-sensitive"",
        dest=""case_sensitive"",
        action=""store_true"",
        help=""Tells isort to include casing when sorting module names"",
    )
    parser.add_argument(
        ""--filter-files"",
        dest=""filter_files"",
        action=""store_true"",
        help=""Tells isort to filter files even when they are explicitly passed in as ""
        ""part of the command"",
    )
    parser.add_argument(
        ""files"", nargs=""*"", help=""One or more Python source files that need their imports sorted.""
    )
    parser.add_argument(
        ""--py"",
        ""--python-version"",
        action=""store"",
        dest=""py_version"",
        choices=tuple(VALID_PY_TARGETS) + (""auto"",),
        help=""Tells isort to set the known standard library based on the the specified Python ""
        ""version. Default is to assume any Python 3 version could be the target, and use a union ""
        ""off all stdlib modules across versions. If auto is specified, the version of the ""
        ""interpreter used to run isort ""
        f""(currently: {sys.version_info.major}{sys.version_info.minor}) will be used."",
    )
    parser.add_argument(
        ""--profile"",
        dest=""profile"",
        choices=list(profiles.keys()),
        type=str,
        help=""Base profile type to use for configuration."",
    )
    parser.add_argument(
        ""--interactive"",
        dest=""ask_to_apply"",
        action=""store_true"",
        help=""Tells isort to apply changes interactively."",
    )
    parser.add_argument(
        ""--old-finders"",
        ""--magic"",
        dest=""old_finders"",
        action=""store_true"",
        help=""Use the old deprecated finder logic that relies on environment introspection magic."",
    )
    parser.add_argument(
        ""--show-config"",
        dest=""show_config"",
        action=""store_true"",
        help=""See isort's determined config, as well as sources of config options."",
    )
    return parser"
"def main(argv: Optional[Sequence[str]] = None, stdin: Optional[TextIOWrapper] = None) -> None:
    arguments = parse_args(argv)
    if arguments.get(""show_version""):
        print(ASCII_ART)
        return

    show_config: bool = arguments.pop(""show_config"", False)

    if ""settings_path"" in arguments:
        if os.path.isfile(arguments[""settings_path""]):
            arguments[""settings_file""] = os.path.abspath(arguments[""settings_path""])
            arguments[""settings_path""] = os.path.dirname(arguments[""settings_file""])
        elif not os.path.isdir(arguments[""settings_path""]):
            warn(f""settings_path dir does not exist: {arguments['settings_path']}"")
        else:
            arguments[""settings_path""] = os.path.abspath(arguments[""settings_path""])

    if ""virtual_env"" in arguments:
        venv = arguments[""virtual_env""]
        arguments[""virtual_env""] = os.path.abspath(venv)
        if not os.path.isdir(arguments[""virtual_env""]):
            warn(f""virtual_env dir does not exist: {arguments['virtual_env']}"")

    file_names = arguments.pop(""files"", [])
    if not file_names and not show_config:
        print(QUICK_GUIDE)
        return
    elif file_names == [""-""] and not show_config:
        api.sort_stream(
            input_stream=sys.stdin if stdin is None else stdin,
            output_stream=sys.stdout,
            **arguments,
        )
        return

    if ""settings_path"" not in arguments:
        arguments[""settings_path""] = (
            os.path.abspath(file_names[0] if file_names else ""."") or os.getcwd()
        )
        if not os.path.isdir(arguments[""settings_path""]):
            arguments[""settings_path""] = os.path.dirname(arguments[""settings_path""])

    config_dict = arguments.copy()
    ask_to_apply = config_dict.pop(""ask_to_apply"", False)
    jobs = config_dict.pop(""jobs"", ())
    filter_files = config_dict.pop(""filter_files"", False)
    check = config_dict.pop(""check"", False)
    show_diff = config_dict.pop(""show_diff"", False)
    write_to_stdout = config_dict.pop(""write_to_stdout"", False)

    src_paths = set(config_dict.setdefault(""src_paths"", ()))
    for file_name in file_names:
        if os.path.isdir(file_name):
            src_paths.add(Path(file_name).resolve())
        else:
            src_paths.add(Path(file_name).parent.resolve())

    config = Config(**config_dict)
    if show_config:
        print(json.dumps(config.__dict__, indent=4, separators=("","", "": ""), default=_preconvert))
        return

    wrong_sorted_files = False
    skipped: List[str] = []

    if filter_files:
        filtered_files = []
        for file_name in file_names:
            if config.is_skipped(Path(file_name)):
                skipped.append(file_name)
            else:
                filtered_files.append(file_name)
        file_names = filtered_files

    file_names = iter_source_code(file_names, config, skipped)
    num_skipped = 0
    if config.verbose:
        print(ASCII_ART)

    if jobs:
        import multiprocessing

        executor = multiprocessing.Pool(jobs)
        attempt_iterator = executor.imap(
            functools.partial(
                sort_imports,
                config=config,
                check=check,
                ask_to_apply=ask_to_apply,
                write_to_stdout=write_to_stdout,
            ),
            file_names,
        )
    else:
        # https://github.com/python/typeshed/pull/2814
        attempt_iterator = (
            sort_imports(  # type: ignore
                file_name,
                config=config,
                check=check,
                ask_to_apply=ask_to_apply,
                show_diff=show_diff,
                write_to_stdout=write_to_stdout,
            )
            for file_name in file_names
        )

    for sort_attempt in attempt_iterator:
        if not sort_attempt:
            continue  # pragma: no cover - shouldn't happen, satisfies type constraint
        incorrectly_sorted = sort_attempt.incorrectly_sorted
        if arguments.get(""check"", False) and incorrectly_sorted:
            wrong_sorted_files = True
        if sort_attempt.skipped:
            num_skipped += 1  # pragma: no cover - shouldn't happen, due to skip in iter_source_code

    if wrong_sorted_files:
        sys.exit(1)

    num_skipped += len(skipped)
    if num_skipped and not arguments.get(""quiet"", False):
        if config.verbose:
            for was_skipped in skipped:
                warn(
                    f""{was_skipped} was skipped as it's listed in 'skip' setting""
                    "" or matches a glob in 'skip_glob' setting""
                )
        print(f""Skipped {num_skipped} files"")","def main(argv: Optional[Sequence[str]] = None, stdin: Optional[TextIOWrapper] = None) -> None:
    arguments = parse_args(argv)
    if arguments.get(""show_version""):
        print(ASCII_ART)
        return

    show_config: bool = arguments.pop(""show_config"", False)

    if ""settings_path"" in arguments:
        if os.path.isfile(arguments[""settings_path""]):
            arguments[""settings_file""] = os.path.abspath(arguments[""settings_path""])
            arguments[""settings_path""] = os.path.dirname(arguments[""settings_file""])
        elif not os.path.isdir(arguments[""settings_path""]):
            warn(f""settings_path dir does not exist: {arguments['settings_path']}"")
        else:
            arguments[""settings_path""] = os.path.abspath(arguments[""settings_path""])

    if ""virtual_env"" in arguments:
        venv = arguments[""virtual_env""]
        arguments[""virtual_env""] = os.path.abspath(venv)
        if not os.path.isdir(arguments[""virtual_env""]):
            warn(f""virtual_env dir does not exist: {arguments['virtual_env']}"")

    file_names = arguments.pop(""files"", [])
    if not file_names and not show_config:
        print(QUICK_GUIDE)
        return
    elif file_names == [""-""] and not show_config:
        api.sort_stream(
            input_stream=sys.stdin if stdin is None else stdin,
            output_stream=sys.stdout,
            **arguments,
        )
        return

    if ""settings_path"" not in arguments:
        arguments[""settings_path""] = (
            os.path.abspath(file_names[0] if file_names else ""."") or os.getcwd()
        )
        if not os.path.isdir(arguments[""settings_path""]):
            arguments[""settings_path""] = os.path.dirname(arguments[""settings_path""])

    config_dict = arguments.copy()
    ask_to_apply = config_dict.pop(""ask_to_apply"", False)
    jobs = config_dict.pop(""jobs"", ())
    filter_files = config_dict.pop(""filter_files"", False)
    check = config_dict.pop(""check"", False)
    show_diff = config_dict.pop(""show_diff"", False)
    write_to_stdout = config_dict.pop(""write_to_stdout"", False)

    src_paths = config_dict.setdefault(""src_paths"", set())
    for file_name in file_names:
        if os.path.isdir(file_name):
            src_paths.add(Path(file_name).resolve())
        else:
            src_paths.add(Path(file_name).parent.resolve())

    config = Config(**config_dict)
    if show_config:
        print(json.dumps(config.__dict__, indent=4, separators=("","", "": ""), default=_preconvert))
        return

    wrong_sorted_files = False
    skipped: List[str] = []

    if filter_files:
        filtered_files = []
        for file_name in file_names:
            if config.is_skipped(Path(file_name)):
                skipped.append(file_name)
            else:
                filtered_files.append(file_name)
        file_names = filtered_files

    file_names = iter_source_code(file_names, config, skipped)
    num_skipped = 0
    if config.verbose:
        print(ASCII_ART)

    if jobs:
        import multiprocessing

        executor = multiprocessing.Pool(jobs)
        attempt_iterator = executor.imap(
            functools.partial(
                sort_imports,
                config=config,
                check=check,
                ask_to_apply=ask_to_apply,
                write_to_stdout=write_to_stdout,
            ),
            file_names,
        )
    else:
        # https://github.com/python/typeshed/pull/2814
        attempt_iterator = (
            sort_imports(  # type: ignore
                file_name,
                config=config,
                check=check,
                ask_to_apply=ask_to_apply,
                show_diff=show_diff,
                write_to_stdout=write_to_stdout,
            )
            for file_name in file_names
        )

    for sort_attempt in attempt_iterator:
        if not sort_attempt:
            continue  # pragma: no cover - shouldn't happen, satisfies type constraint
        incorrectly_sorted = sort_attempt.incorrectly_sorted
        if arguments.get(""check"", False) and incorrectly_sorted:
            wrong_sorted_files = True
        if sort_attempt.skipped:
            num_skipped += 1  # pragma: no cover - shouldn't happen, due to skip in iter_source_code

    if wrong_sorted_files:
        sys.exit(1)

    num_skipped += len(skipped)
    if num_skipped and not arguments.get(""quiet"", False):
        if config.verbose:
            for was_skipped in skipped:
                warn(
                    f""{was_skipped} was skipped as it's listed in 'skip' setting""
                    "" or matches a glob in 'skip_glob' setting""
                )
        print(f""Skipped {num_skipped} files"")"
"    def find(self, module_name):
        for finder in self.finders:
            try:
                section = finder.find(module_name)
            except Exception as exception:
                # isort has to be able to keep trying to identify the correct import section even if one approach fails
                if config.get('verbose', False):
                    print('{} encountered an error ({}) while trying to identify the {} module'.format(finder.__name__,
                                                                                                       str(exception),
                                                                                                       module_name))
            if section is not None:
                return section","    def find(self, module_name):
        for finder in self.finders:
            section = finder.find(module_name)
            if section is not None:
                return section"
"    def create_domain_name(self,
                           protocol,              # type: str
                           domain_name,           # type: str
                           endpoint_type,         # type: str
                           certificate_arn,       # type: str
                           security_policy=None,  # type: Optional[str]
                           tags=None,             # type: StrMap
                           ):
        # type: (...) -> DomainNameResponse
        if protocol == 'HTTP':
            kwargs = {
                'domainName': domain_name,
                'endpointConfiguration': {
                    'types': [endpoint_type],
                },
            }
            if security_policy is not None:
                kwargs['securityPolicy'] = security_policy
            if endpoint_type == 'EDGE':
                kwargs['certificateArn'] = certificate_arn
            else:
                kwargs['regionalCertificateArn'] = certificate_arn
            if tags is not None:
                kwargs['tags'] = tags
            created_domain_name = self._create_domain_name(kwargs)
        elif protocol == 'WEBSOCKET':
            kwargs = self.get_custom_domain_params_v2(
                domain_name=domain_name,
                endpoint_type=endpoint_type,
                security_policy=security_policy,
                certificate_arn=certificate_arn,
                tags=tags
            )
            created_domain_name = self._create_domain_name_v2(kwargs)
        else:
            raise ValueError(""Unsupported protocol value."")
        return created_domain_name","    def create_domain_name(self,
                           protocol,              # type: str
                           domain_name,           # type: str
                           endpoint_type,         # type: str
                           certificate_arn,       # type: str
                           security_policy=None,  # type: Optional[str]
                           tags=None,             # type: StrMap
                           ):
        # type: (...) -> Dict[str, Any]
        if protocol == 'HTTP':
            kwargs = {
                'domainName': domain_name,
                'endpointConfiguration': {
                    'types': [endpoint_type],
                },
            }
            if security_policy is not None:
                kwargs['securityPolicy'] = security_policy
            if endpoint_type == 'EDGE':
                kwargs['certificateArn'] = certificate_arn
            else:
                kwargs['regionalCertificateArn'] = certificate_arn
            if tags is not None:
                kwargs['tags'] = tags
            created_domain_name = self._create_domain_name(kwargs)
        elif protocol == 'WEBSOCKET':
            kwargs = self.get_custom_domain_params_v2(
                domain_name=domain_name,
                endpoint_type=endpoint_type,
                security_policy=security_policy,
                certificate_arn=certificate_arn,
                tags=tags
            )
            created_domain_name = self._create_domain_name_v2(kwargs)
        else:
            raise ValueError(""Unsupported protocol value."")
        return created_domain_name"
"    def _create_domain_name(self, api_args):
        # type: (Dict[str, Any]) -> DomainNameResponse
        client = self._client('apigateway')
        exceptions = (
            client.exceptions.TooManyRequestsException,
        )
        result = self._call_client_method_with_retries(
            client.create_domain_name,
            api_args, max_attempts=6,
            should_retry=lambda x: True,
            retryable_exceptions=exceptions
        )
        if result.get('regionalHostedZoneId'):
            hosted_zone_id = result['regionalHostedZoneId']
        else:
            hosted_zone_id = result['distributionHostedZoneId']

        if result.get('regionalCertificateArn'):
            certificate_arn = result['regionalCertificateArn']
        else:
            certificate_arn = result['certificateArn']

        if result.get('regionalDomainName') is not None:
            alias_domain_name = result['regionalDomainName']
        else:
            alias_domain_name = result['distributionDomainName']
        domain_name = {
            'domain_name': result['domainName'],
            'security_policy': result['securityPolicy'],
            'hosted_zone_id': hosted_zone_id,
            'certificate_arn': certificate_arn,
            'alias_domain_name': alias_domain_name,
        }  # type: DomainNameResponse
        return domain_name","    def _create_domain_name(self, api_args):
        # type: (Dict[str, Any]) -> Dict[str, Any]
        client = self._client('apigateway')
        exceptions = (
            client.exceptions.TooManyRequestsException,
        )
        result = self._call_client_method_with_retries(
            client.create_domain_name,
            api_args, max_attempts=6,
            should_retry=lambda x: True,
            retryable_exceptions=exceptions
        )
        domain_name = {
            'domain_name': result['domainName'],
            'endpoint_configuration': result['endpointConfiguration'],
            'security_policy': result['securityPolicy'],
        }
        if result.get('regionalHostedZoneId'):
            domain_name['hosted_zone_id'] = result['regionalHostedZoneId']
        else:
            domain_name['hosted_zone_id'] = result['distributionHostedZoneId']

        if result.get('regionalCertificateArn'):
            domain_name['certificate_arn'] = result['regionalCertificateArn']
        else:
            domain_name['certificate_arn'] = result['certificateArn']

        if result.get('regionalDomainName') is not None:
            domain_name['alias_domain_name'] = result['regionalDomainName']
        else:
            domain_name['alias_domain_name'] = result['distributionDomainName']
        return domain_name"
"    def _create_domain_name_v2(self, api_args):
        # type: (Dict[str, Any]) -> DomainNameResponse
        client = self._client('apigatewayv2')
        exceptions = (
            client.exceptions.TooManyRequestsException,
        )
        result = self._call_client_method_with_retries(
            client.create_domain_name,
            api_args, max_attempts=6,
            should_retry=lambda x: True,
            retryable_exceptions=exceptions
        )
        result_data = result['DomainNameConfigurations'][0]
        domain_name = {
            'domain_name': result['DomainName'],
            'alias_domain_name': result_data['ApiGatewayDomainName'],
            'security_policy': result_data['SecurityPolicy'],
            'hosted_zone_id': result_data['HostedZoneId'],
            'certificate_arn': result_data['CertificateArn']
        }  # type: DomainNameResponse
        return domain_name","    def _create_domain_name_v2(self, api_args):
        # type: (Dict[str, Any]) -> Dict[str, Any]
        client = self._client('apigatewayv2')
        exceptions = (
            client.exceptions.TooManyRequestsException,
        )
        result = self._call_client_method_with_retries(
            client.create_domain_name,
            api_args, max_attempts=6,
            should_retry=lambda x: True,
            retryable_exceptions=exceptions
        )
        result_data = result['DomainNameConfigurations'][0]
        domain_name = {
            'domain_name': result_data['ApiGatewayDomainName'],
            'endpoint_type': result_data['EndpointType'],
            'security_policy': result_data['SecurityPolicy'],
            'hosted_zone_id': result_data['HostedZoneId'],
            'certificate_arn': result_data['CertificateArn']
        }
        return domain_name"
"    def update_domain_name(self,
                           protocol,                  # type: str
                           domain_name,               # type: str
                           endpoint_type,             # type: str
                           certificate_arn,           # type: str
                           security_policy=None,      # type: Optional[str]
                           tags=None,                 # type: StrMap
                           ):
        # type: (...) -> DomainNameResponse
        if protocol == 'HTTP':
            patch_operations = self.get_custom_domain_patch_operations(
                certificate_arn,
                endpoint_type,
                security_policy,
            )
            updated_domain_name = self._update_domain_name(
                domain_name, patch_operations
            )
        elif protocol == 'WEBSOCKET':
            kwargs = self.get_custom_domain_params_v2(
                domain_name=domain_name,
                endpoint_type=endpoint_type,
                security_policy=security_policy,
                certificate_arn=certificate_arn,
            )
            updated_domain_name = self._update_domain_name_v2(kwargs)
        else:
            raise ValueError('Unsupported protocol value.')
        resource_arn = 'arn:aws:apigateway:{region_name}:' \\
                       ':/domainnames/{domain_name}'\\
            .format(
                region_name=self.region_name,
                domain_name=domain_name
            )
        self._update_resource_tags(resource_arn, tags)
        return updated_domain_name","    def update_domain_name(self,
                           protocol,                  # type: str
                           domain_name,               # type: str
                           endpoint_type,             # type: str
                           certificate_arn,           # type: str
                           security_policy=None,      # type: Optional[str]
                           tags=None,                 # type: StrMap
                           ):
        # type: (...) -> Dict[str, Any]
        if protocol == 'HTTP':
            patch_operations = self.get_custom_domain_patch_operations(
                certificate_arn,
                endpoint_type,
                security_policy,
            )
            updated_domain_name = self._update_domain_name(
                domain_name, patch_operations
            )
        elif protocol == 'WEBSOCKET':
            kwargs = self.get_custom_domain_params_v2(
                domain_name=domain_name,
                endpoint_type=endpoint_type,
                security_policy=security_policy,
                certificate_arn=certificate_arn,
            )
            updated_domain_name = self._update_domain_name_v2(kwargs)
        else:
            raise ValueError('Unsupported protocol value.')
        resource_arn = 'arn:aws:apigateway:{region_name}:' \\
                       ':/domainnames/{domain_name}'\\
            .format(
                region_name=self.region_name,
                domain_name=domain_name
            )
        self._update_resource_tags(resource_arn, tags)
        return updated_domain_name"
"    def _update_domain_name(self, custom_domain_name, patch_operations):
        # type: (str, List[Dict[str, str]]) -> DomainNameResponse
        client = self._client('apigateway')
        exceptions = (
            client.exceptions.TooManyRequestsException,
        )
        result = {}
        for patch_operation in patch_operations:
            api_args = {
                'domainName': custom_domain_name,
                'patchOperations': [patch_operation]
            }
            response = self._call_client_method_with_retries(
                client.update_domain_name,
                api_args, max_attempts=6,
                should_retry=lambda x: True,
                retryable_exceptions=exceptions
            )
            result.update(response)

        if result.get('regionalCertificateArn'):
            certificate_arn = result['regionalCertificateArn']
        else:
            certificate_arn = result['certificateArn']

        if result.get('regionalHostedZoneId'):
            hosted_zone_id = result['regionalHostedZoneId']
        else:
            hosted_zone_id = result['distributionHostedZoneId']

        if result.get('regionalDomainName') is not None:
            alias_domain_name = result['regionalDomainName']
        else:
            alias_domain_name = result['distributionDomainName']
        domain_name = {
            'domain_name': result['domainName'],
            'security_policy': result['securityPolicy'],
            'certificate_arn': certificate_arn,
            'hosted_zone_id': hosted_zone_id,
            'alias_domain_name': alias_domain_name
        }  # type: DomainNameResponse
        return domain_name","    def _update_domain_name(self, custom_domain_name, patch_operations):
        # type: (str, List[Dict[str, str]]) -> Dict[str, Any]
        client = self._client('apigateway')
        exceptions = (
            client.exceptions.TooManyRequestsException,
        )
        result = {}
        for patch_operation in patch_operations:
            api_args = {
                'domainName': custom_domain_name,
                'patchOperations': [patch_operation]
            }
            response = self._call_client_method_with_retries(
                client.update_domain_name,
                api_args, max_attempts=6,
                should_retry=lambda x: True,
                retryable_exceptions=exceptions
            )
            result.update(response)

        domain_name = {
            'domain_name': result['domainName'],
            'endpoint_configuration': result['endpointConfiguration'],
            'security_policy': result['securityPolicy'],
        }
        if result.get('regionalCertificateArn'):
            domain_name['certificate_arn'] = result['regionalCertificateArn']
        else:
            domain_name['certificate_arn'] = result['certificateArn']

        if result.get('regionalHostedZoneId'):
            domain_name['hosted_zone_id'] = result['regionalHostedZoneId']
        else:
            domain_name['hosted_zone_id'] = result['distributionHostedZoneId']

        if result.get('regionalDomainName') is not None:
            domain_name['alias_domain_name'] = result['regionalDomainName']
        else:
            domain_name['alias_domain_name'] = result['distributionDomainName']
        return domain_name"
"    def _update_domain_name_v2(self, api_args):
        # type: (Dict[str, Any]) -> DomainNameResponse
        client = self._client('apigatewayv2')
        exceptions = (
            client.exceptions.TooManyRequestsException,
        )

        result = self._call_client_method_with_retries(
            client.update_domain_name,
            api_args, max_attempts=6,
            should_retry=lambda x: True,
            retryable_exceptions=exceptions
        )
        result_data = result['DomainNameConfigurations'][0]
        domain_name = {
            'domain_name': result['DomainName'],
            'alias_domain_name': result_data['ApiGatewayDomainName'],
            'security_policy': result_data['SecurityPolicy'],
            'hosted_zone_id': result_data['HostedZoneId'],
            'certificate_arn': result_data['CertificateArn']
        }  # type: DomainNameResponse
        return domain_name","    def _update_domain_name_v2(self, api_args):
        # type: (Dict[str, Any]) -> Dict[str, Any]
        client = self._client('apigatewayv2')
        exceptions = (
            client.exceptions.TooManyRequestsException,
        )

        result = self._call_client_method_with_retries(
            client.update_domain_name,
            api_args, max_attempts=6,
            should_retry=lambda x: True,
            retryable_exceptions=exceptions
        )
        result_data = result['DomainNameConfigurations'][0]
        domain_name = {
            'domain_name': result['DomainName'],
            'endpoint_configuration': result_data['EndpointType'],
            'security_policy': result_data['SecurityPolicy'],
            'hosted_zone_id': result_data['HostedZoneId'],
            'certificate_arn': result_data['CertificateArn']
        }
        return domain_name"
"    def iter_log_events(self, log_group_name, interleaved=True):
        # type: (str, bool) -> Iterator[Dict[str, Any]]
        logs = self._client('logs')
        paginator = logs.get_paginator('filter_log_events')
        pages = paginator.paginate(logGroupName=log_group_name,
                                   interleaved=True)
        try:
            for log_message in self._iter_log_messages(pages):
                yield log_message
        except logs.exceptions.ResourceNotFoundException:
            # If the lambda function exists but has not been invoked yet,
            # it's possible that the log group does not exist and we'll get
            # a ResourceNotFoundException.  If this happens we return instead
            # of propagating an exception back to the user.
            pass","    def iter_log_events(self, log_group_name, interleaved=True):
        # type: (str, bool) -> Iterator[Dict[str, Any]]
        logs = self._client('logs')
        paginator = logs.get_paginator('filter_log_events')
        for page in paginator.paginate(logGroupName=log_group_name,
                                       interleaved=True):
            events = page['events']
            for event in events:
                # timestamp is modeled as a 'long', so we'll
                # convert to a datetime to make it easier to use
                # in python.
                event['ingestionTime'] = self._convert_to_datetime(
                    event['ingestionTime'])
                event['timestamp'] = self._convert_to_datetime(
                    event['timestamp'])
                yield event"
"    def __init__(self, osutils=None, import_string=None):
        # type: (Optional[OSUtils], OptStr) -> None
        if osutils is None:
            osutils = OSUtils()
        self._osutils = osutils
        if import_string is None:
            import_string = pip_import_string()
        self._import_string = import_string","    def __init__(self, osutils=None):
        # type: (Optional[OSUtils]) -> None
        if osutils is None:
            osutils = OSUtils()
        self._osutils = osutils"
"    def main(self, args, env_vars=None, shim=None):
        # type: (List[str], EnvVars, OptStr) -> Tuple[int, Optional[bytes]]
        if env_vars is None:
            env_vars = self._osutils.environ()
        if shim is None:
            shim = ''
        python_exe = sys.executable
        run_pip = (
            'import sys; %s; sys.exit(main(%s))'
        ) % (self._import_string, args)
        exec_string = '%s%s' % (shim, run_pip)
        invoke_pip = [python_exe, '-c', exec_string]
        p = self._osutils.popen(invoke_pip,
                                stdout=self._osutils.pipe,
                                stderr=self._osutils.pipe,
                                env=env_vars)
        _, err = p.communicate()
        rc = p.returncode
        return rc, err","    def main(self, args, env_vars=None, shim=None):
        # type: (List[str], EnvVars, OptStr) -> Tuple[int, Optional[bytes]]
        if env_vars is None:
            env_vars = self._osutils.environ()
        if shim is None:
            shim = ''
        python_exe = sys.executable
        run_pip = 'import pip, sys; sys.exit(pip.main(%s))' % args
        exec_string = '%s%s' % (shim, run_pip)
        invoke_pip = [python_exe, '-c', exec_string]
        p = subprocess.Popen(invoke_pip,
                             stdout=subprocess.PIPE, stderr=subprocess.PIPE,
                             env=env_vars)
        _, err = p.communicate()
        rc = p.returncode
        return rc, err"
"    def index(self, locale):
        if locale not in self.appbuilder.bm.languages:
            abort(404, description=""Locale not supported."")
        session[""locale""] = locale
        refresh()
        self.update_redirect()
        return redirect(self.get_redirect())","    def index(self, locale):
        session[""locale""] = locale
        refresh()
        self.update_redirect()
        return redirect(self.get_redirect())"
"    def login(self, flag=True):
        @self.appbuilder.sm.oid.loginhandler
        def login_handler(self):
            if g.user is not None and g.user.is_authenticated:
                return redirect(self.appbuilder.get_url_for_index)
            form = LoginForm_oid()
            if form.validate_on_submit():
                session[""remember_me""] = form.remember_me.data
                return self.appbuilder.sm.oid.try_login(
                    form.openid.data,
                    ask_for=self.oid_ask_for,
                    ask_for_optional=self.oid_ask_for_optional,
                )
            return self.render_template(
                self.login_template,
                title=self.title,
                form=form,
                providers=self.appbuilder.sm.openid_providers,
                appbuilder=self.appbuilder,
            )

        @self.appbuilder.sm.oid.after_login
        def after_login(resp):
            if resp.email is None or resp.email == """":
                flash(as_unicode(self.invalid_login_message), ""warning"")
                return redirect(self.appbuilder.get_url_for_login)
            user = self.appbuilder.sm.auth_user_oid(resp.email)
            if user is None:
                flash(as_unicode(self.invalid_login_message), ""warning"")
                return redirect(self.appbuilder.get_url_for_login)
            remember_me = False
            if ""remember_me"" in session:
                remember_me = session[""remember_me""]
                session.pop(""remember_me"", None)

            login_user(user, remember=remember_me)
            return redirect(self.appbuilder.get_url_for_index)

        return login_handler(self)","    def login(self, flag=True):
        @self.appbuilder.sm.oid.loginhandler
        def login_handler(self):
            if g.user is not None and g.user.is_authenticated:
                return redirect(self.appbuilder.get_url_for_index)
            form = LoginForm_oid()
            if form.validate_on_submit():
                session[""remember_me""] = form.remember_me.data
                return self.appbuilder.sm.oid.try_login(
                    form.openid.data,
                    ask_for=self.oid_ask_for,
                    ask_for_optional=self.oid_ask_for_optional,
                )
            return self.render_template(
                self.login_template,
                title=self.title,
                form=form,
                providers=self.appbuilder.sm.openid_providers,
                appbuilder=self.appbuilder,
            )

        @self.appbuilder.sm.oid.after_login
        def after_login(resp):
            if resp.email is None or resp.email == """":
                flash(as_unicode(self.invalid_login_message), ""warning"")
                return redirect(""login"")
            user = self.appbuilder.sm.auth_user_oid(resp.email)
            if user is None:
                flash(as_unicode(self.invalid_login_message), ""warning"")
                return redirect(""login"")
            remember_me = False
            if ""remember_me"" in session:
                remember_me = session[""remember_me""]
                session.pop(""remember_me"", None)

            login_user(user, remember=remember_me)
            return redirect(self.appbuilder.get_url_for_index)

        return login_handler(self)"
"        def after_login(resp):
            if resp.email is None or resp.email == """":
                flash(as_unicode(self.invalid_login_message), ""warning"")
                return redirect(self.appbuilder.get_url_for_login)
            user = self.appbuilder.sm.auth_user_oid(resp.email)
            if user is None:
                flash(as_unicode(self.invalid_login_message), ""warning"")
                return redirect(self.appbuilder.get_url_for_login)
            remember_me = False
            if ""remember_me"" in session:
                remember_me = session[""remember_me""]
                session.pop(""remember_me"", None)

            login_user(user, remember=remember_me)
            return redirect(self.appbuilder.get_url_for_index)","        def after_login(resp):
            if resp.email is None or resp.email == """":
                flash(as_unicode(self.invalid_login_message), ""warning"")
                return redirect(""login"")
            user = self.appbuilder.sm.auth_user_oid(resp.email)
            if user is None:
                flash(as_unicode(self.invalid_login_message), ""warning"")
                return redirect(""login"")
            remember_me = False
            if ""remember_me"" in session:
                remember_me = session[""remember_me""]
                session.pop(""remember_me"", None)

            login_user(user, remember=remember_me)
            return redirect(self.appbuilder.get_url_for_index)"
"    def oauth_authorized(self, provider):
        log.debug(""Authorized init"")
        resp = self.appbuilder.sm.oauth_remotes[provider].authorized_response()
        if resp is None:
            flash(u""You denied the request to sign in."", ""warning"")
            return redirect(self.appbuilder.get_url_for_login)
        log.debug(""OAUTH Authorized resp: {0}"".format(resp))
        # Retrieves specific user info from the provider
        try:
            self.appbuilder.sm.set_oauth_session(provider, resp)
            userinfo = self.appbuilder.sm.oauth_user_info(provider, resp)
        except Exception as e:
            log.error(""Error returning OAuth user info: {0}"".format(e))
            user = None
        else:
            log.debug(""User info retrieved from {0}: {1}"".format(provider, userinfo))
            # User email is not whitelisted
            if provider in self.appbuilder.sm.oauth_whitelists:
                whitelist = self.appbuilder.sm.oauth_whitelists[provider]
                allow = False
                for e in whitelist:
                    if re.search(e, userinfo[""email""]):
                        allow = True
                        break
                if not allow:
                    flash(u""You are not authorized."", ""warning"")
                    return redirect(self.appbuilder.get_url_for_login)
            else:
                log.debug(""No whitelist for OAuth provider"")
            user = self.appbuilder.sm.auth_user_oauth(userinfo)

        if user is None:
            flash(as_unicode(self.invalid_login_message), ""warning"")
            return redirect(self.appbuilder.get_url_for_login)
        else:
            login_user(user)
            try:
                state = jwt.decode(
                    request.args[""state""],
                    self.appbuilder.app.config[""SECRET_KEY""],
                    algorithms=[""HS256""],
                )
            except jwt.InvalidTokenError:
                raise Exception(""State signature is not valid!"")

            try:
                next_url = state[""next""][0] or self.appbuilder.get_url_for_index
            except (KeyError, IndexError):
                next_url = self.appbuilder.get_url_for_index

            return redirect(next_url)","    def oauth_authorized(self, provider):
        log.debug(""Authorized init"")
        resp = self.appbuilder.sm.oauth_remotes[provider].authorized_response()
        if resp is None:
            flash(u""You denied the request to sign in."", ""warning"")
            return redirect(""login"")
        log.debug(""OAUTH Authorized resp: {0}"".format(resp))
        # Retrieves specific user info from the provider
        try:
            self.appbuilder.sm.set_oauth_session(provider, resp)
            userinfo = self.appbuilder.sm.oauth_user_info(provider, resp)
        except Exception as e:
            log.error(""Error returning OAuth user info: {0}"".format(e))
            user = None
        else:
            log.debug(""User info retrieved from {0}: {1}"".format(provider, userinfo))
            # User email is not whitelisted
            if provider in self.appbuilder.sm.oauth_whitelists:
                whitelist = self.appbuilder.sm.oauth_whitelists[provider]
                allow = False
                for e in whitelist:
                    if re.search(e, userinfo[""email""]):
                        allow = True
                        break
                if not allow:
                    flash(u""You are not authorized."", ""warning"")
                    return redirect(""login"")
            else:
                log.debug(""No whitelist for OAuth provider"")
            user = self.appbuilder.sm.auth_user_oauth(userinfo)

        if user is None:
            flash(as_unicode(self.invalid_login_message), ""warning"")
            return redirect(""login"")
        else:
            login_user(user)
            try:
                state = jwt.decode(
                    request.args[""state""],
                    self.appbuilder.app.config[""SECRET_KEY""],
                    algorithms=[""HS256""],
                )
            except jwt.InvalidTokenError:
                raise Exception(""State signature is not valid!"")

            try:
                next_url = state[""next""][0] or self.appbuilder.get_url_for_index
            except (KeyError, IndexError):
                next_url = self.appbuilder.get_url_for_index

            return redirect(next_url)"
"    def _query_select_options(self, query, select_columns=None):
        """"""
            Add select load options to query. The goal
            is to only SQL select what is requested

        :param query: SQLAlchemy Query obj
        :param select_columns: (list) of columns
        :return: SQLAlchemy Query obj
        """"""
        if select_columns:
            _load_options = list()
            for column in select_columns:
                query, relation_tuple = self._query_join_dotted_column(
                    query,
                    column,
                )
                model_relation, relation_join = relation_tuple or (None, None)
                if model_relation:
                    _load_options.append(
                        Load(model_relation).load_only(column.split(""."")[1])
                    )
                else:
                    # is a custom property method field?
                    if hasattr(getattr(self.obj, column), ""fget""):
                        pass
                    # is not a relation and not a function?
                    elif not self.is_relation(column) and not hasattr(
                        getattr(self.obj, column), ""__call__""
                    ):
                        _load_options.append(Load(self.obj).load_only(column))
                    else:
                        _load_options.append(Load(self.obj))
            query = query.options(*tuple(_load_options))
        return query","    def _query_select_options(self, query, select_columns=None):
        """"""
            Add select load options to query. The goal
            is to only SQL select what is requested

        :param query: SQLAlchemy Query obj
        :param select_columns: (list) of columns
        :return: SQLAlchemy Query obj
        """"""
        if select_columns:
            _load_options = list()
            for column in select_columns:
                if ""."" in column:
                    model_relation = self.get_related_model(column.split(""."")[0])
                    if not self.is_model_already_joinded(query, model_relation):
                        query = query.join(model_relation)
                    _load_options.append(
                        Load(model_relation).load_only(column.split(""."")[1])
                    )
                else:
                    # is a custom property method field?
                    if hasattr(getattr(self.obj, column), ""fget""):
                        pass
                    # is not a relation and not a function?
                    elif not self.is_relation(column) and not hasattr(
                        getattr(self.obj, column), ""__call__""
                    ):
                        _load_options.append(Load(self.obj).load_only(column))
                    else:
                        _load_options.append(Load(self.obj))
            query = query.options(*tuple(_load_options))
        return query"
"    def query(
        self,
        filters=None,
        order_column="""",
        order_direction="""",
        page=None,
        page_size=None,
        select_columns=None,
    ):
        """"""
            QUERY
            :param filters:
                dict with filters {<col_name>:<value,...}
            :param order_column:
                name of the column to order
            :param order_direction:
                the direction to order <'asc'|'desc'>
            :param page:
                the current page
            :param page_size:
                the current page size

        """"""
        query = self.session.query(self.obj)
        query, relation_tuple = self._query_join_dotted_column(query, order_column)
        query = self._query_select_options(query, select_columns)
        query_count = self.session.query(func.count('*')).select_from(self.obj)

        query_count = self._get_base_query(query=query_count, filters=filters)
        query = self._get_base_query(
            query=query,
            filters=filters,
            order_column=order_column,
            order_direction=order_direction,
        )

        count = query_count.scalar()

        if page:
            query = query.offset(page * page_size)
        if page_size:
            query = query.limit(page_size)
        return count, query.all()","    def query(
        self,
        filters=None,
        order_column="""",
        order_direction="""",
        page=None,
        page_size=None,
        select_columns=None,
    ):
        """"""
            QUERY
            :param filters:
                dict with filters {<col_name>:<value,...}
            :param order_column:
                name of the column to order
            :param order_direction:
                the direction to order <'asc'|'desc'>
            :param page:
                the current page
            :param page_size:
                the current page size

        """"""
        query = self.session.query(self.obj)
        query = self._query_select_options(query, select_columns)
        if len(order_column.split('.')) >= 2:
            for join_relation in order_column.split('.')[:-1]:
                relation_tuple = self.get_related_model_and_join(join_relation)
                model_relation, relation_join = relation_tuple
                if not self.is_model_already_joinded(query, model_relation):
                    query = query.join(model_relation, relation_join, isouter=True)
        query_count = self.session.query(func.count('*')).select_from(self.obj)

        query_count = self._get_base_query(query=query_count, filters=filters)
        query = self._get_base_query(
            query=query,
            filters=filters,
            order_column=order_column,
            order_direction=order_direction,
        )

        count = query_count.scalar()

        if page:
            query = query.offset(page * page_size)
        if page_size:
            query = query.limit(page_size)
        return count, query.all()"
"    def auth_user_ldap(self, username, password):
        """"""
            Method for authenticating user, auth LDAP style.
            depends on ldap module that is not mandatory requirement
            for F.A.B.

            :param username:
                The username
            :param password:
                The password
        """"""
        if username is None or username == """":
            return None
        user = self.find_user(username=username)
        if user is not None and (not user.is_active()):
            return None
        else:
            try:
                import ldap
            except:
                raise Exception(""No ldap library for python."")
            try:
                if self.auth_ldap_allow_self_signed:
                    ldap.set_option(ldap.OPT_X_TLS_REQUIRE_CERT, ldap.OPT_X_TLS_ALLOW)
                con = ldap.initialize(self.auth_ldap_server)
                con.set_option(ldap.OPT_REFERRALS, 0)
                if self.auth_ldap_use_tls:
                    try:
                        con.start_tls_s()
                    except Exception:
                        log.info(LOGMSG_ERR_SEC_AUTH_LDAP_TLS.format(self.auth_ldap_server))
                        return None
                # Authenticate user
                if not self._bind_ldap(ldap, con, username, password):
                    if user:
                        self.update_user_auth_stat(user, False)
                    log.info(LOGMSG_WAR_SEC_LOGIN_FAILED.format(username))
                    return None
                # If user does not exist on the DB and not self user registration, go away
                if not user and not self.auth_user_registration:
                    return None
                # User does not exist, create one if self registration.
                elif not user and self.auth_user_registration:
                    new_user = self._search_ldap(ldap, con, username)
                    if not new_user:
                        log.warning(LOGMSG_WAR_SEC_NOLDAP_OBJ.format(username))
                        return None
                    ldap_user_info = new_user[0][1]
                    if self.auth_user_registration and user is None:
                        user = self.add_user(
                            username=username,
                            first_name=self.ldap_extract(ldap_user_info, self.auth_ldap_firstname_field, username),
                            last_name=self.ldap_extract(ldap_user_info, self.auth_ldap_lastname_field, username),
                            email=self.ldap_extract(ldap_user_info, self.auth_ldap_email_field, username + '@email.notfound'),
                            role=self.find_role(self.auth_user_registration_role)
                        )

                self.update_user_auth_stat(user)
                return user

            except ldap.LDAPError as e:
                if type(e.message) == dict and 'desc' in e.message:
                    log.error(LOGMSG_ERR_SEC_AUTH_LDAP.format(e.message['desc']))
                    return None
                else:
                    log.error(e)
                    return None","    def auth_user_ldap(self, username, password):
        """"""
            Method for authenticating user, auth LDAP style.
            depends on ldap module that is not mandatory requirement
            for F.A.B.

            :param username:
                The username
            :param password:
                The password
        """"""
        if username is None or username == """":
            return None
        user = self.find_user(username=username)
        if user is not None and (not user.is_active()):
            return None
        else:
            try:
                import ldap
            except:
                raise Exception(""No ldap library for python."")
            try:
                if self.auth_ldap_allow_self_signed:
                    ldap.set_option(ldap.OPT_X_TLS_REQUIRE_CERT, ldap.OPT_X_TLS_ALLOW)
                con = ldap.initialize(self.auth_ldap_server)
                con.set_option(ldap.OPT_REFERRALS, 0)
                if self.auth_ldap_use_tls:
                    try:
                        con.start_tls_s()
                    except Exception:
                        log.info(LOGMSG_ERR_SEC_AUTH_LDAP_TLS.format(self.auth_ldap_server))
                        return None
                # Authenticate user
                if not self._bind_ldap(ldap, con, username, password):
                    if user:
                        self.update_user_auth_stat(user, False)
                    log.info(LOGMSG_WAR_SEC_LOGIN_FAILED.format(username))
                    return None
                # If user does not exist on the DB and not self user registration, go away
                if not user and not self.auth_user_registration:
                    return None
                # User does not exist, create one if self registration.
                elif not user and self.auth_user_registration:
                    new_user = self._search_ldap(ldap, con, username)
                    if not new_user:
                        log.warning(LOGMSG_WAR_SEC_NOLDAP_OBJ.format(username))
                        return None
                    ldap_user_info = new_user[0][1]
                    if self.auth_user_registration and user is None:
                        user = self.add_user(
                                username=username,
                                first_name=ldap_user_info.get(self.auth_ldap_firstname_field, [username])[0],
                                last_name=ldap_user_info.get(self.auth_ldap_lastname_field, [username])[0],
                                email=ldap_user_info.get(self.auth_ldap_email_field, [username + '@email.notfound'])[0],
                                role=self.find_role(self.auth_user_registration_role)
                            )

                self.update_user_auth_stat(user)
                return user

            except ldap.LDAPError as e:
                if type(e.message) == dict and 'desc' in e.message:
                    log.error(LOGMSG_ERR_SEC_AUTH_LDAP.format(e.message['desc']))
                    return None
                else:
                    log.error(e)
                    return None"
"    def _get_base_query(self, query=None, filters=None, order_column='', order_direction=''):
        if filters:
            query = filters.apply_all(query)
        if order_column != '':
            # if Model has custom decorator **renders('<COL_NAME>')**
            # this decorator will add a property to the method named *_col_name*
            if hasattr(self.obj, order_column):
                if hasattr(getattr(self.obj, order_column), '_col_name'):
                    order_column = getattr(getattr(self.obj, order_column), '_col_name')
            query = query.order_by(""%s %s"" % (order_column, order_direction))
        return query","    def _get_base_query(self, query=None, filters=None, order_column='', order_direction=''):
        if filters:
            query = filters.apply_all(query)
        if order_column != '':
            # if Model has custom decorator **renders('<COL_NAME>')**
            # this decorator will add a property to the method named *_col_name*
            if hasattr(self.obj, order_column):
                if hasattr(getattr(self.obj, order_column), '_col_name'):
                    order_column = getattr(getattr(self.obj, order_column), '_col_name')
            query = query.order_by(order_column + ' ' + order_direction)
        return query"
"def get_order_args():
    """"""
        Get order arguments, return a dictionary
        { <VIEW_NAME>: (ORDER_COL, ORDER_DIRECTION) }

        Arguments are passed like: _oc_<VIEW_NAME>=<COL_NAME>&_od_<VIEW_NAME>='asc'|'desc'

    """"""
    orders = {}
    for arg in request.args:
        re_match = re.findall('_oc_(.*)', arg)
        if re_match:
            order_direction = request.args.get('_od_' + re_match[0])
            if order_direction in ('asc', 'desc'):
                orders[re_match[0]] = (request.args.get(arg), order_direction)
    return orders","def get_order_args():
    """"""
        Get order arguments, return a dictionary
        { <VIEW_NAME>: (ORDER_COL, ORDER_DIRECTION) }

        Arguments are passed like: _oc_<VIEW_NAME>=<COL_NAME>&_od_<VIEW_NAME>='asc'|'desc'

    """"""
    orders = {}
    for arg in request.args:
        re_match = re.findall('_oc_(.*)', arg)
        if re_match:
            orders[re_match[0]] = (request.args.get(arg), request.args.get('_od_' + re_match[0]))
    return orders"
"    def _get_attr_value(item, col):
        if not hasattr(item, col):
            # it's an inner obj attr
            try:
                return reduce(getattr, col.split('.'), item)
            except Exception as e:
                return ''
        if hasattr(getattr(item, col), '__call__'):
            # its a function
            return getattr(item, col)()
        else:
            # its an attribute
            value = getattr(item, col)
            # if value is an Enum instance than list and show widgets should display
            # its .value rather than its .name:
            if _has_enum and isinstance(value, enum.Enum):
                return value.value
            return value","    def _get_attr_value(self, item, col):
        if not hasattr(item, col):
            # it's an inner obj attr
            try:
                return reduce(getattr, col.split('.'), item)
            except Exception as e:
                return ''
        if hasattr(getattr(item, col), '__call__'):
            # its a function
            return getattr(item, col)()
        else:
            # its an attribute
            value = getattr(item, col)
            # if value is an Enum instance than list and show widgets should display
            # its .value rather than its .name:
            if _has_enum and isinstance(value, enum.Enum):
                return value.value
            return value"
"    def _get_base_query(self, query=None, filters=None, order_column='', order_direction=''):
        if filters:
            query = filters.apply_all(query)
        if order_column != '':
            # if Model has custom decorator **renders('<COL_NAME>')**
            # this decorator will add a property to the method named *_col_name*
            if hasattr(self.obj, order_column):
                if hasattr(getattr(self.obj, order_column), '_col_name'):
                    order_column = getattr(self._get_attr(order_column), '_col_name')
            if order_direction == 'asc':
                query = query.order_by(self._get_attr(order_column).asc())
            else:
                query = query.order_by(self._get_attr(order_column).desc())
        return query","    def _get_base_query(self, query=None, filters=None, order_column='', order_direction=''):
        if filters:
            query = filters.apply_all(query)
        if order_column != '':
            # if Model has custom decorator **renders('<COL_NAME>')**
            # this decorator will add a property to the method named *_col_name*
            if hasattr(self.obj, order_column):
                if hasattr(getattr(self.obj, order_column), '_col_name'):
                    order_column = getattr(getattr(self.obj, order_column), '_col_name')
            query = query.order_by(""%s %s"" % (order_column, order_direction))
        return query"
"    def solve(self, problem: QuadraticProgram) -> ADMMOptimizationResult:
        """"""Tries to solves the given problem using ADMM algorithm.

        Args:
            problem: The problem to be solved.

        Returns:
            The result of the optimizer applied to the problem.

        Raises:
            QiskitOptimizationError: If the problem is not compatible with the ADMM optimizer.
        """"""
        self._verify_compatibility(problem)

        # debug
        self._log.debug(""Initial problem: %s"", problem.export_as_lp_string())

        # map integer variables to binary variables
        from ..converters.integer_to_binary import IntegerToBinary
        int2bin = IntegerToBinary()
        original_problem = problem
        problem = int2bin.convert(problem)

        # we deal with minimization in the optimizer, so turn the problem to minimization
        problem, sense = self._turn_to_minimization(problem)

        # create our computation state.
        self._state = ADMMState(problem, self._params.rho_initial)

        # parse problem and convert to an ADMM specific representation.
        self._state.binary_indices = self._get_variable_indices(problem, Variable.Type.BINARY)
        self._state.continuous_indices = self._get_variable_indices(problem,
                                                                    Variable.Type.CONTINUOUS)
        if self._params.warm_start:
            # warm start injection for the initial values of the variables
            self._warm_start(problem)

        # convert optimization problem to a set of matrices and vector that are used
        # at each iteration.
        self._convert_problem_representation()

        start_time = time.time()
        # we have not stated our computations yet, so elapsed time initialized as zero.
        elapsed_time = 0.0
        iteration = 0
        residual = 1.e+2

        while (iteration < self._params.maxiter and residual > self._params.tol) \\
                and (elapsed_time < self._params.max_time):
            if self._state.step1_absolute_indices:
                op1 = self._create_step1_problem()
                self._state.x0 = self._update_x0(op1)
                # debug
                self._log.debug(""Step 1 sub-problem: %s"", op1.export_as_lp_string())
            # else, no binary variables exist, and no update to be done in this case.
            # debug
            self._log.debug(""x0=%s"", self._state.x0)

            op2 = self._create_step2_problem()
            self._state.u, self._state.z = self._update_x1(op2)
            # debug
            self._log.debug(""Step 2 sub-problem: %s"", op2.export_as_lp_string())
            self._log.debug(""u=%s"", self._state.u)
            self._log.debug(""z=%s"", self._state.z)

            if self._params.three_block:
                if self._state.binary_indices:
                    op3 = self._create_step3_problem()
                    self._state.y = self._update_y(op3)
                    # debug
                    self._log.debug(""Step 3 sub-problem: %s"", op3.export_as_lp_string())
                # debug
                self._log.debug(""y=%s"", self._state.y)

            self._state.lambda_mult = self._update_lambda_mult()
            # debug
            self._log.debug(""lambda: %s"", self._state.lambda_mult)

            cost_iterate = self._get_objective_value()
            constraint_residual = self._get_constraint_residual()
            residual, dual_residual = self._get_solution_residuals(iteration)
            merit = self._get_merit(cost_iterate, constraint_residual)
            # debug
            self._log.debug(""cost_iterate=%s, cr=%s, merit=%s"",
                            cost_iterate, constraint_residual, merit)

            # costs are saved with their original sign.
            self._state.cost_iterates.append(cost_iterate)
            self._state.residuals.append(residual)
            self._state.dual_residuals.append(dual_residual)
            self._state.cons_r.append(constraint_residual)
            self._state.merits.append(merit)
            self._state.lambdas.append(np.linalg.norm(self._state.lambda_mult))

            self._state.x0_saved.append(self._state.x0)
            self._state.u_saved.append(self._state.u)
            self._state.z_saved.append(self._state.z)
            self._state.z_saved.append(self._state.y)

            self._update_rho(residual, dual_residual)

            iteration += 1
            elapsed_time = time.time() - start_time

        binary_vars, continuous_vars, objective_value = self._get_best_merit_solution()
        solution = self._revert_solution_indexes(binary_vars, continuous_vars)

        # flip the objective sign again if required
        objective_value = objective_value * sense

        # convert back integer to binary
        base_result = OptimizationResult(solution, objective_value, problem.variables,
                                         OptimizationResultStatus.SUCCESS)
        base_result = int2bin.interpret(base_result)

        # third parameter is our internal state of computations.
        result = ADMMOptimizationResult(x=base_result.x, fval=base_result.fval,
                                        variables=original_problem.variables,
                                        state=self._state,
                                        status=self._get_feasibility_status(original_problem,
                                                                            base_result.x))

        # debug
        self._log.debug(""solution=%s, objective=%s at iteration=%s"",
                        solution, objective_value, iteration)
        return result","    def solve(self, problem: QuadraticProgram) -> ADMMOptimizationResult:
        """"""Tries to solves the given problem using ADMM algorithm.

        Args:
            problem: The problem to be solved.

        Returns:
            The result of the optimizer applied to the problem.

        Raises:
            QiskitOptimizationError: If the problem is not compatible with the ADMM optimizer.
        """"""
        self._verify_compatibility(problem)

        # debug
        self._log.debug(""Initial problem: %s"", problem.export_as_lp_string())

        # map integer variables to binary variables
        from ..converters.integer_to_binary import IntegerToBinary
        int2bin = IntegerToBinary()
        original_variables = problem.variables
        problem = int2bin.convert(problem)

        # we deal with minimization in the optimizer, so turn the problem to minimization
        problem, sense = self._turn_to_minimization(problem)

        # create our computation state.
        self._state = ADMMState(problem, self._params.rho_initial)

        # parse problem and convert to an ADMM specific representation.
        self._state.binary_indices = self._get_variable_indices(problem, Variable.Type.BINARY)
        self._state.continuous_indices = self._get_variable_indices(problem,
                                                                    Variable.Type.CONTINUOUS)
        if self._params.warm_start:
            # warm start injection for the initial values of the variables
            self._warm_start(problem)

        # convert optimization problem to a set of matrices and vector that are used
        # at each iteration.
        self._convert_problem_representation()

        start_time = time.time()
        # we have not stated our computations yet, so elapsed time initialized as zero.
        elapsed_time = 0.0
        iteration = 0
        residual = 1.e+2

        while (iteration < self._params.maxiter and residual > self._params.tol) \\
                and (elapsed_time < self._params.max_time):
            if self._state.step1_absolute_indices:
                op1 = self._create_step1_problem()
                self._state.x0 = self._update_x0(op1)
                # debug
                self._log.debug(""Step 1 sub-problem: %s"", op1.export_as_lp_string())
            # else, no binary variables exist, and no update to be done in this case.
            # debug
            self._log.debug(""x0=%s"", self._state.x0)

            op2 = self._create_step2_problem()
            self._state.u, self._state.z = self._update_x1(op2)
            # debug
            self._log.debug(""Step 2 sub-problem: %s"", op2.export_as_lp_string())
            self._log.debug(""u=%s"", self._state.u)
            self._log.debug(""z=%s"", self._state.z)

            if self._params.three_block:
                if self._state.binary_indices:
                    op3 = self._create_step3_problem()
                    self._state.y = self._update_y(op3)
                    # debug
                    self._log.debug(""Step 3 sub-problem: %s"", op3.export_as_lp_string())
                # debug
                self._log.debug(""y=%s"", self._state.y)

            self._state.lambda_mult = self._update_lambda_mult()
            # debug
            self._log.debug(""lambda: %s"", self._state.lambda_mult)

            cost_iterate = self._get_objective_value()
            constraint_residual = self._get_constraint_residual()
            residual, dual_residual = self._get_solution_residuals(iteration)
            merit = self._get_merit(cost_iterate, constraint_residual)
            # debug
            self._log.debug(""cost_iterate=%s, cr=%s, merit=%s"",
                            cost_iterate, constraint_residual, merit)

            # costs are saved with their original sign.
            self._state.cost_iterates.append(cost_iterate)
            self._state.residuals.append(residual)
            self._state.dual_residuals.append(dual_residual)
            self._state.cons_r.append(constraint_residual)
            self._state.merits.append(merit)
            self._state.lambdas.append(np.linalg.norm(self._state.lambda_mult))

            self._state.x0_saved.append(self._state.x0)
            self._state.u_saved.append(self._state.u)
            self._state.z_saved.append(self._state.z)
            self._state.z_saved.append(self._state.y)

            self._update_rho(residual, dual_residual)

            iteration += 1
            elapsed_time = time.time() - start_time

        binary_vars, continuous_vars, objective_value = self._get_best_merit_solution()
        solution = self._revert_solution_indexes(binary_vars, continuous_vars)

        # flip the objective sign again if required
        objective_value = objective_value * sense

        # convert back integer to binary
        base_result = OptimizationResult(solution, objective_value, original_variables,
                                         OptimizationResultStatus.SUCCESS)
        base_result = int2bin.interpret(base_result)

        # third parameter is our internal state of computations.
        result = ADMMOptimizationResult(x=base_result.x, fval=base_result.fval,
                                        variables=base_result.variables,
                                        state=self._state,
                                        status=self._get_feasibility_status(problem, base_result.x))

        # debug
        self._log.debug(""solution=%s, objective=%s at iteration=%s"",
                        solution, objective_value, iteration)
        return result"
"    def solve(self, problem: QuadraticProgram) -> OptimizationResult:
        """"""Tries to solves the given problem using the grover optimizer.

        Runs the optimizer to try to solve the optimization problem. If the problem cannot be,
        converted to a QUBO, this optimizer raises an exception due to incompatibility.

        Args:
            problem: The problem to be solved.

        Returns:
            The result of the optimizer applied to the problem.

        Raises:
            AttributeError: If the quantum instance has not been set.
            QiskitOptimizationError: If the problem is incompatible with the optimizer.
        """"""
        if self.quantum_instance is None:
            raise AttributeError('The quantum instance or backend has not been set.')

        self._verify_compatibility(problem)

        # convert problem to QUBO
        problem_ = self._convert(problem, self._converters)
        problem_init = deepcopy(problem_)

        # convert to minimization problem
        sense = problem_.objective.sense
        if sense == problem_.objective.Sense.MAXIMIZE:
            problem_.objective.sense = problem_.objective.Sense.MINIMIZE
            problem_.objective.constant = -problem_.objective.constant
            for i, val in problem_.objective.linear.to_dict().items():
                problem_.objective.linear[i] = -val
            for (i, j), val in problem_.objective.quadratic.to_dict().items():
                problem_.objective.quadratic[i, j] = -val
        self._num_key_qubits = len(problem_.objective.linear.to_array())  # type: ignore

        # Variables for tracking the optimum.
        optimum_found = False
        optimum_key = math.inf
        optimum_value = math.inf
        threshold = 0
        n_key = len(problem_.variables)
        n_value = self._num_value_qubits

        # Variables for tracking the solutions encountered.
        num_solutions = 2 ** n_key
        keys_measured = []

        # Variables for result object.
        operation_count = {}
        iteration = 0

        # Variables for stopping if we've hit the rotation max.
        rotations = 0
        max_rotations = int(np.ceil(100 * np.pi / 4))

        # Initialize oracle helper object.
        qr_key_value = QuantumRegister(self._num_key_qubits + self._num_value_qubits)
        orig_constant = problem_.objective.constant
        measurement = not self.quantum_instance.is_statevector
        oracle, is_good_state = self._get_oracle(qr_key_value)

        while not optimum_found:
            m = 1
            improvement_found = False

            # Get oracle O and the state preparation operator A for the current threshold.
            problem_.objective.constant = orig_constant - threshold
            a_operator = self._get_a_operator(qr_key_value, problem_)

            # Iterate until we measure a negative.
            loops_with_no_improvement = 0
            while not improvement_found:
                # Determine the number of rotations.
                loops_with_no_improvement += 1
                rotation_count = int(np.ceil(aqua_globals.random.uniform(0, m - 1)))
                rotations += rotation_count
                # Apply Grover's Algorithm to find values below the threshold.
                # TODO: Utilize Grover's incremental feature - requires changes to Grover.
                grover = Grover(oracle,
                                state_preparation=a_operator,
                                good_state=is_good_state)
                circuit = grover.construct_circuit(rotation_count, measurement=measurement)

                # Get the next outcome.
                outcome = self._measure(circuit)
                k = int(outcome[0:n_key], 2)
                v = outcome[n_key:n_key + n_value]
                int_v = self._bin_to_int(v, n_value) + threshold
                v = self._twos_complement(int_v, n_value)
                logger.info('Outcome: %s', outcome)
                logger.info('Value: %s = %s', v, int_v)

                # If the value is an improvement, we update the iteration parameters (e.g. oracle).
                if int_v < optimum_value:
                    optimum_key = k
                    optimum_value = int_v
                    logger.info('Current Optimum Key: %s', optimum_key)
                    logger.info('Current Optimum Value: %s', optimum_value)
                    if v.startswith('1'):
                        improvement_found = True
                        threshold = optimum_value
                else:
                    # Using Durr and Hoyer method, increase m.
                    m = int(np.ceil(min(m * 8 / 7, 2 ** (n_key / 2))))
                    logger.info('No Improvement. M: %s', m)

                    # Check if we've already seen this value.
                    if k not in keys_measured:
                        keys_measured.append(k)

                    # Assume the optimal if any of the stop parameters are true.
                    if loops_with_no_improvement >= self._n_iterations or \\
                            len(keys_measured) == num_solutions or rotations >= max_rotations:
                        improvement_found = True
                        optimum_found = True

                # Track the operation count.
                operations = circuit.count_ops()
                operation_count[iteration] = operations
                iteration += 1
                logger.info('Operation Count: %s\\n', operations)

        # If the constant is 0 and we didn't find a negative, the answer is likely 0.
        if optimum_value >= 0 and orig_constant == 0:
            optimum_key = 0

        opt_x = np.array([1 if s == '1' else 0 for s in ('{0:%sb}' % n_key).format(optimum_key)])

        # Compute function value
        fval = problem_init.objective.evaluate(opt_x)
        result = OptimizationResult(x=opt_x, fval=fval, variables=problem_.variables,
                                    status=OptimizationResultStatus.SUCCESS)

        # cast binaries back to integers
        result = self._interpret(result, self._converters)

        return GroverOptimizationResult(x=result.x, fval=result.fval, variables=result.variables,
                                        operation_counts=operation_count, n_input_qubits=n_key,
                                        n_output_qubits=n_value, intermediate_fval=fval,
                                        threshold=threshold,
                                        status=self._get_feasibility_status(problem, result.x))","    def solve(self, problem: QuadraticProgram) -> OptimizationResult:
        """"""Tries to solves the given problem using the grover optimizer.

        Runs the optimizer to try to solve the optimization problem. If the problem cannot be,
        converted to a QUBO, this optimizer raises an exception due to incompatibility.

        Args:
            problem: The problem to be solved.

        Returns:
            The result of the optimizer applied to the problem.

        Raises:
            AttributeError: If the quantum instance has not been set.
            QiskitOptimizationError: If the problem is incompatible with the optimizer.
        """"""
        if self.quantum_instance is None:
            raise AttributeError('The quantum instance or backend has not been set.')

        self._verify_compatibility(problem)

        # convert problem to QUBO
        problem_ = self._convert(problem, self._converters)
        problem_init = deepcopy(problem_)

        # convert to minimization problem
        sense = problem_.objective.sense
        if sense == problem_.objective.Sense.MAXIMIZE:
            problem_.objective.sense = problem_.objective.Sense.MINIMIZE
            problem_.objective.constant = -problem_.objective.constant
            for i, val in problem_.objective.linear.to_dict().items():
                problem_.objective.linear[i] = -val
            for (i, j), val in problem_.objective.quadratic.to_dict().items():
                problem_.objective.quadratic[i, j] = -val
        self._num_key_qubits = len(problem_.objective.linear.to_array())  # type: ignore

        # Variables for tracking the optimum.
        optimum_found = False
        optimum_key = math.inf
        optimum_value = math.inf
        threshold = 0
        n_key = len(problem_.variables)
        n_value = self._num_value_qubits

        # Variables for tracking the solutions encountered.
        num_solutions = 2 ** n_key
        keys_measured = []

        # Variables for result object.
        operation_count = {}
        iteration = 0

        # Variables for stopping if we've hit the rotation max.
        rotations = 0
        max_rotations = int(np.ceil(100 * np.pi / 4))

        # Initialize oracle helper object.
        qr_key_value = QuantumRegister(self._num_key_qubits + self._num_value_qubits)
        orig_constant = problem_.objective.constant
        measurement = not self.quantum_instance.is_statevector
        oracle, is_good_state = self._get_oracle(qr_key_value)

        while not optimum_found:
            m = 1
            improvement_found = False

            # Get oracle O and the state preparation operator A for the current threshold.
            problem_.objective.constant = orig_constant - threshold
            a_operator = self._get_a_operator(qr_key_value, problem_)

            # Iterate until we measure a negative.
            loops_with_no_improvement = 0
            while not improvement_found:
                # Determine the number of rotations.
                loops_with_no_improvement += 1
                rotation_count = int(np.ceil(aqua_globals.random.uniform(0, m - 1)))
                rotations += rotation_count

                # Apply Grover's Algorithm to find values below the threshold.
                if rotation_count > 0:
                    # TODO: Utilize Grover's incremental feature - requires changes to Grover.
                    grover = Grover(oracle,
                                    state_preparation=a_operator,
                                    good_state=is_good_state)
                    circuit = grover.construct_circuit(rotation_count, measurement=measurement)
                else:
                    circuit = a_operator

                # Get the next outcome.
                outcome = self._measure(circuit)
                k = int(outcome[0:n_key], 2)
                v = outcome[n_key:n_key + n_value]
                int_v = self._bin_to_int(v, n_value) + threshold
                v = self._twos_complement(int_v, n_value)
                logger.info('Outcome: %s', outcome)
                logger.info('Value: %s = %s', v, int_v)

                # If the value is an improvement, we update the iteration parameters (e.g. oracle).
                if int_v < optimum_value:
                    optimum_key = k
                    optimum_value = int_v
                    logger.info('Current Optimum Key: %s', optimum_key)
                    logger.info('Current Optimum Value: %s', optimum_value)
                    if v.startswith('1'):
                        improvement_found = True
                        threshold = optimum_value
                else:
                    # Using Durr and Hoyer method, increase m.
                    m = int(np.ceil(min(m * 8 / 7, 2 ** (n_key / 2))))
                    logger.info('No Improvement. M: %s', m)

                    # Check if we've already seen this value.
                    if k not in keys_measured:
                        keys_measured.append(k)

                    # Assume the optimal if any of the stop parameters are true.
                    if loops_with_no_improvement >= self._n_iterations or \\
                            len(keys_measured) == num_solutions or rotations >= max_rotations:
                        improvement_found = True
                        optimum_found = True

                # Track the operation count.
                operations = circuit.count_ops()
                operation_count[iteration] = operations
                iteration += 1
                logger.info('Operation Count: %s\\n', operations)

        # If the constant is 0 and we didn't find a negative, the answer is likely 0.
        if optimum_value >= 0 and orig_constant == 0:
            optimum_key = 0

        opt_x = np.array([1 if s == '1' else 0 for s in ('{0:%sb}' % n_key).format(optimum_key)])

        # Compute function value
        fval = problem_init.objective.evaluate(opt_x)
        result = OptimizationResult(x=opt_x, fval=fval, variables=problem_.variables,
                                    status=OptimizationResultStatus.SUCCESS)

        # cast binaries back to integers
        result = self._interpret(result, self._converters)

        return GroverOptimizationResult(x=result.x, fval=result.fval, variables=result.variables,
                                        operation_counts=operation_count, n_input_qubits=n_key,
                                        n_output_qubits=n_value, intermediate_fval=fval,
                                        threshold=threshold,
                                        status=self._get_feasibility_status(problem, result.x))"
"    def _get_probs(self, qc: QuantumCircuit) -> Dict[str, float]:
        """"""Gets probabilities from a given backend.""""""
        # Execute job and filter results.
        result = self.quantum_instance.execute(qc)
        if self.quantum_instance.is_statevector:
            state = np.round(result.get_statevector(qc), 5)
            keys = [bin(i)[2::].rjust(int(np.log2(len(state))), '0')[::-1]
                    for i in range(0, len(state))]
            probs = [np.round(abs(a) * abs(a), 5) for a in state]
            hist = dict(zip(keys, probs))
        else:
            state = result.get_counts(qc)
            shots = self.quantum_instance.run_config.shots
            hist = {}
            for key in state:
                hist[key[::-1]] = state[key] / shots
        hist = dict(filter(lambda p: p[1] > 0, hist.items()))
        return hist","    def _get_probs(self, qc: QuantumCircuit) -> Dict[str, float]:
        """"""Gets probabilities from a given backend.""""""
        # Execute job and filter results.
        result = self.quantum_instance.execute(qc)
        if self.quantum_instance.is_statevector:
            state = np.round(result.get_statevector(qc), 5)
            keys = [bin(i)[2::].rjust(int(np.log2(len(state))), '0')[::-1]
                    for i in range(0, len(state))]
            probs = [np.round(abs(a) * abs(a), 5) for a in state]
            hist = dict(zip(keys, probs))
        else:
            state = result.get_counts(qc)
            shots = self.quantum_instance.run_config.shots
            hist = {}
            for key in state:
                hist[key] = state[key] / shots
        hist = dict(filter(lambda p: p[1] > 0, hist.items()))

        return hist"
"    def get_kernel_matrix(quantum_instance, feature_map, x1_vec, x2_vec=None, enforce_psd=True):
        """"""
        Construct kernel matrix, if x2_vec is None, self-innerproduct is conducted.

        Notes:
            When using `statevector_simulator`,
            we only build the circuits for Psi(x1)|0> rather than
            Psi(x2)^dagger Psi(x1)|0>, and then we perform the inner product classically.
            That is, for `statevector_simulator`,
            the total number of circuits will be O(N) rather than
            O(N^2) for `qasm_simulator`.

        Args:
            quantum_instance (QuantumInstance): quantum backend with all settings
            feature_map (FeatureMap): a feature map that maps data to feature space
            x1_vec (numpy.ndarray): data points, 2-D array, N1xD, where N1 is the number of data,
                                    D is the feature dimension
            x2_vec (numpy.ndarray): data points, 2-D array, N2xD, where N2 is the number of data,
                                    D is the feature dimension
            enforce_psd (bool): enforces that the kernel matrix is positive semi-definite by setting
                                negative eigenvalues to zero. This is only applied in the symmetric
                                case, i.e., if `x2_vec == None`.
        Returns:
            numpy.ndarray: 2-D matrix, N1xN2
        """"""

        if isinstance(feature_map, QuantumCircuit):
            use_parameterized_circuits = True
        else:
            use_parameterized_circuits = feature_map.support_parameterized_circuit

        if x2_vec is None:
            is_symmetric = True
            x2_vec = x1_vec
        else:
            is_symmetric = False

        is_statevector_sim = quantum_instance.is_statevector

        measurement = not is_statevector_sim
        measurement_basis = '0' * feature_map.num_qubits
        mat = np.ones((x1_vec.shape[0], x2_vec.shape[0]))

        # get all indices
        if is_symmetric:
            mus, nus = np.triu_indices(x1_vec.shape[0], k=1)  # remove diagonal term
        else:
            mus, nus = np.indices((x1_vec.shape[0], x2_vec.shape[0]))
            mus = np.asarray(mus.flat)
            nus = np.asarray(nus.flat)

        if is_statevector_sim:
            if is_symmetric:
                to_be_computed_data = x1_vec
            else:
                to_be_computed_data = np.concatenate((x1_vec, x2_vec))

            if use_parameterized_circuits:
                # build parameterized circuits, it could be slower for building circuit
                # but overall it should be faster since it only transpile one circuit
                feature_map_params = ParameterVector('x', feature_map.feature_dimension)
                parameterized_circuit = QSVM._construct_circuit(
                    (feature_map_params, feature_map_params), feature_map, measurement,
                    is_statevector_sim=is_statevector_sim)
                parameterized_circuit = quantum_instance.transpile(parameterized_circuit)[0]
                circuits = [parameterized_circuit.assign_parameters({feature_map_params: x})
                            for x in to_be_computed_data]
            else:
                #  the second x is redundant
                to_be_computed_data_pair = [(x, x) for x in to_be_computed_data]
                if logger.isEnabledFor(logging.DEBUG):
                    logger.debug(""Building circuits:"")
                    TextProgressBar(sys.stderr)
                circuits = parallel_map(QSVM._construct_circuit,
                                        to_be_computed_data_pair,
                                        task_args=(feature_map, measurement, is_statevector_sim),
                                        num_processes=aqua_globals.num_processes)

            results = quantum_instance.execute(circuits,
                                               had_transpiled=use_parameterized_circuits)

            if logger.isEnabledFor(logging.DEBUG):
                logger.debug(""Calculating overlap:"")
                TextProgressBar(sys.stderr)

            offset = 0 if is_symmetric else len(x1_vec)
            matrix_elements = parallel_map(QSVM._compute_overlap, list(zip(mus, nus + offset)),
                                           task_args=(results,
                                                      is_statevector_sim, measurement_basis),
                                           num_processes=aqua_globals.num_processes)

            for i, j, value in zip(mus, nus, matrix_elements):
                mat[i, j] = value
                if is_symmetric:
                    mat[j, i] = mat[i, j]
        else:
            for idx in range(0, len(mus), QSVM.BATCH_SIZE):
                to_be_computed_data_pair = []
                to_be_computed_index = []
                for sub_idx in range(idx, min(idx + QSVM.BATCH_SIZE, len(mus))):
                    i = mus[sub_idx]
                    j = nus[sub_idx]
                    x1 = x1_vec[i]
                    x2 = x2_vec[j]
                    if not np.all(x1 == x2):
                        to_be_computed_data_pair.append((x1, x2))
                        to_be_computed_index.append((i, j))

                if use_parameterized_circuits:
                    # build parameterized circuits, it could be slower for building circuit
                    # but overall it should be faster since it only transpile one circuit
                    feature_map_params_x = ParameterVector('x', feature_map.feature_dimension)
                    feature_map_params_y = ParameterVector('y', feature_map.feature_dimension)
                    parameterized_circuit = QSVM._construct_circuit(
                        (feature_map_params_x, feature_map_params_y), feature_map, measurement,
                        is_statevector_sim=is_statevector_sim)
                    parameterized_circuit = quantum_instance.transpile(parameterized_circuit)[0]
                    circuits = [parameterized_circuit.assign_parameters({feature_map_params_x: x,
                                                                         feature_map_params_y: y})
                                for x, y in to_be_computed_data_pair]
                else:
                    if logger.isEnabledFor(logging.DEBUG):
                        logger.debug(""Building circuits:"")
                        TextProgressBar(sys.stderr)
                    circuits = parallel_map(QSVM._construct_circuit,
                                            to_be_computed_data_pair,
                                            task_args=(feature_map, measurement),
                                            num_processes=aqua_globals.num_processes)

                results = quantum_instance.execute(circuits,
                                                   had_transpiled=use_parameterized_circuits)

                if logger.isEnabledFor(logging.DEBUG):
                    logger.debug(""Calculating overlap:"")
                    TextProgressBar(sys.stderr)
                matrix_elements = parallel_map(QSVM._compute_overlap, range(len(circuits)),
                                               task_args=(results,
                                                          is_statevector_sim, measurement_basis),
                                               num_processes=aqua_globals.num_processes)

                for (i, j), value in zip(to_be_computed_index, matrix_elements):
                    mat[i, j] = value
                    if is_symmetric:
                        mat[j, i] = mat[i, j]

        if enforce_psd and is_symmetric and not is_statevector_sim:
            # Find the closest positive semi-definite approximation to kernel matrix, in case it is
            # symmetric. The (symmetric) matrix should always be positive semi-definite by
            # construction, but this can be violated in case of noise, such as sampling noise, thus,
            # the adjustment is only done if NOT using the statevector simulation.
            D, U = np.linalg.eig(mat)
            mat = U @ np.diag(np.maximum(0, D)) @ U.transpose()

        return mat","    def get_kernel_matrix(quantum_instance, feature_map, x1_vec, x2_vec=None):
        """"""
        Construct kernel matrix, if x2_vec is None, self-innerproduct is conducted.

        Notes:
            When using `statevector_simulator`,
            we only build the circuits for Psi(x1)|0> rather than
            Psi(x2)^dagger Psi(x1)|0>, and then we perform the inner product classically.
            That is, for `statevector_simulator`,
            the total number of circuits will be O(N) rather than
            O(N^2) for `qasm_simulator`.

        Args:
            quantum_instance (QuantumInstance): quantum backend with all settings
            feature_map (FeatureMap): a feature map that maps data to feature space
            x1_vec (numpy.ndarray): data points, 2-D array, N1xD, where N1 is the number of data,
                                    D is the feature dimension
            x2_vec (numpy.ndarray): data points, 2-D array, N2xD, where N2 is the number of data,
                                    D is the feature dimension
        Returns:
            numpy.ndarray: 2-D matrix, N1xN2
        """"""

        if isinstance(feature_map, QuantumCircuit):
            use_parameterized_circuits = True
        else:
            use_parameterized_circuits = feature_map.support_parameterized_circuit

        if x2_vec is None:
            is_symmetric = True
            x2_vec = x1_vec
        else:
            is_symmetric = False

        is_statevector_sim = quantum_instance.is_statevector

        measurement = not is_statevector_sim
        measurement_basis = '0' * feature_map.num_qubits
        mat = np.ones((x1_vec.shape[0], x2_vec.shape[0]))

        # get all indices
        if is_symmetric:
            mus, nus = np.triu_indices(x1_vec.shape[0], k=1)  # remove diagonal term
        else:
            mus, nus = np.indices((x1_vec.shape[0], x2_vec.shape[0]))
            mus = np.asarray(mus.flat)
            nus = np.asarray(nus.flat)

        if is_statevector_sim:
            if is_symmetric:
                to_be_computed_data = x1_vec
            else:
                to_be_computed_data = np.concatenate((x1_vec, x2_vec))

            if use_parameterized_circuits:
                # build parameterized circuits, it could be slower for building circuit
                # but overall it should be faster since it only transpile one circuit
                feature_map_params = ParameterVector('x', feature_map.feature_dimension)
                parameterized_circuit = QSVM._construct_circuit(
                    (feature_map_params, feature_map_params), feature_map, measurement,
                    is_statevector_sim=is_statevector_sim)
                parameterized_circuit = quantum_instance.transpile(parameterized_circuit)[0]
                circuits = [parameterized_circuit.assign_parameters({feature_map_params: x})
                            for x in to_be_computed_data]
            else:
                #  the second x is redundant
                to_be_computed_data_pair = [(x, x) for x in to_be_computed_data]
                if logger.isEnabledFor(logging.DEBUG):
                    logger.debug(""Building circuits:"")
                    TextProgressBar(sys.stderr)
                circuits = parallel_map(QSVM._construct_circuit,
                                        to_be_computed_data_pair,
                                        task_args=(feature_map, measurement, is_statevector_sim),
                                        num_processes=aqua_globals.num_processes)

            results = quantum_instance.execute(circuits,
                                               had_transpiled=use_parameterized_circuits)

            if logger.isEnabledFor(logging.DEBUG):
                logger.debug(""Calculating overlap:"")
                TextProgressBar(sys.stderr)

            offset = 0 if is_symmetric else len(x1_vec)
            matrix_elements = parallel_map(QSVM._compute_overlap, list(zip(mus, nus + offset)),
                                           task_args=(results,
                                                      is_statevector_sim, measurement_basis),
                                           num_processes=aqua_globals.num_processes)

            for i, j, value in zip(mus, nus, matrix_elements):
                mat[i, j] = value
                if is_symmetric:
                    mat[j, i] = mat[i, j]
        else:
            for idx in range(0, len(mus), QSVM.BATCH_SIZE):
                to_be_computed_data_pair = []
                to_be_computed_index = []
                for sub_idx in range(idx, min(idx + QSVM.BATCH_SIZE, len(mus))):
                    i = mus[sub_idx]
                    j = nus[sub_idx]
                    x1 = x1_vec[i]
                    x2 = x2_vec[j]
                    if not np.all(x1 == x2):
                        to_be_computed_data_pair.append((x1, x2))
                        to_be_computed_index.append((i, j))

                if use_parameterized_circuits:
                    # build parameterized circuits, it could be slower for building circuit
                    # but overall it should be faster since it only transpile one circuit
                    feature_map_params_x = ParameterVector('x', feature_map.feature_dimension)
                    feature_map_params_y = ParameterVector('y', feature_map.feature_dimension)
                    parameterized_circuit = QSVM._construct_circuit(
                        (feature_map_params_x, feature_map_params_y), feature_map, measurement,
                        is_statevector_sim=is_statevector_sim)
                    parameterized_circuit = quantum_instance.transpile(parameterized_circuit)[0]
                    circuits = [parameterized_circuit.assign_parameters({feature_map_params_x: x,
                                                                         feature_map_params_y: y})
                                for x, y in to_be_computed_data_pair]
                else:
                    if logger.isEnabledFor(logging.DEBUG):
                        logger.debug(""Building circuits:"")
                        TextProgressBar(sys.stderr)
                    circuits = parallel_map(QSVM._construct_circuit,
                                            to_be_computed_data_pair,
                                            task_args=(feature_map, measurement),
                                            num_processes=aqua_globals.num_processes)

                results = quantum_instance.execute(circuits,
                                                   had_transpiled=use_parameterized_circuits)

                if logger.isEnabledFor(logging.DEBUG):
                    logger.debug(""Calculating overlap:"")
                    TextProgressBar(sys.stderr)
                matrix_elements = parallel_map(QSVM._compute_overlap, range(len(circuits)),
                                               task_args=(results,
                                                          is_statevector_sim, measurement_basis),
                                               num_processes=aqua_globals.num_processes)

                for (i, j), value in zip(to_be_computed_index, matrix_elements):
                    mat[i, j] = value
                    if is_symmetric:
                        mat[j, i] = mat[i, j]

        return mat"
"    def optimize(self, num_vars, objective_function, gradient_function=None,
                 variable_bounds=None, initial_point=None):
        num_procs = multiprocessing.cpu_count() - 1
        num_procs = \\
            num_procs if self._max_processes is None else min(num_procs, self._max_processes)
        num_procs = num_procs if num_procs >= 0 else 0

        if platform.system() == 'Darwin':
            # Changed in version 3.8: On macOS, the spawn start method is now the
            # default. The fork start method should be considered unsafe as it can
            # lead to crashes.
            # However P_BFGS doesn't support spawn, so we revert to single process.
            major, minor, _ = platform.python_version_tuple()
            if major > '3' or (major == '3' and minor >= '8'):
                num_procs = 0
                logger.warning(""For MacOS, python >= 3.8, using only current process. ""
                               ""Multiple core use not supported."")
        elif platform.system() == 'Windows':
            num_procs = 0
            logger.warning(""For Windows, using only current process. ""
                           ""Multiple core use not supported."")

        queue = multiprocessing.Queue()
        # bounds for additional initial points in case bounds has any None values
        threshold = 2 * np.pi
        if variable_bounds is None:
            variable_bounds = [(-threshold, threshold)] * num_vars
        low = [(l if l is not None else -threshold) for (l, u) in variable_bounds]
        high = [(u if u is not None else threshold) for (l, u) in variable_bounds]

        def optimize_runner(_queue, _i_pt):  # Multi-process sampling
            _sol, _opt, _nfev = self._optimize(num_vars, objective_function,
                                               gradient_function, variable_bounds, _i_pt)
            _queue.put((_sol, _opt, _nfev))

        # Start off as many other processes running the optimize (can be 0)
        processes = []
        for _ in range(num_procs):
            i_pt = aqua_globals.random.uniform(low, high)  # Another random point in bounds
            p = multiprocessing.Process(target=optimize_runner, args=(queue, i_pt))
            processes.append(p)
            p.start()

        # While the one _optimize in this process below runs the other processes will
        # be running to. This one runs
        # with the supplied initial point. The process ones have their own random one
        sol, opt, nfev = self._optimize(num_vars, objective_function,
                                        gradient_function, variable_bounds, initial_point)

        for p in processes:
            # For each other process we wait now for it to finish and see if it has
            # a better result than above
            p.join()
            p_sol, p_opt, p_nfev = queue.get()
            if p_opt < opt:
                sol, opt = p_sol, p_opt
            nfev += p_nfev

        return sol, opt, nfev","    def optimize(self, num_vars, objective_function, gradient_function=None,
                 variable_bounds=None, initial_point=None):
        num_procs = multiprocessing.cpu_count() - 1
        num_procs = \\
            num_procs if self._max_processes is None else min(num_procs, self._max_processes)
        num_procs = num_procs if num_procs >= 0 else 0

        if platform.system() == ""Windows"":
            num_procs = 0
            logger.warning(""Using only current process. Multiple core use not supported in Windows"")

        queue = multiprocessing.Queue()
        # bounds for additional initial points in case bounds has any None values
        threshold = 2 * np.pi
        if variable_bounds is None:
            variable_bounds = [(-threshold, threshold)] * num_vars
        low = [(l if l is not None else -threshold) for (l, u) in variable_bounds]
        high = [(u if u is not None else threshold) for (l, u) in variable_bounds]

        def optimize_runner(_queue, _i_pt):  # Multi-process sampling
            _sol, _opt, _nfev = self._optimize(num_vars, objective_function,
                                               gradient_function, variable_bounds, _i_pt)
            _queue.put((_sol, _opt, _nfev))

        # Start off as many other processes running the optimize (can be 0)
        processes = []
        for _ in range(num_procs):
            i_pt = aqua_globals.random.uniform(low, high)  # Another random point in bounds
            p = multiprocessing.Process(target=optimize_runner, args=(queue, i_pt))
            processes.append(p)
            p.start()

        # While the one _optimize in this process below runs the other processes will
        # be running to. This one runs
        # with the supplied initial point. The process ones have their own random one
        sol, opt, nfev = self._optimize(num_vars, objective_function,
                                        gradient_function, variable_bounds, initial_point)

        for p in processes:
            # For each other process we wait now for it to finish and see if it has
            # a better result than above
            p.join()
            p_sol, p_opt, p_nfev = queue.get()
            if p_opt < opt:
                sol, opt = p_sol, p_opt
            nfev += p_nfev

        return sol, opt, nfev"
"    def solve(self, problem: QuadraticProgram) -> MinimumEigenOptimizerResult:
        """"""Tries to solves the given problem using the optimizer.

        Runs the optimizer to try to solve the optimization problem.

        Args:
            problem: The problem to be solved.

        Returns:
            The result of the optimizer applied to the problem.

        Raises:
            QiskitOptimizationError: If problem not compatible.
        """"""
        # check compatibility and raise exception if incompatible
        msg = self.get_compatibility_msg(problem)
        if len(msg) > 0:
            raise QiskitOptimizationError('Incompatible problem: {}'.format(msg))

        # convert problem to QUBO
        qubo_converter = QuadraticProgramToQubo()
        problem_ = qubo_converter.encode(problem)

        # construct operator and offset
        operator_converter = QuadraticProgramToIsing()
        operator, offset = operator_converter.encode(problem_)

        # only try to solve non-empty Ising Hamiltonians
        if operator.num_qubits > 0:

            # approximate ground state of operator using min eigen solver
            eigen_results = self._min_eigen_solver.compute_minimum_eigenvalue(operator)

            # analyze results
            samples = eigenvector_to_solutions(eigen_results.eigenstate, operator)
            samples = [(res[0], problem_.objective.sense.value * (res[1] + offset), res[2])
                       for res in samples]
            samples.sort(key=lambda x: problem_.objective.sense.value * x[1])
            x = samples[0][0]
            fval = samples[0][1]

        # if Hamiltonian is empty, then the objective function is constant to the offset
        else:
            x = [0]*problem_.get_num_binary_vars()
            fval = offset
            x_str = '0'*problem_.get_num_binary_vars()
            samples = [(x_str, offset, 1.0)]

        # translate result back to integers
        opt_res = MinimumEigenOptimizerResult(x, fval, samples, qubo_converter)
        opt_res = qubo_converter.decode(opt_res)

        # translate results back to original problem
        return opt_res","    def solve(self, problem: QuadraticProgram) -> MinimumEigenOptimizerResult:
        """"""Tries to solves the given problem using the optimizer.

        Runs the optimizer to try to solve the optimization problem.

        Args:
            problem: The problem to be solved.

        Returns:
            The result of the optimizer applied to the problem.

        Raises:
            QiskitOptimizationError: If problem not compatible.
        """"""
        # check compatibility and raise exception if incompatible
        msg = self.get_compatibility_msg(problem)
        if len(msg) > 0:
            raise QiskitOptimizationError('Incompatible problem: {}'.format(msg))

        # convert problem to QUBO
        qubo_converter = QuadraticProgramToQubo()
        problem_ = qubo_converter.encode(problem)

        # construct operator and offset
        operator_converter = QuadraticProgramToIsing()
        operator, offset = operator_converter.encode(problem_)

        # approximate ground state of operator using min eigen solver
        eigen_results = self._min_eigen_solver.compute_minimum_eigenvalue(operator)

        # analyze results
        samples = eigenvector_to_solutions(eigen_results.eigenstate, operator)
        samples = [(res[0], problem_.objective.sense.value * (res[1] + offset), res[2])
                   for res in samples]
        samples.sort(key=lambda x: problem_.objective.sense.value * x[1])

        # translate result back to integers
        opt_res = MinimumEigenOptimizerResult(samples[0][0], samples[0][1], samples, qubo_converter)
        opt_res = qubo_converter.decode(opt_res)

        # translate results back to original problem
        return opt_res"
"    def solve(self, problem: QuadraticProgram) -> OptimizationResult:
        """"""Tries to solve the given problem using the recursive optimizer.

        Runs the optimizer to try to solve the optimization problem.

        Args:
            problem: The problem to be solved.

        Returns:
            The result of the optimizer applied to the problem.

        Raises:
            QiskitOptimizationError: Incompatible problem.
            QiskitOptimizationError: Infeasible due to variable substitution
        """"""
        # check compatibility and raise exception if incompatible
        msg = self.get_compatibility_msg(problem)
        if len(msg) > 0:
            raise QiskitOptimizationError('Incompatible problem: {}'.format(msg))

        # convert problem to QUBO, this implicitly checks if the problem is compatible
        qubo_converter = QuadraticProgramToQubo()
        problem_ = qubo_converter.encode(problem)
        problem_ref = deepcopy(problem_)

        # run recursive optimization until the resulting problem is small enough
        replacements = {}
        while problem_.get_num_vars() > self._min_num_vars:

            # solve current problem with optimizer
            result = self._min_eigen_optimizer.solve(problem_)

            # analyze results to get strongest correlation
            correlations = result.get_correlations()
            i, j = self._find_strongest_correlation(correlations)

            x_i = problem_.variables[i].name
            x_j = problem_.variables[j].name
            if correlations[i, j] > 0:
                # set x_i = x_j
                problem_ = problem_.substitute_variables(variables={i: (j, 1)})
                if problem_.status == QuadraticProgram.Status.INFEASIBLE:
                    raise QiskitOptimizationError('Infeasible due to variable substitution')
                replacements[x_i] = (x_j, 1)
            else:
                # set x_i = 1 - x_j, this is done in two steps:
                # 1. set x_i = 1 + x_i
                # 2. set x_i = -x_j

                # 1a. get additional offset
                constant = problem_.objective.constant
                constant += problem_.objective.linear[i]
                constant += problem_.objective.quadratic[i, i]
                problem_.objective.constant = constant

                # 1b. get additional linear part
                for k in range(problem_.get_num_vars()):
                    coeff = problem_.objective.linear[k]
                    if k == i:
                        coeff += 2*problem_.objective.quadratic[i, k]
                    else:
                        coeff += problem_.objective.quadratic[i, k]

                    # set new coefficient if not too small
                    if np.abs(coeff) > 1e-10:
                        problem_.objective.linear[k] = coeff
                    else:
                        problem_.objective.linear[k] = 0

                # 2. replace x_i by -x_j
                problem_ = problem_.substitute_variables(variables={i: (j, -1)})
                if problem_.status == QuadraticProgram.Status.INFEASIBLE:
                    raise QiskitOptimizationError('Infeasible due to variable substitution')
                replacements[x_i] = (x_j, -1)

        # solve remaining problem
        result = self._min_num_vars_optimizer.solve(problem_)

        # unroll replacements
        var_values = {}
        for i, x in enumerate(problem_.variables):
            var_values[x.name] = result.x[i]

        def find_value(x, replacements, var_values):
            if x in var_values:
                # if value for variable is known, return it
                return var_values[x]
            elif x in replacements:
                # get replacement for variable
                (y, sgn) = replacements[x]
                # find details for replacing variable
                value = find_value(y, replacements, var_values)
                # construct, set, and return new value
                var_values[x] = value if sgn == 1 else 1 - value
                return var_values[x]
            else:
                raise QiskitOptimizationError('Invalid values!')

        # loop over all variables to set their values
        for x_i in problem_ref.variables:
            if x_i.name not in var_values:
                find_value(x_i.name, replacements, var_values)

        # construct result
        x = [var_values[x_aux.name] for x_aux in problem_ref.variables]
        fval = result.fval
        results = OptimizationResult(x, fval, (replacements, qubo_converter))
        results = qubo_converter.decode(results)
        return results","    def solve(self, problem: QuadraticProgram) -> OptimizationResult:
        """"""Tries to solve the given problem using the recursive optimizer.

        Runs the optimizer to try to solve the optimization problem.

        Args:
            problem: The problem to be solved.

        Returns:
            The result of the optimizer applied to the problem.

        Raises:
            QiskitOptimizationError: Incompatible problem.
            QiskitOptimizationError: Infeasible due to variable substitution
        """"""
        # check compatibility and raise exception if incompatible
        msg = self.get_compatibility_msg(problem)
        if len(msg) > 0:
            raise QiskitOptimizationError('Incompatible problem: {}'.format(msg))

        # convert problem to QUBO, this implicitly checks if the problem is compatible
        qubo_converter = QuadraticProgramToQubo()
        problem_ = qubo_converter.encode(problem)
        problem_ref = deepcopy(problem_)

        # run recursive optimization until the resulting problem is small enough
        replacements = {}
        while problem_.get_num_vars() > self._min_num_vars:

            # solve current problem with optimizer
            result = self._min_eigen_optimizer.solve(problem_)

            # analyze results to get strongest correlation
            correlations = result.get_correlations()
            i, j = self._find_strongest_correlation(correlations)

            x_i = problem_.variables[i].name
            x_j = problem_.variables[j].name
            if correlations[i, j] > 0:
                # set x_i = x_j
                problem_.substitute_variables()
                problem_ = problem_.substitute_variables(variables={i: (j, 1)})
                if problem_.status == QuadraticProgram.Status.INFEASIBLE:
                    raise QiskitOptimizationError('Infeasible due to variable substitution')
                replacements[x_i] = (x_j, 1)
            else:
                # set x_i = 1 - x_j, this is done in two steps:
                # 1. set x_i = 1 + x_i
                # 2. set x_i = -x_j

                # 1a. get additional offset
                constant = problem_.objective.constant
                constant += problem_.objective.quadratic[i, i]
                constant += problem_.objective.linear[i]
                problem_.objective.constant = constant

                # 1b. get additional linear part
                for k in range(problem_.get_num_vars()):
                    coeff = problem_.objective.quadratic[i, k]
                    if np.abs(coeff) > 1e-10:
                        coeff += problem_.objective.linear[k]
                        problem_.objective.linear[k] = coeff

                # 2. replace x_i by -x_j
                problem_ = problem_.substitute_variables(variables={i: (j, -1)})
                if problem_.status == QuadraticProgram.Status.INFEASIBLE:
                    raise QiskitOptimizationError('Infeasible due to variable substitution')
                replacements[x_i] = (x_j, -1)

        # solve remaining problem
        result = self._min_num_vars_optimizer.solve(problem_)

        # unroll replacements
        var_values = {}
        for i, x in enumerate(problem_.variables):
            var_values[x.name] = result.x[i]

        def find_value(x, replacements, var_values):
            if x in var_values:
                # if value for variable is known, return it
                return var_values[x]
            elif x in replacements:
                # get replacement for variable
                (y, sgn) = replacements[x]
                # find details for replacing variable
                value = find_value(y, replacements, var_values)
                # construct, set, and return new value
                var_values[x] = value if sgn == 1 else 1 - value
                return var_values[x]
            else:
                raise QiskitOptimizationError('Invalid values!')

        # loop over all variables to set their values
        for x_i in problem_ref.variables:
            if x_i.name not in var_values:
                find_value(x_i.name, replacements, var_values)

        # construct result
        x = [var_values[x_aux.name] for x_aux in problem_ref.variables]
        fval = result.fval
        results = OptimizationResult(x, fval, (replacements, qubo_converter))
        results = qubo_converter.decode(results)
        return results"
"    def _find_strongest_correlation(self, correlations):

        # get absolute values and set diagonal to -1 to make sure maximum is always on off-diagonal
        abs_correlations = np.abs(correlations)
        for i in range(len(correlations)):
            abs_correlations[i, i] = -1

        # get index of maximum (by construction on off-diagonal)
        m_max = np.argmax(abs_correlations.flatten())

        # translate back to indices
        i = int(m_max // len(correlations))
        j = int(m_max - i*len(correlations))
        return (i, j)","    def _find_strongest_correlation(self, correlations):
        m_max = np.argmax(np.abs(correlations.flatten()))
        i = int(m_max // len(correlations))
        j = int(m_max - i*len(correlations))
        return (i, j)"
"def _livereload(host, port, config, builder, site_dir):

    # We are importing here for anyone that has issues with livereload. Even if
    # this fails, the --no-livereload alternative should still work.
    _init_asyncio_patch()
    from livereload import Server
    import livereload.handlers

    class LiveReloadServer(Server):

        def get_web_handlers(self, script):
            handlers = super(LiveReloadServer, self).get_web_handlers(script)
            # replace livereload handler
            return [(handlers[0][0], _get_handler(site_dir, livereload.handlers.StaticFileHandler), handlers[0][2],)]

    server = LiveReloadServer()

    # Watch the documentation files, the config file and the theme files.
    server.watch(config['docs_dir'], builder)
    server.watch(config['config_file_path'], builder)

    for d in config['theme'].dirs:
        server.watch(d, builder)

    # Run `serve` plugin events.
    server = config['plugins'].run_event('serve', server, config=config)

    server.serve(root=site_dir, host=host, port=port, restart_delay=0)","def _livereload(host, port, config, builder, site_dir):

    # We are importing here for anyone that has issues with livereload. Even if
    # this fails, the --no-livereload alternative should still work.
    from livereload import Server
    import livereload.handlers

    class LiveReloadServer(Server):

        def get_web_handlers(self, script):
            handlers = super(LiveReloadServer, self).get_web_handlers(script)
            # replace livereload handler
            return [(handlers[0][0], _get_handler(site_dir, livereload.handlers.StaticFileHandler), handlers[0][2],)]

    server = LiveReloadServer()

    # Watch the documentation files, the config file and the theme files.
    server.watch(config['docs_dir'], builder)
    server.watch(config['config_file_path'], builder)

    for d in config['theme'].dirs:
        server.watch(d, builder)

    # Run `serve` plugin events.
    server = config['plugins'].run_event('serve', server, config=config)

    server.serve(root=site_dir, host=host, port=port, restart_delay=0)"
"def _static_server(host, port, site_dir):

    # Importing here to seperate the code paths from the --livereload
    # alternative.
    _init_asyncio_patch()
    from tornado import ioloop
    from tornado import web

    application = web.Application([
        (r""/(.*)"", _get_handler(site_dir, web.StaticFileHandler), {
            ""path"": site_dir,
            ""default_filename"": ""index.html""
        }),
    ])
    application.listen(port=port, address=host)

    log.info('Running at: http://%s:%s/', host, port)
    log.info('Hold ctrl+c to quit.')
    try:
        ioloop.IOLoop.instance().start()
    except KeyboardInterrupt:
        log.info('Stopping server...')","def _static_server(host, port, site_dir):

    # Importing here to seperate the code paths from the --livereload
    # alternative.
    from tornado import ioloop
    from tornado import web

    application = web.Application([
        (r""/(.*)"", _get_handler(site_dir, web.StaticFileHandler), {
            ""path"": site_dir,
            ""default_filename"": ""index.html""
        }),
    ])
    application.listen(port=port, address=host)

    log.info('Running at: http://%s:%s/', host, port)
    log.info('Hold ctrl+c to quit.')
    try:
        ioloop.IOLoop.instance().start()
    except KeyboardInterrupt:
        log.info('Stopping server...')"
"def serve(config_file=None, dev_addr=None, strict=None, theme=None,
          theme_dir=None, livereload='livereload'):
    """"""
    Start the MkDocs development server

    By default it will serve the documentation on http://localhost:8000/ and
    it will rebuild the documentation and refresh the page automatically
    whenever a file is edited.
    """"""

    # Create a temporary build directory, and set some options to serve it
    # PY2 returns a byte string by default. The Unicode prefix ensures a Unicode
    # string is returned. And it makes MkDocs temp dirs easier to identify.
    site_dir = tempfile.mkdtemp(prefix='mkdocs_')

    def builder():
        log.info(""Building documentation..."")
        config = load_config(
            config_file=config_file,
            dev_addr=dev_addr,
            strict=strict,
            theme=theme,
            theme_dir=theme_dir,
            site_dir=site_dir
        )
        # Override a few config settings after validation
        config['site_url'] = 'http://{0}/'.format(config['dev_addr'])

        live_server = livereload in ['dirty', 'livereload']
        dirty = livereload == 'dirty'
        build(config, live_server=live_server, dirty=dirty)
        return config

    try:
        # Perform the initial build
        config = builder()

        host, port = config['dev_addr']

        if livereload in ['livereload', 'dirty']:
            _livereload(host, port, config, builder, site_dir)
        else:
            _static_server(host, port, site_dir)
    finally:
        shutil.rmtree(site_dir)","def serve(config_file=None, dev_addr=None, strict=None, theme=None,
          theme_dir=None, livereload='livereload'):
    """"""
    Start the MkDocs development server

    By default it will serve the documentation on http://localhost:8000/ and
    it will rebuild the documentation and refresh the page automatically
    whenever a file is edited.
    """"""

    # Create a temporary build directory, and set some options to serve it
    tempdir = tempfile.mkdtemp()

    def builder():
        log.info(""Building documentation..."")
        config = load_config(
            config_file=config_file,
            dev_addr=dev_addr,
            strict=strict,
            theme=theme,
            theme_dir=theme_dir
        )
        # Override a few config settings after validation
        config['site_dir'] = tempdir
        config['site_url'] = 'http://{0}/'.format(config['dev_addr'])

        live_server = livereload in ['dirty', 'livereload']
        dirty = livereload == 'dirty'
        build(config, live_server=live_server, dirty=dirty)
        return config

    try:
        # Perform the initial build
        config = builder()

        host, port = config['dev_addr']

        if livereload in ['livereload', 'dirty']:
            _livereload(host, port, config, builder, tempdir)
        else:
            _static_server(host, port, tempdir)
    finally:
        shutil.rmtree(tempdir)"
"    def builder():
        log.info(""Building documentation..."")
        config = load_config(
            config_file=config_file,
            dev_addr=dev_addr,
            strict=strict,
            theme=theme,
            theme_dir=theme_dir,
            site_dir=site_dir
        )
        # Override a few config settings after validation
        config['site_url'] = 'http://{0}/'.format(config['dev_addr'])

        live_server = livereload in ['dirty', 'livereload']
        dirty = livereload == 'dirty'
        build(config, live_server=live_server, dirty=dirty)
        return config","    def builder():
        log.info(""Building documentation..."")
        config = load_config(
            config_file=config_file,
            dev_addr=dev_addr,
            strict=strict,
            theme=theme,
            theme_dir=theme_dir
        )
        # Override a few config settings after validation
        config['site_dir'] = tempdir
        config['site_url'] = 'http://{0}/'.format(config['dev_addr'])

        live_server = livereload in ['dirty', 'livereload']
        dirty = livereload == 'dirty'
        build(config, live_server=live_server, dirty=dirty)
        return config"
"def copy_file(source_path, output_path):
    """"""
    Copy source_path to output_path, making sure any parent directories exist.

    The output_path may be a directory.
    """"""
    output_dir = os.path.dirname(output_path)
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    if os.path.isdir(output_path):
        output_path = os.path.join(output_path, os.path.basename(source_path))
    shutil.copyfile(source_path, output_path)","def copy_file(source_path, output_path):
    """"""
    Copy source_path to output_path, making sure any parent directories exist.
    """"""

    output_dir = os.path.dirname(output_path)
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    shutil.copy(source_path, output_path)"
"def _livereload(host, port, config, builder, site_dir):

    # We are importing here for anyone that has issues with livereload. Even if
    # this fails, the --no-livereload alternative should still work.
    from livereload import Server
    import livereload.handlers

    class LiveReloadServer(Server):

        def get_web_handlers(self, script):
            handlers = super(LiveReloadServer, self).get_web_handlers(script)
            # replace livereload handler
            return [(handlers[0][0], _get_handler(site_dir, livereload.handlers.StaticFileHandler), handlers[0][2],)]

    server = LiveReloadServer()

    # Watch the documentation files, the config file and the theme files.
    server.watch(config['docs_dir'], builder)
    server.watch(config['config_file_path'], builder)

    for d in config['theme_dir']:
        server.watch(d, builder)

    server.serve(root=site_dir, host=host, port=port, restart_delay=0)","def _livereload(host, port, config, builder, site_dir):

    # We are importing here for anyone that has issues with livereload. Even if
    # this fails, the --no-livereload alternative should still work.
    from livereload import Server
    import livereload.handlers

    class LiveReloadServer(Server):

        def get_web_handlers(self, script):
            handlers = super(LiveReloadServer, self).get_web_handlers(script)
            # replace livereload handler
            return [(handlers[0][0], _get_handler(site_dir, livereload.handlers.StaticFileHandler), handlers[0][2],)]

    server = LiveReloadServer()

    # Watch the documentation files, the config file and the theme files.
    server.watch(config['docs_dir'], builder)
    server.watch(config['config_file_path'], builder)

    for d in config['theme_dir']:
        server.watch(d, builder)

    server.serve(root=site_dir, host=host, port=int(port), restart_delay=0)"
"def _static_server(host, port, site_dir):

    # Importing here to seperate the code paths from the --livereload
    # alternative.
    from tornado import ioloop
    from tornado import web

    application = web.Application([
        (r""/(.*)"", _get_handler(site_dir, web.StaticFileHandler), {
            ""path"": site_dir,
            ""default_filename"": ""index.html""
        }),
    ])
    application.listen(port=port, address=host)

    log.info('Running at: http://%s:%s/', host, port)
    log.info('Hold ctrl+c to quit.')
    try:
        ioloop.IOLoop.instance().start()
    except KeyboardInterrupt:
        log.info('Stopping server...')","def _static_server(host, port, site_dir):

    # Importing here to seperate the code paths from the --livereload
    # alternative.
    from tornado import ioloop
    from tornado import web

    application = web.Application([
        (r""/(.*)"", _get_handler(site_dir, web.StaticFileHandler), {
            ""path"": site_dir,
            ""default_filename"": ""index.html""
        }),
    ])
    application.listen(port=int(port), address=host)

    log.info('Running at: http://%s:%s/', host, port)
    log.info('Hold ctrl+c to quit.')
    try:
        ioloop.IOLoop.instance().start()
    except KeyboardInterrupt:
        log.info('Stopping server...')"
"def serve(config_file=None, dev_addr=None, strict=None, theme=None,
          theme_dir=None, livereload='livereload'):
    """"""
    Start the MkDocs development server

    By default it will serve the documentation on http://localhost:8000/ and
    it will rebuild the documentation and refresh the page automatically
    whenever a file is edited.
    """"""

    # Create a temporary build directory, and set some options to serve it
    tempdir = tempfile.mkdtemp()

    def builder():
        log.info(""Building documentation..."")
        config = load_config(
            config_file=config_file,
            dev_addr=dev_addr,
            strict=strict,
            theme=theme,
            theme_dir=theme_dir
        )
        config['site_dir'] = tempdir
        live_server = livereload in ['dirty', 'livereload']
        dirty = livereload == 'dirty'
        build(config, live_server=live_server, dirty=dirty)
        return config

    try:
        # Perform the initial build
        config = builder()

        host, port = config['dev_addr']

        if livereload in ['livereload', 'dirty']:
            _livereload(host, port, config, builder, tempdir)
        else:
            _static_server(host, port, tempdir)
    finally:
        shutil.rmtree(tempdir)","def serve(config_file=None, dev_addr=None, strict=None, theme=None,
          theme_dir=None, livereload='livereload'):
    """"""
    Start the MkDocs development server

    By default it will serve the documentation on http://localhost:8000/ and
    it will rebuild the documentation and refresh the page automatically
    whenever a file is edited.
    """"""

    # Create a temporary build directory, and set some options to serve it
    tempdir = tempfile.mkdtemp()

    def builder():
        log.info(""Building documentation..."")
        config = load_config(
            config_file=config_file,
            dev_addr=dev_addr,
            strict=strict,
            theme=theme,
            theme_dir=theme_dir
        )
        config['site_dir'] = tempdir
        live_server = livereload in ['dirty', 'livereload']
        dirty = livereload == 'dirty'
        build(config, live_server=live_server, dirty=dirty)
        return config

    # Perform the initial build
    config = builder()

    host, port = config['dev_addr'].split(':', 1)

    try:
        if livereload in ['livereload', 'dirty']:
            _livereload(host, port, config, builder, tempdir)
        else:
            _static_server(host, port, tempdir)
    finally:
        shutil.rmtree(tempdir)"
"def try_rebase(remote, branch):
    cmd = ['git', 'rev-list', '--max-count=1', '%s/%s' % (remote, branch)]
    p = sp.Popen(cmd, stdin=sp.PIPE, stdout=sp.PIPE, stderr=sp.PIPE)
    (rev, _) = p.communicate()
    if p.wait() != 0:
        return True
    cmd = ['git', 'update-ref', 'refs/heads/%s' % branch, dec(rev.strip())]
    if sp.call(cmd) != 0:
        return False
    return True","def try_rebase(remote, branch):
    cmd = ['git', 'rev-list', '--max-count=1', '%s/%s' % (remote, branch)]
    p = sp.Popen(cmd, stdin=sp.PIPE, stdout=sp.PIPE, stderr=sp.PIPE)
    (rev, _) = p.communicate()
    if p.wait() != 0:
        return True
    cmd = ['git', 'update-ref', 'refs/heads/%s' % branch, rev.strip()]
    if sp.call(cmd) != 0:
        return False
    return True"
"def path_to_url(path):
    """"""Convert a system path to a URL.""""""

    if os.path.sep == '/':
        return path

    if sys.version_info < (3, 0):
        path = path.encode('utf8')
    return pathname2url(path)","def path_to_url(path):
    """"""Convert a system path to a URL.""""""

    if os.path.sep == '/':
        return path

    return pathname2url(path)"
"    def walk_docs_dir(self, docs_dir):

        if self.file_match is None:
            raise StopIteration

        for (dirpath, dirs, filenames) in os.walk(docs_dir):
            dirs.sort()
            for filename in sorted(filenames):
                fullpath = os.path.join(dirpath, filename)

                # Some editors (namely Emacs) will create temporary symlinks
                # for internal magic. We can just ignore these files.
                if os.path.islink(fullpath):
                    if not os.path.exists(os.readlink(fullpath)):
                        continue

                relpath = os.path.normpath(os.path.relpath(fullpath, docs_dir))
                if self.file_match(relpath):
                    yield relpath","    def walk_docs_dir(self, docs_dir):

        if self.file_match is None:
            raise StopIteration

        for (dirpath, dirs, filenames) in os.walk(docs_dir):
            dirs.sort()
            for filename in sorted(filenames):
                fullpath = os.path.join(dirpath, filename)
                relpath = os.path.normpath(os.path.relpath(fullpath, docs_dir))
                if self.file_match(relpath):
                    yield relpath"
"    def walk_docs_dir(self, docs_dir):

        if self.file_match is None:
            raise StopIteration

        for (dirpath, dirs, filenames) in os.walk(docs_dir):
            dirs.sort()
            for filename in sorted(filenames):
                fullpath = os.path.join(dirpath, filename)

                # Some editors (namely Emacs) will create temporary symlinks
                # for internal magic. We can just ignore these files.
                if os.path.islink(fullpath):
                    fp = os.path.join(dirpath, os.readlink(fullpath))
                    if not os.path.exists(fp):
                        continue

                relpath = os.path.normpath(os.path.relpath(fullpath, docs_dir))
                if self.file_match(relpath):
                    yield relpath","    def walk_docs_dir(self, docs_dir):

        if self.file_match is None:
            raise StopIteration

        for (dirpath, dirs, filenames) in os.walk(docs_dir):
            dirs.sort()
            for filename in sorted(filenames):
                fullpath = os.path.join(dirpath, filename)

                # Some editors (namely Emacs) will create temporary symlinks
                # for internal magic. We can just ignore these files.
                if os.path.islink(fullpath):
                    if not os.path.exists(os.readlink(fullpath)):
                        continue

                relpath = os.path.normpath(os.path.relpath(fullpath, docs_dir))
                if self.file_match(relpath):
                    yield relpath"
"    def __init__(self, file_match=None, **kwargs):
        super(Extras, self).__init__(**kwargs)
        self.file_match = file_match","    def __init__(self, file_match, **kwargs):
        super(Extras, self).__init__(**kwargs)
        self.file_match = file_match"
"    def walk_docs_dir(self, docs_dir):

        if self.file_match is None:
            raise StopIteration

        for (dirpath, _, filenames) in os.walk(docs_dir):
            for filename in sorted(filenames):
                fullpath = os.path.join(dirpath, filename)
                relpath = os.path.normpath(os.path.relpath(fullpath, docs_dir))
                if self.file_match(relpath):
                    yield relpath","    def walk_docs_dir(self, docs_dir):
        for (dirpath, _, filenames) in os.walk(docs_dir):
            for filename in sorted(filenames):
                fullpath = os.path.join(dirpath, filename)
                relpath = os.path.normpath(os.path.relpath(fullpath, docs_dir))
                if self.file_match(relpath):
                    yield relpath"
"    def run_validation(self, value):

        if not isinstance(value, list):
            raise ValidationError(
                ""Expected a list, got {0}"".format(type(value)))

        if len(value) == 0:
            return

        # TODO: Remove in 1.0
        config_types = set(type(l) for l in value)

        if config_types.issubset(set([six.text_type, dict, ])):
            return value

        if config_types.issubset(set([six.text_type, list, ])):
            return legacy.pages_compat_shim(value)

        raise ValidationError(""Invalid pages config. {0} {1}"".format(
            config_types,
            set([six.text_type, dict, ])
        ))","    def run_validation(self, value):

        if not isinstance(value, list):
            raise ValidationError(
                ""Expected a list, got {0}"".format(type(value)))

        if len(value) == 0:
            return

        # TODO: Remove in 1.0
        config_types = set(type(l) for l in value)

        if config_types.issubset(set([str, dict, ])):
            return value

        if config_types.issubset(set([str, list, ])):
            return legacy.pages_compat_shim(value)

        raise ValidationError(""Invalid pages config."")"
"def _follow(config_line, url_context, use_dir_urls, header=None, title=None):

    if isinstance(config_line, six.string_types):
        path = os.path.normpath(config_line)
        page = _path_to_page(path, title, url_context, use_dir_urls)

        if header:
            page.ancestors = [header]
            header.children.append(page)

        yield page
        raise StopIteration

    elif not isinstance(config_line, dict):
        msg = (""Line in 'page' config is of type {0}, dict or string ""
               ""expected. Config: {1}"").format(type(config_line), config_line)
        raise exceptions.ConfigurationError(msg)

    if len(config_line) > 1:
        raise exceptions.ConfigurationError(
            ""Page configs should be in the format 'name: markdown.md'. The ""
            ""config contains an invalid entry: {0}"".format(config_line))
    elif len(config_line) == 0:
        log.warning(""Ignoring empty line in the pages config."")
        raise StopIteration

    next_cat_or_title, subpages_or_path = next(iter(config_line.items()))

    if isinstance(subpages_or_path, six.string_types):
        path = subpages_or_path
        for sub in _follow(path, url_context, use_dir_urls, header=header, title=next_cat_or_title):
            yield sub
        raise StopIteration

    elif not isinstance(subpages_or_path, list):
        msg = (""Line in 'page' config is of type {0}, list or string ""
               ""expected for sub pages. Config: {1}""
               ).format(type(config_line), config_line)
        raise exceptions.ConfigurationError(msg)

    next_header = Header(title=next_cat_or_title, children=[])
    if header:
        next_header.ancestors = [header]
        header.children.append(next_header)
    yield next_header

    subpages = subpages_or_path

    for subpage in subpages:
        for sub in _follow(subpage, url_context, use_dir_urls, next_header):
            yield sub","def _follow(config_line, url_context, use_dir_urls, header=None, title=None):

    if isinstance(config_line, str):
        path = os.path.normpath(config_line)
        page = _path_to_page(path, title, url_context, use_dir_urls)

        if header:
            page.ancestors = [header]
            header.children.append(page)

        yield page
        raise StopIteration

    elif not isinstance(config_line, dict):
        msg = (""Line in 'page' config is of type {0}, dict or string ""
               ""expected. Config: {1}"").format(type(config_line), config_line)
        raise exceptions.ConfigurationError(msg)

    if len(config_line) > 1:
        raise exceptions.ConfigurationError(
            ""Page configs should be in the format 'name: markdown.md'. The ""
            ""config contains an invalid entry: {0}"".format(config_line))
    elif len(config_line) == 0:
        log.warning(""Ignoring empty line in the pages config."")
        raise StopIteration

    next_cat_or_title, subpages_or_path = next(iter(config_line.items()))

    if isinstance(subpages_or_path, str):
        path = subpages_or_path
        for sub in _follow(path, url_context, use_dir_urls, header=header, title=next_cat_or_title):
            yield sub
        raise StopIteration

    elif not isinstance(subpages_or_path, list):
        msg = (""Line in 'page' config is of type {0}, list or string ""
               ""expected for sub pages. Config: {1}""
               ).format(type(config_line), config_line)
        raise exceptions.ConfigurationError(msg)

    next_header = Header(title=next_cat_or_title, children=[])
    if header:
        next_header.ancestors = [header]
        header.children.append(next_header)
    yield next_header

    subpages = subpages_or_path

    for subpage in subpages:
        for sub in _follow(subpage, url_context, use_dir_urls, next_header):
            yield sub"
"def _generate_site_navigation(pages_config, url_context, use_directory_urls=True):
    """"""
    Returns a list of Page and Header instances that represent the
    top level site navigation.
    """"""
    nav_items = []
    pages = []
    previous = None

    for config_line in pages_config:
        if isinstance(config_line, str):
            path = os.path.normpath(config_line)
            title, child_title = None, None
        elif len(config_line) in (1, 2, 3):
            # Pad any items that don't exist with 'None'
            padded_config = (list(config_line) + [None, None])[:3]
            path, title, child_title = padded_config
            path = os.path.normpath(path)
        else:
            msg = (
                ""Line in 'page' config contained %d items.  ""
                ""Expected 1, 2 or 3 strings."" % len(config_line)
            )
            raise exceptions.ConfigurationError(msg)

        # If both the title and child_title are None, then we
        # have just been given a path. If that path contains a /
        # then lets automatically nest it.
        if title is None and child_title is None and os.path.sep in path:
            filename = path.split(os.path.sep)[-1]
            child_title = filename_to_title(filename)

        if title is None:
            filename = path.split(os.path.sep)[0]
            title = filename_to_title(filename)

        # If we don't have a child title but the other title is the same, we
        # should be within a section and the child title needs to be inferred
        # from the filename.
        if len(nav_items) and title == nav_items[-1].title == title and child_title is None:
            filename = path.split(os.path.sep)[-1]
            child_title = filename_to_title(filename)

        url = utils.get_url_path(path, use_directory_urls)

        if not child_title:
            # New top level page.
            page = Page(title=title, url=url, path=path, url_context=url_context)
            nav_items.append(page)
        elif not nav_items or (nav_items[-1].title != title):
            # New second level page.
            page = Page(title=child_title, url=url, path=path, url_context=url_context)
            header = Header(title=title, children=[page])
            nav_items.append(header)
            page.ancestors = [header]
        else:
            # Additional second level page.
            page = Page(title=child_title, url=url, path=path, url_context=url_context)
            header = nav_items[-1]
            header.children.append(page)
            page.ancestors = [header]

        # Add in previous and next information.
        if previous:
            page.previous_page = previous
            previous.next_page = page
        previous = page

        pages.append(page)

    return (nav_items, pages)","def _generate_site_navigation(pages_config, url_context, use_directory_urls=True):
    """"""
    Returns a list of Page and Header instances that represent the
    top level site navigation.
    """"""
    nav_items = []
    pages = []
    previous = None

    for config_line in pages_config:
        if isinstance(config_line, str):
            path = os.path.normpath(config_line)
            title, child_title = None, None
        elif len(config_line) in (1, 2, 3):
            # Pad any items that don't exist with 'None'
            padded_config = (list(config_line) + [None, None])[:3]
            path, title, child_title = padded_config
            path = os.path.normpath(path)
        else:
            msg = (
                ""Line in 'page' config contained %d items.  ""
                ""Expected 1, 2 or 3 strings."" % len(config_line)
            )
            raise exceptions.ConfigurationError(msg)

        # If both the title and child_title are None, then we
        # have just been given a path. If that path contains a /
        # then lets automatically nest it.
        if title is None and child_title is None and os.path.sep in path:
            filename = path.split(os.path.sep)[-1]
            child_title = filename_to_title(filename)

        if title is None:
            filename = path.split(os.path.sep)[0]
            title = filename_to_title(filename)

        url = utils.get_url_path(path, use_directory_urls)

        if not child_title:
            # New top level page.
            page = Page(title=title, url=url, path=path, url_context=url_context)
            nav_items.append(page)
        elif not nav_items or (nav_items[-1].title != title):
            # New second level page.
            page = Page(title=child_title, url=url, path=path, url_context=url_context)
            header = Header(title=title, children=[page])
            nav_items.append(header)
            page.ancestors = [header]
        else:
            # Additional second level page.
            page = Page(title=child_title, url=url, path=path, url_context=url_context)
            header = nav_items[-1]
            header.children.append(page)
            page.ancestors = [header]

        # Add in previous and next information.
        if previous:
            page.previous_page = previous
            previous.next_page = page
        previous = page

        pages.append(page)

    return (nav_items, pages)"
"def convert_markdown(markdown_source, site_navigation=None, extensions=(), strict=False):
    """"""
    Convert the Markdown source file to HTML content, and additionally
    return the parsed table of contents, and a dictionary of any metadata
    that was specified in the Markdown file.

    `extensions` is an optional sequence of Python Markdown extensions to add
    to the default set.
    """"""

    # Generate the HTML from the markdown source
    builtin_extensions = ['meta', 'toc', 'tables', 'fenced_code']
    mkdocs_extensions = [RelativePathExtension(site_navigation, strict), ]
    extensions = builtin_extensions + mkdocs_extensions + list(extensions)
    md = markdown.Markdown(
        extensions=extensions
    )
    html_content = md.convert(markdown_source)

    # On completely blank markdown files, no Meta or tox properties are added
    # to the generated document.
    meta = getattr(md, 'Meta', {})
    toc_html = getattr(md, 'toc', '')

    # Post process the generated table of contents into a data structure
    table_of_contents = toc.TableOfContents(toc_html)

    return (html_content, table_of_contents, meta)","def convert_markdown(markdown_source, site_navigation=None, extensions=(), strict=False):
    """"""
    Convert the Markdown source file to HTML content, and additionally
    return the parsed table of contents, and a dictionary of any metadata
    that was specified in the Markdown file.

    `extensions` is an optional sequence of Python Markdown extensions to add
    to the default set.
    """"""

    # Generate the HTML from the markdown source
    builtin_extensions = ['meta', 'toc', 'tables', 'fenced_code']
    mkdocs_extensions = [RelativePathExtension(site_navigation, strict), ]
    extensions = builtin_extensions + mkdocs_extensions + list(extensions)
    md = markdown.Markdown(
        extensions=extensions
    )
    html_content = md.convert(markdown_source)
    meta = md.Meta
    toc_html = md.toc

    # Post process the generated table of contents into a data structure
    table_of_contents = toc.TableOfContents(toc_html)

    return (html_content, table_of_contents, meta)"
"def serve(config, options=None):
    """"""
    Start the devserver, and rebuild the docs whenever any changes take effect.
    """"""
    # Create a temporary build directory, and set some options to serve it
    tempdir = tempfile.mkdtemp()
    options['site_dir'] = tempdir

    # Only use user-friendly URLs when running the live server
    options['use_directory_urls'] = True

    # Perform the initial build
    config = load_config(options=options)
    build(config, live_server=True)

    # Note: We pass any command-line options through so that we
    #       can re-apply them if the config file is reloaded.
    event_handler = BuildEventHandler(options)
    config_event_handler = ConfigEventHandler(options)

    # We could have used `Observer()`, which can be faster, but
    # `PollingObserver()` works more universally.
    observer = PollingObserver()
    observer.schedule(event_handler, config['docs_dir'], recursive=True)
    for theme_dir in config['theme_dir']:
        if not os.path.exists(theme_dir):
            continue
        observer.schedule(event_handler, theme_dir, recursive=True)
    observer.schedule(config_event_handler, '.')
    observer.start()

    class TCPServer(socketserver.TCPServer):
        allow_reuse_address = True

    class DocsDirectoryHandler(FixedDirectoryHandler):
        base_dir = config['site_dir']

    host, port = config['dev_addr'].split(':', 1)
    server = TCPServer((host, int(port)), DocsDirectoryHandler)

    print('Running at: http://%s:%s/' % (host, port))
    print('Live reload enabled.')
    print('Hold ctrl+c to quit.')
    try:
        server.serve_forever()
    except KeyboardInterrupt:
        print('Stopping server...')

    # Clean up
    observer.stop()
    observer.join()
    shutil.rmtree(tempdir)
    print('Quit complete')","def serve(config, options=None):
    """"""
    Start the devserver, and rebuild the docs whenever any changes take effect.
    """"""
    # Create a temporary build directory, and set some options to serve it
    tempdir = tempfile.mkdtemp()
    options['site_dir'] = tempdir

    # Only use user-friendly URLs when running the live server
    options['use_directory_urls'] = True

    # Perform the initial build
    config = load_config(options=options)
    build(config, live_server=True)

    # Note: We pass any command-line options through so that we
    #       can re-apply them if the config file is reloaded.
    event_handler = BuildEventHandler(options)
    config_event_handler = ConfigEventHandler(options)

    # We could have used `Observer()`, which can be faster, but
    # `PollingObserver()` works more universally.
    observer = PollingObserver()
    observer.schedule(event_handler, config['docs_dir'], recursive=True)
    for theme_dir in config['theme_dir']:
        observer.schedule(event_handler, theme_dir, recursive=True)
    observer.schedule(config_event_handler, '.')
    observer.start()

    class TCPServer(socketserver.TCPServer):
        allow_reuse_address = True

    class DocsDirectoryHandler(FixedDirectoryHandler):
        base_dir = config['site_dir']

    host, port = config['dev_addr'].split(':', 1)
    server = TCPServer((host, int(port)), DocsDirectoryHandler)

    print('Running at: http://%s:%s/' % (host, port))
    print('Live reload enabled.')
    print('Hold ctrl+c to quit.')
    try:
        server.serve_forever()
    except KeyboardInterrupt:
        print('Stopping server...')

    # Clean up
    observer.stop()
    observer.join()
    shutil.rmtree(tempdir)
    print('Quit complete')"
"def load_config(filename='mkdocs.yml', options=None):
    user_config = options or {}
    if 'config' in user_config:
        filename = user_config.pop('config')
    if not os.path.exists(filename):
        raise ConfigurationError(""Config file '%s' does not exist."" % filename)
    with open(filename, 'r') as fp:
        local_config = yaml.load(fp)
    if local_config:
        user_config.update(local_config)
    return validate_config(user_config)","def load_config(filename='mkdocs.yml', options=None):
    options = options or {}
    if 'config' in options:
        filename = options['config']
    if not os.path.exists(filename):
        raise ConfigurationError(""Config file '%s' does not exist."" % filename)
    with open(filename, 'r') as fp:
        user_config = yaml.load(fp)
    user_config.update(options)
    return validate_config(user_config)"
"def base(net: network.TensorNetwork,
         algorithm: Callable[[List[Set[int]], Set[int], Dict[int, int]],
                             List]) -> network.TensorNetwork:
  """"""Base method for all `opt_einsum` contractors.

  Args:
    net: a TensorNetwork object. Should be connected.
    algorithm: `opt_einsum` contraction method to use.

  Returns:
    The network after full contraction.
  """"""
  net.check_connected()
  # First contract all trace edges
  edges = net.get_all_nondangling()
  for edge in edges:
    if edge in net and edge.is_trace():
      net.contract_parallel(edge)
  if not net.get_all_nondangling():
    # There's nothing to contract.
    return net

  # Then apply `opt_einsum`'s algorithm
  nodes = sorted(net.nodes_set)
  input_sets = utils.get_input_sets(net)
  output_set = utils.get_output_set(net)
  size_dict = utils.get_size_dict(net)
  path = algorithm(input_sets, output_set, size_dict)
  for a, b in path:
    new_node = nodes[a] @ nodes[b]
    nodes.append(new_node)
    nodes = utils.multi_remove(nodes, [a, b])
  return net","def base(net: network.TensorNetwork,
         algorithm: Callable[[List[Set[int]], Set[int], Dict[int, int]],
                             List]) -> network.TensorNetwork:
  """"""Base method for all `opt_einsum` contractors.

  Args:
    net: a TensorNetwork object. Should be connected.
    algorithm: `opt_einsum` contraction method to use.

  Returns:
    The network after full contraction.
  """"""
  net.check_connected()
  # First contract all trace edges
  edges = net.get_all_nondangling()
  for edge in edges:
    if edge in net and edge.is_trace():
      net.contract_parallel(edge)

  # Then apply `opt_einsum`'s algorithm
  nodes = sorted(net.nodes_set)
  input_sets = utils.get_input_sets(net)
  output_set = utils.get_output_set(net)
  size_dict = utils.get_size_dict(net)
  path = algorithm(input_sets, output_set, size_dict)
  for a, b in path:
    new_node = nodes[a] @ nodes[b]
    nodes.append(new_node)
    nodes = utils.multi_remove(nodes, [a, b])
  return net"
"def build(preprocessor_step_config):
  """"""Builds preprocessing step based on the configuration.

  Args:
    preprocessor_step_config: PreprocessingStep configuration proto.

  Returns:
    function, argmap: A callable function and an argument map to call function
                      with.

  Raises:
    ValueError: On invalid configuration.
  """"""
  step_type = preprocessor_step_config.WhichOneof('preprocessing_step')

  if step_type in PREPROCESSING_FUNCTION_MAP:
    preprocessing_function = PREPROCESSING_FUNCTION_MAP[step_type]
    step_config = _get_step_config_from_proto(preprocessor_step_config,
                                              step_type)
    function_args = _get_dict_from_proto(step_config)
    return (preprocessing_function, function_args)

  if step_type == 'random_horizontal_flip':
    config = preprocessor_step_config.random_horizontal_flip
    return (preprocessor.random_horizontal_flip,
            {
                'keypoint_flip_permutation': tuple(
                    config.keypoint_flip_permutation),
            })

  if step_type == 'random_vertical_flip':
    config = preprocessor_step_config.random_vertical_flip
    return (preprocessor.random_vertical_flip,
            {
                'keypoint_flip_permutation': tuple(
                    config.keypoint_flip_permutation),
            })

  if step_type == 'random_rotation90':
    return (preprocessor.random_rotation90, {})

  if step_type == 'random_crop_image':
    config = preprocessor_step_config.random_crop_image
    return (preprocessor.random_crop_image,
            {
                'min_object_covered': config.min_object_covered,
                'aspect_ratio_range': (config.min_aspect_ratio,
                                       config.max_aspect_ratio),
                'area_range': (config.min_area, config.max_area),
                'overlap_thresh': config.overlap_thresh,
                'random_coef': config.random_coef,
            })

  if step_type == 'random_pad_image':
    config = preprocessor_step_config.random_pad_image
    min_image_size = None
    if (config.HasField('min_image_height') !=
        config.HasField('min_image_width')):
      raise ValueError('min_image_height and min_image_width should be either '
                       'both set or both unset.')
    if config.HasField('min_image_height'):
      min_image_size = (config.min_image_height, config.min_image_width)

    max_image_size = None
    if (config.HasField('max_image_height') !=
        config.HasField('max_image_width')):
      raise ValueError('max_image_height and max_image_width should be either '
                       'both set or both unset.')
    if config.HasField('max_image_height'):
      max_image_size = (config.max_image_height, config.max_image_width)

    pad_color = config.pad_color
    if pad_color and len(pad_color) != 3:
      raise ValueError('pad_color should have 3 elements (RGB) if set!')
    if not pad_color:
      pad_color = None
    return (preprocessor.random_pad_image,
            {
                'min_image_size': min_image_size,
                'max_image_size': max_image_size,
                'pad_color': pad_color,
            })

  if step_type == 'random_crop_pad_image':
    config = preprocessor_step_config.random_crop_pad_image
    min_padded_size_ratio = config.min_padded_size_ratio
    if min_padded_size_ratio and len(min_padded_size_ratio) != 2:
      raise ValueError('min_padded_size_ratio should have 2 elements if set!')
    max_padded_size_ratio = config.max_padded_size_ratio
    if max_padded_size_ratio and len(max_padded_size_ratio) != 2:
      raise ValueError('max_padded_size_ratio should have 2 elements if set!')
    pad_color = config.pad_color
    if pad_color and len(pad_color) != 3:
      raise ValueError('pad_color should have 3 elements if set!')
    kwargs = {
        'min_object_covered': config.min_object_covered,
        'aspect_ratio_range': (config.min_aspect_ratio,
                               config.max_aspect_ratio),
        'area_range': (config.min_area, config.max_area),
        'overlap_thresh': config.overlap_thresh,
        'random_coef': config.random_coef,
    }
    if min_padded_size_ratio:
      kwargs['min_padded_size_ratio'] = tuple(min_padded_size_ratio)
    if max_padded_size_ratio:
      kwargs['max_padded_size_ratio'] = tuple(max_padded_size_ratio)
    if pad_color:
      kwargs['pad_color'] = tuple(pad_color)
    return (preprocessor.random_crop_pad_image, kwargs)

  if step_type == 'random_resize_method':
    config = preprocessor_step_config.random_resize_method
    return (preprocessor.random_resize_method,
            {
                'target_size': [config.target_height, config.target_width],
            })

  if step_type == 'resize_image':
    config = preprocessor_step_config.resize_image
    method = RESIZE_METHOD_MAP[config.method]
    return (preprocessor.resize_image,
            {
                'new_height': config.new_height,
                'new_width': config.new_width,
                'method': method
            })

  if step_type == 'ssd_random_crop':
    config = preprocessor_step_config.ssd_random_crop
    if config.operations:
      min_object_covered = [op.min_object_covered for op in config.operations]
      aspect_ratio_range = [(op.min_aspect_ratio, op.max_aspect_ratio)
                            for op in config.operations]
      area_range = [(op.min_area, op.max_area) for op in config.operations]
      overlap_thresh = [op.overlap_thresh for op in config.operations]
      random_coef = [op.random_coef for op in config.operations]
      return (preprocessor.ssd_random_crop,
              {
                  'min_object_covered': min_object_covered,
                  'aspect_ratio_range': aspect_ratio_range,
                  'area_range': area_range,
                  'overlap_thresh': overlap_thresh,
                  'random_coef': random_coef,
              })
    return (preprocessor.ssd_random_crop, {})

  if step_type == 'ssd_random_crop_pad':
    config = preprocessor_step_config.ssd_random_crop_pad
    if config.operations:
      min_object_covered = [op.min_object_covered for op in config.operations]
      aspect_ratio_range = [(op.min_aspect_ratio, op.max_aspect_ratio)
                            for op in config.operations]
      area_range = [(op.min_area, op.max_area) for op in config.operations]
      overlap_thresh = [op.overlap_thresh for op in config.operations]
      random_coef = [op.random_coef for op in config.operations]
      min_padded_size_ratio = [
          (op.min_padded_size_ratio[0], op.min_padded_size_ratio[1])
          for op in config.operations]
      max_padded_size_ratio = [
          (op.max_padded_size_ratio[0], op.max_padded_size_ratio[1])
          for op in config.operations]
      pad_color = [(op.pad_color_r, op.pad_color_g, op.pad_color_b)
                   for op in config.operations]
      return (preprocessor.ssd_random_crop_pad,
              {
                  'min_object_covered': min_object_covered,
                  'aspect_ratio_range': aspect_ratio_range,
                  'area_range': area_range,
                  'overlap_thresh': overlap_thresh,
                  'random_coef': random_coef,
                  'min_padded_size_ratio': min_padded_size_ratio,
                  'max_padded_size_ratio': max_padded_size_ratio,
                  'pad_color': pad_color,
              })
    return (preprocessor.ssd_random_crop_pad, {})

  if step_type == 'ssd_random_crop_fixed_aspect_ratio':
    config = preprocessor_step_config.ssd_random_crop_fixed_aspect_ratio
    if config.operations:
      min_object_covered = [op.min_object_covered for op in config.operations]
      area_range = [(op.min_area, op.max_area) for op in config.operations]
      overlap_thresh = [op.overlap_thresh for op in config.operations]
      random_coef = [op.random_coef for op in config.operations]
      return (preprocessor.ssd_random_crop_fixed_aspect_ratio,
              {
                  'min_object_covered': min_object_covered,
                  'aspect_ratio': config.aspect_ratio,
                  'area_range': area_range,
                  'overlap_thresh': overlap_thresh,
                  'random_coef': random_coef,
              })
    return (preprocessor.ssd_random_crop_fixed_aspect_ratio, {})

  if step_type == 'ssd_random_crop_pad_fixed_aspect_ratio':
    config = preprocessor_step_config.ssd_random_crop_pad_fixed_aspect_ratio
    if config.operations:
      min_object_covered = [op.min_object_covered for op in config.operations]
      aspect_ratio_range = [(op.min_aspect_ratio, op.max_aspect_ratio)
                            for op in config.operations]
      area_range = [(op.min_area, op.max_area) for op in config.operations]
      overlap_thresh = [op.overlap_thresh for op in config.operations]
      random_coef = [op.random_coef for op in config.operations]
      min_padded_size_ratio = [
          (op.min_padded_size_ratio[0], op.min_padded_size_ratio[1])
          for op in config.operations]
      max_padded_size_ratio = [
          (op.max_padded_size_ratio[0], op.max_padded_size_ratio[1])
          for op in config.operations]
      return (preprocessor.ssd_random_crop_pad_fixed_aspect_ratio,
              {
                  'min_object_covered': min_object_covered,
                  'aspect_ratio': config.aspect_ratio,
                  'aspect_ratio_range': aspect_ratio_range,
                  'area_range': area_range,
                  'overlap_thresh': overlap_thresh,
                  'random_coef': random_coef,
                  'min_padded_size_ratio': min_padded_size_ratio,
                  'max_padded_size_ratio': max_padded_size_ratio,
              })
    return (preprocessor.ssd_random_crop_pad_fixed_aspect_ratio, {})

  raise ValueError('Unknown preprocessing step.')","def build(preprocessor_step_config):
  """"""Builds preprocessing step based on the configuration.

  Args:
    preprocessor_step_config: PreprocessingStep configuration proto.

  Returns:
    function, argmap: A callable function and an argument map to call function
                      with.

  Raises:
    ValueError: On invalid configuration.
  """"""
  step_type = preprocessor_step_config.WhichOneof('preprocessing_step')

  if step_type in PREPROCESSING_FUNCTION_MAP:
    preprocessing_function = PREPROCESSING_FUNCTION_MAP[step_type]
    step_config = _get_step_config_from_proto(preprocessor_step_config,
                                              step_type)
    function_args = _get_dict_from_proto(step_config)
    return (preprocessing_function, function_args)

  if step_type == 'random_horizontal_flip':
    config = preprocessor_step_config.random_horizontal_flip
    return (preprocessor.random_horizontal_flip,
            {
                'keypoint_flip_permutation': tuple(
                    config.keypoint_flip_permutation),
            })

  if step_type == 'random_vertical_flip':
    config = preprocessor_step_config.random_vertical_flip
    return (preprocessor.random_vertical_flip,
            {
                'keypoint_flip_permutation': tuple(
                    config.keypoint_flip_permutation),
            })

  if step_type == 'random_rotation90':
    return (preprocessor.random_rotation90, {})

  if step_type == 'random_crop_image':
    config = preprocessor_step_config.random_crop_image
    return (preprocessor.random_crop_image,
            {
                'min_object_covered': config.min_object_covered,
                'aspect_ratio_range': (config.min_aspect_ratio,
                                       config.max_aspect_ratio),
                'area_range': (config.min_area, config.max_area),
                'overlap_thresh': config.overlap_thresh,
                'random_coef': config.random_coef,
            })

  if step_type == 'random_pad_image':
    config = preprocessor_step_config.random_pad_image
    min_image_size = None
    if (config.HasField('min_image_height') !=
        config.HasField('min_image_width')):
      raise ValueError('min_image_height and min_image_width should be either '
                       'both set or both unset.')
    if config.HasField('min_image_height'):
      min_image_size = (config.min_image_height, config.min_image_width)

    max_image_size = None
    if (config.HasField('max_image_height') !=
        config.HasField('max_image_width')):
      raise ValueError('max_image_height and max_image_width should be either '
                       'both set or both unset.')
    if config.HasField('max_image_height'):
      max_image_size = (config.max_image_height, config.max_image_width)

    pad_color = config.pad_color
    if pad_color and len(pad_color) != 3:
      raise ValueError('pad_color should have 3 elements (RGB) if set!')
    if not pad_color:
      pad_color = None
    return (preprocessor.random_pad_image,
            {
                'min_image_size': min_image_size,
                'max_image_size': max_image_size,
                'pad_color': pad_color,
            })

  if step_type == 'random_crop_pad_image':
    config = preprocessor_step_config.random_crop_pad_image
    min_padded_size_ratio = config.min_padded_size_ratio
    if min_padded_size_ratio and len(min_padded_size_ratio) != 2:
      raise ValueError('min_padded_size_ratio should have 3 elements if set!')
    max_padded_size_ratio = config.max_padded_size_ratio
    if max_padded_size_ratio and len(max_padded_size_ratio) != 2:
      raise ValueError('max_padded_size_ratio should have 3 elements if set!')
    pad_color = config.pad_color
    if pad_color and len(pad_color) != 3:
      raise ValueError('pad_color should have 3 elements if set!')
    return (preprocessor.random_crop_pad_image,
            {
                'min_object_covered': config.min_object_covered,
                'aspect_ratio_range': (config.min_aspect_ratio,
                                       config.max_aspect_ratio),
                'area_range': (config.min_area, config.max_area),
                'overlap_thresh': config.overlap_thresh,
                'random_coef': config.random_coef,
                'min_padded_size_ratio': (min_padded_size_ratio if
                                          min_padded_size_ratio else None),
                'max_padded_size_ratio': (max_padded_size_ratio if
                                          max_padded_size_ratio else None),
                'pad_color': (pad_color if pad_color else None),
            })

  if step_type == 'random_resize_method':
    config = preprocessor_step_config.random_resize_method
    return (preprocessor.random_resize_method,
            {
                'target_size': [config.target_height, config.target_width],
            })

  if step_type == 'resize_image':
    config = preprocessor_step_config.resize_image
    method = RESIZE_METHOD_MAP[config.method]
    return (preprocessor.resize_image,
            {
                'new_height': config.new_height,
                'new_width': config.new_width,
                'method': method
            })

  if step_type == 'ssd_random_crop':
    config = preprocessor_step_config.ssd_random_crop
    if config.operations:
      min_object_covered = [op.min_object_covered for op in config.operations]
      aspect_ratio_range = [(op.min_aspect_ratio, op.max_aspect_ratio)
                            for op in config.operations]
      area_range = [(op.min_area, op.max_area) for op in config.operations]
      overlap_thresh = [op.overlap_thresh for op in config.operations]
      random_coef = [op.random_coef for op in config.operations]
      return (preprocessor.ssd_random_crop,
              {
                  'min_object_covered': min_object_covered,
                  'aspect_ratio_range': aspect_ratio_range,
                  'area_range': area_range,
                  'overlap_thresh': overlap_thresh,
                  'random_coef': random_coef,
              })
    return (preprocessor.ssd_random_crop, {})

  if step_type == 'ssd_random_crop_pad':
    config = preprocessor_step_config.ssd_random_crop_pad
    if config.operations:
      min_object_covered = [op.min_object_covered for op in config.operations]
      aspect_ratio_range = [(op.min_aspect_ratio, op.max_aspect_ratio)
                            for op in config.operations]
      area_range = [(op.min_area, op.max_area) for op in config.operations]
      overlap_thresh = [op.overlap_thresh for op in config.operations]
      random_coef = [op.random_coef for op in config.operations]
      min_padded_size_ratio = [
          (op.min_padded_size_ratio[0], op.min_padded_size_ratio[1])
          for op in config.operations]
      max_padded_size_ratio = [
          (op.max_padded_size_ratio[0], op.max_padded_size_ratio[1])
          for op in config.operations]
      pad_color = [(op.pad_color_r, op.pad_color_g, op.pad_color_b)
                   for op in config.operations]
      return (preprocessor.ssd_random_crop_pad,
              {
                  'min_object_covered': min_object_covered,
                  'aspect_ratio_range': aspect_ratio_range,
                  'area_range': area_range,
                  'overlap_thresh': overlap_thresh,
                  'random_coef': random_coef,
                  'min_padded_size_ratio': min_padded_size_ratio,
                  'max_padded_size_ratio': max_padded_size_ratio,
                  'pad_color': pad_color,
              })
    return (preprocessor.ssd_random_crop_pad, {})

  if step_type == 'ssd_random_crop_fixed_aspect_ratio':
    config = preprocessor_step_config.ssd_random_crop_fixed_aspect_ratio
    if config.operations:
      min_object_covered = [op.min_object_covered for op in config.operations]
      area_range = [(op.min_area, op.max_area) for op in config.operations]
      overlap_thresh = [op.overlap_thresh for op in config.operations]
      random_coef = [op.random_coef for op in config.operations]
      return (preprocessor.ssd_random_crop_fixed_aspect_ratio,
              {
                  'min_object_covered': min_object_covered,
                  'aspect_ratio': config.aspect_ratio,
                  'area_range': area_range,
                  'overlap_thresh': overlap_thresh,
                  'random_coef': random_coef,
              })
    return (preprocessor.ssd_random_crop_fixed_aspect_ratio, {})

  if step_type == 'ssd_random_crop_pad_fixed_aspect_ratio':
    config = preprocessor_step_config.ssd_random_crop_pad_fixed_aspect_ratio
    if config.operations:
      min_object_covered = [op.min_object_covered for op in config.operations]
      aspect_ratio_range = [(op.min_aspect_ratio, op.max_aspect_ratio)
                            for op in config.operations]
      area_range = [(op.min_area, op.max_area) for op in config.operations]
      overlap_thresh = [op.overlap_thresh for op in config.operations]
      random_coef = [op.random_coef for op in config.operations]
      min_padded_size_ratio = [
          (op.min_padded_size_ratio[0], op.min_padded_size_ratio[1])
          for op in config.operations]
      max_padded_size_ratio = [
          (op.max_padded_size_ratio[0], op.max_padded_size_ratio[1])
          for op in config.operations]
      return (preprocessor.ssd_random_crop_pad_fixed_aspect_ratio,
              {
                  'min_object_covered': min_object_covered,
                  'aspect_ratio': config.aspect_ratio,
                  'aspect_ratio_range': aspect_ratio_range,
                  'area_range': area_range,
                  'overlap_thresh': overlap_thresh,
                  'random_coef': random_coef,
                  'min_padded_size_ratio': min_padded_size_ratio,
                  'max_padded_size_ratio': max_padded_size_ratio,
              })
    return (preprocessor.ssd_random_crop_pad_fixed_aspect_ratio, {})

  raise ValueError('Unknown preprocessing step.')"
"def train():
  """"""Train CIFAR-10 for a number of steps.""""""
  with tf.Graph().as_default(), tf.device('/cpu:0'):
    # Create a variable to count the number of train() calls. This equals the
    # number of batches processed * FLAGS.num_gpus.
    global_step = tf.get_variable(
        'global_step', [],
        initializer=tf.constant_initializer(0), trainable=False)

    # Calculate the learning rate schedule.
    num_batches_per_epoch = (cifar10.NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN /
                             FLAGS.batch_size)
    decay_steps = int(num_batches_per_epoch * cifar10.NUM_EPOCHS_PER_DECAY)

    # Decay the learning rate exponentially based on the number of steps.
    lr = tf.train.exponential_decay(cifar10.INITIAL_LEARNING_RATE,
                                    global_step,
                                    decay_steps,
                                    cifar10.LEARNING_RATE_DECAY_FACTOR,
                                    staircase=True)

    # Create an optimizer that performs gradient descent.
    opt = tf.train.GradientDescentOptimizer(lr)

    # Calculate the gradients for each model tower.
    tower_grads = []
    with tf.variable_scope(tf.get_variable_scope()):
      for i in xrange(FLAGS.num_gpus):
        with tf.device('/gpu:%d' % i):
          with tf.name_scope('%s_%d' % (cifar10.TOWER_NAME, i)) as scope:
            # Calculate the loss for one tower of the CIFAR model. This function
            # constructs the entire CIFAR model but shares the variables across
            # all towers.
            loss = tower_loss(scope)

            # Reuse variables for the next tower.
            tf.get_variable_scope().reuse_variables()

            # Retain the summaries from the final tower.
            summaries = tf.get_collection(tf.GraphKeys.SUMMARIES, scope)

            # Calculate the gradients for the batch of data on this CIFAR tower.
            grads = opt.compute_gradients(loss)

            # Keep track of the gradients across all towers.
            tower_grads.append(grads)

    # We must calculate the mean of each gradient. Note that this is the
    # synchronization point across all towers.
    grads = average_gradients(tower_grads)

    # Add a summary to track the learning rate.
    summaries.append(tf.summary.scalar('learning_rate', lr))

    # Add histograms for gradients.
    for grad, var in grads:
      if grad is not None:
        summaries.append(tf.summary.histogram(var.op.name + '/gradients', grad))

    # Apply the gradients to adjust the shared variables.
    apply_gradient_op = opt.apply_gradients(grads, global_step=global_step)

    # Add histograms for trainable variables.
    for var in tf.trainable_variables():
      summaries.append(tf.summary.histogram(var.op.name, var))

    # Track the moving averages of all trainable variables.
    variable_averages = tf.train.ExponentialMovingAverage(
        cifar10.MOVING_AVERAGE_DECAY, global_step)
    variables_averages_op = variable_averages.apply(tf.trainable_variables())

    # Group all updates to into a single train op.
    train_op = tf.group(apply_gradient_op, variables_averages_op)

    # Create a saver.
    saver = tf.train.Saver(tf.global_variables())

    # Build the summary operation from the last tower summaries.
    summary_op = tf.summary.merge(summaries)

    # Build an initialization operation to run below.
    init = tf.global_variables_initializer()

    # Start running operations on the Graph. allow_soft_placement must be set to
    # True to build towers on GPU, as some of the ops do not have GPU
    # implementations.
    sess = tf.Session(config=tf.ConfigProto(
        allow_soft_placement=True,
        log_device_placement=FLAGS.log_device_placement))
    sess.run(init)

    # Start the queue runners.
    tf.train.start_queue_runners(sess=sess)

    summary_writer = tf.summary.FileWriter(FLAGS.train_dir, sess.graph)

    for step in xrange(FLAGS.max_steps):
      start_time = time.time()
      _, loss_value = sess.run([train_op, loss])
      duration = time.time() - start_time

      assert not np.isnan(loss_value), 'Model diverged with loss = NaN'

      if step % 10 == 0:
        num_examples_per_step = FLAGS.batch_size * FLAGS.num_gpus
        examples_per_sec = num_examples_per_step / duration
        sec_per_batch = duration / FLAGS.num_gpus

        format_str = ('%s: step %d, loss = %.2f (%.1f examples/sec; %.3f '
                      'sec/batch)')
        print (format_str % (datetime.now(), step, loss_value,
                             examples_per_sec, sec_per_batch))

      if step % 100 == 0:
        summary_str = sess.run(summary_op)
        summary_writer.add_summary(summary_str, step)

      # Save the model checkpoint periodically.
      if step % 1000 == 0 or (step + 1) == FLAGS.max_steps:
        checkpoint_path = os.path.join(FLAGS.train_dir, 'model.ckpt')
        saver.save(sess, checkpoint_path, global_step=step)","def train():
  """"""Train CIFAR-10 for a number of steps.""""""
  with tf.Graph().as_default(), tf.device('/cpu:0'):
    # Create a variable to count the number of train() calls. This equals the
    # number of batches processed * FLAGS.num_gpus.
    global_step = tf.get_variable(
        'global_step', [],
        initializer=tf.constant_initializer(0), trainable=False)

    # Calculate the learning rate schedule.
    num_batches_per_epoch = (cifar10.NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN /
                             FLAGS.batch_size)
    decay_steps = int(num_batches_per_epoch * cifar10.NUM_EPOCHS_PER_DECAY)

    # Decay the learning rate exponentially based on the number of steps.
    lr = tf.train.exponential_decay(cifar10.INITIAL_LEARNING_RATE,
                                    global_step,
                                    decay_steps,
                                    cifar10.LEARNING_RATE_DECAY_FACTOR,
                                    staircase=True)

    # Create an optimizer that performs gradient descent.
    opt = tf.train.GradientDescentOptimizer(lr)

    # Calculate the gradients for each model tower.
    tower_grads = []
    for i in xrange(FLAGS.num_gpus):
      with tf.device('/gpu:%d' % i):
        with tf.name_scope('%s_%d' % (cifar10.TOWER_NAME, i)) as scope:
          # Calculate the loss for one tower of the CIFAR model. This function
          # constructs the entire CIFAR model but shares the variables across
          # all towers.
          loss = tower_loss(scope)

          # Reuse variables for the next tower.
          tf.get_variable_scope().reuse_variables()

          # Retain the summaries from the final tower.
          summaries = tf.get_collection(tf.GraphKeys.SUMMARIES, scope)

          # Calculate the gradients for the batch of data on this CIFAR tower.
          grads = opt.compute_gradients(loss)

          # Keep track of the gradients across all towers.
          tower_grads.append(grads)

    # We must calculate the mean of each gradient. Note that this is the
    # synchronization point across all towers.
    grads = average_gradients(tower_grads)

    # Add a summary to track the learning rate.
    summaries.append(tf.summary.scalar('learning_rate', lr))

    # Add histograms for gradients.
    for grad, var in grads:
      if grad is not None:
        summaries.append(tf.summary.histogram(var.op.name + '/gradients', grad))

    # Apply the gradients to adjust the shared variables.
    apply_gradient_op = opt.apply_gradients(grads, global_step=global_step)

    # Add histograms for trainable variables.
    for var in tf.trainable_variables():
      summaries.append(tf.summary.histogram(var.op.name, var))

    # Track the moving averages of all trainable variables.
    variable_averages = tf.train.ExponentialMovingAverage(
        cifar10.MOVING_AVERAGE_DECAY, global_step)
    variables_averages_op = variable_averages.apply(tf.trainable_variables())

    # Group all updates to into a single train op.
    train_op = tf.group(apply_gradient_op, variables_averages_op)

    # Create a saver.
    saver = tf.train.Saver(tf.global_variables())

    # Build the summary operation from the last tower summaries.
    summary_op = tf.summary.merge(summaries)

    # Build an initialization operation to run below.
    init = tf.global_variables_initializer()

    # Start running operations on the Graph. allow_soft_placement must be set to
    # True to build towers on GPU, as some of the ops do not have GPU
    # implementations.
    sess = tf.Session(config=tf.ConfigProto(
        allow_soft_placement=True,
        log_device_placement=FLAGS.log_device_placement))
    sess.run(init)

    # Start the queue runners.
    tf.train.start_queue_runners(sess=sess)

    summary_writer = tf.summary.FileWriter(FLAGS.train_dir, sess.graph)

    for step in xrange(FLAGS.max_steps):
      start_time = time.time()
      _, loss_value = sess.run([train_op, loss])
      duration = time.time() - start_time

      assert not np.isnan(loss_value), 'Model diverged with loss = NaN'

      if step % 10 == 0:
        num_examples_per_step = FLAGS.batch_size * FLAGS.num_gpus
        examples_per_sec = num_examples_per_step / duration
        sec_per_batch = duration / FLAGS.num_gpus

        format_str = ('%s: step %d, loss = %.2f (%.1f examples/sec; %.3f '
                      'sec/batch)')
        print (format_str % (datetime.now(), step, loss_value,
                             examples_per_sec, sec_per_batch))

      if step % 100 == 0:
        summary_str = sess.run(summary_op)
        summary_writer.add_summary(summary_str, step)

      # Save the model checkpoint periodically.
      if step % 1000 == 0 or (step + 1) == FLAGS.max_steps:
        checkpoint_path = os.path.join(FLAGS.train_dir, 'model.ckpt')
        saver.save(sess, checkpoint_path, global_step=step)"
"    def __call__(self, direction, factor, values):
        angles_deg = np.asarray(values)/factor
        damping_ratios = np.cos((180-angles_deg) * np.pi/180)
        ret = [""%.2f"" % val for val in damping_ratios]
        return ret","    def __call__(self, direction, factor, values):
        angles_deg = values/factor
        damping_ratios = np.cos((180-angles_deg) * np.pi/180)
        ret = [""%.2f"" % val for val in damping_ratios]
        return ret"
"    def __call__(self, transform_xy, x1, y1, x2, y2):
        x, y = np.meshgrid(
            np.linspace(x1, x2, self.nx), np.linspace(y1, y2, self.ny))
        lon, lat = transform_xy(np.ravel(x), np.ravel(y))

        with np.errstate(invalid='ignore'):
            if self.lon_cycle is not None:
                lon0 = np.nanmin(lon)
                # Changed from 180 to 360 to be able to span only
                # 90-270 (left hand side)
                lon -= 360. * ((lon - lon0) > 360.)
            if self.lat_cycle is not None:  # pragma: no cover
                lat0 = np.nanmin(lat)
                lat -= 360. * ((lat - lat0) > 180.)

        lon_min, lon_max = np.nanmin(lon), np.nanmax(lon)
        lat_min, lat_max = np.nanmin(lat), np.nanmax(lat)

        lon_min, lon_max, lat_min, lat_max = \\
            self._add_pad(lon_min, lon_max, lat_min, lat_max)

        # check cycle
        if self.lon_cycle:
            lon_max = min(lon_max, lon_min + self.lon_cycle)
        if self.lat_cycle:  # pragma: no cover
            lat_max = min(lat_max, lat_min + self.lat_cycle)

        if self.lon_minmax is not None:
            min0 = self.lon_minmax[0]
            lon_min = max(min0, lon_min)
            max0 = self.lon_minmax[1]
            lon_max = min(max0, lon_max)

        if self.lat_minmax is not None:
            min0 = self.lat_minmax[0]
            lat_min = max(min0, lat_min)
            max0 = self.lat_minmax[1]
            lat_max = min(max0, lat_max)

        return lon_min, lon_max, lat_min, lat_max","    def __call__(self, transform_xy, x1, y1, x2, y2):
        x_, y_ = np.linspace(x1, x2, self.nx), np.linspace(y1, y2, self.ny)
        x, y = np.meshgrid(x_, y_)
        lon, lat = transform_xy(np.ravel(x), np.ravel(y))

        with np.errstate(invalid='ignore'):
            if self.lon_cycle is not None:
                lon0 = np.nanmin(lon)
                # Changed from 180 to 360 to be able to span only
                # 90-270 (left hand side)
                lon -= 360. * ((lon - lon0) > 360.)
            if self.lat_cycle is not None:
                lat0 = np.nanmin(lat)
                # Changed from 180 to 360 to be able to span only
                # 90-270 (left hand side)
                lat -= 360. * ((lat - lat0) > 360.)

        lon_min, lon_max = np.nanmin(lon), np.nanmax(lon)
        lat_min, lat_max = np.nanmin(lat), np.nanmax(lat)

        lon_min, lon_max, lat_min, lat_max = \\
            self._adjust_extremes(lon_min, lon_max, lat_min, lat_max)

        return lon_min, lon_max, lat_min, lat_max"
"def pzmap(sys, plot=None, grid=None, title='Pole Zero Map', **kwargs):
    """"""
    Plot a pole/zero map for a linear system.

    Parameters
    ----------
    sys: LTI (StateSpace or TransferFunction)
        Linear system for which poles and zeros are computed.
    plot: bool, optional
        If ``True`` a graph is generated with Matplotlib,
        otherwise the poles and zeros are only computed and returned.
    grid: boolean (default = False)
        If True plot omega-damping grid.

    Returns
    -------
    pole: array
        The systems poles
    zeros: array
        The system's zeros.
    """"""
    # Check to see if legacy 'Plot' keyword was used
    if 'Plot' in kwargs:
        import warnings
        warnings.warn(""'Plot' keyword is deprecated in pzmap; use 'plot'"",
                      FutureWarning)
        plot = kwargs['Plot']

    # Get parameter values
    plot = config._get_param('pzmap', 'plot', plot, True)
    grid = config._get_param('pzmap', 'grid', grid, False)

    if not isinstance(sys, LTI):
        raise TypeError('Argument ``sys``: must be a linear system.')

    poles = sys.pole()
    zeros = sys.zero()

    if (plot):
        import matplotlib.pyplot as plt

        if grid:
            if isdtime(sys, strict=True):
                ax, fig = zgrid()
            else:
                ax, fig = sgrid()
        else:
            ax, fig = nogrid()

        # Plot the locations of the poles and zeros
        if len(poles) > 0:
            ax.scatter(real(poles), imag(poles), s=50, marker='x',
                       facecolors='k')
        if len(zeros) > 0:
            ax.scatter(real(zeros), imag(zeros), s=50, marker='o',
                       facecolors='none', edgecolors='k')

        plt.title(title)

    # Return locations of poles and zeros as a tuple
    return poles, zeros","def pzmap(sys, plot=True, grid=False, title='Pole Zero Map', **kwargs):
    """"""
    Plot a pole/zero map for a linear system.

    Parameters
    ----------
    sys: LTI (StateSpace or TransferFunction)
        Linear system for which poles and zeros are computed.
    plot: bool, optional
        If ``True`` a graph is generated with Matplotlib,
        otherwise the poles and zeros are only computed and returned.
    grid: boolean (default = False)
        If True plot omega-damping grid.

    Returns
    -------
    pole: array
        The systems poles
    zeros: array
        The system's zeros.
    """"""
    # Check to see if legacy 'Plot' keyword was used
    if 'Plot' in kwargs:
        import warnings
        warnings.warn(""'Plot' keyword is deprecated in pzmap; use 'plot'"",
                      FutureWarning)
        plot = kwargs['Plot']

    # Get parameter values
    plot = config._get_param('rlocus', 'plot', plot, True)
    grid = config._get_param('rlocus', 'grid', grid, False)

    if not isinstance(sys, LTI):
        raise TypeError('Argument ``sys``: must be a linear system.')

    poles = sys.pole()
    zeros = sys.zero()

    if (plot):
        import matplotlib.pyplot as plt

        if grid:
            if isdtime(sys, strict=True):
                ax, fig = zgrid()
            else:
                ax, fig = sgrid()
        else:
            ax, fig = nogrid()

        # Plot the locations of the poles and zeros
        if len(poles) > 0:
            ax.scatter(real(poles), imag(poles), s=50, marker='x',
                       facecolors='k')
        if len(zeros) > 0:
            ax.scatter(real(zeros), imag(zeros), s=50, marker='o',
                       facecolors='none', edgecolors='k')

        plt.title(title)

    # Return locations of poles and zeros as a tuple
    return poles, zeros"
"    def __str__(self):
        """"""String representation of an input/output system""""""
        str = ""System: "" + (self.name if self.name else ""(None)"") + ""\\n""
        str += ""Inputs (%s): "" % self.ninputs
        for key in self.input_index: str += key + "", ""
        str += ""\\nOutputs (%s): "" % self.noutputs
        for key in self.output_index: str += key + "", ""
        str += ""\\nStates (%s): "" % self.nstates
        for key in self.state_index: str += key + "", ""
        return str","    def __str__(self):
        """"""String representation of an input/output system""""""
        str = ""System: "" + (self.name if self.name else ""(none)"") + ""\\n""
        str += ""Inputs (%d): "" % self.ninputs
        for key in self.input_index: str += key + "", ""
        str += ""\\nOutputs (%d): "" % self.noutputs
        for key in self.output_index: str += key + "", ""
        str += ""\\nStates (%d): "" % self.nstates
        for key in self.state_index: str += key + "", ""
        return str"
"    def __add__(sys1, sys2):
        """"""Add two input/output systems (parallel interconnection)""""""
        # TODO: Allow addition of scalars and matrices
        if not isinstance(sys2, InputOutputSystem):
            raise ValueError(""Unknown I/O system object "", sys2)
        elif isinstance(sys1, StateSpace) and isinstance(sys2, StateSpace):
            # Special case: maintain linear systems structure
            new_ss_sys = StateSpace.__add__(sys1, sys2)
            # TODO: set input and output names
            new_io_sys = LinearIOSystem(new_ss_sys)

            return new_io_sys

        # Make sure number of input and outputs match
        if sys1.ninputs != sys2.ninputs or sys1.noutputs != sys2.noutputs:
            raise ValueError(""Can't add systems with different numbers of ""
                             ""inputs or outputs."")
        ninputs = sys1.ninputs
        noutputs = sys1.noutputs

        # Create a new system to handle the composition
        newsys = InterconnectedSystem((sys1, sys2))

        # Set up the input map
        newsys.set_input_map(np.concatenate(
            (np.eye(ninputs), np.eye(ninputs)), axis=0))
        # TODO: set up input names

        # Set up the output map
        newsys.set_output_map(np.concatenate(
            (np.eye(noutputs), np.eye(noutputs)), axis=1))
        # TODO: set up output names

        # Return the newly created system
        return newsys","    def __add__(sys1, sys2):
        """"""Add two input/output systems (parallel interconnection)""""""
        # TODO: Allow addition of scalars and matrices
        if not isinstance(sys2, InputOutputSystem):
            raise ValueError(""Unknown I/O system object "", sys2)
        elif isinstance(sys1, StateSpace) and isinstance(sys2, StateSpace):
            # Special case: maintain linear systems structure
            new_ss_sys = StateSpace.__add__(sys1, sys2)
            # TODO: set input and output names
            new_io_sys = LinearIOSystem(new_ss_sys)

            return new_io_sys

        # Make sure number of input and outputs match
        if sys1.ninputs != sys2.ninputs or sys1.noutputs != sys2.noutputs:
            raise ValueError(""Can't add systems with different numbers of ""
                             ""inputs or outputs."")
        ninputs = sys1.ninputs
        noutputs = sys1.noutputs

        # Make sure timebase are compatible
        dt = _find_timebase(sys1, sys2)
        if dt is False:
            raise ValueError(""System timebases are not compabile"")

        # Create a new system to handle the composition
        newsys = InterconnectedSystem((sys1, sys2), dt=dt)

        # Set up the input map
        newsys.set_input_map(np.concatenate(
            (np.eye(ninputs), np.eye(ninputs)), axis=0))
        # TODO: set up input names

        # Set up the output map
        newsys.set_output_map(np.concatenate(
            (np.eye(noutputs), np.eye(noutputs)), axis=1))
        # TODO: set up output names

        # Return the newly created system
        return newsys"
"    def __init__(self, syslist, connections=[], inplist=[], outlist=[],
                 inputs=None, outputs=None, states=None,
                 params={}, dt=None, name=None):
        """"""Create an I/O system from a list of systems + connection info.

        The InterconnectedSystem class is used to represent an input/output
        system that consists of an interconnection between a set of subystems.
        The outputs of each subsystem can be summed together to to provide
        inputs to other subsystems.  The overall system inputs and outputs can
        be any subset of subsystem inputs and outputs.

        Parameters
        ----------
        syslist : array_like of InputOutputSystems
            The list of input/output systems to be connected

        connections : tuple of connection specifications, optional
            Description of the internal connections between the subsystems.
            Each element of the tuple describes an input to one of the
            subsystems.  The entries are are of the form:

                (input-spec, output-spec1, output-spec2, ...)

            The input-spec should be a tuple of the form `(subsys_i, inp_j)`
            where `subsys_i` is the index into `syslist` and `inp_j` is the
            index into the input vector for the subsystem.  If `subsys_i` has
            a single input, then the subsystem index `subsys_i` can be listed
            as the input-spec.  If systems and signals are given names, then
            the form 'sys.sig' or ('sys', 'sig') are also recognized.

            Each output-spec should be a tuple of the form `(subsys_i, out_j,
            gain)`.  The input will be constructed by summing the listed
            outputs after multiplying by the gain term.  If the gain term is
            omitted, it is assumed to be 1.  If the system has a single
            output, then the subsystem index `subsys_i` can be listed as the
            input-spec.  If systems and signals are given names, then the form
            'sys.sig', ('sys', 'sig') or ('sys', 'sig', gain) are also
            recognized, and the special form '-sys.sig' can be used to specify
            a signal with gain -1.

            If omitted, the connection map (matrix) can be specified using the
            :func:`~control.InterconnectedSystem.set_connect_map` method.

        inplist : tuple of input specifications, optional
            List of specifications for how the inputs for the overall system
            are mapped to the subsystem inputs.  The input specification is
            the same as the form defined in the connection specification.
            Each system input is added to the input for the listed subsystem.

            If omitted, the input map can be specified using the
            `set_input_map` method.

        outlist : tuple of output specifications, optional
            List of specifications for how the outputs for the subsystems are
            mapped to overall system outputs.  The output specification is the
            same as the form defined in the connection specification
            (including the optional gain term).  Numbered outputs must be
            chosen from the list of subsystem outputs, but named outputs can
            also be contained in the list of subsystem inputs.

            If omitted, the output map can be specified using the
            `set_output_map` method.

        params : dict, optional
            Parameter values for the systems.  Passed to the evaluation
            functions for the system as default values, overriding internal
            defaults.

        dt : timebase, optional
            The timebase for the system, used to specify whether the system is
            operating in continuous or discrete time.  It can have the
            following values:

            * dt = None       No timebase specified
            * dt = 0          Continuous time system
            * dt > 0          Discrete time system with sampling time dt
            * dt = True       Discrete time with unspecified sampling time

        name : string, optional
            System name (used for specifying signals).

        """"""
        # Convert input and output names to lists if they aren't already
        if not isinstance(inplist, (list, tuple)): inplist = [inplist]
        if not isinstance(outlist, (list, tuple)): outlist = [outlist]

        # Check to make sure all systems are consistent
        self.syslist = syslist
        self.syslist_index = {}
        dt = None
        nstates = 0; self.state_offset = []
        ninputs = 0; self.input_offset = []
        noutputs = 0; self.output_offset = []
        system_count = 0
        for sys in syslist:
            # Make sure time bases are consistent
            # TODO: Use lti._find_timebase() instead?
            if dt is None and sys.dt is not None:
                # Timebase was not specified; set to match this system
                dt = sys.dt
            elif dt != sys.dt:
                raise TypeError(""System timebases are not compatible"")

            # Make sure number of inputs, outputs, states is given
            if sys.ninputs is None or sys.noutputs is None or \\
               sys.nstates is None:
                raise TypeError(""System '%s' must define number of inputs, ""
                                ""outputs, states in order to be connected"" %
                                sys.name)

            # Keep track of the offsets into the states, inputs, outputs
            self.input_offset.append(ninputs)
            self.output_offset.append(noutputs)
            self.state_offset.append(nstates)

            # Keep track of the total number of states, inputs, outputs
            nstates += sys.nstates
            ninputs += sys.ninputs
            noutputs += sys.noutputs

            # Store the index to the system for later retrieval
            # TODO: look for duplicated system names
            self.syslist_index[sys.name] = system_count
            system_count += 1

        # Check for duplicate systems or duplicate names
        sysobj_list = []
        sysname_list = []
        for sys in syslist:
            if sys in sysobj_list:
                warn(""Duplicate object found in system list: %s"" % str(sys))
            elif sys.name is not None and sys.name in sysname_list:
                warn(""Duplicate name found in system list: %s"" % sys.name)
            sysobj_list.append(sys)
            sysname_list.append(sys.name)

        # Create the I/O system
        super(InterconnectedSystem, self).__init__(
            inputs=len(inplist), outputs=len(outlist),
            states=nstates, params=params, dt=dt)

        # If input or output list was specified, update it
        nsignals, self.input_index = \\
            self._process_signal_list(inputs, prefix='u')
        if nsignals is not None and len(inplist) != nsignals:
            raise ValueError(""Wrong number/type of inputs given."")
        nsignals, self.output_index = \\
            self._process_signal_list(outputs, prefix='y')
        if nsignals is not None and len(outlist) != nsignals:
            raise ValueError(""Wrong number/type of outputs given."")

        # Convert the list of interconnections to a connection map (matrix)
        self.connect_map = np.zeros((ninputs, noutputs))
        for connection in connections:
            input_index = self._parse_input_spec(connection[0])
            for output_spec in connection[1:]:
                output_index, gain = self._parse_output_spec(output_spec)
                self.connect_map[input_index, output_index] = gain

        # Convert the input list to a matrix: maps system to subsystems
        self.input_map = np.zeros((ninputs, self.ninputs))
        for index, inpspec in enumerate(inplist):
            if isinstance(inpspec, (int, str, tuple)): inpspec = [inpspec]
            for spec in inpspec:
                self.input_map[self._parse_input_spec(spec), index] = 1

        # Convert the output list to a matrix: maps subsystems to system
        self.output_map = np.zeros((self.noutputs, noutputs + ninputs))
        for index in range(len(outlist)):
            ylist_index, gain = self._parse_output_spec(outlist[index])
            self.output_map[index, ylist_index] = gain

        # Save the parameters for the system
        self.params = params.copy()","    def __init__(self, syslist, connections=[], inplist=[], outlist=[],
                 inputs=None, outputs=None, states=None,
                 params={}, dt=None, name=None):
        """"""Create an I/O system from a list of systems + connection info.

        The InterconnectedSystem class is used to represent an input/output
        system that consists of an interconnection between a set of subystems.
        The outputs of each subsystem can be summed together to to provide
        inputs to other subsystems.  The overall system inputs and outputs can
        be any subset of subsystem inputs and outputs.

        Parameters
        ----------
        syslist : array_like of InputOutputSystems
            The list of input/output systems to be connected

        connections : tuple of connection specifications, optional
            Description of the internal connections between the subsystems.
            Each element of the tuple describes an input to one of the
            subsystems.  The entries are are of the form:

                (input-spec, output-spec1, output-spec2, ...)

            The input-spec should be a tuple of the form `(subsys_i, inp_j)`
            where `subsys_i` is the index into `syslist` and `inp_j` is the
            index into the input vector for the subsystem.  If `subsys_i` has
            a single input, then the subsystem index `subsys_i` can be listed
            as the input-spec.  If systems and signals are given names, then
            the form 'sys.sig' or ('sys', 'sig') are also recognized.

            Each output-spec should be a tuple of the form `(subsys_i, out_j,
            gain)`.  The input will be constructed by summing the listed
            outputs after multiplying by the gain term.  If the gain term is
            omitted, it is assumed to be 1.  If the system has a single
            output, then the subsystem index `subsys_i` can be listed as the
            input-spec.  If systems and signals are given names, then the form
            'sys.sig', ('sys', 'sig') or ('sys', 'sig', gain) are also
            recognized, and the special form '-sys.sig' can be used to specify
            a signal with gain -1.

            If omitted, the connection map (matrix) can be specified using the
            :func:`~control.InterconnectedSystem.set_connect_map` method.

        inplist : tuple of input specifications, optional
            List of specifications for how the inputs for the overall system
            are mapped to the subsystem inputs.  The input specification is
            the same as the form defined in the connection specification.
            Each system input is added to the input for the listed subsystem.

            If omitted, the input map can be specified using the
            `set_input_map` method.

        outlist : tuple of output specifications, optional
            List of specifications for how the outputs for the subsystems are
            mapped to overall system outputs.  The output specification is the
            same as the form defined in the connection specification
            (including the optional gain term).  Numbered outputs must be
            chosen from the list of subsystem outputs, but named outputs can
            also be contained in the list of subsystem inputs.

            If omitted, the output map can be specified using the
            `set_output_map` method.

        params : dict, optional
            Parameter values for the systems.  Passed to the evaluation
            functions for the system as default values, overriding internal
            defaults.

        dt : timebase, optional
            The timebase for the system, used to specify whether the system is
            operating in continuous or discrete time.  It can have the
            following values:

            * dt = None       No timebase specified
            * dt = 0          Continuous time system
            * dt > 0          Discrete time system with sampling time dt
            * dt = True       Discrete time with unspecified sampling time

        name : string, optional
            System name (used for specifying signals).

        """"""
        # Convert input and output names to lists if they aren't already
        if not isinstance(inplist, (list, tuple)): inplist = [inplist]
        if not isinstance(outlist, (list, tuple)): outlist = [outlist]

        # Check to make sure all systems are consistent
        self.syslist = syslist
        self.syslist_index = {}
        dt = None
        nstates = 0; self.state_offset = []
        ninputs = 0; self.input_offset = []
        noutputs = 0; self.output_offset = []
        system_count = 0
        for sys in syslist:
            # Make sure time bases are consistent
            if dt is None and sys.dt is not None:
                # Timebase was not specified; set to match this system
                dt = sys.dt
            elif dt != sys.dt:
                raise TypeError(""System timebases are not compatible"")

            # Make sure number of inputs, outputs, states is given
            if sys.ninputs is None or sys.noutputs is None or \\
               sys.nstates is None:
                raise TypeError(""System '%s' must define number of inputs, ""
                                ""outputs, states in order to be connected"" %
                                sys)

            # Keep track of the offsets into the states, inputs, outputs
            self.input_offset.append(ninputs)
            self.output_offset.append(noutputs)
            self.state_offset.append(nstates)

            # Keep track of the total number of states, inputs, outputs
            nstates += sys.nstates
            ninputs += sys.ninputs
            noutputs += sys.noutputs

            # Store the index to the system for later retrieval
            # TODO: look for duplicated system names
            self.syslist_index[sys.name] = system_count
            system_count += 1

        # Check for duplicate systems or duplicate names
        sysobj_list = []
        sysname_list = []
        for sys in syslist:
            if sys in sysobj_list:
                warn(""Duplicate object found in system list: %s"" % str(sys))
            elif sys.name is not None and sys.name in sysname_list:
                warn(""Duplicate name found in system list: %s"" % sys.name)
            sysobj_list.append(sys)
            sysname_list.append(sys.name)

        # Create the I/O system
        super(InterconnectedSystem, self).__init__(
            inputs=len(inplist), outputs=len(outlist),
            states=nstates, params=params, dt=dt)

        # If input or output list was specified, update it
        nsignals, self.input_index = \\
            self._process_signal_list(inputs, prefix='u')
        if nsignals is not None and len(inplist) != nsignals:
            raise ValueError(""Wrong number/type of inputs given."")
        nsignals, self.output_index = \\
            self._process_signal_list(outputs, prefix='y')
        if nsignals is not None and len(outlist) != nsignals:
            raise ValueError(""Wrong number/type of outputs given."")

        # Convert the list of interconnections to a connection map (matrix)
        self.connect_map = np.zeros((ninputs, noutputs))
        for connection in connections:
            input_index = self._parse_input_spec(connection[0])
            for output_spec in connection[1:]:
                output_index, gain = self._parse_output_spec(output_spec)
                self.connect_map[input_index, output_index] = gain

        # Convert the input list to a matrix: maps system to subsystems
        self.input_map = np.zeros((ninputs, self.ninputs))
        for index, inpspec in enumerate(inplist):
            if isinstance(inpspec, (int, str, tuple)): inpspec = [inpspec]
            for spec in inpspec:
                self.input_map[self._parse_input_spec(spec), index] = 1

        # Convert the output list to a matrix: maps subsystems to system
        self.output_map = np.zeros((self.noutputs, noutputs + ninputs))
        for index in range(len(outlist)):
            ylist_index, gain = self._parse_output_spec(outlist[index])
            self.output_map[index, ylist_index] = gain

        # Save the parameters for the system
        self.params = params.copy()"
"    def __init__(self,
                 node_ip_address,
                 redis_address,
                 dashboard_agent_port,
                 redis_password=None,
                 temp_dir=None,
                 log_dir=None,
                 metrics_export_port=None,
                 node_manager_port=None,
                 object_store_name=None,
                 raylet_name=None):
        """"""Initialize the DashboardAgent object.""""""
        # Public attributes are accessible for all agent modules.
        self.ip = node_ip_address
        self.redis_address = dashboard_utils.address_tuple(redis_address)
        self.redis_password = redis_password
        self.temp_dir = temp_dir
        self.log_dir = log_dir
        self.dashboard_agent_port = dashboard_agent_port
        self.metrics_export_port = metrics_export_port
        self.node_manager_port = node_manager_port
        self.object_store_name = object_store_name
        self.raylet_name = raylet_name
        self.node_id = os.environ[""RAY_NODE_ID""]
        assert self.node_id, ""Empty node id (RAY_NODE_ID).""
        self.server = aiogrpc.server(options=((""grpc.so_reuseport"", 0), ))
        self.grpc_port = self.server.add_insecure_port(
            f""[::]:{self.dashboard_agent_port}"")
        logger.info(""Dashboard agent grpc address: %s:%s"", self.ip,
                    self.grpc_port)
        self.aioredis_client = None
        self.aiogrpc_raylet_channel = aiogrpc.insecure_channel(
            f""{self.ip}:{self.node_manager_port}"")
        self.http_session = None","    def __init__(self,
                 redis_address,
                 dashboard_agent_port,
                 redis_password=None,
                 temp_dir=None,
                 log_dir=None,
                 metrics_export_port=None,
                 node_manager_port=None,
                 object_store_name=None,
                 raylet_name=None):
        """"""Initialize the DashboardAgent object.""""""
        # Public attributes are accessible for all agent modules.
        self.redis_address = dashboard_utils.address_tuple(redis_address)
        self.redis_password = redis_password
        self.temp_dir = temp_dir
        self.log_dir = log_dir
        self.dashboard_agent_port = dashboard_agent_port
        self.metrics_export_port = metrics_export_port
        self.node_manager_port = node_manager_port
        self.object_store_name = object_store_name
        self.raylet_name = raylet_name
        self.node_id = os.environ[""RAY_NODE_ID""]
        assert self.node_id, ""Empty node id (RAY_NODE_ID).""
        self.ip = ray._private.services.get_node_ip_address()
        self.server = aiogrpc.server(options=((""grpc.so_reuseport"", 0), ))
        self.grpc_port = self.server.add_insecure_port(
            f""[::]:{self.dashboard_agent_port}"")
        logger.info(""Dashboard agent grpc address: %s:%s"", self.ip,
                    self.grpc_port)
        self.aioredis_client = None
        self.aiogrpc_raylet_channel = aiogrpc.insecure_channel(""{}:{}"".format(
            self.ip, self.node_manager_port))
        self.http_session = None"
"def start_raylet(redis_address,
                 node_ip_address,
                 node_manager_port,
                 raylet_name,
                 plasma_store_name,
                 worker_path,
                 temp_dir,
                 session_dir,
                 log_dir,
                 resource_spec,
                 plasma_directory,
                 object_store_memory,
                 min_worker_port=None,
                 max_worker_port=None,
                 worker_port_list=None,
                 object_manager_port=None,
                 redis_password=None,
                 metrics_agent_port=None,
                 metrics_export_port=None,
                 use_valgrind=False,
                 use_profiler=False,
                 stdout_file=None,
                 stderr_file=None,
                 config=None,
                 java_worker_options=None,
                 load_code_from_local=False,
                 huge_pages=False,
                 fate_share=None,
                 socket_to_use=None,
                 head_node=False,
                 start_initial_python_workers_for_first_job=False,
                 code_search_path=None):
    """"""Start a raylet, which is a combined local scheduler and object manager.

    Args:
        redis_address (str): The address of the primary Redis server.
        node_ip_address (str): The IP address of this node.
        node_manager_port(int): The port to use for the node manager. This must
            not be 0.
        raylet_name (str): The name of the raylet socket to create.
        plasma_store_name (str): The name of the plasma store socket to connect
             to.
        worker_path (str): The path of the Python file that new worker
            processes will execute.
        temp_dir (str): The path of the temporary directory Ray will use.
        session_dir (str): The path of this session.
        log_dir (str): The path of the dir where log files are created.
        resource_spec (ResourceSpec): Resources for this raylet.
        object_manager_port: The port to use for the object manager. If this is
            None, then the object manager will choose its own port.
        min_worker_port (int): The lowest port number that workers will bind
            on. If not set, random ports will be chosen.
        max_worker_port (int): The highest port number that workers will bind
            on. If set, min_worker_port must also be set.
        redis_password: The password to use when connecting to Redis.
        metrics_agent_port(int): The port where metrics agent is bound to.
        metrics_export_port(int): The port at which metrics are exposed to.
        use_valgrind (bool): True if the raylet should be started inside
            of valgrind. If this is True, use_profiler must be False.
        use_profiler (bool): True if the raylet should be started inside
            a profiler. If this is True, use_valgrind must be False.
        stdout_file: A file handle opened for writing to redirect stdout to. If
            no redirection should happen, then this should be None.
        stderr_file: A file handle opened for writing to redirect stderr to. If
            no redirection should happen, then this should be None.
        config (dict|None): Optional Raylet configuration that will
            override defaults in RayConfig.
        java_worker_options (list): The command options for Java worker.
        code_search_path (list): Code search path for worker. code_search_path
            is added to worker command in non-multi-tenancy mode and job_config
            in multi-tenancy mode.
    Returns:
        ProcessInfo for the process that was started.
    """"""
    # The caller must provide a node manager port so that we can correctly
    # populate the command to start a worker.
    assert node_manager_port is not None and node_manager_port != 0
    config_str = serialize_config(config)

    if use_valgrind and use_profiler:
        raise ValueError(""Cannot use valgrind and profiler at the same time."")

    assert resource_spec.resolved()
    num_initial_workers = resource_spec.num_cpus
    static_resources = resource_spec.to_resource_dict()

    # Limit the number of workers that can be started in parallel by the
    # raylet. However, make sure it is at least 1.
    num_cpus_static = static_resources.get(""CPU"", 0)
    maximum_startup_concurrency = max(
        1, min(multiprocessing.cpu_count(), num_cpus_static))

    # Format the resource argument in a form like 'CPU,1.0,GPU,0,Custom,3'.
    resource_argument = "","".join(
        [""{},{}"".format(*kv) for kv in static_resources.items()])

    gcs_ip_address, gcs_port = redis_address.split("":"")

    has_java_command = False
    if shutil.which(""java"") is not None:
        has_java_command = True

    ray_java_installed = False
    try:
        jars_dir = get_ray_jars_dir()
        if os.path.exists(jars_dir):
            ray_java_installed = True
    except Exception:
        pass

    include_java = has_java_command and ray_java_installed
    if include_java is True:
        java_worker_command = build_java_worker_command(
            json.loads(java_worker_options) if java_worker_options else [],
            redis_address,
            node_manager_port,
            plasma_store_name,
            raylet_name,
            redis_password,
            session_dir,
            code_search_path,
        )
    else:
        java_worker_command = []

    if os.path.exists(DEFAULT_WORKER_EXECUTABLE):
        cpp_worker_command = build_cpp_worker_command(
            """",
            redis_address,
            node_manager_port,
            plasma_store_name,
            raylet_name,
            redis_password,
            session_dir,
        )
    else:
        cpp_worker_command = []

    # Create the command that the Raylet will use to start workers.
    start_worker_command = [
        sys.executable, worker_path, f""--node-ip-address={node_ip_address}"",
        f""--node-manager-port={node_manager_port}"",
        f""--object-store-name={plasma_store_name}"",
        f""--raylet-name={raylet_name}"", f""--redis-address={redis_address}"",
        f""--config-list={config_str}"", f""--temp-dir={temp_dir}"",
        f""--metrics-agent-port={metrics_agent_port}""
    ]
    if code_search_path:
        start_worker_command.append(f""--code-search-path={code_search_path}"")
    if redis_password:
        start_worker_command += [f""--redis-password={redis_password}""]

    # If the object manager port is None, then use 0 to cause the object
    # manager to choose its own port.
    if object_manager_port is None:
        object_manager_port = 0

    if min_worker_port is None:
        min_worker_port = 0

    if max_worker_port is None:
        max_worker_port = 0

    if code_search_path is not None and len(code_search_path) > 0:
        load_code_from_local = True

    if load_code_from_local:
        start_worker_command += [""--load-code-from-local""]

    # Create agent command
    agent_command = [
        sys.executable,
        ""-u"",
        os.path.join(RAY_PATH, ""new_dashboard/agent.py""),
        f""--node-ip-address={node_ip_address}"",
        f""--redis-address={redis_address}"",
        f""--metrics-export-port={metrics_export_port}"",
        f""--dashboard-agent-port={metrics_agent_port}"",
        f""--node-manager-port={node_manager_port}"",
        f""--object-store-name={plasma_store_name}"",
        f""--raylet-name={raylet_name}"",
        f""--temp-dir={temp_dir}"",
        f""--log-dir={log_dir}"",
    ]

    if redis_password is not None and len(redis_password) != 0:
        agent_command.append(""--redis-password={}"".format(redis_password))

    command = [
        RAYLET_EXECUTABLE,
        f""--raylet_socket_name={raylet_name}"",
        f""--store_socket_name={plasma_store_name}"",
        f""--object_manager_port={object_manager_port}"",
        f""--min_worker_port={min_worker_port}"",
        f""--max_worker_port={max_worker_port}"",
        f""--node_manager_port={node_manager_port}"",
        f""--node_ip_address={node_ip_address}"",
        f""--redis_address={gcs_ip_address}"",
        f""--redis_port={gcs_port}"",
        f""--num_initial_workers={num_initial_workers}"",
        f""--maximum_startup_concurrency={maximum_startup_concurrency}"",
        f""--static_resource_list={resource_argument}"",
        f""--config_list={config_str}"",
        f""--python_worker_command={subprocess.list2cmdline(start_worker_command)}"",  # noqa
        f""--java_worker_command={subprocess.list2cmdline(java_worker_command)}"",  # noqa
        f""--cpp_worker_command={subprocess.list2cmdline(cpp_worker_command)}"",  # noqa
        f""--redis_password={redis_password or ''}"",
        f""--temp_dir={temp_dir}"",
        f""--session_dir={session_dir}"",
        f""--metrics-agent-port={metrics_agent_port}"",
        f""--metrics_export_port={metrics_export_port}"",
    ]
    if worker_port_list is not None:
        command.append(f""--worker_port_list={worker_port_list}"")
    if start_initial_python_workers_for_first_job:
        command.append(""--num_initial_python_workers_for_first_job={}"".format(
            resource_spec.num_cpus))
    command.append(""--agent_command={}"".format(
        subprocess.list2cmdline(agent_command)))
    if config.get(""plasma_store_as_thread""):
        # command related to the plasma store
        command += [
            f""--object_store_memory={object_store_memory}"",
            f""--plasma_directory={plasma_directory}"",
        ]
        if huge_pages:
            command.append(""--huge_pages"")
    if socket_to_use:
        socket_to_use.close()
    if head_node:
        command.append(""--head_node"")
    process_info = start_ray_process(
        command,
        ray_constants.PROCESS_TYPE_RAYLET,
        use_valgrind=use_valgrind,
        use_gdb=False,
        use_valgrind_profiler=use_profiler,
        use_perftools_profiler=(""RAYLET_PERFTOOLS_PATH"" in os.environ),
        stdout_file=stdout_file,
        stderr_file=stderr_file,
        fate_share=fate_share)

    return process_info","def start_raylet(redis_address,
                 node_ip_address,
                 node_manager_port,
                 raylet_name,
                 plasma_store_name,
                 worker_path,
                 temp_dir,
                 session_dir,
                 log_dir,
                 resource_spec,
                 plasma_directory,
                 object_store_memory,
                 min_worker_port=None,
                 max_worker_port=None,
                 worker_port_list=None,
                 object_manager_port=None,
                 redis_password=None,
                 metrics_agent_port=None,
                 metrics_export_port=None,
                 use_valgrind=False,
                 use_profiler=False,
                 stdout_file=None,
                 stderr_file=None,
                 config=None,
                 java_worker_options=None,
                 load_code_from_local=False,
                 huge_pages=False,
                 fate_share=None,
                 socket_to_use=None,
                 head_node=False,
                 start_initial_python_workers_for_first_job=False,
                 code_search_path=None):
    """"""Start a raylet, which is a combined local scheduler and object manager.

    Args:
        redis_address (str): The address of the primary Redis server.
        node_ip_address (str): The IP address of this node.
        node_manager_port(int): The port to use for the node manager. This must
            not be 0.
        raylet_name (str): The name of the raylet socket to create.
        plasma_store_name (str): The name of the plasma store socket to connect
             to.
        worker_path (str): The path of the Python file that new worker
            processes will execute.
        temp_dir (str): The path of the temporary directory Ray will use.
        session_dir (str): The path of this session.
        log_dir (str): The path of the dir where log files are created.
        resource_spec (ResourceSpec): Resources for this raylet.
        object_manager_port: The port to use for the object manager. If this is
            None, then the object manager will choose its own port.
        min_worker_port (int): The lowest port number that workers will bind
            on. If not set, random ports will be chosen.
        max_worker_port (int): The highest port number that workers will bind
            on. If set, min_worker_port must also be set.
        redis_password: The password to use when connecting to Redis.
        metrics_agent_port(int): The port where metrics agent is bound to.
        metrics_export_port(int): The port at which metrics are exposed to.
        use_valgrind (bool): True if the raylet should be started inside
            of valgrind. If this is True, use_profiler must be False.
        use_profiler (bool): True if the raylet should be started inside
            a profiler. If this is True, use_valgrind must be False.
        stdout_file: A file handle opened for writing to redirect stdout to. If
            no redirection should happen, then this should be None.
        stderr_file: A file handle opened for writing to redirect stderr to. If
            no redirection should happen, then this should be None.
        config (dict|None): Optional Raylet configuration that will
            override defaults in RayConfig.
        java_worker_options (list): The command options for Java worker.
        code_search_path (list): Code search path for worker. code_search_path
            is added to worker command in non-multi-tenancy mode and job_config
            in multi-tenancy mode.
    Returns:
        ProcessInfo for the process that was started.
    """"""
    # The caller must provide a node manager port so that we can correctly
    # populate the command to start a worker.
    assert node_manager_port is not None and node_manager_port != 0
    config_str = serialize_config(config)

    if use_valgrind and use_profiler:
        raise ValueError(""Cannot use valgrind and profiler at the same time."")

    assert resource_spec.resolved()
    num_initial_workers = resource_spec.num_cpus
    static_resources = resource_spec.to_resource_dict()

    # Limit the number of workers that can be started in parallel by the
    # raylet. However, make sure it is at least 1.
    num_cpus_static = static_resources.get(""CPU"", 0)
    maximum_startup_concurrency = max(
        1, min(multiprocessing.cpu_count(), num_cpus_static))

    # Format the resource argument in a form like 'CPU,1.0,GPU,0,Custom,3'.
    resource_argument = "","".join(
        [""{},{}"".format(*kv) for kv in static_resources.items()])

    gcs_ip_address, gcs_port = redis_address.split("":"")

    has_java_command = False
    if shutil.which(""java"") is not None:
        has_java_command = True

    ray_java_installed = False
    try:
        jars_dir = get_ray_jars_dir()
        if os.path.exists(jars_dir):
            ray_java_installed = True
    except Exception:
        pass

    include_java = has_java_command and ray_java_installed
    if include_java is True:
        java_worker_command = build_java_worker_command(
            json.loads(java_worker_options) if java_worker_options else [],
            redis_address,
            node_manager_port,
            plasma_store_name,
            raylet_name,
            redis_password,
            session_dir,
            code_search_path,
        )
    else:
        java_worker_command = []

    if os.path.exists(DEFAULT_WORKER_EXECUTABLE):
        cpp_worker_command = build_cpp_worker_command(
            """",
            redis_address,
            node_manager_port,
            plasma_store_name,
            raylet_name,
            redis_password,
            session_dir,
        )
    else:
        cpp_worker_command = []

    # Create the command that the Raylet will use to start workers.
    start_worker_command = [
        sys.executable, worker_path, f""--node-ip-address={node_ip_address}"",
        f""--node-manager-port={node_manager_port}"",
        f""--object-store-name={plasma_store_name}"",
        f""--raylet-name={raylet_name}"", f""--redis-address={redis_address}"",
        f""--config-list={config_str}"", f""--temp-dir={temp_dir}"",
        f""--metrics-agent-port={metrics_agent_port}""
    ]
    if code_search_path:
        start_worker_command.append(f""--code-search-path={code_search_path}"")
    if redis_password:
        start_worker_command += [f""--redis-password={redis_password}""]

    # If the object manager port is None, then use 0 to cause the object
    # manager to choose its own port.
    if object_manager_port is None:
        object_manager_port = 0

    if min_worker_port is None:
        min_worker_port = 0

    if max_worker_port is None:
        max_worker_port = 0

    if code_search_path is not None and len(code_search_path) > 0:
        load_code_from_local = True

    if load_code_from_local:
        start_worker_command += [""--load-code-from-local""]

    # Create agent command
    agent_command = [
        sys.executable,
        ""-u"",
        os.path.join(RAY_PATH, ""new_dashboard/agent.py""),
        f""--redis-address={redis_address}"",
        f""--metrics-export-port={metrics_export_port}"",
        f""--dashboard-agent-port={metrics_agent_port}"",
        f""--node-manager-port={node_manager_port}"",
        f""--object-store-name={plasma_store_name}"",
        f""--raylet-name={raylet_name}"",
        f""--temp-dir={temp_dir}"",
        f""--log-dir={log_dir}"",
    ]

    if redis_password is not None and len(redis_password) != 0:
        agent_command.append(""--redis-password={}"".format(redis_password))

    command = [
        RAYLET_EXECUTABLE,
        f""--raylet_socket_name={raylet_name}"",
        f""--store_socket_name={plasma_store_name}"",
        f""--object_manager_port={object_manager_port}"",
        f""--min_worker_port={min_worker_port}"",
        f""--max_worker_port={max_worker_port}"",
        f""--node_manager_port={node_manager_port}"",
        f""--node_ip_address={node_ip_address}"",
        f""--redis_address={gcs_ip_address}"",
        f""--redis_port={gcs_port}"",
        f""--num_initial_workers={num_initial_workers}"",
        f""--maximum_startup_concurrency={maximum_startup_concurrency}"",
        f""--static_resource_list={resource_argument}"",
        f""--config_list={config_str}"",
        f""--python_worker_command={subprocess.list2cmdline(start_worker_command)}"",  # noqa
        f""--java_worker_command={subprocess.list2cmdline(java_worker_command)}"",  # noqa
        f""--cpp_worker_command={subprocess.list2cmdline(cpp_worker_command)}"",  # noqa
        f""--redis_password={redis_password or ''}"",
        f""--temp_dir={temp_dir}"",
        f""--session_dir={session_dir}"",
        f""--metrics-agent-port={metrics_agent_port}"",
        f""--metrics_export_port={metrics_export_port}"",
    ]
    if worker_port_list is not None:
        command.append(f""--worker_port_list={worker_port_list}"")
    if start_initial_python_workers_for_first_job:
        command.append(""--num_initial_python_workers_for_first_job={}"".format(
            resource_spec.num_cpus))
    command.append(""--agent_command={}"".format(
        subprocess.list2cmdline(agent_command)))
    if config.get(""plasma_store_as_thread""):
        # command related to the plasma store
        command += [
            f""--object_store_memory={object_store_memory}"",
            f""--plasma_directory={plasma_directory}"",
        ]
        if huge_pages:
            command.append(""--huge_pages"")
    if socket_to_use:
        socket_to_use.close()
    if head_node:
        command.append(""--head_node"")
    process_info = start_ray_process(
        command,
        ray_constants.PROCESS_TYPE_RAYLET,
        use_valgrind=use_valgrind,
        use_gdb=False,
        use_valgrind_profiler=use_profiler,
        use_perftools_profiler=(""RAYLET_PERFTOOLS_PATH"" in os.environ),
        stdout_file=stdout_file,
        stderr_file=stderr_file,
        fate_share=fate_share)

    return process_info"
"    def __init__(self,
                 redis_address,
                 autoscaling_config,
                 redis_password=None,
                 prefix_cluster_info=False):
        # Initialize the Redis clients.
        ray.state.state._initialize_global_state(
            redis_address, redis_password=redis_password)
        self.redis = ray._private.services.create_redis_client(
            redis_address, password=redis_password)

        # Initialize the gcs stub for getting all node resource usage.
        gcs_address = self.redis.get(""GcsServerAddress"").decode(""utf-8"")
        options = ((""grpc.enable_http_proxy"", 0), )
        gcs_channel = grpc.insecure_channel(gcs_address, options=options)
        self.gcs_node_resources_stub = \\
            gcs_service_pb2_grpc.NodeResourceInfoGcsServiceStub(gcs_channel)

        # Set the redis client and mode so _internal_kv works for autoscaler.
        worker = ray.worker.global_worker
        worker.redis_client = self.redis
        worker.mode = 0
        head_node_ip = redis_address.split("":"")[0]
        self.load_metrics = LoadMetrics(local_ip=head_node_ip)
        self.last_avail_resources = None
        self.event_summarizer = EventSummarizer()
        self.prefix_cluster_info = prefix_cluster_info
        self.autoscaling_config = autoscaling_config
        self.autoscaler = None

        logger.info(""Monitor: Started"")","    def __init__(self,
                 redis_address,
                 autoscaling_config,
                 redis_password=None,
                 prefix_cluster_info=False):
        # Initialize the Redis clients.
        ray.state.state._initialize_global_state(
            redis_address, redis_password=redis_password)
        self.redis = ray._private.services.create_redis_client(
            redis_address, password=redis_password)

        # Initialize the gcs stub for getting all node resource usage.
        gcs_address = self.redis.get(""GcsServerAddress"").decode(""utf-8"")
        gcs_channel = grpc.insecure_channel(gcs_address)
        self.gcs_node_resources_stub = \\
            gcs_service_pb2_grpc.NodeResourceInfoGcsServiceStub(gcs_channel)

        # Set the redis client and mode so _internal_kv works for autoscaler.
        worker = ray.worker.global_worker
        worker.redis_client = self.redis
        worker.mode = 0
        head_node_ip = redis_address.split("":"")[0]
        self.load_metrics = LoadMetrics(local_ip=head_node_ip)
        self.last_avail_resources = None
        self.event_summarizer = EventSummarizer()
        self.prefix_cluster_info = prefix_cluster_info
        self.autoscaling_config = autoscaling_config
        self.autoscaler = None

        logger.info(""Monitor: Started"")"
"    def terminate_node(self, node_id):
        node = self._get_cached_node(node_id)
        if self.cache_stopped_nodes:
            if node.spot_instance_request_id:
                cli_logger.print(
                    ""Terminating instance {} "" +
                    cf.dimmed(""(cannot stop spot instances, only terminate)""),
                    node_id)  # todo: show node name?
                node.terminate()
            else:
                cli_logger.print(""Stopping instance {} "" + cf.dimmed(
                    ""(to terminate instead, ""
                    ""set `cache_stopped_nodes: False` ""
                    ""under `provider` in the cluster configuration)""),
                                 node_id)  # todo: show node name?
                node.stop()
        else:
            node.terminate()

        # TODO (Alex): We are leaking the tag cache here. Naively, we would
        # want to just remove the cache entry here, but terminating can be
        # asyncrhonous or error, which would result in a use after free error.
        # If this leak becomes bad, we can garbage collect the tag cache when
        # the node cache is updated.
        pass","    def terminate_node(self, node_id):
        node = self._get_cached_node(node_id)
        if self.cache_stopped_nodes:
            if node.spot_instance_request_id:
                cli_logger.print(
                    ""Terminating instance {} "" +
                    cf.dimmed(""(cannot stop spot instances, only terminate)""),
                    node_id)  # todo: show node name?
                node.terminate()
            else:
                cli_logger.print(""Stopping instance {} "" + cf.dimmed(
                    ""(to terminate instead, ""
                    ""set `cache_stopped_nodes: False` ""
                    ""under `provider` in the cluster configuration)""),
                                 node_id)  # todo: show node name?
                node.stop()
        else:
            node.terminate()

        self.tag_cache.pop(node_id, None)
        self.tag_cache_pending.pop(node_id, None)"
"    def terminate_nodes(self, node_ids):
        if not node_ids:
            return
        if self.cache_stopped_nodes:
            spot_ids = []
            on_demand_ids = []

            for node_id in node_ids:
                if self._get_cached_node(node_id).spot_instance_request_id:
                    spot_ids += [node_id]
                else:
                    on_demand_ids += [node_id]

            if on_demand_ids:
                # todo: show node names?
                cli_logger.print(
                    ""Stopping instances {} "" + cf.dimmed(
                        ""(to terminate instead, ""
                        ""set `cache_stopped_nodes: False` ""
                        ""under `provider` in the cluster configuration)""),
                    cli_logger.render_list(on_demand_ids))

                self.ec2.meta.client.stop_instances(InstanceIds=on_demand_ids)
            if spot_ids:
                cli_logger.print(
                    ""Terminating instances {} "" +
                    cf.dimmed(""(cannot stop spot instances, only terminate)""),
                    cli_logger.render_list(spot_ids))

                self.ec2.meta.client.terminate_instances(InstanceIds=spot_ids)
        else:
            self.ec2.meta.client.terminate_instances(InstanceIds=node_ids)","    def terminate_nodes(self, node_ids):
        if not node_ids:
            return
        if self.cache_stopped_nodes:
            spot_ids = []
            on_demand_ids = []

            for node_id in node_ids:
                if self._get_cached_node(node_id).spot_instance_request_id:
                    spot_ids += [node_id]
                else:
                    on_demand_ids += [node_id]

            if on_demand_ids:
                # todo: show node names?
                cli_logger.print(
                    ""Stopping instances {} "" + cf.dimmed(
                        ""(to terminate instead, ""
                        ""set `cache_stopped_nodes: False` ""
                        ""under `provider` in the cluster configuration)""),
                    cli_logger.render_list(on_demand_ids))

                self.ec2.meta.client.stop_instances(InstanceIds=on_demand_ids)
            if spot_ids:
                cli_logger.print(
                    ""Terminating instances {} "" +
                    cf.dimmed(""(cannot stop spot instances, only terminate)""),
                    cli_logger.render_list(spot_ids))

                self.ec2.meta.client.terminate_instances(InstanceIds=spot_ids)
        else:
            self.ec2.meta.client.terminate_instances(InstanceIds=node_ids)

        for node_id in node_ids:
            self.tag_cache.pop(node_id, None)
            self.tag_cache_pending.pop(node_id, None)"
"    def _print(self,
               msg: str,
               _level_str: str = ""INFO"",
               _linefeed: bool = True):
        """"""Proxy for printing messages.

        Args:
            msg (str): Message to print.
            linefeed (bool):
                If `linefeed` is `False` no linefeed is printed at the
                end of the message.
        """"""
        if self.pretty:
            rendered_message = ""  "" * self.indent_level + msg
        else:
            if msg.strip() == """":
                return
            caller_info = _external_caller_info()
            record = logging.LogRecord(
                name=""cli"",
                # We override the level name later
                # TODO(maximsmol): give approximate level #s to our log levels
                level=0,
                # The user-facing logs do not need this information anyway
                # and it would be very tedious to extract since _print
                # can be at varying depths in the call stack
                # TODO(maximsmol): do it anyway to be extra
                pathname=caller_info[""filename""],
                lineno=caller_info[""lineno""],
                msg=msg,
                args={},
                # No exception
                exc_info=None)
            record.levelname = _level_str
            rendered_message = self._formatter.format(record)

        # We aren't using standard python logging convention, so we hardcode
        # the log levels for now.
        if _level_str in [""WARNING"", ""ERROR"", ""PANIC""]:
            stream = sys.stderr
        else:
            stream = sys.stdout

        if not _linefeed:
            stream.write(rendered_message)
            stream.flush()
            return

        print(rendered_message, file=stream)","    def _print(self,
               msg: str,
               _level_str: str = ""INFO"",
               _linefeed: bool = True):
        """"""Proxy for printing messages.

        Args:
            msg (str): Message to print.
            linefeed (bool):
                If `linefeed` is `False` no linefeed is printed at the
                end of the message.
        """"""
        if self.pretty:
            rendered_message = ""  "" * self.indent_level + msg
        else:
            if msg.strip() == """":
                return
            caller_info = _external_caller_info()
            record = logging.LogRecord(
                name=""cli"",
                # We override the level name later
                # TODO(maximsmol): give approximate level #s to our log levels
                level=0,
                # The user-facing logs do not need this information anyway
                # and it would be very tedious to extract since _print
                # can be at varying depths in the call stack
                # TODO(maximsmol): do it anyway to be extra
                pathname=caller_info[""filename""],
                lineno=caller_info[""lineno""],
                msg=msg,
                args={},
                # No exception
                exc_info=None)
            record.levelname = _level_str
            rendered_message = self._formatter.format(record)

        if not _linefeed:
            sys.stdout.write(rendered_message)
            sys.stdout.flush()
            return

        print(rendered_message)"
"    def _handle_failure(self, error):
        logger.exception(""Error in monitor loop"")
        if self.autoscaler is not None and \\
           os.environ.get(""RAY_AUTOSCALER_FATESHARE_WORKERS"", """") == ""1"":
            self.autoscaler.kill_workers()
            # Take down autoscaler workers if necessary.
            self.destroy_autoscaler_workers()

        # Something went wrong, so push an error to all current and future
        # drivers.
        message = f""The autoscaler failed with the following error:\\n{error}""
        if _internal_kv_initialized():
            _internal_kv_put(DEBUG_AUTOSCALING_ERROR, message, overwrite=True)
        redis_client = ray._private.services.create_redis_client(
            args.redis_address, password=args.redis_password)
        from ray.utils import push_error_to_driver_through_redis
        push_error_to_driver_through_redis(
            redis_client, ray_constants.MONITOR_DIED_ERROR, message)","    def _handle_failure(self, error):
        logger.exception(""Error in monitor loop"")
        if self.autoscaler is not None:
            self.autoscaler.kill_workers()
            # Take down autoscaler workers if necessary.
            self.destroy_autoscaler_workers()

        # Something went wrong, so push an error to all current and future
        # drivers.
        message = f""The autoscaler failed with the following error:\\n{error}""
        if _internal_kv_initialized():
            _internal_kv_put(DEBUG_AUTOSCALING_ERROR, message, overwrite=True)
        redis_client = ray._private.services.create_redis_client(
            args.redis_address, password=args.redis_password)
        from ray.utils import push_error_to_driver_through_redis
        push_error_to_driver_through_redis(
            redis_client, ray_constants.MONITOR_DIED_ERROR, message)"
"    def close_all_files(self):
        """"""Close all open files (so that we can open more).""""""
        while len(self.open_file_infos) > 0:
            file_info = self.open_file_infos.pop(0)
            file_info.file_handle.close()
            file_info.file_handle = None
            try:
                # Test if the worker process that generated the log file
                # is still alive. Only applies to worker processes.
                if (file_info.worker_pid != ""raylet""
                        and file_info.worker_pid != ""gcs_server""
                        and file_info.worker_pid != ""autoscaler""):
                    os.kill(file_info.worker_pid, 0)
            except OSError:
                # The process is not alive any more, so move the log file
                # out of the log directory so glob.glob will not be slowed
                # by it.
                target = os.path.join(self.logs_dir, ""old"",
                                      os.path.basename(file_info.filename))
                try:
                    shutil.move(file_info.filename, target)
                except (IOError, OSError) as e:
                    if e.errno == errno.ENOENT:
                        logger.warning(
                            f""Warning: The file {file_info.filename} ""
                            ""was not found."")
                    else:
                        raise e
            else:
                self.closed_file_infos.append(file_info)
        self.can_open_more_files = True","    def close_all_files(self):
        """"""Close all open files (so that we can open more).""""""
        while len(self.open_file_infos) > 0:
            file_info = self.open_file_infos.pop(0)
            file_info.file_handle.close()
            file_info.file_handle = None
            try:
                # Test if the worker process that generated the log file
                # is still alive. Only applies to worker processes.
                if (file_info.worker_pid != ""raylet""
                        and file_info.worker_pid != ""gcs_server""):
                    os.kill(file_info.worker_pid, 0)
            except OSError:
                # The process is not alive any more, so move the log file
                # out of the log directory so glob.glob will not be slowed
                # by it.
                target = os.path.join(self.logs_dir, ""old"",
                                      os.path.basename(file_info.filename))
                try:
                    shutil.move(file_info.filename, target)
                except (IOError, OSError) as e:
                    if e.errno == errno.ENOENT:
                        logger.warning(
                            f""Warning: The file {file_info.filename} ""
                            ""was not found."")
                    else:
                        raise e
            else:
                self.closed_file_infos.append(file_info)
        self.can_open_more_files = True"
"    def update_log_filenames(self):
        """"""Update the list of log files to monitor.""""""
        # output of user code is written here
        log_file_paths = glob.glob(f""{self.logs_dir}/worker*[.out|.err]"")
        # segfaults and other serious errors are logged here
        raylet_err_paths = glob.glob(f""{self.logs_dir}/raylet*.err"")
        # If gcs server restarts, there can be multiple log files.
        gcs_err_path = glob.glob(f""{self.logs_dir}/gcs_server*.err"")
        for file_path in log_file_paths + raylet_err_paths + gcs_err_path:
            if os.path.isfile(
                    file_path) and file_path not in self.log_filenames:
                job_match = JOB_LOG_PATTERN.match(file_path)
                if job_match:
                    job_id = job_match.group(2)
                    worker_pid = int(job_match.group(3))
                else:
                    job_id = None
                    worker_pid = None

                is_err_file = file_path.endswith(""err"")

                self.log_filenames.add(file_path)
                self.closed_file_infos.append(
                    LogFileInfo(
                        filename=file_path,
                        size_when_last_opened=0,
                        file_position=0,
                        file_handle=None,
                        is_err_file=is_err_file,
                        job_id=job_id,
                        worker_pid=worker_pid))
                log_filename = os.path.basename(file_path)
                logger.info(f""Beginning to track file {log_filename}"")","    def update_log_filenames(self):
        """"""Update the list of log files to monitor.""""""
        # output of user code is written here
        log_file_paths = glob.glob(f""{self.logs_dir}/worker*[.out|.err]"")
        # segfaults and other serious errors are logged here
        raylet_err_paths = glob.glob(f""{self.logs_dir}/raylet*.err"")
        # If gcs server restarts, there can be multiple log files.
        gcs_err_path = glob.glob(f""{self.logs_dir}/gcs_server*.err"")
        for file_path in log_file_paths + raylet_err_paths + gcs_err_path:
            if os.path.isfile(
                    file_path) and file_path not in self.log_filenames:
                job_match = JOB_LOG_PATTERN.match(file_path)
                if job_match:
                    job_id = job_match.group(2)
                    worker_pid = job_match.group(3)
                else:
                    job_id = None
                    worker_pid = None

                is_err_file = file_path.endswith(""err"")

                self.log_filenames.add(file_path)
                self.closed_file_infos.append(
                    LogFileInfo(
                        filename=file_path,
                        size_when_last_opened=0,
                        file_position=0,
                        file_handle=None,
                        is_err_file=is_err_file,
                        job_id=job_id,
                        worker_pid=worker_pid))
                log_filename = os.path.basename(file_path)
                logger.info(f""Beginning to track file {log_filename}"")"
"    async def run(self):
        async def _check_parent():
            """"""Check if raylet is dead and fate-share if it is.""""""
            try:
                curr_proc = psutil.Process()
                while True:
                    parent = curr_proc.parent()
                    if (parent is None or parent.pid == 1
                            or self.ppid != parent.pid):
                        logger.error(""Raylet is dead, exiting."")
                        sys.exit(0)
                    await asyncio.sleep(
                        dashboard_consts.
                        DASHBOARD_AGENT_CHECK_PARENT_INTERVAL_SECONDS)
            except Exception:
                logger.error(""Failed to check parent PID, exiting."")
                sys.exit(1)

        if sys.platform not in [""win32"", ""cygwin""]:
            check_parent_task = create_task(_check_parent())

        # Create an aioredis client for all modules.
        try:
            self.aioredis_client = await dashboard_utils.get_aioredis_client(
                self.redis_address, self.redis_password,
                dashboard_consts.CONNECT_REDIS_INTERNAL_SECONDS,
                dashboard_consts.RETRY_REDIS_CONNECTION_TIMES)
        except (socket.gaierror, ConnectionRefusedError):
            logger.error(
                ""Dashboard agent exiting: ""
                ""Failed to connect to redis at %s"", self.redis_address)
            sys.exit(-1)

        # Create a http session for all modules.
        self.http_session = aiohttp.ClientSession(
            loop=asyncio.get_event_loop())

        # Start a grpc asyncio server.
        await self.server.start()

        modules = self._load_modules()

        # Http server should be initialized after all modules loaded.
        app = aiohttp.web.Application()
        app.add_routes(routes=routes.bound_routes())

        # Enable CORS on all routes.
        cors = aiohttp_cors.setup(
            app,
            defaults={
                ""*"": aiohttp_cors.ResourceOptions(
                    allow_credentials=True,
                    expose_headers=""*"",
                    allow_methods=""*"",
                    allow_headers=(""Content-Type"", ""X-Header""),
                )
            })
        for route in list(app.router.routes()):
            cors.add(route)

        runner = aiohttp.web.AppRunner(app)
        await runner.setup()
        site = aiohttp.web.TCPSite(runner, self.ip, 0)
        await site.start()
        http_host, http_port, *_ = site._server.sockets[0].getsockname()
        logger.info(""Dashboard agent http address: %s:%s"", http_host,
                    http_port)

        # Dump registered http routes.
        dump_routes = [
            r for r in app.router.routes() if r.method != hdrs.METH_HEAD
        ]
        for r in dump_routes:
            logger.info(r)
        logger.info(""Registered %s routes."", len(dump_routes))

        # Write the dashboard agent port to redis.
        await self.aioredis_client.set(
            f""{dashboard_consts.DASHBOARD_AGENT_PORT_PREFIX}{self.node_id}"",
            json.dumps([http_port, self.grpc_port]))

        # Register agent to agent manager.
        raylet_stub = agent_manager_pb2_grpc.AgentManagerServiceStub(
            self.aiogrpc_raylet_channel)

        await raylet_stub.RegisterAgent(
            agent_manager_pb2.RegisterAgentRequest(
                agent_pid=os.getpid(),
                agent_port=self.grpc_port,
                agent_ip_address=self.ip))

        tasks = [m.run(self.server) for m in modules]
        if sys.platform not in [""win32"", ""cygwin""]:
            tasks.append(check_parent_task)
        await asyncio.gather(*tasks)

        await self.server.wait_for_termination()
        # Wait for finish signal.
        await runner.cleanup()","    async def run(self):
        async def _check_parent():
            """"""Check if raylet is dead and fate-share if it is.""""""
            try:
                curr_proc = psutil.Process()
                while True:
                    parent = curr_proc.parent()
                    if (parent is None or parent.pid == 1
                            or self.ppid != parent.pid):
                        logger.error(""Raylet is dead, exiting."")
                        sys.exit(0)
                    await asyncio.sleep(
                        dashboard_consts.
                        DASHBOARD_AGENT_CHECK_PARENT_INTERVAL_SECONDS)
            except Exception:
                logger.error(""Failed to check parent PID, exiting."")
                sys.exit(1)

        if sys.platform not in [""win32"", ""cygwin""]:
            check_parent_task = create_task(_check_parent())

        # Create an aioredis client for all modules.
        try:
            self.aioredis_client = await dashboard_utils.get_aioredis_client(
                self.redis_address, self.redis_password,
                dashboard_consts.CONNECT_REDIS_INTERNAL_SECONDS,
                dashboard_consts.RETRY_REDIS_CONNECTION_TIMES)
        except (socket.gaierror, ConnectionRefusedError):
            logger.error(
                ""Dashboard agent exiting: ""
                ""Failed to connect to redis at %s"", self.redis_address)
            sys.exit(-1)

        # Create a http session for all modules.
        self.http_session = aiohttp.ClientSession(
            loop=asyncio.get_event_loop())

        # Start a grpc asyncio server.
        await self.server.start()

        modules = self._load_modules()

        # Http server should be initialized after all modules loaded.
        app = aiohttp.web.Application()
        app.add_routes(routes=routes.bound_routes())

        # Enable CORS on all routes.
        cors = aiohttp_cors.setup(
            app,
            defaults={
                ""*"": aiohttp_cors.ResourceOptions(
                    allow_credentials=True,
                    expose_headers=""*"",
                    allow_methods=""*"",
                    allow_headers=(""Content-Type"", ""X-Header""),
                )
            })
        for route in list(app.router.routes()):
            cors.add(route)

        runner = aiohttp.web.AppRunner(app)
        await runner.setup()
        site = aiohttp.web.TCPSite(runner, self.ip, 0)
        await site.start()
        http_host, http_port = site._server.sockets[0].getsockname()
        logger.info(""Dashboard agent http address: %s:%s"", http_host,
                    http_port)

        # Dump registered http routes.
        dump_routes = [
            r for r in app.router.routes() if r.method != hdrs.METH_HEAD
        ]
        for r in dump_routes:
            logger.info(r)
        logger.info(""Registered %s routes."", len(dump_routes))

        # Write the dashboard agent port to redis.
        await self.aioredis_client.set(
            f""{dashboard_consts.DASHBOARD_AGENT_PORT_PREFIX}{self.node_id}"",
            json.dumps([http_port, self.grpc_port]))

        # Register agent to agent manager.
        raylet_stub = agent_manager_pb2_grpc.AgentManagerServiceStub(
            self.aiogrpc_raylet_channel)

        await raylet_stub.RegisterAgent(
            agent_manager_pb2.RegisterAgentRequest(
                agent_pid=os.getpid(),
                agent_port=self.grpc_port,
                agent_ip_address=self.ip))

        tasks = [m.run(self.server) for m in modules]
        if sys.platform not in [""win32"", ""cygwin""]:
            tasks.append(check_parent_task)
        await asyncio.gather(*tasks)

        await self.server.wait_for_termination()
        # Wait for finish signal.
        await runner.cleanup()"
"def setup_static_dir():
    build_dir = os.path.join(
        os.path.dirname(os.path.abspath(__file__)), ""client"", ""build"")
    module_name = os.path.basename(os.path.dirname(__file__))
    if not os.path.isdir(build_dir):
        raise FrontendNotFoundError(
            errno.ENOENT, ""Dashboard build directory not found. If installing ""
            ""from source, please follow the additional steps ""
            ""required to build the dashboard""
            f""(cd python/ray/{module_name}/client ""
            ""&& npm install ""
            ""&& npm ci ""
            ""&& npm run build)"", build_dir)

    static_dir = os.path.join(build_dir, ""static"")
    routes.static(""/static"", static_dir, follow_symlinks=True)
    return build_dir","def setup_static_dir():
    build_dir = os.path.join(
        os.path.dirname(os.path.abspath(__file__)), ""client"", ""build"")
    module_name = os.path.basename(os.path.dirname(__file__))
    if not os.path.isdir(build_dir):
        raise OSError(
            errno.ENOENT, ""Dashboard build directory not found. If installing ""
            ""from source, please follow the additional steps ""
            ""required to build the dashboard""
            f""(cd python/ray/{module_name}/client ""
            ""&& npm install ""
            ""&& npm ci ""
            ""&& npm run build)"", build_dir)

    static_dir = os.path.join(build_dir, ""static"")
    routes.static(""/static"", static_dir, follow_symlinks=True)
    return build_dir"
"    def __init__(self,
                 host,
                 port,
                 port_retries,
                 redis_address,
                 redis_password=None,
                 log_dir=None):
        self.dashboard_head = dashboard_head.DashboardHead(
            http_host=host,
            http_port=port,
            http_port_retries=port_retries,
            redis_address=redis_address,
            redis_password=redis_password,
            log_dir=log_dir)

        # Setup Dashboard Routes
        try:
            build_dir = setup_static_dir()
            logger.info(""Setup static dir for dashboard: %s"", build_dir)
        except FrontendNotFoundError as ex:
            # Not to raise FrontendNotFoundError due to NPM incompatibilities
            # with Windows.
            # Please refer to ci.sh::build_dashboard_front_end()
            if sys.platform in [""win32"", ""cygwin""]:
                logger.warning(ex)
            else:
                raise ex
        dashboard_utils.ClassMethodRouteTable.bind(self)","    def __init__(self,
                 host,
                 port,
                 redis_address,
                 redis_password=None,
                 log_dir=None):
        self.dashboard_head = dashboard_head.DashboardHead(
            http_host=host,
            http_port=port,
            redis_address=redis_address,
            redis_password=redis_password,
            log_dir=log_dir)

        # Setup Dashboard Routes
        build_dir = setup_static_dir()
        logger.info(""Setup static dir for dashboard: %s"", build_dir)
        dashboard_utils.ClassMethodRouteTable.bind(self)"
"    def __init__(self, http_host, http_port, http_port_retries, redis_address,
                 redis_password, log_dir):
        # NodeInfoGcsService
        self._gcs_node_info_stub = None
        self._gcs_rpc_error_counter = 0
        # Public attributes are accessible for all head modules.
        # Walkaround for issue: https://github.com/ray-project/ray/issues/7084
        self.http_host = ""127.0.0.1"" if http_host == ""localhost"" else http_host
        self.http_port = http_port
        self.http_port_retries = http_port_retries
        self.redis_address = dashboard_utils.address_tuple(redis_address)
        self.redis_password = redis_password
        self.log_dir = log_dir
        self.aioredis_client = None
        self.aiogrpc_gcs_channel = None
        self.http_session = None
        self.ip = ray._private.services.get_node_ip_address()
        self.server = aiogrpc.server(options=((""grpc.so_reuseport"", 0), ))
        self.grpc_port = self.server.add_insecure_port(""[::]:0"")
        logger.info(""Dashboard head grpc address: %s:%s"", self.ip,
                    self.grpc_port)","    def __init__(self, http_host, http_port, redis_address, redis_password,
                 log_dir):
        # NodeInfoGcsService
        self._gcs_node_info_stub = None
        self._gcs_rpc_error_counter = 0
        # Public attributes are accessible for all head modules.
        self.http_host = http_host
        self.http_port = http_port
        self.redis_address = dashboard_utils.address_tuple(redis_address)
        self.redis_password = redis_password
        self.log_dir = log_dir
        self.aioredis_client = None
        self.aiogrpc_gcs_channel = None
        self.http_session = None
        self.ip = ray._private.services.get_node_ip_address()
        self.server = aiogrpc.server(options=((""grpc.so_reuseport"", 0), ))
        self.grpc_port = self.server.add_insecure_port(""[::]:0"")
        logger.info(""Dashboard head grpc address: %s:%s"", self.ip,
                    self.grpc_port)
        logger.info(""Dashboard head http address: %s:%s"", self.http_host,
                    self.http_port)"
"    async def run(self):
        # Create an aioredis client for all modules.
        try:
            self.aioredis_client = await dashboard_utils.get_aioredis_client(
                self.redis_address, self.redis_password,
                dashboard_consts.CONNECT_REDIS_INTERNAL_SECONDS,
                dashboard_consts.RETRY_REDIS_CONNECTION_TIMES)
        except (socket.gaierror, ConnectionError):
            logger.error(
                ""Dashboard head exiting: ""
                ""Failed to connect to redis at %s"", self.redis_address)
            sys.exit(-1)

        # Create a http session for all modules.
        self.http_session = aiohttp.ClientSession(
            loop=asyncio.get_event_loop())

        # Waiting for GCS is ready.
        while True:
            try:
                gcs_address = await self.aioredis_client.get(
                    dashboard_consts.REDIS_KEY_GCS_SERVER_ADDRESS)
                if not gcs_address:
                    raise Exception(""GCS address not found."")
                logger.info(""Connect to GCS at %s"", gcs_address)
                options = ((""grpc.enable_http_proxy"", 0), )
                channel = aiogrpc.insecure_channel(
                    gcs_address, options=options)
            except Exception as ex:
                logger.error(""Connect to GCS failed: %s, retry..."", ex)
                await asyncio.sleep(
                    dashboard_consts.CONNECT_GCS_INTERVAL_SECONDS)
            else:
                self.aiogrpc_gcs_channel = channel
                break

        # Create a NodeInfoGcsServiceStub.
        self._gcs_node_info_stub = gcs_service_pb2_grpc.NodeInfoGcsServiceStub(
            self.aiogrpc_gcs_channel)

        # Start a grpc asyncio server.
        await self.server.start()

        async def _async_notify():
            """"""Notify signals from queue.""""""
            while True:
                co = await dashboard_utils.NotifyQueue.get()
                try:
                    await co
                except Exception:
                    logger.exception(f""Error notifying coroutine {co}"")

        modules = self._load_modules()

        # Http server should be initialized after all modules loaded.
        app = aiohttp.web.Application()
        app.add_routes(routes=routes.bound_routes())

        runner = aiohttp.web.AppRunner(app)
        await runner.setup()
        last_ex = None
        for i in range(1 + self.http_port_retries):
            try:
                site = aiohttp.web.TCPSite(runner, self.http_host,
                                           self.http_port)
                await site.start()
                break
            except OSError as e:
                last_ex = e
                self.http_port += 1
                logger.warning(""Try to use port %s: %s"", self.http_port, e)
        else:
            raise Exception(f""Failed to find a valid port for dashboard after ""
                            f""{self.http_port_retries} retries: {last_ex}"")
        http_host, http_port, *_ = site._server.sockets[0].getsockname()
        logger.info(""Dashboard head http address: %s:%s"", http_host, http_port)

        # Write the dashboard head port to redis.
        await self.aioredis_client.set(ray_constants.REDIS_KEY_DASHBOARD,
                                       f""{http_host}:{http_port}"")
        await self.aioredis_client.set(
            dashboard_consts.REDIS_KEY_DASHBOARD_RPC,
            f""{self.ip}:{self.grpc_port}"")

        # Dump registered http routes.
        dump_routes = [
            r for r in app.router.routes() if r.method != hdrs.METH_HEAD
        ]
        for r in dump_routes:
            logger.info(r)
        logger.info(""Registered %s routes."", len(dump_routes))

        # Freeze signal after all modules loaded.
        dashboard_utils.SignalManager.freeze()
        concurrent_tasks = [
            self._update_nodes(),
            _async_notify(),
            DataOrganizer.purge(),
            DataOrganizer.organize(),
        ]
        await asyncio.gather(*concurrent_tasks,
                             *(m.run(self.server) for m in modules))
        await self.server.wait_for_termination()","    async def run(self):
        # Create an aioredis client for all modules.
        try:
            self.aioredis_client = await dashboard_utils.get_aioredis_client(
                self.redis_address, self.redis_password,
                dashboard_consts.CONNECT_REDIS_INTERNAL_SECONDS,
                dashboard_consts.RETRY_REDIS_CONNECTION_TIMES)
        except (socket.gaierror, ConnectionError):
            logger.error(
                ""Dashboard head exiting: ""
                ""Failed to connect to redis at %s"", self.redis_address)
            sys.exit(-1)

        # Create a http session for all modules.
        self.http_session = aiohttp.ClientSession(
            loop=asyncio.get_event_loop())

        # Waiting for GCS is ready.
        while True:
            try:
                gcs_address = await self.aioredis_client.get(
                    dashboard_consts.REDIS_KEY_GCS_SERVER_ADDRESS)
                if not gcs_address:
                    raise Exception(""GCS address not found."")
                logger.info(""Connect to GCS at %s"", gcs_address)
                options = ((""grpc.enable_http_proxy"", 0), )
                channel = aiogrpc.insecure_channel(
                    gcs_address, options=options)
            except Exception as ex:
                logger.error(""Connect to GCS failed: %s, retry..."", ex)
                await asyncio.sleep(
                    dashboard_consts.CONNECT_GCS_INTERVAL_SECONDS)
            else:
                self.aiogrpc_gcs_channel = channel
                break

        # Create a NodeInfoGcsServiceStub.
        self._gcs_node_info_stub = gcs_service_pb2_grpc.NodeInfoGcsServiceStub(
            self.aiogrpc_gcs_channel)

        # Start a grpc asyncio server.
        await self.server.start()

        # Write the dashboard head port to redis.
        await self.aioredis_client.set(dashboard_consts.REDIS_KEY_DASHBOARD,
                                       self.ip + "":"" + str(self.http_port))
        await self.aioredis_client.set(
            dashboard_consts.REDIS_KEY_DASHBOARD_RPC,
            self.ip + "":"" + str(self.grpc_port))

        async def _async_notify():
            """"""Notify signals from queue.""""""
            while True:
                co = await dashboard_utils.NotifyQueue.get()
                try:
                    await co
                except Exception:
                    logger.exception(f""Error notifying coroutine {co}"")

        modules = self._load_modules()

        # Http server should be initialized after all modules loaded.
        app = aiohttp.web.Application()
        app.add_routes(routes=routes.bound_routes())
        web_server = aiohttp.web._run_app(
            app, host=self.http_host, port=self.http_port)

        # Dump registered http routes.
        dump_routes = [
            r for r in app.router.routes() if r.method != hdrs.METH_HEAD
        ]
        for r in dump_routes:
            logger.info(r)
        logger.info(""Registered %s routes."", len(dump_routes))

        # Freeze signal after all modules loaded.
        dashboard_utils.SignalManager.freeze()
        concurrent_tasks = [
            self._update_nodes(),
            _async_notify(),
            DataOrganizer.purge(),
            DataOrganizer.organize(),
            web_server,
        ]
        await asyncio.gather(*concurrent_tasks,
                             *(m.run(self.server) for m in modules))
        await self.server.wait_for_termination()"
"def start_dashboard(require_dashboard,
                    host,
                    redis_address,
                    temp_dir,
                    logdir,
                    port=ray_constants.DEFAULT_DASHBOARD_PORT,
                    stdout_file=None,
                    stderr_file=None,
                    redis_password=None,
                    fate_share=None,
                    max_bytes=0,
                    backup_count=0):
    """"""Start a dashboard process.

    Args:
        require_dashboard (bool): If true, this will raise an exception if we
            fail to start the dashboard. Otherwise it will print a warning if
            we fail to start the dashboard.
        host (str): The host to bind the dashboard web server to.
        port (str): The port to bind the dashboard web server to.
            Defaults to 8265.
        redis_address (str): The address of the Redis instance.
        temp_dir (str): The temporary directory used for log files and
            information for this Ray session.
        logdir (str): The log directory used to generate dashboard log.
        stdout_file: A file handle opened for writing to redirect stdout to. If
            no redirection should happen, then this should be None.
        stderr_file: A file handle opened for writing to redirect stderr to. If
            no redirection should happen, then this should be None.
        redis_password (str): The password of the redis server.
        max_bytes (int): Log rotation parameter. Corresponding to
            RotatingFileHandler's maxBytes.
        backup_count (int): Log rotation parameter. Corresponding to
            RotatingFileHandler's backupCount.

    Returns:
        ProcessInfo for the process that was started.
    """"""
    port_retries = 10
    if port != ray_constants.DEFAULT_DASHBOARD_PORT:
        port_test_socket = socket.socket()
        port_test_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
        try:
            port_test_socket.bind((""127.0.0.1"", port))
            port_test_socket.close()
        except socket.error:
            raise ValueError(
                f""The given dashboard port {port} is already in use"")
        port_retries = 0

    dashboard_dir = ""new_dashboard""
    dashboard_filepath = os.path.join(RAY_PATH, dashboard_dir, ""dashboard.py"")
    command = [
        sys.executable, ""-u"", dashboard_filepath, f""--host={host}"",
        f""--port={port}"", f""--port-retries={port_retries}"",
        f""--redis-address={redis_address}"", f""--temp-dir={temp_dir}"",
        f""--log-dir={logdir}"", f""--logging-rotate-bytes={max_bytes}"",
        f""--logging-rotate-backup-count={backup_count}""
    ]

    if redis_password:
        command += [""--redis-password"", redis_password]

    dashboard_dependencies_present = True
    try:
        import aiohttp  # noqa: F401
        import grpc  # noqa: F401
    except ImportError:
        dashboard_dependencies_present = False
        warning_message = (
            ""Failed to start the dashboard. The dashboard requires Python 3 ""
            ""as well as 'pip install aiohttp grpcio'."")
        if require_dashboard:
            raise ImportError(warning_message)
        else:
            logger.warning(warning_message)
    if dashboard_dependencies_present:
        process_info = start_ray_process(
            command,
            ray_constants.PROCESS_TYPE_DASHBOARD,
            stdout_file=stdout_file,
            stderr_file=stderr_file,
            fate_share=fate_share)

        redis_client = ray._private.services.create_redis_client(
            redis_address, redis_password)

        dashboard_url = None
        dashboard_returncode = None
        for _ in range(20):
            dashboard_url = redis_client.get(ray_constants.REDIS_KEY_DASHBOARD)
            if dashboard_url is not None:
                dashboard_url = dashboard_url.decode(""utf-8"")
                break
            dashboard_returncode = process_info.process.poll()
            if dashboard_returncode is not None:
                break
            time.sleep(1)
        if dashboard_url is None:
            dashboard_log = os.path.join(logdir, ""dashboard.log"")
            returncode_str = (f"", return code {dashboard_returncode}""
                              if dashboard_returncode is not None else """")
            # Read last n lines of dashboard log. The log file may be large.
            n = 10
            lines = []
            try:
                with open(dashboard_log, ""rb"") as f:
                    with mmap.mmap(
                            f.fileno(), 0, access=mmap.ACCESS_READ) as mm:
                        end = mm.size()
                        for _ in range(n):
                            sep = mm.rfind(b""\\n"", 0, end - 1)
                            if sep == -1:
                                break
                            lines.append(mm[sep + 1:end].decode(""utf-8""))
                            end = sep
                lines.append(f"" The last {n} lines of {dashboard_log}:"")
            except Exception:
                pass
            last_log_str = ""\\n"".join(reversed(lines[-n:]))
            raise Exception(""Failed to start the dashboard""
                            f""{returncode_str}.{last_log_str}"")

        logger.info(""View the Ray dashboard at %s%shttp://%s%s%s"",
                    colorama.Style.BRIGHT, colorama.Fore.GREEN, dashboard_url,
                    colorama.Fore.RESET, colorama.Style.NORMAL)

        return dashboard_url, process_info
    else:
        return None, None","def start_dashboard(require_dashboard,
                    host,
                    redis_address,
                    temp_dir,
                    logdir,
                    port=ray_constants.DEFAULT_DASHBOARD_PORT,
                    stdout_file=None,
                    stderr_file=None,
                    redis_password=None,
                    fate_share=None,
                    max_bytes=0,
                    backup_count=0):
    """"""Start a dashboard process.

    Args:
        require_dashboard (bool): If true, this will raise an exception if we
            fail to start the dashboard. Otherwise it will print a warning if
            we fail to start the dashboard.
        host (str): The host to bind the dashboard web server to.
        port (str): The port to bind the dashboard web server to.
            Defaults to 8265.
        redis_address (str): The address of the Redis instance.
        temp_dir (str): The temporary directory used for log files and
            information for this Ray session.
        logdir (str): The log directory used to generate dashboard log.
        stdout_file: A file handle opened for writing to redirect stdout to. If
            no redirection should happen, then this should be None.
        stderr_file: A file handle opened for writing to redirect stderr to. If
            no redirection should happen, then this should be None.
        redis_password (str): The password of the redis server.
        max_bytes (int): Log rotation parameter. Corresponding to
            RotatingFileHandler's maxBytes.
        backup_count (int): Log rotation parameter. Corresponding to
            RotatingFileHandler's backupCount.

    Returns:
        ProcessInfo for the process that was started.
    """"""
    port_test_socket = socket.socket()
    port_test_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
    if port == ray_constants.DEFAULT_DASHBOARD_PORT:
        while True:
            try:
                port_test_socket.bind((""127.0.0.1"", port))
                port_test_socket.close()
                break
            except socket.error:
                port += 1
    else:
        try:
            port_test_socket.bind((""127.0.0.1"", port))
            port_test_socket.close()
        except socket.error:
            raise ValueError(
                f""The given dashboard port {port} is already in use"")

    dashboard_dir = ""new_dashboard""
    dashboard_filepath = os.path.join(RAY_PATH, dashboard_dir, ""dashboard.py"")
    command = [
        sys.executable, ""-u"", dashboard_filepath, f""--host={host}"",
        f""--port={port}"", f""--redis-address={redis_address}"",
        f""--temp-dir={temp_dir}"", f""--log-dir={logdir}"",
        f""--logging-rotate-bytes={max_bytes}"",
        f""--logging-rotate-backup-count={backup_count}""
    ]

    if redis_password:
        command += [""--redis-password"", redis_password]

    dashboard_dependencies_present = True
    try:
        import aiohttp  # noqa: F401
        import grpc  # noqa: F401
    except ImportError:
        dashboard_dependencies_present = False
        warning_message = (
            ""Failed to start the dashboard. The dashboard requires Python 3 ""
            ""as well as 'pip install aiohttp grpcio'."")
        if require_dashboard:
            raise ImportError(warning_message)
        else:
            logger.warning(warning_message)
    if dashboard_dependencies_present:
        process_info = start_ray_process(
            command,
            ray_constants.PROCESS_TYPE_DASHBOARD,
            stdout_file=stdout_file,
            stderr_file=stderr_file,
            fate_share=fate_share)

        dashboard_url = (
            f""{host if host != '0.0.0.0' else get_node_ip_address()}:{port}"")

        logger.info(""View the Ray dashboard at {}{}http://{}{}{}"".format(
            colorama.Style.BRIGHT, colorama.Fore.GREEN, dashboard_url,
            colorama.Fore.RESET, colorama.Style.NORMAL))

        return dashboard_url, process_info
    else:
        return None, None"
"    def learn_on_batch(self, samples: SampleBatchType) -> dict:
        """"""Update policies based on the given batch.

        This is the equivalent to apply_gradients(compute_gradients(samples)),
        but can be optimized to avoid pulling gradients into CPU memory.

        Returns:
            info: dictionary of extra metadata from compute_gradients().

        Examples:
            >>> batch = worker.sample()
            >>> worker.learn_on_batch(samples)
        """"""
        if log_once(""learn_on_batch""):
            logger.info(
                ""Training on concatenated sample batches:\\n\\n{}\\n"".format(
                    summarize(samples)))
        if isinstance(samples, MultiAgentBatch):
            info_out = {}
            to_fetch = {}
            if self.tf_sess is not None:
                builder = TFRunBuilder(self.tf_sess, ""learn_on_batch"")
            else:
                builder = None
            for pid, batch in samples.policy_batches.items():
                if pid not in self.policies_to_train:
                    continue
                # Decompress SampleBatch, in case some columns are compressed.
                batch.decompress_if_needed()
                policy = self.policy_map[pid]
                if builder and hasattr(policy, ""_build_learn_on_batch""):
                    to_fetch[pid] = policy._build_learn_on_batch(
                        builder, batch)
                else:
                    info_out[pid] = policy.learn_on_batch(batch)
            info_out.update({k: builder.get(v) for k, v in to_fetch.items()})
        else:
            info_out = {
                DEFAULT_POLICY_ID: self.policy_map[DEFAULT_POLICY_ID]
                .learn_on_batch(samples)
            }
        if log_once(""learn_out""):
            logger.debug(""Training out:\\n\\n{}\\n"".format(summarize(info_out)))
        return info_out","    def learn_on_batch(self, samples: SampleBatchType) -> dict:
        """"""Update policies based on the given batch.

        This is the equivalent to apply_gradients(compute_gradients(samples)),
        but can be optimized to avoid pulling gradients into CPU memory.

        Returns:
            info: dictionary of extra metadata from compute_gradients().

        Examples:
            >>> batch = worker.sample()
            >>> worker.learn_on_batch(samples)
        """"""
        if log_once(""learn_on_batch""):
            logger.info(
                ""Training on concatenated sample batches:\\n\\n{}\\n"".format(
                    summarize(samples)))
        if isinstance(samples, MultiAgentBatch):
            info_out = {}
            to_fetch = {}
            if self.tf_sess is not None:
                builder = TFRunBuilder(self.tf_sess, ""learn_on_batch"")
            else:
                builder = None
            for pid, batch in samples.policy_batches.items():
                if pid not in self.policies_to_train:
                    continue
                policy = self.policy_map[pid]
                if builder and hasattr(policy, ""_build_learn_on_batch""):
                    to_fetch[pid] = policy._build_learn_on_batch(
                        builder, batch)
                else:
                    info_out[pid] = policy.learn_on_batch(batch)
            info_out.update({k: builder.get(v) for k, v in to_fetch.items()})
        else:
            info_out = {
                DEFAULT_POLICY_ID: self.policy_map[DEFAULT_POLICY_ID]
                .learn_on_batch(samples)
            }
        if log_once(""learn_out""):
            logger.debug(""Training out:\\n\\n{}\\n"".format(summarize(info_out)))
        return info_out"
"    def __call__(self,
                 samples: SampleBatchType) -> (SampleBatchType, List[dict]):
        _check_sample_batch_type(samples)

        # Handle everything as if multiagent
        if isinstance(samples, SampleBatch):
            samples = MultiAgentBatch({
                DEFAULT_POLICY_ID: samples
            }, samples.count)

        metrics = _get_shared_metrics()
        load_timer = metrics.timers[LOAD_BATCH_TIMER]
        learn_timer = metrics.timers[LEARN_ON_BATCH_TIMER]
        with load_timer:
            # (1) Load data into GPUs.
            num_loaded_tuples = {}
            for policy_id, batch in samples.policy_batches.items():
                # Not a policy-to-train.
                if policy_id not in self.policies:
                    continue

                # Decompress SampleBatch, in case some columns are compressed.
                batch.decompress_if_needed()

                policy = self.workers.local_worker().get_policy(policy_id)
                policy._debug_vars()
                tuples = policy._get_loss_inputs_dict(
                    batch, shuffle=self.shuffle_sequences)
                data_keys = list(policy._loss_input_dict_no_rnn.values())
                if policy._state_inputs:
                    state_keys = policy._state_inputs + [policy._seq_lens]
                else:
                    state_keys = []
                num_loaded_tuples[policy_id] = (
                    self.optimizers[policy_id].load_data(
                        self.sess, [tuples[k] for k in data_keys],
                        [tuples[k] for k in state_keys]))

        with learn_timer:
            # (2) Execute minibatch SGD on loaded data.
            fetches = {}
            for policy_id, tuples_per_device in num_loaded_tuples.items():
                optimizer = self.optimizers[policy_id]
                num_batches = max(
                    1,
                    int(tuples_per_device) // int(self.per_device_batch_size))
                logger.debug(""== sgd epochs for {} =="".format(policy_id))
                for i in range(self.num_sgd_iter):
                    iter_extra_fetches = defaultdict(list)
                    permutation = np.random.permutation(num_batches)
                    for batch_index in range(num_batches):
                        batch_fetches = optimizer.optimize(
                            self.sess, permutation[batch_index] *
                            self.per_device_batch_size)
                        for k, v in batch_fetches[LEARNER_STATS_KEY].items():
                            iter_extra_fetches[k].append(v)
                    if logger.getEffectiveLevel() <= logging.DEBUG:
                        avg = averaged(iter_extra_fetches)
                        logger.debug(""{} {}"".format(i, avg))
                fetches[policy_id] = averaged(iter_extra_fetches, axis=0)

        load_timer.push_units_processed(samples.count)
        learn_timer.push_units_processed(samples.count)

        metrics.counters[STEPS_TRAINED_COUNTER] += samples.count
        metrics.info[LEARNER_INFO] = fetches
        if self.workers.remote_workers():
            with metrics.timers[WORKER_UPDATE_TIMER]:
                weights = ray.put(self.workers.local_worker().get_weights(
                    self.policies))
                for e in self.workers.remote_workers():
                    e.set_weights.remote(weights, _get_global_vars())
        # Also update global vars of the local worker.
        self.workers.local_worker().set_global_vars(_get_global_vars())
        return samples, fetches","    def __call__(self,
                 samples: SampleBatchType) -> (SampleBatchType, List[dict]):
        _check_sample_batch_type(samples)

        # Handle everything as if multiagent
        if isinstance(samples, SampleBatch):
            samples = MultiAgentBatch({
                DEFAULT_POLICY_ID: samples
            }, samples.count)

        metrics = _get_shared_metrics()
        load_timer = metrics.timers[LOAD_BATCH_TIMER]
        learn_timer = metrics.timers[LEARN_ON_BATCH_TIMER]
        with load_timer:
            # (1) Load data into GPUs.
            num_loaded_tuples = {}
            for policy_id, batch in samples.policy_batches.items():
                if policy_id not in self.policies:
                    continue

                policy = self.workers.local_worker().get_policy(policy_id)
                policy._debug_vars()
                tuples = policy._get_loss_inputs_dict(
                    batch, shuffle=self.shuffle_sequences)
                data_keys = list(policy._loss_input_dict_no_rnn.values())
                if policy._state_inputs:
                    state_keys = policy._state_inputs + [policy._seq_lens]
                else:
                    state_keys = []
                num_loaded_tuples[policy_id] = (
                    self.optimizers[policy_id].load_data(
                        self.sess, [tuples[k] for k in data_keys],
                        [tuples[k] for k in state_keys]))

        with learn_timer:
            # (2) Execute minibatch SGD on loaded data.
            fetches = {}
            for policy_id, tuples_per_device in num_loaded_tuples.items():
                optimizer = self.optimizers[policy_id]
                num_batches = max(
                    1,
                    int(tuples_per_device) // int(self.per_device_batch_size))
                logger.debug(""== sgd epochs for {} =="".format(policy_id))
                for i in range(self.num_sgd_iter):
                    iter_extra_fetches = defaultdict(list)
                    permutation = np.random.permutation(num_batches)
                    for batch_index in range(num_batches):
                        batch_fetches = optimizer.optimize(
                            self.sess, permutation[batch_index] *
                            self.per_device_batch_size)
                        for k, v in batch_fetches[LEARNER_STATS_KEY].items():
                            iter_extra_fetches[k].append(v)
                    if logger.getEffectiveLevel() <= logging.DEBUG:
                        avg = averaged(iter_extra_fetches)
                        logger.debug(""{} {}"".format(i, avg))
                fetches[policy_id] = averaged(iter_extra_fetches, axis=0)

        load_timer.push_units_processed(samples.count)
        learn_timer.push_units_processed(samples.count)

        metrics.counters[STEPS_TRAINED_COUNTER] += samples.count
        metrics.info[LEARNER_INFO] = fetches
        if self.workers.remote_workers():
            with metrics.timers[WORKER_UPDATE_TIMER]:
                weights = ray.put(self.workers.local_worker().get_weights(
                    self.policies))
                for e in self.workers.remote_workers():
                    e.set_weights.remote(weights, _get_global_vars())
        # Also update global vars of the local worker.
        self.workers.local_worker().set_global_vars(_get_global_vars())
        return samples, fetches"
"    def _get(self, ref: ClientObjectRef, timeout: float):
        req = ray_client_pb2.GetRequest(id=ref.id, timeout=timeout)
        try:
            data = self.data_client.GetObject(req)
        except grpc.RpcError as e:
            raise e.details()
        if not data.valid:
            try:
                err = cloudpickle.loads(data.error)
            except pickle.UnpicklingError:
                logger.exception(""Failed to deserialize {}"".format(data.error))
                raise
            logger.error(err)
            raise err
        return loads_from_server(data.data)","    def _get(self, ref: ClientObjectRef, timeout: float):
        req = ray_client_pb2.GetRequest(id=ref.id, timeout=timeout)
        try:
            data = self.data_client.GetObject(req)
        except grpc.RpcError as e:
            raise e.details()
        if not data.valid:
            try:
                err = cloudpickle.loads(data.error)
            except Exception:
                logger.exception(""Failed to deserialize {}"".format(data.error))
                raise
            logger.error(err)
            raise err
        return loads_from_server(data.data)"
"    def _call_schedule_for_task(
            self, task: ray_client_pb2.ClientTask) -> List[bytes]:
        logger.debug(""Scheduling %s"" % task)
        task.client_id = self._client_id
        try:
            ticket = self.server.Schedule(task, metadata=self.metadata)
        except grpc.RpcError as e:
            raise decode_exception(e.details)
        if not ticket.valid:
            try:
                raise cloudpickle.loads(ticket.error)
            except pickle.UnpicklingError:
                logger.exception(""Failed to deserialize {}"".format(
                    ticket.error))
                raise
        return ticket.return_ids","    def _call_schedule_for_task(
            self, task: ray_client_pb2.ClientTask) -> List[bytes]:
        logger.debug(""Scheduling %s"" % task)
        task.client_id = self._client_id
        try:
            ticket = self.server.Schedule(task, metadata=self.metadata)
        except grpc.RpcError as e:
            raise decode_exception(e.details)
        if not ticket.valid:
            try:
                raise cloudpickle.loads(ticket.error)
            except Exception:
                logger.exception(""Failed to deserialize {}"".format(
                    ticket.error))
                raise
        return ticket.return_ids"
"def normalize(data, wrt):
    """""" Normalize data to be in range (0,1), with respect to (wrt) boundaries,
        which can be specified.
    """"""
    return (data - np.min(wrt, axis=0)) / (
        np.max(wrt, axis=0) - np.min(wrt, axis=0) + 1e-8)","def normalize(data, wrt):
    """""" Normalize data to be in range (0,1), with respect to (wrt) boundaries,
        which can be specified.
    """"""
    return (data - np.min(wrt, axis=0)) / (
        np.max(wrt, axis=0) - np.min(wrt, axis=0))"
"    def on_step_begin(self, **info):
        import click
        from ray.autoscaler._private.commands import kill_node
        failures = 0
        max_failures = 3
        # With 10% probability inject failure to a worker.
        if random.random() < self.probability and not self.disable:
            # With 10% probability fully terminate the node.
            should_terminate = random.random() < self.probability
            while failures < max_failures:
                try:
                    kill_node(
                        self.config_path,
                        yes=True,
                        hard=should_terminate,
                        override_cluster_name=None)
                except click.exceptions.ClickException:
                    failures += 1
                    logger.exception(""Killing random node failed in attempt ""
                                     ""{}. ""
                                     ""Retrying {} more times"".format(
                                         str(failures),
                                         str(max_failures - failures)))","    def on_step_begin(self, **info):
        from ray.autoscaler._private.commands import kill_node
        # With 10% probability inject failure to a worker.
        if random.random() < self.probability and not self.disable:
            # With 10% probability fully terminate the node.
            should_terminate = random.random() < self.probability
            kill_node(
                self.config_path,
                yes=True,
                hard=should_terminate,
                override_cluster_name=None)"
"def _check_ami(config):
    """"""Provide helpful message for missing ImageId for node configuration.""""""

    _set_config_info(head_ami_src=""config"", workers_ami_src=""config"")

    region = config[""provider""][""region""]
    default_ami = DEFAULT_AMI.get(region)
    if not default_ami:
        # If we do not provide a default AMI for the given region, noop.
        return

    head_ami = config[""head_node""].get(""ImageId"", """").lower()
    if head_ami in ["""", ""latest_dlami""]:
        config[""head_node""][""ImageId""] = default_ami
        _set_config_info(head_ami_src=""dlami"")

    worker_ami = config[""worker_nodes""].get(""ImageId"", """").lower()
    if worker_ami in ["""", ""latest_dlami""]:
        config[""worker_nodes""][""ImageId""] = default_ami
        _set_config_info(workers_ami_src=""dlami"")","def _check_ami(config):
    """"""Provide helpful message for missing ImageId for node configuration.""""""

    _set_config_info(head_ami_src=""config"", workers_ami_src=""config"")

    region = config[""provider""][""region""]
    default_ami = DEFAULT_AMI.get(region)
    if not default_ami:
        # If we do not provide a default AMI for the given region, noop.
        return

    if config[""head_node""].get(""ImageId"", """").lower() == ""latest_dlami"":
        config[""head_node""][""ImageId""] = default_ami
        _set_config_info(head_ami_src=""dlami"")

    if config[""worker_nodes""].get(""ImageId"", """").lower() == ""latest_dlami"":
        config[""worker_nodes""][""ImageId""] = default_ami
        _set_config_info(workers_ami_src=""dlami"")"
"    def on_checkpoint(self, checkpoint):
        """"""Starts tracking checkpoint metadata on checkpoint.

        Sets the newest checkpoint. For PERSISTENT checkpoints: Deletes
        previous checkpoint as long as it isn't one of the best ones. Also
        deletes the worst checkpoint if at capacity.

        Args:
            checkpoint (Checkpoint): Trial state checkpoint.
        """"""
        if checkpoint.storage == Checkpoint.MEMORY:
            self.newest_memory_checkpoint = checkpoint
            return

        old_checkpoint = self.newest_persistent_checkpoint

        if old_checkpoint.value == checkpoint.value:
            return

        self.newest_persistent_checkpoint = checkpoint

        # Remove the old checkpoint if it isn't one of the best ones.
        if old_checkpoint.value and old_checkpoint not in self._membership:
            self.delete(old_checkpoint)

        try:
            queue_item = QueueItem(self._priority(checkpoint), checkpoint)
        except KeyError:
            logger.error(""Result dict has no key: {}. ""
                         ""checkpoint_score_attr must be set to a key in the ""
                         ""result dict."".format(self._checkpoint_score_attr))
            return

        if len(self._best_checkpoints) < self.keep_checkpoints_num:
            heapq.heappush(self._best_checkpoints, queue_item)
            self._membership.add(checkpoint)
        elif queue_item.priority >= self._best_checkpoints[0].priority:
            worst = heapq.heappushpop(self._best_checkpoints, queue_item).value
            self._membership.add(checkpoint)
            if worst in self._membership:
                self._membership.remove(worst)
            # Don't delete the newest checkpoint. It will be deleted on the
            # next on_checkpoint() call since it isn't in self._membership.
            if worst != checkpoint:
                self.delete(worst)","    def on_checkpoint(self, checkpoint):
        """"""Starts tracking checkpoint metadata on checkpoint.

        Sets the newest checkpoint. For PERSISTENT checkpoints: Deletes
        previous checkpoint as long as it isn't one of the best ones. Also
        deletes the worst checkpoint if at capacity.

        Args:
            checkpoint (Checkpoint): Trial state checkpoint.
        """"""
        if checkpoint.storage == Checkpoint.MEMORY:
            self.newest_memory_checkpoint = checkpoint
            return

        old_checkpoint = self.newest_persistent_checkpoint
        self.newest_persistent_checkpoint = checkpoint

        # Remove the old checkpoint if it isn't one of the best ones.
        if old_checkpoint.value and old_checkpoint not in self._membership:
            self.delete(old_checkpoint)

        try:
            queue_item = QueueItem(self._priority(checkpoint), checkpoint)
        except KeyError:
            logger.error(""Result dict has no key: {}. ""
                         ""checkpoint_score_attr must be set to a key in the ""
                         ""result dict."".format(self._checkpoint_score_attr))
            return

        if len(self._best_checkpoints) < self.keep_checkpoints_num:
            heapq.heappush(self._best_checkpoints, queue_item)
            self._membership.add(checkpoint)
        elif queue_item.priority >= self._best_checkpoints[0].priority:
            worst = heapq.heappushpop(self._best_checkpoints, queue_item).value
            self._membership.add(checkpoint)
            if worst in self._membership:
                self._membership.remove(worst)
            # Don't delete the newest checkpoint. It will be deleted on the
            # next on_checkpoint() call since it isn't in self._membership.
            if worst != checkpoint:
                self.delete(worst)"
"    def __init__(self, controller_handle, sync: bool):
        self.controller_handle = controller_handle
        self.sync = sync
        self.router = Router(controller_handle)

        if sync:
            self.async_loop = create_or_get_async_loop_in_thread()
            asyncio.run_coroutine_threadsafe(
                self.router.setup_in_async_loop(),
                self.async_loop,
            )
        else:
            self.async_loop = asyncio.get_event_loop()
            self.async_loop.create_task(self.router.setup_in_async_loop())","    def __init__(self, controller_handle, sync: bool):
        self.router = Router(controller_handle)

        if sync:
            self.async_loop = create_or_get_async_loop_in_thread()
            asyncio.run_coroutine_threadsafe(
                self.router.setup_in_async_loop(),
                self.async_loop,
            )
        else:
            self.async_loop = asyncio.get_event_loop()
            self.async_loop.create_task(self.router.setup_in_async_loop())"
"    def __init__(
            self,
            router,  # ThreadProxiedRouter
            endpoint_name,
            handle_options: Optional[HandleOptions] = None):
        self.router = router
        self.endpoint_name = endpoint_name
        self.handle_options = handle_options or HandleOptions()","    def __init__(self,
                 router: Router,
                 endpoint_name,
                 handle_options: Optional[HandleOptions] = None):
        self.router = router
        self.endpoint_name = endpoint_name
        self.handle_options = handle_options or HandleOptions()"
"    async def remote(self,
                     request_data: Optional[Union[Dict, Any]] = None,
                     **kwargs):
        """"""Issue an asynchronous request to the endpoint.

        Returns a Ray ObjectRef whose results can be waited for or retrieved
        using ray.wait or ray.get (or ``await object_ref``), respectively.

        Returns:
            ray.ObjectRef
        Args:
            request_data(dict, Any): If it's a dictionary, the data will be
                available in ``request.json()`` or ``request.form()``.
                Otherwise, it will be available in ``request.body()``.
            ``**kwargs``: All keyword arguments will be available in
                ``request.query_params``.
        """"""
        return await self.router._remote(
            self.endpoint_name, self.handle_options, request_data, kwargs)","    async def remote(self,
                     request_data: Optional[Union[Dict, Any]] = None,
                     **kwargs):
        """"""Issue an asynchrounous request to the endpoint.

        Returns a Ray ObjectRef whose results can be waited for or retrieved
        using ray.wait or ray.get (or ``await object_ref``), respectively.

        Returns:
            ray.ObjectRef
        Args:
            request_data(dict, Any): If it's a dictionary, the data will be
                available in ``request.json()`` or ``request.form()``.
                Otherwise, it will be available in ``request.body()``.
            ``**kwargs``: All keyword arguments will be available in
                ``request.query_params``.
        """"""
        return await self.router._remote(
            self.endpoint_name, self.handle_options, request_data, kwargs)"
"    def __init__(self,
                 node_ip_address,
                 redis_address,
                 dashboard_agent_port,
                 redis_password=None,
                 temp_dir=None,
                 log_dir=None,
                 metrics_export_port=None,
                 node_manager_port=None,
                 object_store_name=None,
                 raylet_name=None):
        """"""Initialize the DashboardAgent object.""""""
        # Public attributes are accessible for all agent modules.
        self.ip = node_ip_address
        self.redis_address = dashboard_utils.address_tuple(redis_address)
        self.redis_password = redis_password
        self.temp_dir = temp_dir
        self.log_dir = log_dir
        self.dashboard_agent_port = dashboard_agent_port
        self.metrics_export_port = metrics_export_port
        self.node_manager_port = node_manager_port
        self.object_store_name = object_store_name
        self.raylet_name = raylet_name
        self.node_id = os.environ[""RAY_NODE_ID""]
        self.ppid = int(os.environ[""RAY_RAYLET_PID""])
        assert self.ppid > 0
        logger.info(""Parent pid is %s"", self.ppid)
        self.server = aiogrpc.server(options=((""grpc.so_reuseport"", 0), ))
        self.grpc_port = self.server.add_insecure_port(
            f""[::]:{self.dashboard_agent_port}"")
        logger.info(""Dashboard agent grpc address: %s:%s"", self.ip,
                    self.grpc_port)
        self.aioredis_client = None
        options = ((""grpc.enable_http_proxy"", 0), )
        self.aiogrpc_raylet_channel = aiogrpc.insecure_channel(
            f""{self.ip}:{self.node_manager_port}"", options=options)
        self.http_session = None","    def __init__(self,
                 node_ip_address,
                 redis_address,
                 dashboard_agent_port,
                 redis_password=None,
                 temp_dir=None,
                 log_dir=None,
                 metrics_export_port=None,
                 node_manager_port=None,
                 object_store_name=None,
                 raylet_name=None):
        """"""Initialize the DashboardAgent object.""""""
        # Public attributes are accessible for all agent modules.
        self.ip = node_ip_address
        self.redis_address = dashboard_utils.address_tuple(redis_address)
        self.redis_password = redis_password
        self.temp_dir = temp_dir
        self.log_dir = log_dir
        self.dashboard_agent_port = dashboard_agent_port
        self.metrics_export_port = metrics_export_port
        self.node_manager_port = node_manager_port
        self.object_store_name = object_store_name
        self.raylet_name = raylet_name
        self.node_id = os.environ[""RAY_NODE_ID""]
        # TODO(edoakes): RAY_RAYLET_PID isn't properly set on Windows. This is
        # only used for fate-sharing with the raylet and we need a different
        # fate-sharing mechanism for Windows anyways.
        if sys.platform not in [""win32"", ""cygwin""]:
            self.ppid = int(os.environ[""RAY_RAYLET_PID""])
            assert self.ppid > 0
            logger.info(""Parent pid is %s"", self.ppid)
        self.server = aiogrpc.server(options=((""grpc.so_reuseport"", 0), ))
        self.grpc_port = self.server.add_insecure_port(
            f""[::]:{self.dashboard_agent_port}"")
        logger.info(""Dashboard agent grpc address: %s:%s"", self.ip,
                    self.grpc_port)
        self.aioredis_client = None
        options = ((""grpc.enable_http_proxy"", 0), )
        self.aiogrpc_raylet_channel = aiogrpc.insecure_channel(
            f""{self.ip}:{self.node_manager_port}"", options=options)
        self.http_session = None"
"    async def run(self):
        async def _check_parent():
            """"""Check if raylet is dead and fate-share if it is.""""""
            try:
                curr_proc = psutil.Process()
                while True:
                    parent = curr_proc.parent()
                    if (parent is None or parent.pid == 1
                            or self.ppid != parent.pid):
                        logger.error(""Raylet is dead, exiting."")
                        sys.exit(0)
                    await asyncio.sleep(
                        dashboard_consts.
                        DASHBOARD_AGENT_CHECK_PARENT_INTERVAL_SECONDS)
            except Exception:
                logger.error(""Failed to check parent PID, exiting."")
                sys.exit(1)

        check_parent_task = create_task(_check_parent())

        # Create an aioredis client for all modules.
        try:
            self.aioredis_client = await dashboard_utils.get_aioredis_client(
                self.redis_address, self.redis_password,
                dashboard_consts.CONNECT_REDIS_INTERNAL_SECONDS,
                dashboard_consts.RETRY_REDIS_CONNECTION_TIMES)
        except (socket.gaierror, ConnectionRefusedError):
            logger.error(
                ""Dashboard agent exiting: ""
                ""Failed to connect to redis at %s"", self.redis_address)
            sys.exit(-1)

        # Create a http session for all modules.
        self.http_session = aiohttp.ClientSession(
            loop=asyncio.get_event_loop())

        # Start a grpc asyncio server.
        await self.server.start()

        modules = self._load_modules()

        # Http server should be initialized after all modules loaded.
        app = aiohttp.web.Application()
        app.add_routes(routes=routes.bound_routes())

        # Enable CORS on all routes.
        cors = aiohttp_cors.setup(
            app,
            defaults={
                ""*"": aiohttp_cors.ResourceOptions(
                    allow_credentials=True,
                    expose_headers=""*"",
                    allow_methods=""*"",
                    allow_headers=(""Content-Type"", ""X-Header""),
                )
            })
        for route in list(app.router.routes()):
            cors.add(route)

        runner = aiohttp.web.AppRunner(app)
        await runner.setup()
        site = aiohttp.web.TCPSite(runner, self.ip, 0)
        await site.start()
        http_host, http_port = site._server.sockets[0].getsockname()
        logger.info(""Dashboard agent http address: %s:%s"", http_host,
                    http_port)

        # Dump registered http routes.
        dump_routes = [
            r for r in app.router.routes() if r.method != hdrs.METH_HEAD
        ]
        for r in dump_routes:
            logger.info(r)
        logger.info(""Registered %s routes."", len(dump_routes))

        # Write the dashboard agent port to redis.
        await self.aioredis_client.set(
            f""{dashboard_consts.DASHBOARD_AGENT_PORT_PREFIX}{self.node_id}"",
            json.dumps([http_port, self.grpc_port]))

        # Register agent to agent manager.
        raylet_stub = agent_manager_pb2_grpc.AgentManagerServiceStub(
            self.aiogrpc_raylet_channel)

        await raylet_stub.RegisterAgent(
            agent_manager_pb2.RegisterAgentRequest(
                agent_pid=os.getpid(),
                agent_port=self.grpc_port,
                agent_ip_address=self.ip))

        await asyncio.gather(check_parent_task,
                             *(m.run(self.server) for m in modules))
        await self.server.wait_for_termination()
        # Wait for finish signal.
        await runner.cleanup()","    async def run(self):
        async def _check_parent():
            """"""Check if raylet is dead and fate-share if it is.""""""
            try:
                curr_proc = psutil.Process()
                while True:
                    parent = curr_proc.parent()
                    if (parent is None or parent.pid == 1
                            or self.ppid != parent.pid):
                        logger.error(""Raylet is dead, exiting."")
                        sys.exit(0)
                    await asyncio.sleep(
                        dashboard_consts.
                        DASHBOARD_AGENT_CHECK_PARENT_INTERVAL_SECONDS)
            except Exception:
                logger.error(""Failed to check parent PID, exiting."")
                sys.exit(1)

        if sys.platform not in [""win32"", ""cygwin""]:
            check_parent_task = create_task(_check_parent())

        # Create an aioredis client for all modules.
        try:
            self.aioredis_client = await dashboard_utils.get_aioredis_client(
                self.redis_address, self.redis_password,
                dashboard_consts.CONNECT_REDIS_INTERNAL_SECONDS,
                dashboard_consts.RETRY_REDIS_CONNECTION_TIMES)
        except (socket.gaierror, ConnectionRefusedError):
            logger.error(
                ""Dashboard agent exiting: ""
                ""Failed to connect to redis at %s"", self.redis_address)
            sys.exit(-1)

        # Create a http session for all modules.
        self.http_session = aiohttp.ClientSession(
            loop=asyncio.get_event_loop())

        # Start a grpc asyncio server.
        await self.server.start()

        modules = self._load_modules()

        # Http server should be initialized after all modules loaded.
        app = aiohttp.web.Application()
        app.add_routes(routes=routes.bound_routes())

        # Enable CORS on all routes.
        cors = aiohttp_cors.setup(
            app,
            defaults={
                ""*"": aiohttp_cors.ResourceOptions(
                    allow_credentials=True,
                    expose_headers=""*"",
                    allow_methods=""*"",
                    allow_headers=(""Content-Type"", ""X-Header""),
                )
            })
        for route in list(app.router.routes()):
            cors.add(route)

        runner = aiohttp.web.AppRunner(app)
        await runner.setup()
        site = aiohttp.web.TCPSite(runner, self.ip, 0)
        await site.start()
        http_host, http_port = site._server.sockets[0].getsockname()
        logger.info(""Dashboard agent http address: %s:%s"", http_host,
                    http_port)

        # Dump registered http routes.
        dump_routes = [
            r for r in app.router.routes() if r.method != hdrs.METH_HEAD
        ]
        for r in dump_routes:
            logger.info(r)
        logger.info(""Registered %s routes."", len(dump_routes))

        # Write the dashboard agent port to redis.
        await self.aioredis_client.set(
            f""{dashboard_consts.DASHBOARD_AGENT_PORT_PREFIX}{self.node_id}"",
            json.dumps([http_port, self.grpc_port]))

        # Register agent to agent manager.
        raylet_stub = agent_manager_pb2_grpc.AgentManagerServiceStub(
            self.aiogrpc_raylet_channel)

        await raylet_stub.RegisterAgent(
            agent_manager_pb2.RegisterAgentRequest(
                agent_pid=os.getpid(),
                agent_port=self.grpc_port,
                agent_ip_address=self.ip))

        await asyncio.gather(check_parent_task,
                             *(m.run(self.server) for m in modules))
        await self.server.wait_for_termination()
        # Wait for finish signal.
        await runner.cleanup()"
"    def is_connected(self) -> bool:
        if self.client_worker is None:
            return False
        return self.client_worker.is_connected()","    def is_connected(self) -> bool:
        return self.client_worker is not None"
"    def __init__(self, channel: ""grpc._channel.Channel"", client_id: str,
                 metadata: list):
        """"""Initializes a thread-safe datapath over a Ray Client gRPC channel.

        Args:
            channel: connected gRPC channel
            client_id: the generated ID representing this client
            metadata: metadata to pass to gRPC requests
        """"""
        self.channel = channel
        self.request_queue = queue.Queue()
        self.data_thread = self._start_datathread()
        self.ready_data: Dict[int, Any] = {}
        self.cv = threading.Condition()
        self._req_id = 0
        self._client_id = client_id
        self._metadata = metadata
        self._in_shutdown = False
        self.data_thread.start()","    def __init__(self, channel: ""grpc._channel.Channel"", client_id: str,
                 metadata: list):
        """"""Initializes a thread-safe datapath over a Ray Client gRPC channel.

        Args:
            channel: connected gRPC channel
            client_id: the generated ID representing this client
            metadata: metadata to pass to gRPC requests
        """"""
        self.channel = channel
        self.request_queue = queue.Queue()
        self.data_thread = self._start_datathread()
        self.ready_data: Dict[int, Any] = {}
        self.cv = threading.Condition()
        self._req_id = 0
        self._client_id = client_id
        self._metadata = metadata
        self.data_thread.start()"
"    def _data_main(self) -> None:
        stub = ray_client_pb2_grpc.RayletDataStreamerStub(self.channel)
        resp_stream = stub.Datapath(
            iter(self.request_queue.get, None),
            metadata=[(""client_id"", self._client_id)] + self._metadata,
            wait_for_ready=True)
        try:
            for response in resp_stream:
                if response.req_id == 0:
                    # This is not being waited for.
                    logger.debug(f""Got unawaited response {response}"")
                    continue
                with self.cv:
                    self.ready_data[response.req_id] = response
                    self.cv.notify_all()
        except grpc.RpcError as e:
            with self.cv:
                self._in_shutdown = True
                self.cv.notify_all()
            if e.code() == grpc.StatusCode.CANCELLED:
                # Gracefully shutting down
                logger.info(""Cancelling data channel"")
            elif e.code() == grpc.StatusCode.UNAVAILABLE:
                # TODO(barakmich): The server may have
                # dropped. In theory, we can retry, as per
                # https://grpc.github.io/grpc/core/md_doc_statuscodes.html but
                # in practice we may need to think about the correct semantics
                # here.
                logger.info(""Server disconnected from data channel"")
            else:
                logger.error(
                    f""Got Error from data channel -- shutting down: {e}"")
                raise e","    def _data_main(self) -> None:
        stub = ray_client_pb2_grpc.RayletDataStreamerStub(self.channel)
        resp_stream = stub.Datapath(
            iter(self.request_queue.get, None),
            metadata=[(""client_id"", self._client_id)] + self._metadata,
            wait_for_ready=True)
        try:
            for response in resp_stream:
                if response.req_id == 0:
                    # This is not being waited for.
                    logger.debug(f""Got unawaited response {response}"")
                    continue
                with self.cv:
                    self.ready_data[response.req_id] = response
                    self.cv.notify_all()
        except grpc.RpcError as e:
            if grpc.StatusCode.CANCELLED == e.code():
                # Gracefully shutting down
                logger.info(""Cancelling data channel"")
            else:
                logger.error(
                    f""Got Error from data channel -- shutting down: {e}"")
                raise e"
"    def _blocking_send(self, req: ray_client_pb2.DataRequest
                       ) -> ray_client_pb2.DataResponse:
        req_id = self._next_id()
        req.req_id = req_id
        self.request_queue.put(req)
        data = None
        with self.cv:
            self.cv.wait_for(
                lambda: req_id in self.ready_data or self._in_shutdown)
            if self._in_shutdown:
                raise ConnectionError(
                    f""cannot send request {req}: data channel shutting down"")
            data = self.ready_data[req_id]
            del self.ready_data[req_id]
        return data","    def _blocking_send(self, req: ray_client_pb2.DataRequest
                       ) -> ray_client_pb2.DataResponse:
        req_id = self._next_id()
        req.req_id = req_id
        self.request_queue.put(req)
        data = None
        with self.cv:
            self.cv.wait_for(lambda: req_id in self.ready_data)
            data = self.ready_data[req_id]
            del self.ready_data[req_id]
        return data"
"    def _log_main(self) -> None:
        stub = ray_client_pb2_grpc.RayletLogStreamerStub(self.channel)
        log_stream = stub.Logstream(
            iter(self.request_queue.get, None), metadata=self._metadata)
        try:
            for record in log_stream:
                if record.level < 0:
                    self.stdstream(level=record.level, msg=record.msg)
                self.log(level=record.level, msg=record.msg)
        except grpc.RpcError as e:
            if e.code() == grpc.StatusCode.CANCELLED:
                # Graceful shutdown. We've cancelled our own connection.
                logger.info(""Cancelling logs channel"")
            elif e.code() == grpc.StatusCode.UNAVAILABLE:
                # TODO(barakmich): The server may have
                # dropped. In theory, we can retry, as per
                # https://grpc.github.io/grpc/core/md_doc_statuscodes.html but
                # in practice we may need to think about the correct semantics
                # here.
                logger.info(""Server disconnected from logs channel"")
            else:
                # Some other, unhandled, gRPC error
                logger.error(
                    f""Got Error from logger channel -- shutting down: {e}"")
                raise e","    def _log_main(self) -> None:
        stub = ray_client_pb2_grpc.RayletLogStreamerStub(self.channel)
        log_stream = stub.Logstream(
            iter(self.request_queue.get, None), metadata=self._metadata)
        try:
            for record in log_stream:
                if record.level < 0:
                    self.stdstream(level=record.level, msg=record.msg)
                self.log(level=record.level, msg=record.msg)
        except grpc.RpcError as e:
            if grpc.StatusCode.CANCELLED != e.code():
                # Not just shutting down normally
                logger.error(
                    f""Got Error from logger channel -- shutting down: {e}"")
                raise e"
"    def __init__(self,
                 conn_str: str = """",
                 secure: bool = False,
                 metadata: List[Tuple[str, str]] = None,
                 connection_retries: int = 3):
        """"""Initializes the worker side grpc client.

        Args:
            conn_str: The host:port connection string for the ray server.
            secure: whether to use SSL secure channel or not.
            metadata: additional metadata passed in the grpc request headers.
            connection_retries: Number of times to attempt to reconnect to the
              ray server if it doesn't respond immediately. Setting to 0 tries
              at least once.  For infinite retries, catch the ConnectionError
              exception.
        """"""
        self.metadata = metadata if metadata else []
        self.channel = None
        self._conn_state = grpc.ChannelConnectivity.IDLE
        self._client_id = make_client_id()
        if secure:
            credentials = grpc.ssl_channel_credentials()
            self.channel = grpc.secure_channel(conn_str, credentials)
        else:
            self.channel = grpc.insecure_channel(conn_str)

        self.channel.subscribe(self._on_channel_state_change)

        # Retry the connection until the channel responds to something
        # looking like a gRPC connection, though it may be a proxy.
        conn_attempts = 0
        timeout = INITIAL_TIMEOUT_SEC
        ray_ready = False
        while conn_attempts < max(connection_retries, 1):
            conn_attempts += 1
            try:
                # Let gRPC wait for us to see if the channel becomes ready.
                # If it throws, we couldn't connect.
                grpc.channel_ready_future(self.channel).result(timeout=timeout)
                # The HTTP2 channel is ready. Wrap the channel with the
                # RayletDriverStub, allowing for unary requests.
                self.server = ray_client_pb2_grpc.RayletDriverStub(
                    self.channel)
                # Now the HTTP2 channel is ready, or proxied, but the
                # servicer may not be ready. Call is_initialized() and if
                # it throws, the servicer is not ready. On success, the
                # `ray_ready` result is checked.
                ray_ready = self.is_initialized()
                if ray_ready:
                    # Ray is ready! Break out of the retry loop
                    break
                # Ray is not ready yet, wait a timeout
                time.sleep(timeout)
            except grpc.FutureTimeoutError:
                logger.info(
                    f""Couldn't connect channel in {timeout} seconds, retrying"")
                # Note that channel_ready_future constitutes its own timeout,
                # which is why we do not sleep here.
            except grpc.RpcError as e:
                if e.code() == grpc.StatusCode.UNAVAILABLE:
                    # UNAVAILABLE is gRPC's retryable error,
                    # so we do that here.
                    logger.info(""Ray client server unavailable, ""
                                f""retrying in {timeout}s..."")
                    logger.debug(f""Received when checking init: {e.details()}"")
                    # Ray is not ready yet, wait a timeout
                    time.sleep(timeout)
                else:
                    # Any other gRPC error gets a reraise
                    raise e
            # Fallthrough, backoff, and retry at the top of the loop
            logger.info(""Waiting for Ray to become ready on the server, ""
                        f""retry in {timeout}s..."")
            timeout = backoff(timeout)

        # If we made it through the loop without ray_ready it means we've used
        # up our retries and should error back to the user.
        if not ray_ready:
            raise ConnectionError(""ray client connection timeout"")

        # Initialize the streams to finish protocol negotiation.
        self.data_client = DataClient(self.channel, self._client_id,
                                      self.metadata)
        self.reference_count: Dict[bytes, int] = defaultdict(int)

        self.log_client = LogstreamClient(self.channel, self.metadata)
        self.log_client.set_logstream_level(logging.INFO)
        self.closed = False","    def __init__(self,
                 conn_str: str = """",
                 secure: bool = False,
                 metadata: List[Tuple[str, str]] = None,
                 connection_retries: int = 3):
        """"""Initializes the worker side grpc client.

        Args:
            conn_str: The host:port connection string for the ray server.
            secure: whether to use SSL secure channel or not.
            metadata: additional metadata passed in the grpc request headers.
            connection_retries: Number of times to attempt to reconnect to the
              ray server if it doesn't respond immediately. Setting to 0 tries
              at least once.  For infinite retries, catch the ConnectionError
              exception.
        """"""
        self.metadata = metadata if metadata else []
        self.channel = None
        self._client_id = make_client_id()
        if secure:
            credentials = grpc.ssl_channel_credentials()
            self.channel = grpc.secure_channel(conn_str, credentials)
        else:
            self.channel = grpc.insecure_channel(conn_str)

        # Retry the connection until the channel responds to something
        # looking like a gRPC connection, though it may be a proxy.
        conn_attempts = 0
        timeout = INITIAL_TIMEOUT_SEC
        ray_ready = False
        while conn_attempts < max(connection_retries, 1):
            conn_attempts += 1
            try:
                # Let gRPC wait for us to see if the channel becomes ready.
                # If it throws, we couldn't connect.
                grpc.channel_ready_future(self.channel).result(timeout=timeout)
                # The HTTP2 channel is ready. Wrap the channel with the
                # RayletDriverStub, allowing for unary requests.
                self.server = ray_client_pb2_grpc.RayletDriverStub(
                    self.channel)
                # Now the HTTP2 channel is ready, or proxied, but the
                # servicer may not be ready. Call is_initialized() and if
                # it throws, the servicer is not ready. On success, the
                # `ray_ready` result is checked.
                ray_ready = self.is_initialized()
                if ray_ready:
                    # Ray is ready! Break out of the retry loop
                    break
                # Ray is not ready yet, wait a timeout
                time.sleep(timeout)
            except grpc.FutureTimeoutError:
                logger.info(
                    f""Couldn't connect channel in {timeout} seconds, retrying"")
                # Note that channel_ready_future constitutes its own timeout,
                # which is why we do not sleep here.
            except grpc.RpcError as e:
                if e.code() == grpc.StatusCode.UNAVAILABLE:
                    # UNAVAILABLE is gRPC's retryable error,
                    # so we do that here.
                    logger.info(""Ray client server unavailable, ""
                                f""retrying in {timeout}s..."")
                    logger.debug(f""Received when checking init: {e.details()}"")
                    # Ray is not ready yet, wait a timeout
                    time.sleep(timeout)
                else:
                    # Any other gRPC error gets a reraise
                    raise e
            # Fallthrough, backoff, and retry at the top of the loop
            logger.info(""Waiting for Ray to become ready on the server, ""
                        f""retry in {timeout}s..."")
            timeout = backoff(timeout)

        # If we made it through the loop without ray_ready it means we've used
        # up our retries and should error back to the user.
        if not ray_ready:
            raise ConnectionError(""ray client connection timeout"")

        # Initialize the streams to finish protocol negotiation.
        self.data_client = DataClient(self.channel, self._client_id,
                                      self.metadata)
        self.reference_count: Dict[bytes, int] = defaultdict(int)

        self.log_client = LogstreamClient(self.channel, self.metadata)
        self.log_client.set_logstream_level(logging.INFO)
        self.closed = False"
"def translate(configuration: Dict[str, Any],
              dictionary: Dict[str, str]) -> Dict[str, Any]:
    return {
        dictionary[field]: configuration[field]
        for field in dictionary if field in configuration
    }","def translate(configuration: Dict[str, Any],
              dictionary: Dict[str, str]) -> Dict[str, Any]:
    return {dictionary[field]: configuration[field] for field in dictionary}"
"    def __init__(self,
                 conn_str: str = """",
                 secure: bool = False,
                 metadata: List[Tuple[str, str]] = None,
                 connection_retries: int = 3):
        """"""Initializes the worker side grpc client.

        Args:
            conn_str: The host:port connection string for the ray server.
            secure: whether to use SSL secure channel or not.
            metadata: additional metadata passed in the grpc request headers.
            connection_retries: Number of times to attempt to reconnect to the
              ray server if it doesn't respond immediately. Setting to 0 tries
              at least once.  For infinite retries, catch the ConnectionError
              exception.
        """"""
        self.metadata = metadata if metadata else []
        self.channel = None
        self._client_id = make_client_id()
        if secure:
            credentials = grpc.ssl_channel_credentials()
            self.channel = grpc.secure_channel(conn_str, credentials)
        else:
            self.channel = grpc.insecure_channel(conn_str)

        # Retry the connection until the channel responds to something
        # looking like a gRPC connection, though it may be a proxy.
        conn_attempts = 0
        timeout = INITIAL_TIMEOUT_SEC
        ray_ready = False
        while conn_attempts < max(connection_retries, 1):
            conn_attempts += 1
            try:
                # Let gRPC wait for us to see if the channel becomes ready.
                # If it throws, we couldn't connect.
                grpc.channel_ready_future(self.channel).result(timeout=timeout)
                # The HTTP2 channel is ready. Wrap the channel with the
                # RayletDriverStub, allowing for unary requests.
                self.server = ray_client_pb2_grpc.RayletDriverStub(
                    self.channel)
                # Now the HTTP2 channel is ready, or proxied, but the
                # servicer may not be ready. Call is_initialized() and if
                # it throws, the servicer is not ready. On success, the
                # `ray_ready` result is checked.
                ray_ready = self.is_initialized()
                if ray_ready:
                    # Ray is ready! Break out of the retry loop
                    break
                # Ray is not ready yet, wait a timeout
                time.sleep(timeout)
            except grpc.FutureTimeoutError:
                logger.info(
                    f""Couldn't connect channel in {timeout} seconds, retrying"")
                # Note that channel_ready_future constitutes its own timeout,
                # which is why we do not sleep here.
            except grpc.RpcError as e:
                if e.code() == grpc.StatusCode.UNAVAILABLE:
                    # UNAVAILABLE is gRPC's retryable error,
                    # so we do that here.
                    logger.info(""Ray client server unavailable, ""
                                f""retrying in {timeout}s..."")
                    logger.debug(f""Received when checking init: {e.details()}"")
                    # Ray is not ready yet, wait a timeout
                    time.sleep(timeout)
                else:
                    # Any other gRPC error gets a reraise
                    raise e
            # Fallthrough, backoff, and retry at the top of the loop
            logger.info(""Waiting for Ray to become ready on the server, ""
                        f""retry in {timeout}s..."")
            timeout = backoff(timeout)

        # If we made it through the loop without ray_ready it means we've used
        # up our retries and should error back to the user.
        if not ray_ready:
            raise ConnectionError(""ray client connection timeout"")

        # Initialize the streams to finish protocol negotiation.
        self.data_client = DataClient(self.channel, self._client_id,
                                      self.metadata)
        self.reference_count: Dict[bytes, int] = defaultdict(int)

        self.log_client = LogstreamClient(self.channel, self.metadata)
        self.log_client.set_logstream_level(logging.INFO)
        self.closed = False","    def __init__(self,
                 conn_str: str = """",
                 secure: bool = False,
                 metadata: List[Tuple[str, str]] = None,
                 connection_retries: int = 3):
        """"""Initializes the worker side grpc client.

        Args:
            conn_str: The host:port connection string for the ray server.
            secure: whether to use SSL secure channel or not.
            metadata: additional metadata passed in the grpc request headers.
            connection_retries: Number of times to attempt to reconnect to the
              ray server if it doesn't respond immediately. Setting to 0 tries
              at least once.  For infinite retries, catch the ConnectionError
              exception.
        """"""
        self.metadata = metadata if metadata else []
        self.channel = None
        self._client_id = make_client_id()
        if secure:
            credentials = grpc.ssl_channel_credentials()
            self.channel = grpc.secure_channel(conn_str, credentials)
        else:
            self.channel = grpc.insecure_channel(conn_str)

        conn_attempts = 0
        timeout = INITIAL_TIMEOUT_SEC
        while conn_attempts < connection_retries + 1:
            conn_attempts += 1
            try:
                grpc.channel_ready_future(self.channel).result(timeout=timeout)
                break
            except grpc.FutureTimeoutError:
                if conn_attempts >= connection_retries:
                    raise ConnectionError(""ray client connection timeout"")
                logger.info(f""Couldn't connect in {timeout} seconds, retrying"")
                timeout = timeout + 5
                if timeout > MAX_TIMEOUT_SEC:
                    timeout = MAX_TIMEOUT_SEC

        self.server = ray_client_pb2_grpc.RayletDriverStub(self.channel)

        self.data_client = DataClient(self.channel, self._client_id,
                                      self.metadata)
        self.reference_count: Dict[bytes, int] = defaultdict(int)

        self.log_client = LogstreamClient(self.channel, self.metadata)
        self.log_client.set_logstream_level(logging.INFO)
        self.closed = False"
"    def __init__(self,
                 name: str,
                 description: str = """",
                 tag_keys: Optional[Tuple[str]] = None):
        if len(name) == 0:
            raise ValueError(""Empty name is not allowed. ""
                             ""Please provide a metric name."")
        self._name = name
        self._description = description
        # We don't specify unit because it won't be
        # exported to Prometheus anyway.
        self._unit = """"
        # The default tags key-value pair.
        self._default_tags = {}
        # Keys of tags.
        self._tag_keys = tag_keys or tuple()
        # The Cython metric class. This should be set in the child class.
        self._metric = None

        if not isinstance(self._tag_keys, tuple):
            raise TypeError(""tag_keys should be a tuple type, got: ""
                            f""{type(self._tag_keys)}"")

        for key in self._tag_keys:
            if not isinstance(key, str):
                raise TypeError(f""Tag keys must be str, got {type(key)}."")","    def __init__(self,
                 name: str,
                 description: str = """",
                 tag_keys: Optional[Tuple[str]] = None):
        if len(name) == 0:
            raise ValueError(""Empty name is not allowed. ""
                             ""Please provide a metric name."")
        self._name = name
        self._description = description
        # We don't specify unit because it won't be
        # exported to Prometheus anyway.
        self._unit = """"
        # The default tags key-value pair.
        self._default_tags = {}
        # Keys of tags.
        self._tag_keys = tag_keys or tuple()
        # The Cython metric class. This should be set in the child class.
        self._metric = None

        if not isinstance(self._tag_keys, tuple):
            raise ValueError(""tag_keys should be a tuple type, got: ""
                             f""{type(self._tag_keys)}"")"
"    def set_default_tags(self, default_tags: Dict[str, str]):
        """"""Set default tags of metrics.

        Example:
            >>> # Note that set_default_tags returns the instance itself.
            >>> counter = Counter(""name"")
            >>> counter2 = counter.set_default_tags({""a"": ""b""})
            >>> assert counter is counter2
            >>> # this means you can instantiate it in this way.
            >>> counter = Counter(""name"").set_default_tags({""a"": ""b""})

        Args:
            default_tags(dict): Default tags that are
                used for every record method.

        Returns:
            Metric: it returns the instance itself.
        """"""
        for key, val in default_tags.items():
            if key not in self._tag_keys:
                raise ValueError(f""Unrecognized tag key {key}."")
            if not isinstance(val, str):
                raise TypeError(f""Tag values must be str, got {type(val)}."")

        self._default_tags = default_tags
        return self","    def set_default_tags(self, default_tags: Dict[str, str]):
        """"""Set default tags of metrics.

        Example:
            >>> # Note that set_default_tags returns the instance itself.
            >>> counter = Counter(""name"")
            >>> counter2 = counter.set_default_tags({""a"": ""b""})
            >>> assert counter is counter2
            >>> # this means you can instantiate it in this way.
            >>> counter = Counter(""name"").set_default_tags({""a"": ""b""})

        Args:
            default_tags(dict): Default tags that are
                used for every record method.

        Returns:
            Metric: it returns the instance itself.
        """"""
        self._default_tags = default_tags
        return self"
"    def record(self, value: float, tags: dict = None) -> None:
        """"""Record the metric point of the metric.

        Args:
            value(float): The value to be recorded as a metric point.
        """"""
        assert self._metric is not None
        if tags is not None:
            for val in tags.values():
                if not isinstance(val, str):
                    raise TypeError(
                        f""Tag values must be str, got {type(val)}."")

        default_tag_copy = self._default_tags.copy()
        default_tag_copy.update(tags or {})
        self._metric.record(value, tags=default_tag_copy)","    def record(self, value: float, tags: dict = None) -> None:
        """"""Record the metric point of the metric.

        Args:
            value(float): The value to be recorded as a metric point.
        """"""
        assert self._metric is not None
        default_tag_copy = self._default_tags.copy()
        default_tag_copy.update(tags or {})
        self._metric.record(value, tags=default_tag_copy)"
"def postprocess_advantages(policy,
                           sample_batch,
                           other_agent_batches=None,
                           episode=None):

    # Stub serving backward compatibility.
    deprecation_warning(
        old=""rllib.agents.a3c.a3c_tf_policy.postprocess_advantages"",
        new=""rllib.evaluation.postprocessing.compute_gae_for_sample_batch"",
        error=False)

    return compute_gae_for_sample_batch(policy, sample_batch,
                                        other_agent_batches, episode)","def postprocess_advantages(policy,
                           sample_batch,
                           other_agent_batches=None,
                           episode=None):
    completed = sample_batch[SampleBatch.DONES][-1]
    if completed:
        last_r = 0.0
    else:
        next_state = []
        for i in range(policy.num_state_tensors()):
            next_state.append(sample_batch[""state_out_{}"".format(i)][-1])
        last_r = policy._value(sample_batch[SampleBatch.NEXT_OBS][-1],
                               sample_batch[SampleBatch.ACTIONS][-1],
                               sample_batch[SampleBatch.REWARDS][-1],
                               *next_state)
    return compute_advantages(
        sample_batch, last_r, policy.config[""gamma""], policy.config[""lambda""],
        policy.config[""use_gae""], policy.config[""use_critic""])"
"def setup_mixins(policy, obs_space, action_space, config):
    ValueNetworkMixin.__init__(policy, obs_space, action_space, config)
    LearningRateSchedule.__init__(policy, config[""lr""], config[""lr_schedule""])","def setup_mixins(policy, obs_space, action_space, config):
    ValueNetworkMixin.__init__(policy)
    LearningRateSchedule.__init__(policy, config[""lr""], config[""lr_schedule""])"
"    def __init__(self,
                 action_dist,
                 actions,
                 advantages,
                 v_target,
                 vf,
                 vf_loss_coeff=0.5,
                 entropy_coeff=0.01):
        log_prob = action_dist.logp(actions)

        # The ""policy gradients"" loss
        self.pi_loss = -tf.reduce_sum(log_prob * advantages)

        delta = vf - v_target
        self.vf_loss = 0.5 * tf.reduce_sum(tf.math.square(delta))
        self.entropy = tf.reduce_sum(action_dist.entropy())
        self.total_loss = (self.pi_loss + self.vf_loss * vf_loss_coeff -
                           self.entropy * entropy_coeff)","    def __init__(self):
        @make_tf_callable(self.get_session())
        def value(ob, prev_action, prev_reward, *state):
            model_out, _ = self.model({
                SampleBatch.CUR_OBS: tf.convert_to_tensor([ob]),
                SampleBatch.PREV_ACTIONS: tf.convert_to_tensor([prev_action]),
                SampleBatch.PREV_REWARDS: tf.convert_to_tensor([prev_reward]),
                ""is_training"": tf.convert_to_tensor(False),
            }, [tf.convert_to_tensor([s]) for s in state],
                                      tf.convert_to_tensor([1]))
            return self.model.value_function()[0]

        self._value = value"
"def add_advantages(policy,
                   sample_batch,
                   other_agent_batches=None,
                   episode=None):

    # Stub serving backward compatibility.
    deprecation_warning(
        old=""rllib.agents.a3c.a3c_torch_policy.add_advantages"",
        new=""rllib.evaluation.postprocessing.compute_gae_for_sample_batch"",
        error=False)

    return compute_gae_for_sample_batch(policy, sample_batch,
                                        other_agent_batches, episode)","def add_advantages(policy,
                   sample_batch,
                   other_agent_batches=None,
                   episode=None):

    completed = sample_batch[SampleBatch.DONES][-1]
    if completed:
        last_r = 0.0
    else:
        last_r = policy._value(sample_batch[SampleBatch.NEXT_OBS][-1])

    return compute_advantages(
        sample_batch, last_r, policy.config[""gamma""], policy.config[""lambda""],
        policy.config[""use_gae""], policy.config[""use_critic""])"
"def clip_gradients(policy: Policy, optimizer: ""tf.keras.optimizers.Optimizer"",
                   loss: TensorType) -> ModelGradients:
    return minimize_and_clip(
        optimizer,
        loss,
        var_list=policy.q_func_vars,
        clip_val=policy.config[""grad_clip""])","def clip_gradients(policy: Policy, optimizer: ""tf.keras.optimizers.Optimizer"",
                   loss: TensorType) -> ModelGradients:
    if policy.config[""grad_clip""] is not None:
        grads_and_vars = minimize_and_clip(
            optimizer,
            loss,
            var_list=policy.q_func_vars,
            clip_val=policy.config[""grad_clip""])
    else:
        grads_and_vars = optimizer.compute_gradients(
            loss, var_list=policy.q_func_vars)
    grads_and_vars = [(g, v) for (g, v) in grads_and_vars if g is not None]
    return grads_and_vars"
"def get_policy_class(config):
    if config[""framework""] == ""torch"":
        if config[""vtrace""]:
            from ray.rllib.agents.impala.vtrace_torch_policy import \\
                VTraceTorchPolicy
            return VTraceTorchPolicy
        else:
            from ray.rllib.agents.a3c.a3c_torch_policy import \\
                A3CTorchPolicy
            return A3CTorchPolicy
    else:
        if config[""vtrace""]:
            return VTraceTFPolicy
        else:
            from ray.rllib.agents.a3c.a3c_tf_policy import A3CTFPolicy
            return A3CTFPolicy","def get_policy_class(config):
    if config[""framework""] == ""torch"":
        if config[""vtrace""]:
            from ray.rllib.agents.impala.vtrace_torch_policy import \\
                VTraceTorchPolicy
            return VTraceTorchPolicy
        else:
            from ray.rllib.agents.a3c.a3c_torch_policy import \\
                A3CTorchPolicy
            return A3CTorchPolicy
    else:
        if config[""vtrace""]:
            return VTraceTFPolicy
        else:
            return A3CTFPolicy"
"def postprocess_trajectory(
        policy: Policy,
        sample_batch: SampleBatch,
        other_agent_batches: Optional[Dict[AgentID, SampleBatch]] = None,
        episode: Optional[MultiAgentEpisode] = None) -> SampleBatch:
    """"""Postprocesses a trajectory and returns the processed trajectory.

    The trajectory contains only data from one episode and from one agent.
    - If  `config.batch_mode=truncate_episodes` (default), sample_batch may
    contain a truncated (at-the-end) episode, in case the
    `config.rollout_fragment_length` was reached by the sampler.
    - If `config.batch_mode=complete_episodes`, sample_batch will contain
    exactly one episode (no matter how long).
    New columns can be added to sample_batch and existing ones may be altered.

    Args:
        policy (Policy): The Policy used to generate the trajectory
            (`sample_batch`)
        sample_batch (SampleBatch): The SampleBatch to postprocess.
        other_agent_batches (Optional[Dict[PolicyID, SampleBatch]]): Optional
            dict of AgentIDs mapping to other agents' trajectory data (from the
            same episode). NOTE: The other agents use the same policy.
        episode (Optional[MultiAgentEpisode]): Optional multi-agent episode
            object in which the agents operated.

    Returns:
        SampleBatch: The postprocessed, modified SampleBatch (or a new one).
    """"""
    if not policy.config[""vtrace""]:
        sample_batch = compute_gae_for_sample_batch(
            policy, sample_batch, other_agent_batches, episode)

    # TODO: (sven) remove this del once we have trajectory view API fully in
    #  place.
    del sample_batch.data[""new_obs""]  # not used, so save some bandwidth

    return sample_batch","def postprocess_trajectory(
        policy: Policy,
        sample_batch: SampleBatch,
        other_agent_batches: Optional[Dict[AgentID, SampleBatch]] = None,
        episode: Optional[MultiAgentEpisode] = None) -> SampleBatch:
    """"""Postprocesses a trajectory and returns the processed trajectory.

    The trajectory contains only data from one episode and from one agent.
    - If  `config.batch_mode=truncate_episodes` (default), sample_batch may
    contain a truncated (at-the-end) episode, in case the
    `config.rollout_fragment_length` was reached by the sampler.
    - If `config.batch_mode=complete_episodes`, sample_batch will contain
    exactly one episode (no matter how long).
    New columns can be added to sample_batch and existing ones may be altered.

    Args:
        policy (Policy): The Policy used to generate the trajectory
            (`sample_batch`)
        sample_batch (SampleBatch): The SampleBatch to postprocess.
        other_agent_batches (Optional[Dict[PolicyID, SampleBatch]]): Optional
            dict of AgentIDs mapping to other agents' trajectory data (from the
            same episode). NOTE: The other agents use the same policy.
        episode (Optional[MultiAgentEpisode]): Optional multi-agent episode
            object in which the agents operated.

    Returns:
        SampleBatch: The postprocessed, modified SampleBatch (or a new one).
    """"""
    if not policy.config[""vtrace""]:
        sample_batch = postprocess_ppo_gae(policy, sample_batch,
                                           other_agent_batches, episode)

    # TODO: (sven) remove this del once we have trajectory view API fully in
    #  place.
    del sample_batch.data[""new_obs""]  # not used, so save some bandwidth

    return sample_batch"
"def postprocess_ppo_gae(
        policy: Policy,
        sample_batch: SampleBatch,
        other_agent_batches: Optional[Dict[AgentID, SampleBatch]] = None,
        episode: Optional[MultiAgentEpisode] = None) -> SampleBatch:

    # Stub serving backward compatibility.
    deprecation_warning(
        old=""rllib.agents.ppo.ppo_tf_policy.postprocess_ppo_gae"",
        new=""rllib.evaluation.postprocessing.compute_gae_for_sample_batch"",
        error=False)

    return compute_gae_for_sample_batch(policy, sample_batch,
                                        other_agent_batches, episode)","def postprocess_ppo_gae(
        policy: Policy,
        sample_batch: SampleBatch,
        other_agent_batches: Optional[Dict[AgentID, SampleBatch]] = None,
        episode: Optional[MultiAgentEpisode] = None) -> SampleBatch:
    """"""Postprocesses a trajectory and returns the processed trajectory.

    The trajectory contains only data from one episode and from one agent.
    - If  `config.batch_mode=truncate_episodes` (default), sample_batch may
    contain a truncated (at-the-end) episode, in case the
    `config.rollout_fragment_length` was reached by the sampler.
    - If `config.batch_mode=complete_episodes`, sample_batch will contain
    exactly one episode (no matter how long).
    New columns can be added to sample_batch and existing ones may be altered.

    Args:
        policy (Policy): The Policy used to generate the trajectory
            (`sample_batch`)
        sample_batch (SampleBatch): The SampleBatch to postprocess.
        other_agent_batches (Optional[Dict[PolicyID, SampleBatch]]): Optional
            dict of AgentIDs mapping to other agents' trajectory data (from the
            same episode). NOTE: The other agents use the same policy.
        episode (Optional[MultiAgentEpisode]): Optional multi-agent episode
            object in which the agents operated.

    Returns:
        SampleBatch: The postprocessed, modified SampleBatch (or a new one).
    """"""

    # Trajectory is actually complete -> last r=0.0.
    if sample_batch[SampleBatch.DONES][-1]:
        last_r = 0.0
    # Trajectory has been truncated -> last r=VF estimate of last obs.
    else:
        # Input dict is provided to us automatically via the Model's
        # requirements. It's a single-timestep (last one in trajectory)
        # input_dict.
        if policy.config[""_use_trajectory_view_api""]:
            # Create an input dict according to the Model's requirements.
            input_dict = policy.model.get_input_dict(
                sample_batch, index=""last"")
            last_r = policy._value(**input_dict)
        # TODO: (sven) Remove once trajectory view API is all-algo default.
        else:
            next_state = []
            for i in range(policy.num_state_tensors()):
                next_state.append(sample_batch[""state_out_{}"".format(i)][-1])
            last_r = policy._value(sample_batch[SampleBatch.NEXT_OBS][-1],
                                   sample_batch[SampleBatch.ACTIONS][-1],
                                   sample_batch[SampleBatch.REWARDS][-1],
                                   *next_state)

    # Adds the policy logits, VF preds, and advantages to the batch,
    # using GAE (""generalized advantage estimation"") or not.
    batch = compute_advantages(
        sample_batch,
        last_r,
        policy.config[""gamma""],
        policy.config[""lambda""],
        use_gae=policy.config[""use_gae""],
        use_critic=policy.config.get(""use_critic"", True))

    return batch"
"    def gradients(self, optimizer, loss):
        self.gvs = {
            k: minimize_and_clip(optimizer, self.losses[k], self.vars[k],
                                 self.config[""grad_norm_clipping""])
            for k, optimizer in self.optimizers.items()
        }
        return self.gvs[""critic""] + self.gvs[""actor""]","    def gradients(self, optimizer, loss):
        if self.config[""grad_norm_clipping""] is not None:
            self.gvs = {
                k: minimize_and_clip(optimizer, self.losses[k], self.vars[k],
                                     self.config[""grad_norm_clipping""])
                for k, optimizer in self.optimizers.items()
            }
        else:
            self.gvs = {
                k: optimizer.compute_gradients(self.losses[k], self.vars[k])
                for k, optimizer in self.optimizers.items()
            }
        return self.gvs[""critic""] + self.gvs[""actor""]"
"def minimize_and_clip(optimizer, objective, var_list, clip_val=10.0):
    """"""Minimized `objective` using `optimizer` w.r.t. variables in
    `var_list` while ensure the norm of the gradients for each
    variable is clipped to `clip_val`
    """"""
    # Accidentally passing values < 0.0 will break all gradients.
    assert clip_val is None or clip_val > 0.0, clip_val

    if tf.executing_eagerly():
        tape = optimizer.tape
        grads_and_vars = list(
            zip(list(tape.gradient(objective, var_list)), var_list))
    else:
        grads_and_vars = optimizer.compute_gradients(
            objective, var_list=var_list)

    return [(tf.clip_by_norm(g, clip_val) if clip_val is not None else g, v)
            for (g, v) in grads_and_vars if g is not None]","def minimize_and_clip(optimizer, objective, var_list, clip_val=10.0):
    """"""Minimized `objective` using `optimizer` w.r.t. variables in
    `var_list` while ensure the norm of the gradients for each
    variable is clipped to `clip_val`
    """"""
    # Accidentally passing values < 0.0 will break all gradients.
    assert clip_val > 0.0, clip_val

    if tf.executing_eagerly():
        tape = optimizer.tape
        grads_and_vars = list(
            zip(list(tape.gradient(objective, var_list)), var_list))
    else:
        grads_and_vars = optimizer.compute_gradients(
            objective, var_list=var_list)

    for i, (grad, var) in enumerate(grads_and_vars):
        if grad is not None:
            grads_and_vars[i] = (tf.clip_by_norm(grad, clip_val), var)
    return grads_and_vars"
"    def persistent_id(self, obj):
        if isinstance(obj, ray.ObjectRef):
            obj_id = obj.binary()
            if obj_id not in self.server.object_refs[self.client_id]:
                # We're passing back a reference, probably inside a reference.
                # Let's hold onto it.
                self.server.object_refs[self.client_id][obj_id] = obj
            return PickleStub(
                type=""Object"",
                client_id=self.client_id,
                ref_id=obj_id,
                name=None,
                baseline_options=None,
            )
        elif isinstance(obj, ray.actor.ActorHandle):
            actor_id = obj._actor_id.binary()
            if actor_id not in self.server.actor_refs:
                # We're passing back a handle, probably inside a reference.
                self.server.actor_refs[actor_id] = obj
            if actor_id not in self.server.actor_owners[self.client_id]:
                self.server.actor_owners[self.client_id].add(actor_id)
            return PickleStub(
                type=""Actor"",
                client_id=self.client_id,
                ref_id=obj._actor_id.binary(),
                name=None,
                baseline_options=None,
            )
        return None","    def persistent_id(self, obj):
        if isinstance(obj, ray.ObjectRef):
            obj_id = obj.binary()
            if obj_id not in self.server.object_refs[self.client_id]:
                # We're passing back a reference, probably inside a reference.
                # Let's hold onto it.
                self.server.object_refs[self.client_id][obj_id] = obj
            return PickleStub(
                type=""Object"",
                client_id=self.client_id,
                ref_id=obj_id,
                name=None,
                baseline_options=None,
            )
        elif isinstance(obj, ray.actor.ActorHandle):
            actor_id = obj._actor_id.binary()
            if actor_id not in self.server.actor_refs:
                # We're passing back a handle, probably inside a reference.
                self.actor_refs[actor_id] = obj
            if actor_id not in self.actor_owners[self.client_id]:
                self.actor_owners[self.client_id].add(actor_id)
            return PickleStub(
                type=""Actor"",
                client_id=self.client_id,
                ref_id=obj._actor_id.binary(),
                name=None,
                baseline_options=None,
            )
        return None"
"    def ClusterInfo(self, request,
                    context=None) -> ray_client_pb2.ClusterInfoResponse:
        resp = ray_client_pb2.ClusterInfoResponse()
        resp.type = request.type
        if request.type == ray_client_pb2.ClusterInfoType.CLUSTER_RESOURCES:
            with disable_client_hook():
                resources = ray.cluster_resources()
            # Normalize resources into floats
            # (the function may return values that are ints)
            float_resources = {k: float(v) for k, v in resources.items()}
            resp.resource_table.CopyFrom(
                ray_client_pb2.ClusterInfoResponse.ResourceTable(
                    table=float_resources))
        elif request.type == \\
                ray_client_pb2.ClusterInfoType.AVAILABLE_RESOURCES:
            with disable_client_hook():
                resources = ray.available_resources()
            # Normalize resources into floats
            # (the function may return values that are ints)
            float_resources = {k: float(v) for k, v in resources.items()}
            resp.resource_table.CopyFrom(
                ray_client_pb2.ClusterInfoResponse.ResourceTable(
                    table=float_resources))
        elif request.type == ray_client_pb2.ClusterInfoType.RUNTIME_CONTEXT:
            ctx = ray_client_pb2.ClusterInfoResponse.RuntimeContext()
            with disable_client_hook():
                rtc = ray.get_runtime_context()
                ctx.job_id = rtc.job_id.binary()
                ctx.node_id = rtc.node_id.binary()
                ctx.capture_client_tasks = \\
                    rtc.should_capture_child_tasks_in_placement_group
            resp.runtime_context.CopyFrom(ctx)
        else:
            with disable_client_hook():
                resp.json = self._return_debug_cluster_info(request, context)
        return resp","    def ClusterInfo(self, request,
                    context=None) -> ray_client_pb2.ClusterInfoResponse:
        resp = ray_client_pb2.ClusterInfoResponse()
        resp.type = request.type
        if request.type == ray_client_pb2.ClusterInfoType.CLUSTER_RESOURCES:
            with disable_client_hook():
                resources = ray.cluster_resources()
            # Normalize resources into floats
            # (the function may return values that are ints)
            float_resources = {k: float(v) for k, v in resources.items()}
            resp.resource_table.CopyFrom(
                ray_client_pb2.ClusterInfoResponse.ResourceTable(
                    table=float_resources))
        elif request.type == \\
                ray_client_pb2.ClusterInfoType.AVAILABLE_RESOURCES:
            with disable_client_hook():
                resources = ray.available_resources()
            # Normalize resources into floats
            # (the function may return values that are ints)
            float_resources = {k: float(v) for k, v in resources.items()}
            resp.resource_table.CopyFrom(
                ray_client_pb2.ClusterInfoResponse.ResourceTable(
                    table=float_resources))
        else:
            with disable_client_hook():
                resp.json = self._return_debug_cluster_info(request, context)
        return resp"
"    def get_cluster_info(self, type: ray_client_pb2.ClusterInfoType.TypeEnum):
        req = ray_client_pb2.ClusterInfoRequest()
        req.type = type
        resp = self.server.ClusterInfo(req, metadata=self.metadata)
        if resp.WhichOneof(""response_type"") == ""resource_table"":
            # translate from a proto map to a python dict
            output_dict = {k: v for k, v in resp.resource_table.table.items()}
            return output_dict
        elif resp.WhichOneof(""response_type"") == ""runtime_context"":
            return resp.runtime_context
        return json.loads(resp.json)","    def get_cluster_info(self, type: ray_client_pb2.ClusterInfoType.TypeEnum):
        req = ray_client_pb2.ClusterInfoRequest()
        req.type = type
        resp = self.server.ClusterInfo(req, metadata=self.metadata)
        if resp.WhichOneof(""response_type"") == ""resource_table"":
            # translate from a proto map to a python dict
            output_dict = {k: v for k, v in resp.resource_table.table.items()}
            return output_dict
        return json.loads(resp.json)"
"    def get(self):
        """"""Get a dictionary of the current context.

        Fields that are not available (e.g., actor ID inside a task) won't be
        included in the field.

        Returns:
            dict: Dictionary of the current context.
        """"""
        context = {
            ""job_id"": self.job_id,
            ""node_id"": self.node_id,
        }
        if self.worker.mode == ray.worker.WORKER_MODE:
            if self.task_id is not None:
                context[""task_id""] = self.task_id
            if self.actor_id is not None:
                context[""actor_id""] = self.actor_id

        return context","    def get(self):
        """"""Get a dictionary of the current_context.

        For fields that are not available (for example actor id inside a task)
        won't be included in the field.

        Returns:
            dict: Dictionary of the current context.
        """"""
        context = {
            ""job_id"": self.job_id,
            ""node_id"": self.node_id,
            ""task_id"": self.task_id,
            ""actor_id"": self.actor_id
        }
        # Remove fields that are None.
        return {
            key: value
            for key, value in context.items() if value is not None
        }"
"    def __init__(self,
                 node_ip_address,
                 redis_address,
                 dashboard_agent_port,
                 redis_password=None,
                 temp_dir=None,
                 log_dir=None,
                 metrics_export_port=None,
                 node_manager_port=None,
                 object_store_name=None,
                 raylet_name=None):
        """"""Initialize the DashboardAgent object.""""""
        # Public attributes are accessible for all agent modules.
        self.ip = node_ip_address
        self.redis_address = dashboard_utils.address_tuple(redis_address)
        self.redis_password = redis_password
        self.temp_dir = temp_dir
        self.log_dir = log_dir
        self.dashboard_agent_port = dashboard_agent_port
        self.metrics_export_port = metrics_export_port
        self.node_manager_port = node_manager_port
        self.object_store_name = object_store_name
        self.raylet_name = raylet_name
        self.node_id = os.environ[""RAY_NODE_ID""]
        # TODO(edoakes): RAY_RAYLET_PID isn't properly set on Windows. This is
        # only used for fate-sharing with the raylet and we need a different
        # fate-sharing mechanism for Windows anyways.
        if sys.platform not in [""win32"", ""cygwin""]:
            self.ppid = int(os.environ[""RAY_RAYLET_PID""])
            assert self.ppid > 0
            logger.info(""Parent pid is %s"", self.ppid)
        self.server = aiogrpc.server(options=((""grpc.so_reuseport"", 0), ))
        self.grpc_port = self.server.add_insecure_port(
            f""[::]:{self.dashboard_agent_port}"")
        logger.info(""Dashboard agent grpc address: %s:%s"", self.ip,
                    self.grpc_port)
        self.aioredis_client = None
        self.aiogrpc_raylet_channel = aiogrpc.insecure_channel(
            f""{self.ip}:{self.node_manager_port}"")
        self.http_session = None","    def __init__(self,
                 node_ip_address,
                 redis_address,
                 dashboard_agent_port,
                 redis_password=None,
                 temp_dir=None,
                 log_dir=None,
                 metrics_export_port=None,
                 node_manager_port=None,
                 object_store_name=None,
                 raylet_name=None):
        """"""Initialize the DashboardAgent object.""""""
        # Public attributes are accessible for all agent modules.
        self.ip = node_ip_address
        self.redis_address = dashboard_utils.address_tuple(redis_address)
        self.redis_password = redis_password
        self.temp_dir = temp_dir
        self.log_dir = log_dir
        self.dashboard_agent_port = dashboard_agent_port
        self.metrics_export_port = metrics_export_port
        self.node_manager_port = node_manager_port
        self.object_store_name = object_store_name
        self.raylet_name = raylet_name
        self.node_id = os.environ[""RAY_NODE_ID""]
        self.ppid = int(os.environ[""RAY_RAYLET_PID""])
        assert self.ppid > 0
        logger.info(""Parent pid is %s"", self.ppid)
        self.server = aiogrpc.server(options=((""grpc.so_reuseport"", 0), ))
        self.grpc_port = self.server.add_insecure_port(
            f""[::]:{self.dashboard_agent_port}"")
        logger.info(""Dashboard agent grpc address: %s:%s"", self.ip,
                    self.grpc_port)
        self.aioredis_client = None
        self.aiogrpc_raylet_channel = aiogrpc.insecure_channel(
            f""{self.ip}:{self.node_manager_port}"")
        self.http_session = None"
"    async def run(self):
        async def _check_parent():
            """"""Check if raylet is dead and fate-share if it is.""""""
            try:
                curr_proc = psutil.Process()
                while True:
                    parent = curr_proc.parent()
                    if (parent is None or parent.pid == 1
                            or self.ppid != parent.pid):
                        logger.error(""Raylet is dead, exiting."")
                        sys.exit(0)
                    await asyncio.sleep(
                        dashboard_consts.
                        DASHBOARD_AGENT_CHECK_PARENT_INTERVAL_SECONDS)
            except Exception:
                logger.error(""Failed to check parent PID, exiting."")
                sys.exit(1)

        if sys.platform not in [""win32"", ""cygwin""]:
            check_parent_task = create_task(_check_parent())

        # Create an aioredis client for all modules.
        try:
            self.aioredis_client = await dashboard_utils.get_aioredis_client(
                self.redis_address, self.redis_password,
                dashboard_consts.CONNECT_REDIS_INTERNAL_SECONDS,
                dashboard_consts.RETRY_REDIS_CONNECTION_TIMES)
        except (socket.gaierror, ConnectionRefusedError):
            logger.error(
                ""Dashboard agent exiting: ""
                ""Failed to connect to redis at %s"", self.redis_address)
            sys.exit(-1)

        # Create a http session for all modules.
        self.http_session = aiohttp.ClientSession(
            loop=asyncio.get_event_loop())

        # Start a grpc asyncio server.
        await self.server.start()

        modules = self._load_modules()

        # Http server should be initialized after all modules loaded.
        app = aiohttp.web.Application()
        app.add_routes(routes=routes.bound_routes())

        # Enable CORS on all routes.
        cors = aiohttp_cors.setup(
            app,
            defaults={
                ""*"": aiohttp_cors.ResourceOptions(
                    allow_credentials=True,
                    expose_headers=""*"",
                    allow_methods=""*"",
                    allow_headers=(""Content-Type"", ""X-Header""),
                )
            })
        for route in list(app.router.routes()):
            cors.add(route)

        runner = aiohttp.web.AppRunner(app)
        await runner.setup()
        site = aiohttp.web.TCPSite(runner, self.ip, 0)
        await site.start()
        http_host, http_port = site._server.sockets[0].getsockname()
        logger.info(""Dashboard agent http address: %s:%s"", http_host,
                    http_port)

        # Dump registered http routes.
        dump_routes = [
            r for r in app.router.routes() if r.method != hdrs.METH_HEAD
        ]
        for r in dump_routes:
            logger.info(r)
        logger.info(""Registered %s routes."", len(dump_routes))

        # Write the dashboard agent port to redis.
        await self.aioredis_client.set(
            f""{dashboard_consts.DASHBOARD_AGENT_PORT_PREFIX}{self.node_id}"",
            json.dumps([http_port, self.grpc_port]))

        # Register agent to agent manager.
        raylet_stub = agent_manager_pb2_grpc.AgentManagerServiceStub(
            self.aiogrpc_raylet_channel)

        await raylet_stub.RegisterAgent(
            agent_manager_pb2.RegisterAgentRequest(
                agent_pid=os.getpid(),
                agent_port=self.grpc_port,
                agent_ip_address=self.ip))

        await asyncio.gather(check_parent_task,
                             *(m.run(self.server) for m in modules))
        await self.server.wait_for_termination()
        # Wait for finish signal.
        await runner.cleanup()","    async def run(self):
        async def _check_parent():
            """"""Check if raylet is dead and fate-share if it is.""""""
            try:
                curr_proc = psutil.Process()
                while True:
                    parent = curr_proc.parent()
                    if (parent is None or parent.pid == 1
                            or self.ppid != parent.pid):
                        logger.error(""Raylet is dead, exiting."")
                        sys.exit(0)
                    await asyncio.sleep(
                        dashboard_consts.
                        DASHBOARD_AGENT_CHECK_PARENT_INTERVAL_SECONDS)
            except Exception:
                logger.error(""Failed to check parent PID, exiting."")
                sys.exit(1)

        check_parent_task = create_task(_check_parent())

        # Create an aioredis client for all modules.
        try:
            self.aioredis_client = await dashboard_utils.get_aioredis_client(
                self.redis_address, self.redis_password,
                dashboard_consts.CONNECT_REDIS_INTERNAL_SECONDS,
                dashboard_consts.RETRY_REDIS_CONNECTION_TIMES)
        except (socket.gaierror, ConnectionRefusedError):
            logger.error(
                ""Dashboard agent exiting: ""
                ""Failed to connect to redis at %s"", self.redis_address)
            sys.exit(-1)

        # Create a http session for all modules.
        self.http_session = aiohttp.ClientSession(
            loop=asyncio.get_event_loop())

        # Start a grpc asyncio server.
        await self.server.start()

        modules = self._load_modules()

        # Http server should be initialized after all modules loaded.
        app = aiohttp.web.Application()
        app.add_routes(routes=routes.bound_routes())

        # Enable CORS on all routes.
        cors = aiohttp_cors.setup(
            app,
            defaults={
                ""*"": aiohttp_cors.ResourceOptions(
                    allow_credentials=True,
                    expose_headers=""*"",
                    allow_methods=""*"",
                    allow_headers=(""Content-Type"", ""X-Header""),
                )
            })
        for route in list(app.router.routes()):
            cors.add(route)

        runner = aiohttp.web.AppRunner(app)
        await runner.setup()
        site = aiohttp.web.TCPSite(runner, self.ip, 0)
        await site.start()
        http_host, http_port = site._server.sockets[0].getsockname()
        logger.info(""Dashboard agent http address: %s:%s"", http_host,
                    http_port)

        # Dump registered http routes.
        dump_routes = [
            r for r in app.router.routes() if r.method != hdrs.METH_HEAD
        ]
        for r in dump_routes:
            logger.info(r)
        logger.info(""Registered %s routes."", len(dump_routes))

        # Write the dashboard agent port to redis.
        await self.aioredis_client.set(
            f""{dashboard_consts.DASHBOARD_AGENT_PORT_PREFIX}{self.node_id}"",
            json.dumps([http_port, self.grpc_port]))

        # Register agent to agent manager.
        raylet_stub = agent_manager_pb2_grpc.AgentManagerServiceStub(
            self.aiogrpc_raylet_channel)

        await raylet_stub.RegisterAgent(
            agent_manager_pb2.RegisterAgentRequest(
                agent_pid=os.getpid(),
                agent_port=self.grpc_port,
                agent_ip_address=self.ip))

        await asyncio.gather(check_parent_task,
                             *(m.run(self.server) for m in modules))
        await self.server.wait_for_termination()
        # Wait for finish signal.
        await runner.cleanup()"
"    def __str__(self):
        return (""The worker died unexpectedly while executing this task. ""
                ""Check python-core-worker-*.log files for more information."")","    def __str__(self):
        return ""The worker died unexpectedly while executing this task."""
"    def __str__(self):
        return (""The actor died unexpectedly before finishing this task. ""
                ""Check python-core-worker-*.log files for more information."")","    def __str__(self):
        return ""The actor died unexpectedly before finishing this task."""
"def _try_to_compute_deterministic_class_id(cls, depth=5):
    """"""Attempt to produce a deterministic class ID for a given class.

    The goal here is for the class ID to be the same when this is run on
    different worker processes. Pickling, loading, and pickling again seems to
    produce more consistent results than simply pickling. This is a bit crazy
    and could cause problems, in which case we should revert it and figure out
    something better.

    Args:
        cls: The class to produce an ID for.
        depth: The number of times to repeatedly try to load and dump the
            string while trying to reach a fixed point.

    Returns:
        A class ID for this class. We attempt to make the class ID the same
            when this function is run on different workers, but that is not
            guaranteed.

    Raises:
        Exception: This could raise an exception if cloudpickle raises an
            exception.
    """"""
    # Pickling, loading, and pickling again seems to produce more consistent
    # results than simply pickling. This is a bit
    class_id = pickle.dumps(cls)
    for _ in range(depth):
        new_class_id = pickle.dumps(pickle.loads(class_id))
        if new_class_id == class_id:
            # We appear to have reached a fix point, so use this as the ID.
            return hashlib.shake_128(new_class_id).digest(
                ray_constants.ID_SIZE)
        class_id = new_class_id

    # We have not reached a fixed point, so we may end up with a different
    # class ID for this custom class on each worker, which could lead to the
    # same class definition being exported many many times.
    logger.warning(
        f""WARNING: Could not produce a deterministic class ID for class {cls}"")
    return hashlib.shake_128(new_class_id).digest(ray_constants.ID_SIZE)","def _try_to_compute_deterministic_class_id(cls, depth=5):
    """"""Attempt to produce a deterministic class ID for a given class.

    The goal here is for the class ID to be the same when this is run on
    different worker processes. Pickling, loading, and pickling again seems to
    produce more consistent results than simply pickling. This is a bit crazy
    and could cause problems, in which case we should revert it and figure out
    something better.

    Args:
        cls: The class to produce an ID for.
        depth: The number of times to repeatedly try to load and dump the
            string while trying to reach a fixed point.

    Returns:
        A class ID for this class. We attempt to make the class ID the same
            when this function is run on different workers, but that is not
            guaranteed.

    Raises:
        Exception: This could raise an exception if cloudpickle raises an
            exception.
    """"""
    # Pickling, loading, and pickling again seems to produce more consistent
    # results than simply pickling. This is a bit
    class_id = pickle.dumps(cls)
    for _ in range(depth):
        new_class_id = pickle.dumps(pickle.loads(class_id))
        if new_class_id == class_id:
            # We appear to have reached a fix point, so use this as the ID.
            return hashlib.sha1(new_class_id).digest()
        class_id = new_class_id

    # We have not reached a fixed point, so we may end up with a different
    # class ID for this custom class on each worker, which could lead to the
    # same class definition being exported many many times.
    logger.warning(
        f""WARNING: Could not produce a deterministic class ID for class {cls}"")
    return hashlib.sha1(new_class_id).digest()"
"def _random_string():
    id_hash = hashlib.shake_128()
    id_hash.update(uuid.uuid4().bytes)
    id_bytes = id_hash.digest(ray_constants.ID_SIZE)
    assert len(id_bytes) == ray_constants.ID_SIZE
    return id_bytes","def _random_string():
    id_hash = hashlib.sha1()
    id_hash.update(uuid.uuid4().bytes)
    id_bytes = id_hash.digest()
    assert len(id_bytes) == ray_constants.ID_SIZE
    return id_bytes"
"    def run_function_on_all_workers(self, function,
                                    run_on_other_drivers=False):
        """"""Run arbitrary code on all of the workers.

        This function will first be run on the driver, and then it will be
        exported to all of the workers to be run. It will also be run on any
        new workers that register later. If ray.init has not been called yet,
        then cache the function and export it later.

        Args:
            function (Callable): The function to run on all of the workers. It
                takes only one argument, a worker info dict. If it returns
                anything, its return values will not be used.
            run_on_other_drivers: The boolean that indicates whether we want to
                run this function on other drivers. One case is we may need to
                share objects across drivers.
        """"""
        # If ray.init has not been called yet, then cache the function and
        # export it when connect is called. Otherwise, run the function on all
        # workers.
        if self.mode is None:
            self.cached_functions_to_run.append(function)
        else:
            # Attempt to pickle the function before we need it. This could
            # fail, and it is more convenient if the failure happens before we
            # actually run the function locally.
            pickled_function = pickle.dumps(function)

            function_to_run_id = hashlib.shake_128(pickled_function).digest(
                ray_constants.ID_SIZE)
            key = b""FunctionsToRun:"" + function_to_run_id
            # First run the function on the driver.
            # We always run the task locally.
            function({""worker"": self})
            # Check if the function has already been put into redis.
            function_exported = self.redis_client.setnx(b""Lock:"" + key, 1)
            if not function_exported:
                # In this case, the function has already been exported, so
                # we don't need to export it again.
                return

            check_oversized_pickle(pickled_function, function.__name__,
                                   ""function"", self)

            # Run the function on all workers.
            self.redis_client.hset(
                key,
                mapping={
                    ""job_id"": self.current_job_id.binary(),
                    ""function_id"": function_to_run_id,
                    ""function"": pickled_function,
                    ""run_on_other_drivers"": str(run_on_other_drivers),
                })
            self.redis_client.rpush(""Exports"", key)","    def run_function_on_all_workers(self, function,
                                    run_on_other_drivers=False):
        """"""Run arbitrary code on all of the workers.

        This function will first be run on the driver, and then it will be
        exported to all of the workers to be run. It will also be run on any
        new workers that register later. If ray.init has not been called yet,
        then cache the function and export it later.

        Args:
            function (Callable): The function to run on all of the workers. It
                takes only one argument, a worker info dict. If it returns
                anything, its return values will not be used.
            run_on_other_drivers: The boolean that indicates whether we want to
                run this function on other drivers. One case is we may need to
                share objects across drivers.
        """"""
        # If ray.init has not been called yet, then cache the function and
        # export it when connect is called. Otherwise, run the function on all
        # workers.
        if self.mode is None:
            self.cached_functions_to_run.append(function)
        else:
            # Attempt to pickle the function before we need it. This could
            # fail, and it is more convenient if the failure happens before we
            # actually run the function locally.
            pickled_function = pickle.dumps(function)

            function_to_run_id = hashlib.sha1(pickled_function).digest()
            key = b""FunctionsToRun:"" + function_to_run_id
            # First run the function on the driver.
            # We always run the task locally.
            function({""worker"": self})
            # Check if the function has already been put into redis.
            function_exported = self.redis_client.setnx(b""Lock:"" + key, 1)
            if not function_exported:
                # In this case, the function has already been exported, so
                # we don't need to export it again.
                return

            check_oversized_pickle(pickled_function, function.__name__,
                                   ""function"", self)

            # Run the function on all workers.
            self.redis_client.hset(
                key,
                mapping={
                    ""job_id"": self.current_job_id.binary(),
                    ""function_id"": function_to_run_id,
                    ""function"": pickled_function,
                    ""run_on_other_drivers"": str(run_on_other_drivers),
                })
            self.redis_client.rpush(""Exports"", key)"
"    def _home(self):
        if self._home_cached is not None:
            return self._home_cached
        for _ in range(MAX_HOME_RETRIES - 1):
            try:
                self._home_cached = self._try_to_get_home()
                return self._home_cached
            except Exception:
                # TODO (Dmitri): Identify the exception we're trying to avoid.
                logger.info(""Error reading container's home directory. ""
                            f""Retrying in {HOME_RETRY_DELAY_S} seconds."")
                time.sleep(HOME_RETRY_DELAY_S)
        # Last try
        self._home_cached = self._try_to_get_home()
        return self._home_cached","    def _home(self):
        # TODO (Dmitri): Think about how to use the node's HOME variable
        # without making an extra kubectl exec call.
        if self._home_cached is None:
            cmd = self.kubectl + [
                ""exec"", ""-it"", self.node_id, ""--"", ""printenv"", ""HOME""
            ]
            joined_cmd = "" "".join(cmd)
            raw_out = self.process_runner.check_output(joined_cmd, shell=True)
            self._home_cached = raw_out.decode().strip(""\\n\\r"")
        return self._home_cached"
"def init(
        address=None,
        *,
        num_cpus=None,
        num_gpus=None,
        resources=None,
        object_store_memory=None,
        local_mode=False,
        ignore_reinit_error=False,
        include_dashboard=None,
        dashboard_host=ray_constants.DEFAULT_DASHBOARD_IP,
        dashboard_port=ray_constants.DEFAULT_DASHBOARD_PORT,
        job_config=None,
        configure_logging=True,
        logging_level=logging.INFO,
        logging_format=ray_constants.LOGGER_FORMAT,
        log_to_driver=True,
        # The following are unstable parameters and their use is discouraged.
        _enable_object_reconstruction=False,
        _redis_max_memory=None,
        _plasma_directory=None,
        _node_ip_address=ray_constants.NODE_DEFAULT_IP,
        _driver_object_store_memory=None,
        _memory=None,
        _redis_password=ray_constants.REDIS_DEFAULT_PASSWORD,
        _java_worker_options=None,
        _temp_dir=None,
        _lru_evict=False,
        _metrics_export_port=None,
        _system_config=None):
    """"""
    Connect to an existing Ray cluster or start one and connect to it.

    This method handles two cases; either a Ray cluster already exists and we
    just attach this driver to it or we start all of the processes associated
    with a Ray cluster and attach to the newly started cluster.

    To start Ray and all of the relevant processes, use this as follows:

    .. code-block:: python

        ray.init()

    To connect to an existing Ray cluster, use this as follows (substituting
    in the appropriate address):

    .. code-block:: python

        ray.init(address=""123.45.67.89:6379"")

    You can also define an environment variable called `RAY_ADDRESS` in
    the same format as the `address` parameter to connect to an existing
    cluster with ray.init().

    Args:
        address (str): The address of the Ray cluster to connect to. If
            this address is not provided, then this command will start Redis,
            a raylet, a plasma store, a plasma manager, and some workers.
            It will also kill these processes when Python exits. If the driver
            is running on a node in a Ray cluster, using `auto` as the value
            tells the driver to detect the the cluster, removing the need to
            specify a specific node address.
        num_cpus (int): Number of CPUs the user wishes to assign to each
            raylet. By default, this is set based on virtual cores.
        num_gpus (int): Number of GPUs the user wishes to assign to each
            raylet. By default, this is set based on detected GPUs.
        resources: A dictionary mapping the names of custom resources to the
            quantities for them available.
        object_store_memory: The amount of memory (in bytes) to start the
            object store with. By default, this is automatically set based on
            available system memory.
        local_mode (bool): If true, the code will be executed serially. This
            is useful for debugging.
        ignore_reinit_error: If true, Ray suppresses errors from calling
            ray.init() a second time. Ray won't be restarted.
        include_dashboard: Boolean flag indicating whether or not to start the
            Ray dashboard, which displays the status of the Ray
            cluster. If this argument is None, then the UI will be started if
            the relevant dependencies are present.
        dashboard_host: The host to bind the dashboard server to. Can either be
            localhost (127.0.0.1) or 0.0.0.0 (available from all interfaces).
            By default, this is set to localhost to prevent access from
            external machines.
        dashboard_port: The port to bind the dashboard server to. Defaults to
            8265.
        job_config (ray.job_config.JobConfig): The job configuration.
        configure_logging: True (default) if configuration of logging is
            allowed here. Otherwise, the user may want to configure it
            separately.
        logging_level: Logging level, defaults to logging.INFO. Ignored unless
            ""configure_logging"" is true.
        logging_format: Logging format, defaults to string containing a
            timestamp, filename, line number, and message. See the source file
            ray_constants.py for details. Ignored unless ""configure_logging""
            is true.
        log_to_driver (bool): If true, the output from all of the worker
            processes on all nodes will be directed to the driver.
        _enable_object_reconstruction (bool): If True, when an object stored in
            the distributed plasma store is lost due to node failure, Ray will
            attempt to reconstruct the object by re-executing the task that
            created the object. Arguments to the task will be recursively
            reconstructed. If False, then ray.ObjectLostError will be
            thrown.
        _redis_max_memory: Redis max memory.
        _plasma_directory: Override the plasma mmap file directory.
        _node_ip_address (str): The IP address of the node that we are on.
        _driver_object_store_memory (int): Limit the amount of memory the
            driver can use in the object store for creating objects.
        _memory: Amount of reservable memory resource to create.
        _redis_password (str): Prevents external clients without the password
            from connecting to Redis if provided.
        _temp_dir (str): If provided, specifies the root temporary
            directory for the Ray process. Defaults to an OS-specific
            conventional location, e.g., ""/tmp/ray"".
        _java_worker_options: Overwrite the options to start Java workers.
        _lru_evict (bool): If True, when an object store is full, it will evict
            objects in LRU order to make more space and when under memory
            pressure, ray.ObjectLostError may be thrown. If False, then
            reference counting will be used to decide which objects are safe
            to evict and when under memory pressure, ray.ObjectStoreFullError
            may be thrown.
        _metrics_export_port(int): Port number Ray exposes system metrics
            through a Prometheus endpoint. It is currently under active
            development, and the API is subject to change.
        _system_config (dict): Configuration for overriding
            RayConfig defaults. For testing purposes ONLY.

    Returns:
        Address information about the started processes.

    Raises:
        Exception: An exception is raised if an inappropriate combination of
            arguments is passed in.
    """"""

    # Try to increase the file descriptor limit, which is too low by
    # default for Ray: https://github.com/ray-project/ray/issues/11239
    try:
        import resource
        soft, hard = resource.getrlimit(resource.RLIMIT_NOFILE)
        if soft < hard:
            # https://github.com/ray-project/ray/issues/12059
            soft = max(soft, min(hard, 65536))
            logger.debug(""Automatically increasing RLIMIT_NOFILE to max ""
                         ""value of {}"".format(hard))
            try:
                resource.setrlimit(resource.RLIMIT_NOFILE, (hard, hard))
            except ValueError:
                logger.debug(""Failed to raise limit."")
        soft, _ = resource.getrlimit(resource.RLIMIT_NOFILE)
        if soft < 4096:
            logger.warning(
                ""File descriptor limit {} is too low for production ""
                ""servers and may result in connection errors. ""
                ""At least 8192 is recommended. --- ""
                ""Fix with 'ulimit -n 8192'"".format(soft))
    except ImportError:
        logger.debug(""Could not import resource module (on Windows)"")
        pass

    if ""RAY_ADDRESS"" in os.environ:
        if address is None or address == ""auto"":
            address = os.environ[""RAY_ADDRESS""]
        else:
            raise RuntimeError(
                ""Cannot use both the RAY_ADDRESS environment variable and ""
                ""the address argument of ray.init simultaneously. If you ""
                ""use RAY_ADDRESS to connect to a specific Ray cluster, ""
                ""please call ray.init() or ray.init(address=\\""auto\\"") on the ""
                ""driver."")

    # Convert hostnames to numerical IP address.
    if _node_ip_address is not None:
        node_ip_address = services.address_to_ip(_node_ip_address)
    raylet_ip_address = node_ip_address

    if address:
        redis_address, _, _ = services.validate_redis_address(address)
    else:
        redis_address = None

    if configure_logging:
        setup_logger(logging_level, logging_format)

    if redis_address is not None:
        logger.info(
            f""Connecting to existing Ray cluster at address: {redis_address}"")

    if local_mode:
        driver_mode = LOCAL_MODE
    else:
        driver_mode = SCRIPT_MODE

    if global_worker.connected:
        if ignore_reinit_error:
            logger.info(
                ""Calling ray.init() again after it has already been called."")
            return
        else:
            raise RuntimeError(""Maybe you called ray.init twice by accident? ""
                               ""This error can be suppressed by passing in ""
                               ""'ignore_reinit_error=True' or by calling ""
                               ""'ray.shutdown()' prior to 'ray.init()'."")

    _system_config = _system_config or {}
    if not isinstance(_system_config, dict):
        raise TypeError(""The _system_config must be a dict."")

    global _global_node
    if redis_address is None:
        # In this case, we need to start a new cluster.
        ray_params = ray.parameter.RayParams(
            redis_address=redis_address,
            node_ip_address=node_ip_address,
            raylet_ip_address=raylet_ip_address,
            object_ref_seed=None,
            driver_mode=driver_mode,
            redirect_worker_output=None,
            redirect_output=None,
            num_cpus=num_cpus,
            num_gpus=num_gpus,
            resources=resources,
            num_redis_shards=None,
            redis_max_clients=None,
            redis_password=_redis_password,
            plasma_directory=_plasma_directory,
            huge_pages=None,
            include_dashboard=include_dashboard,
            dashboard_host=dashboard_host,
            dashboard_port=dashboard_port,
            memory=_memory,
            object_store_memory=object_store_memory,
            redis_max_memory=_redis_max_memory,
            plasma_store_socket_name=None,
            temp_dir=_temp_dir,
            java_worker_options=_java_worker_options,
            start_initial_python_workers_for_first_job=True,
            _system_config=_system_config,
            lru_evict=_lru_evict,
            enable_object_reconstruction=_enable_object_reconstruction,
            metrics_export_port=_metrics_export_port)
        # Start the Ray processes. We set shutdown_at_exit=False because we
        # shutdown the node in the ray.shutdown call that happens in the atexit
        # handler. We still spawn a reaper process in case the atexit handler
        # isn't called.
        _global_node = ray.node.Node(
            head=True,
            shutdown_at_exit=False,
            spawn_reaper=True,
            ray_params=ray_params)
    else:
        # In this case, we are connecting to an existing cluster.
        if num_cpus is not None or num_gpus is not None:
            raise ValueError(
                ""When connecting to an existing cluster, num_cpus ""
                ""and num_gpus must not be provided."")
        if resources is not None:
            raise ValueError(""When connecting to an existing cluster, ""
                             ""resources must not be provided."")
        if object_store_memory is not None:
            raise ValueError(""When connecting to an existing cluster, ""
                             ""object_store_memory must not be provided."")
        if _system_config is not None and len(_system_config) != 0:
            raise ValueError(""When connecting to an existing cluster, ""
                             ""_system_config must not be provided."")
        if _lru_evict:
            raise ValueError(""When connecting to an existing cluster, ""
                             ""_lru_evict must not be provided."")
        if _enable_object_reconstruction:
            raise ValueError(
                ""When connecting to an existing cluster, ""
                ""_enable_object_reconstruction must not be provided."")

        # In this case, we only need to connect the node.
        ray_params = ray.parameter.RayParams(
            node_ip_address=node_ip_address,
            raylet_ip_address=raylet_ip_address,
            redis_address=redis_address,
            redis_password=_redis_password,
            object_ref_seed=None,
            temp_dir=_temp_dir,
            _system_config=_system_config,
            lru_evict=_lru_evict,
            enable_object_reconstruction=_enable_object_reconstruction,
            metrics_export_port=_metrics_export_port)
        _global_node = ray.node.Node(
            ray_params,
            head=False,
            shutdown_at_exit=False,
            spawn_reaper=False,
            connect_only=True)

    connect(
        _global_node,
        mode=driver_mode,
        log_to_driver=log_to_driver,
        worker=global_worker,
        driver_object_store_memory=_driver_object_store_memory,
        job_id=None,
        job_config=job_config)

    for hook in _post_init_hooks:
        hook()

    node_id = global_worker.core_worker.get_current_node_id()
    return dict(_global_node.address_info, node_id=node_id.hex())","def init(
        address=None,
        *,
        num_cpus=None,
        num_gpus=None,
        resources=None,
        object_store_memory=None,
        local_mode=False,
        ignore_reinit_error=False,
        include_dashboard=None,
        dashboard_host=ray_constants.DEFAULT_DASHBOARD_IP,
        dashboard_port=ray_constants.DEFAULT_DASHBOARD_PORT,
        job_config=None,
        configure_logging=True,
        logging_level=logging.INFO,
        logging_format=ray_constants.LOGGER_FORMAT,
        log_to_driver=True,
        # The following are unstable parameters and their use is discouraged.
        _enable_object_reconstruction=False,
        _redis_max_memory=None,
        _plasma_directory=None,
        _node_ip_address=ray_constants.NODE_DEFAULT_IP,
        _driver_object_store_memory=None,
        _memory=None,
        _redis_password=ray_constants.REDIS_DEFAULT_PASSWORD,
        _java_worker_options=None,
        _temp_dir=None,
        _lru_evict=False,
        _metrics_export_port=None,
        _system_config=None):
    """"""
    Connect to an existing Ray cluster or start one and connect to it.

    This method handles two cases; either a Ray cluster already exists and we
    just attach this driver to it or we start all of the processes associated
    with a Ray cluster and attach to the newly started cluster.

    To start Ray and all of the relevant processes, use this as follows:

    .. code-block:: python

        ray.init()

    To connect to an existing Ray cluster, use this as follows (substituting
    in the appropriate address):

    .. code-block:: python

        ray.init(address=""123.45.67.89:6379"")

    You can also define an environment variable called `RAY_ADDRESS` in
    the same format as the `address` parameter to connect to an existing
    cluster with ray.init().

    Args:
        address (str): The address of the Ray cluster to connect to. If
            this address is not provided, then this command will start Redis,
            a raylet, a plasma store, a plasma manager, and some workers.
            It will also kill these processes when Python exits. If the driver
            is running on a node in a Ray cluster, using `auto` as the value
            tells the driver to detect the the cluster, removing the need to
            specify a specific node address.
        num_cpus (int): Number of CPUs the user wishes to assign to each
            raylet. By default, this is set based on virtual cores.
        num_gpus (int): Number of GPUs the user wishes to assign to each
            raylet. By default, this is set based on detected GPUs.
        resources: A dictionary mapping the names of custom resources to the
            quantities for them available.
        object_store_memory: The amount of memory (in bytes) to start the
            object store with. By default, this is automatically set based on
            available system memory.
        local_mode (bool): If true, the code will be executed serially. This
            is useful for debugging.
        ignore_reinit_error: If true, Ray suppresses errors from calling
            ray.init() a second time. Ray won't be restarted.
        include_dashboard: Boolean flag indicating whether or not to start the
            Ray dashboard, which displays the status of the Ray
            cluster. If this argument is None, then the UI will be started if
            the relevant dependencies are present.
        dashboard_host: The host to bind the dashboard server to. Can either be
            localhost (127.0.0.1) or 0.0.0.0 (available from all interfaces).
            By default, this is set to localhost to prevent access from
            external machines.
        dashboard_port: The port to bind the dashboard server to. Defaults to
            8265.
        job_config (ray.job_config.JobConfig): The job configuration.
        configure_logging: True (default) if configuration of logging is
            allowed here. Otherwise, the user may want to configure it
            separately.
        logging_level: Logging level, defaults to logging.INFO. Ignored unless
            ""configure_logging"" is true.
        logging_format: Logging format, defaults to string containing a
            timestamp, filename, line number, and message. See the source file
            ray_constants.py for details. Ignored unless ""configure_logging""
            is true.
        log_to_driver (bool): If true, the output from all of the worker
            processes on all nodes will be directed to the driver.
        _enable_object_reconstruction (bool): If True, when an object stored in
            the distributed plasma store is lost due to node failure, Ray will
            attempt to reconstruct the object by re-executing the task that
            created the object. Arguments to the task will be recursively
            reconstructed. If False, then ray.ObjectLostError will be
            thrown.
        _redis_max_memory: Redis max memory.
        _plasma_directory: Override the plasma mmap file directory.
        _node_ip_address (str): The IP address of the node that we are on.
        _driver_object_store_memory (int): Limit the amount of memory the
            driver can use in the object store for creating objects.
        _memory: Amount of reservable memory resource to create.
        _redis_password (str): Prevents external clients without the password
            from connecting to Redis if provided.
        _temp_dir (str): If provided, specifies the root temporary
            directory for the Ray process. Defaults to an OS-specific
            conventional location, e.g., ""/tmp/ray"".
        _java_worker_options: Overwrite the options to start Java workers.
        _lru_evict (bool): If True, when an object store is full, it will evict
            objects in LRU order to make more space and when under memory
            pressure, ray.ObjectLostError may be thrown. If False, then
            reference counting will be used to decide which objects are safe
            to evict and when under memory pressure, ray.ObjectStoreFullError
            may be thrown.
        _metrics_export_port(int): Port number Ray exposes system metrics
            through a Prometheus endpoint. It is currently under active
            development, and the API is subject to change.
        _system_config (dict): Configuration for overriding
            RayConfig defaults. For testing purposes ONLY.

    Returns:
        Address information about the started processes.

    Raises:
        Exception: An exception is raised if an inappropriate combination of
            arguments is passed in.
    """"""

    # Try to increase the file descriptor limit, which is too low by
    # default for Ray: https://github.com/ray-project/ray/issues/11239
    try:
        import resource
        soft, hard = resource.getrlimit(resource.RLIMIT_NOFILE)
        if soft < hard:
            logger.debug(""Automatically increasing RLIMIT_NOFILE to max ""
                         ""value of {}"".format(hard))
            try:
                resource.setrlimit(resource.RLIMIT_NOFILE, (hard, hard))
            except ValueError:
                logger.debug(""Failed to raise limit."")
        soft, _ = resource.getrlimit(resource.RLIMIT_NOFILE)
        if soft < 4096:
            logger.warning(
                ""File descriptor limit {} is too low for production ""
                ""servers and may result in connection errors. ""
                ""At least 8192 is recommended. --- ""
                ""Fix with 'ulimit -n 8192'"".format(soft))
    except ImportError:
        logger.debug(""Could not import resource module (on Windows)"")
        pass

    if ""RAY_ADDRESS"" in os.environ:
        if address is None or address == ""auto"":
            address = os.environ[""RAY_ADDRESS""]
        else:
            raise RuntimeError(
                ""Cannot use both the RAY_ADDRESS environment variable and ""
                ""the address argument of ray.init simultaneously. If you ""
                ""use RAY_ADDRESS to connect to a specific Ray cluster, ""
                ""please call ray.init() or ray.init(address=\\""auto\\"") on the ""
                ""driver."")

    # Convert hostnames to numerical IP address.
    if _node_ip_address is not None:
        node_ip_address = services.address_to_ip(_node_ip_address)
    raylet_ip_address = node_ip_address

    if address:
        redis_address, _, _ = services.validate_redis_address(address)
    else:
        redis_address = None

    if configure_logging:
        setup_logger(logging_level, logging_format)

    if redis_address is not None:
        logger.info(
            f""Connecting to existing Ray cluster at address: {redis_address}"")

    if local_mode:
        driver_mode = LOCAL_MODE
    else:
        driver_mode = SCRIPT_MODE

    if global_worker.connected:
        if ignore_reinit_error:
            logger.info(
                ""Calling ray.init() again after it has already been called."")
            return
        else:
            raise RuntimeError(""Maybe you called ray.init twice by accident? ""
                               ""This error can be suppressed by passing in ""
                               ""'ignore_reinit_error=True' or by calling ""
                               ""'ray.shutdown()' prior to 'ray.init()'."")

    _system_config = _system_config or {}
    if not isinstance(_system_config, dict):
        raise TypeError(""The _system_config must be a dict."")

    global _global_node
    if redis_address is None:
        # In this case, we need to start a new cluster.
        ray_params = ray.parameter.RayParams(
            redis_address=redis_address,
            node_ip_address=node_ip_address,
            raylet_ip_address=raylet_ip_address,
            object_ref_seed=None,
            driver_mode=driver_mode,
            redirect_worker_output=None,
            redirect_output=None,
            num_cpus=num_cpus,
            num_gpus=num_gpus,
            resources=resources,
            num_redis_shards=None,
            redis_max_clients=None,
            redis_password=_redis_password,
            plasma_directory=_plasma_directory,
            huge_pages=None,
            include_dashboard=include_dashboard,
            dashboard_host=dashboard_host,
            dashboard_port=dashboard_port,
            memory=_memory,
            object_store_memory=object_store_memory,
            redis_max_memory=_redis_max_memory,
            plasma_store_socket_name=None,
            temp_dir=_temp_dir,
            java_worker_options=_java_worker_options,
            start_initial_python_workers_for_first_job=True,
            _system_config=_system_config,
            lru_evict=_lru_evict,
            enable_object_reconstruction=_enable_object_reconstruction,
            metrics_export_port=_metrics_export_port)
        # Start the Ray processes. We set shutdown_at_exit=False because we
        # shutdown the node in the ray.shutdown call that happens in the atexit
        # handler. We still spawn a reaper process in case the atexit handler
        # isn't called.
        _global_node = ray.node.Node(
            head=True,
            shutdown_at_exit=False,
            spawn_reaper=True,
            ray_params=ray_params)
    else:
        # In this case, we are connecting to an existing cluster.
        if num_cpus is not None or num_gpus is not None:
            raise ValueError(
                ""When connecting to an existing cluster, num_cpus ""
                ""and num_gpus must not be provided."")
        if resources is not None:
            raise ValueError(""When connecting to an existing cluster, ""
                             ""resources must not be provided."")
        if object_store_memory is not None:
            raise ValueError(""When connecting to an existing cluster, ""
                             ""object_store_memory must not be provided."")
        if _system_config is not None and len(_system_config) != 0:
            raise ValueError(""When connecting to an existing cluster, ""
                             ""_system_config must not be provided."")
        if _lru_evict:
            raise ValueError(""When connecting to an existing cluster, ""
                             ""_lru_evict must not be provided."")
        if _enable_object_reconstruction:
            raise ValueError(
                ""When connecting to an existing cluster, ""
                ""_enable_object_reconstruction must not be provided."")

        # In this case, we only need to connect the node.
        ray_params = ray.parameter.RayParams(
            node_ip_address=node_ip_address,
            raylet_ip_address=raylet_ip_address,
            redis_address=redis_address,
            redis_password=_redis_password,
            object_ref_seed=None,
            temp_dir=_temp_dir,
            _system_config=_system_config,
            lru_evict=_lru_evict,
            enable_object_reconstruction=_enable_object_reconstruction,
            metrics_export_port=_metrics_export_port)
        _global_node = ray.node.Node(
            ray_params,
            head=False,
            shutdown_at_exit=False,
            spawn_reaper=False,
            connect_only=True)

    connect(
        _global_node,
        mode=driver_mode,
        log_to_driver=log_to_driver,
        worker=global_worker,
        driver_object_store_memory=_driver_object_store_memory,
        job_id=None,
        job_config=job_config)

    for hook in _post_init_hooks:
        hook()

    node_id = global_worker.core_worker.get_current_node_id()
    return dict(_global_node.address_info, node_id=node_id.hex())"
"def init(
        address=None,
        *,
        num_cpus=None,
        num_gpus=None,
        resources=None,
        object_store_memory=None,
        local_mode=False,
        ignore_reinit_error=False,
        include_dashboard=None,
        dashboard_host=ray_constants.DEFAULT_DASHBOARD_IP,
        dashboard_port=ray_constants.DEFAULT_DASHBOARD_PORT,
        job_config=None,
        configure_logging=True,
        logging_level=logging.INFO,
        logging_format=ray_constants.LOGGER_FORMAT,
        log_to_driver=True,
        # The following are unstable parameters and their use is discouraged.
        _enable_object_reconstruction=False,
        _redis_max_memory=None,
        _plasma_directory=None,
        _node_ip_address=ray_constants.NODE_DEFAULT_IP,
        _driver_object_store_memory=None,
        _memory=None,
        _redis_password=ray_constants.REDIS_DEFAULT_PASSWORD,
        _java_worker_options=None,
        _temp_dir=None,
        _lru_evict=False,
        _metrics_export_port=None,
        _system_config=None):
    """"""
    Connect to an existing Ray cluster or start one and connect to it.

    This method handles two cases; either a Ray cluster already exists and we
    just attach this driver to it or we start all of the processes associated
    with a Ray cluster and attach to the newly started cluster.

    To start Ray and all of the relevant processes, use this as follows:

    .. code-block:: python

        ray.init()

    To connect to an existing Ray cluster, use this as follows (substituting
    in the appropriate address):

    .. code-block:: python

        ray.init(address=""123.45.67.89:6379"")

    You can also define an environment variable called `RAY_ADDRESS` in
    the same format as the `address` parameter to connect to an existing
    cluster with ray.init().

    Args:
        address (str): The address of the Ray cluster to connect to. If
            this address is not provided, then this command will start Redis,
            a raylet, a plasma store, a plasma manager, and some workers.
            It will also kill these processes when Python exits. If the driver
            is running on a node in a Ray cluster, using `auto` as the value
            tells the driver to detect the the cluster, removing the need to
            specify a specific node address.
        num_cpus (int): Number of CPUs the user wishes to assign to each
            raylet. By default, this is set based on virtual cores.
        num_gpus (int): Number of GPUs the user wishes to assign to each
            raylet. By default, this is set based on detected GPUs.
        resources: A dictionary mapping the names of custom resources to the
            quantities for them available.
        object_store_memory: The amount of memory (in bytes) to start the
            object store with. By default, this is automatically set based on
            available system memory.
        local_mode (bool): If true, the code will be executed serially. This
            is useful for debugging.
        ignore_reinit_error: If true, Ray suppresses errors from calling
            ray.init() a second time. Ray won't be restarted.
        include_dashboard: Boolean flag indicating whether or not to start the
            Ray dashboard, which displays the status of the Ray
            cluster. If this argument is None, then the UI will be started if
            the relevant dependencies are present.
        dashboard_host: The host to bind the dashboard server to. Can either be
            localhost (127.0.0.1) or 0.0.0.0 (available from all interfaces).
            By default, this is set to localhost to prevent access from
            external machines.
        dashboard_port: The port to bind the dashboard server to. Defaults to
            8265.
        job_config (ray.job_config.JobConfig): The job configuration.
        configure_logging: True (default) if configuration of logging is
            allowed here. Otherwise, the user may want to configure it
            separately.
        logging_level: Logging level, defaults to logging.INFO. Ignored unless
            ""configure_logging"" is true.
        logging_format: Logging format, defaults to string containing a
            timestamp, filename, line number, and message. See the source file
            ray_constants.py for details. Ignored unless ""configure_logging""
            is true.
        log_to_driver (bool): If true, the output from all of the worker
            processes on all nodes will be directed to the driver.
        _enable_object_reconstruction (bool): If True, when an object stored in
            the distributed plasma store is lost due to node failure, Ray will
            attempt to reconstruct the object by re-executing the task that
            created the object. Arguments to the task will be recursively
            reconstructed. If False, then ray.ObjectLostError will be
            thrown.
        _redis_max_memory: Redis max memory.
        _plasma_directory: Override the plasma mmap file directory.
        _node_ip_address (str): The IP address of the node that we are on.
        _driver_object_store_memory (int): Limit the amount of memory the
            driver can use in the object store for creating objects.
        _memory: Amount of reservable memory resource to create.
        _redis_password (str): Prevents external clients without the password
            from connecting to Redis if provided.
        _temp_dir (str): If provided, specifies the root temporary
            directory for the Ray process. Defaults to an OS-specific
            conventional location, e.g., ""/tmp/ray"".
        _java_worker_options: Overwrite the options to start Java workers.
        _lru_evict (bool): If True, when an object store is full, it will evict
            objects in LRU order to make more space and when under memory
            pressure, ray.ObjectLostError may be thrown. If False, then
            reference counting will be used to decide which objects are safe
            to evict and when under memory pressure, ray.ObjectStoreFullError
            may be thrown.
        _metrics_export_port(int): Port number Ray exposes system metrics
            through a Prometheus endpoint. It is currently under active
            development, and the API is subject to change.
        _system_config (dict): Configuration for overriding
            RayConfig defaults. For testing purposes ONLY.

    Returns:
        Address information about the started processes.

    Raises:
        Exception: An exception is raised if an inappropriate combination of
            arguments is passed in.
    """"""

    # Try to increase the file descriptor limit, which is too low by
    # default for Ray: https://github.com/ray-project/ray/issues/11239
    try:
        import resource
        soft, hard = resource.getrlimit(resource.RLIMIT_NOFILE)
        if soft < hard:
            # https://github.com/ray-project/ray/issues/12059
            soft = max(soft, min(hard, 65536))
            logger.debug(""Automatically increasing RLIMIT_NOFILE to max ""
                         ""value of {}"".format(hard))
            try:
                resource.setrlimit(resource.RLIMIT_NOFILE, (soft, hard))
            except ValueError:
                logger.debug(""Failed to raise limit."")
        soft, _ = resource.getrlimit(resource.RLIMIT_NOFILE)
        if soft < 4096:
            logger.warning(
                ""File descriptor limit {} is too low for production ""
                ""servers and may result in connection errors. ""
                ""At least 8192 is recommended. --- ""
                ""Fix with 'ulimit -n 8192'"".format(soft))
    except ImportError:
        logger.debug(""Could not import resource module (on Windows)"")
        pass

    if ""RAY_ADDRESS"" in os.environ:
        if address is None or address == ""auto"":
            address = os.environ[""RAY_ADDRESS""]
        else:
            raise RuntimeError(
                ""Cannot use both the RAY_ADDRESS environment variable and ""
                ""the address argument of ray.init simultaneously. If you ""
                ""use RAY_ADDRESS to connect to a specific Ray cluster, ""
                ""please call ray.init() or ray.init(address=\\""auto\\"") on the ""
                ""driver."")

    # Convert hostnames to numerical IP address.
    if _node_ip_address is not None:
        node_ip_address = services.address_to_ip(_node_ip_address)
    raylet_ip_address = node_ip_address

    if address:
        redis_address, _, _ = services.validate_redis_address(address)
    else:
        redis_address = None

    if configure_logging:
        setup_logger(logging_level, logging_format)

    if redis_address is not None:
        logger.info(
            f""Connecting to existing Ray cluster at address: {redis_address}"")

    if local_mode:
        driver_mode = LOCAL_MODE
    else:
        driver_mode = SCRIPT_MODE

    if global_worker.connected:
        if ignore_reinit_error:
            logger.info(
                ""Calling ray.init() again after it has already been called."")
            return
        else:
            raise RuntimeError(""Maybe you called ray.init twice by accident? ""
                               ""This error can be suppressed by passing in ""
                               ""'ignore_reinit_error=True' or by calling ""
                               ""'ray.shutdown()' prior to 'ray.init()'."")

    _system_config = _system_config or {}
    if not isinstance(_system_config, dict):
        raise TypeError(""The _system_config must be a dict."")

    global _global_node
    if redis_address is None:
        # In this case, we need to start a new cluster.
        ray_params = ray.parameter.RayParams(
            redis_address=redis_address,
            node_ip_address=node_ip_address,
            raylet_ip_address=raylet_ip_address,
            object_ref_seed=None,
            driver_mode=driver_mode,
            redirect_worker_output=None,
            redirect_output=None,
            num_cpus=num_cpus,
            num_gpus=num_gpus,
            resources=resources,
            num_redis_shards=None,
            redis_max_clients=None,
            redis_password=_redis_password,
            plasma_directory=_plasma_directory,
            huge_pages=None,
            include_dashboard=include_dashboard,
            dashboard_host=dashboard_host,
            dashboard_port=dashboard_port,
            memory=_memory,
            object_store_memory=object_store_memory,
            redis_max_memory=_redis_max_memory,
            plasma_store_socket_name=None,
            temp_dir=_temp_dir,
            java_worker_options=_java_worker_options,
            start_initial_python_workers_for_first_job=True,
            _system_config=_system_config,
            lru_evict=_lru_evict,
            enable_object_reconstruction=_enable_object_reconstruction,
            metrics_export_port=_metrics_export_port)
        # Start the Ray processes. We set shutdown_at_exit=False because we
        # shutdown the node in the ray.shutdown call that happens in the atexit
        # handler. We still spawn a reaper process in case the atexit handler
        # isn't called.
        _global_node = ray.node.Node(
            head=True,
            shutdown_at_exit=False,
            spawn_reaper=True,
            ray_params=ray_params)
    else:
        # In this case, we are connecting to an existing cluster.
        if num_cpus is not None or num_gpus is not None:
            raise ValueError(
                ""When connecting to an existing cluster, num_cpus ""
                ""and num_gpus must not be provided."")
        if resources is not None:
            raise ValueError(""When connecting to an existing cluster, ""
                             ""resources must not be provided."")
        if object_store_memory is not None:
            raise ValueError(""When connecting to an existing cluster, ""
                             ""object_store_memory must not be provided."")
        if _system_config is not None and len(_system_config) != 0:
            raise ValueError(""When connecting to an existing cluster, ""
                             ""_system_config must not be provided."")
        if _lru_evict:
            raise ValueError(""When connecting to an existing cluster, ""
                             ""_lru_evict must not be provided."")
        if _enable_object_reconstruction:
            raise ValueError(
                ""When connecting to an existing cluster, ""
                ""_enable_object_reconstruction must not be provided."")

        # In this case, we only need to connect the node.
        ray_params = ray.parameter.RayParams(
            node_ip_address=node_ip_address,
            raylet_ip_address=raylet_ip_address,
            redis_address=redis_address,
            redis_password=_redis_password,
            object_ref_seed=None,
            temp_dir=_temp_dir,
            _system_config=_system_config,
            lru_evict=_lru_evict,
            enable_object_reconstruction=_enable_object_reconstruction,
            metrics_export_port=_metrics_export_port)
        _global_node = ray.node.Node(
            ray_params,
            head=False,
            shutdown_at_exit=False,
            spawn_reaper=False,
            connect_only=True)

    connect(
        _global_node,
        mode=driver_mode,
        log_to_driver=log_to_driver,
        worker=global_worker,
        driver_object_store_memory=_driver_object_store_memory,
        job_id=None,
        job_config=job_config)

    for hook in _post_init_hooks:
        hook()

    node_id = global_worker.core_worker.get_current_node_id()
    return dict(_global_node.address_info, node_id=node_id.hex())","def init(
        address=None,
        *,
        num_cpus=None,
        num_gpus=None,
        resources=None,
        object_store_memory=None,
        local_mode=False,
        ignore_reinit_error=False,
        include_dashboard=None,
        dashboard_host=ray_constants.DEFAULT_DASHBOARD_IP,
        dashboard_port=ray_constants.DEFAULT_DASHBOARD_PORT,
        job_config=None,
        configure_logging=True,
        logging_level=logging.INFO,
        logging_format=ray_constants.LOGGER_FORMAT,
        log_to_driver=True,
        # The following are unstable parameters and their use is discouraged.
        _enable_object_reconstruction=False,
        _redis_max_memory=None,
        _plasma_directory=None,
        _node_ip_address=ray_constants.NODE_DEFAULT_IP,
        _driver_object_store_memory=None,
        _memory=None,
        _redis_password=ray_constants.REDIS_DEFAULT_PASSWORD,
        _java_worker_options=None,
        _temp_dir=None,
        _lru_evict=False,
        _metrics_export_port=None,
        _system_config=None):
    """"""
    Connect to an existing Ray cluster or start one and connect to it.

    This method handles two cases; either a Ray cluster already exists and we
    just attach this driver to it or we start all of the processes associated
    with a Ray cluster and attach to the newly started cluster.

    To start Ray and all of the relevant processes, use this as follows:

    .. code-block:: python

        ray.init()

    To connect to an existing Ray cluster, use this as follows (substituting
    in the appropriate address):

    .. code-block:: python

        ray.init(address=""123.45.67.89:6379"")

    You can also define an environment variable called `RAY_ADDRESS` in
    the same format as the `address` parameter to connect to an existing
    cluster with ray.init().

    Args:
        address (str): The address of the Ray cluster to connect to. If
            this address is not provided, then this command will start Redis,
            a raylet, a plasma store, a plasma manager, and some workers.
            It will also kill these processes when Python exits. If the driver
            is running on a node in a Ray cluster, using `auto` as the value
            tells the driver to detect the the cluster, removing the need to
            specify a specific node address.
        num_cpus (int): Number of CPUs the user wishes to assign to each
            raylet. By default, this is set based on virtual cores.
        num_gpus (int): Number of GPUs the user wishes to assign to each
            raylet. By default, this is set based on detected GPUs.
        resources: A dictionary mapping the names of custom resources to the
            quantities for them available.
        object_store_memory: The amount of memory (in bytes) to start the
            object store with. By default, this is automatically set based on
            available system memory.
        local_mode (bool): If true, the code will be executed serially. This
            is useful for debugging.
        ignore_reinit_error: If true, Ray suppresses errors from calling
            ray.init() a second time. Ray won't be restarted.
        include_dashboard: Boolean flag indicating whether or not to start the
            Ray dashboard, which displays the status of the Ray
            cluster. If this argument is None, then the UI will be started if
            the relevant dependencies are present.
        dashboard_host: The host to bind the dashboard server to. Can either be
            localhost (127.0.0.1) or 0.0.0.0 (available from all interfaces).
            By default, this is set to localhost to prevent access from
            external machines.
        dashboard_port: The port to bind the dashboard server to. Defaults to
            8265.
        job_config (ray.job_config.JobConfig): The job configuration.
        configure_logging: True (default) if configuration of logging is
            allowed here. Otherwise, the user may want to configure it
            separately.
        logging_level: Logging level, defaults to logging.INFO. Ignored unless
            ""configure_logging"" is true.
        logging_format: Logging format, defaults to string containing a
            timestamp, filename, line number, and message. See the source file
            ray_constants.py for details. Ignored unless ""configure_logging""
            is true.
        log_to_driver (bool): If true, the output from all of the worker
            processes on all nodes will be directed to the driver.
        _enable_object_reconstruction (bool): If True, when an object stored in
            the distributed plasma store is lost due to node failure, Ray will
            attempt to reconstruct the object by re-executing the task that
            created the object. Arguments to the task will be recursively
            reconstructed. If False, then ray.ObjectLostError will be
            thrown.
        _redis_max_memory: Redis max memory.
        _plasma_directory: Override the plasma mmap file directory.
        _node_ip_address (str): The IP address of the node that we are on.
        _driver_object_store_memory (int): Limit the amount of memory the
            driver can use in the object store for creating objects.
        _memory: Amount of reservable memory resource to create.
        _redis_password (str): Prevents external clients without the password
            from connecting to Redis if provided.
        _temp_dir (str): If provided, specifies the root temporary
            directory for the Ray process. Defaults to an OS-specific
            conventional location, e.g., ""/tmp/ray"".
        _java_worker_options: Overwrite the options to start Java workers.
        _lru_evict (bool): If True, when an object store is full, it will evict
            objects in LRU order to make more space and when under memory
            pressure, ray.ObjectLostError may be thrown. If False, then
            reference counting will be used to decide which objects are safe
            to evict and when under memory pressure, ray.ObjectStoreFullError
            may be thrown.
        _metrics_export_port(int): Port number Ray exposes system metrics
            through a Prometheus endpoint. It is currently under active
            development, and the API is subject to change.
        _system_config (dict): Configuration for overriding
            RayConfig defaults. For testing purposes ONLY.

    Returns:
        Address information about the started processes.

    Raises:
        Exception: An exception is raised if an inappropriate combination of
            arguments is passed in.
    """"""

    # Try to increase the file descriptor limit, which is too low by
    # default for Ray: https://github.com/ray-project/ray/issues/11239
    try:
        import resource
        soft, hard = resource.getrlimit(resource.RLIMIT_NOFILE)
        if soft < hard:
            # https://github.com/ray-project/ray/issues/12059
            soft = max(soft, min(hard, 65536))
            logger.debug(""Automatically increasing RLIMIT_NOFILE to max ""
                         ""value of {}"".format(hard))
            try:
                resource.setrlimit(resource.RLIMIT_NOFILE, (hard, hard))
            except ValueError:
                logger.debug(""Failed to raise limit."")
        soft, _ = resource.getrlimit(resource.RLIMIT_NOFILE)
        if soft < 4096:
            logger.warning(
                ""File descriptor limit {} is too low for production ""
                ""servers and may result in connection errors. ""
                ""At least 8192 is recommended. --- ""
                ""Fix with 'ulimit -n 8192'"".format(soft))
    except ImportError:
        logger.debug(""Could not import resource module (on Windows)"")
        pass

    if ""RAY_ADDRESS"" in os.environ:
        if address is None or address == ""auto"":
            address = os.environ[""RAY_ADDRESS""]
        else:
            raise RuntimeError(
                ""Cannot use both the RAY_ADDRESS environment variable and ""
                ""the address argument of ray.init simultaneously. If you ""
                ""use RAY_ADDRESS to connect to a specific Ray cluster, ""
                ""please call ray.init() or ray.init(address=\\""auto\\"") on the ""
                ""driver."")

    # Convert hostnames to numerical IP address.
    if _node_ip_address is not None:
        node_ip_address = services.address_to_ip(_node_ip_address)
    raylet_ip_address = node_ip_address

    if address:
        redis_address, _, _ = services.validate_redis_address(address)
    else:
        redis_address = None

    if configure_logging:
        setup_logger(logging_level, logging_format)

    if redis_address is not None:
        logger.info(
            f""Connecting to existing Ray cluster at address: {redis_address}"")

    if local_mode:
        driver_mode = LOCAL_MODE
    else:
        driver_mode = SCRIPT_MODE

    if global_worker.connected:
        if ignore_reinit_error:
            logger.info(
                ""Calling ray.init() again after it has already been called."")
            return
        else:
            raise RuntimeError(""Maybe you called ray.init twice by accident? ""
                               ""This error can be suppressed by passing in ""
                               ""'ignore_reinit_error=True' or by calling ""
                               ""'ray.shutdown()' prior to 'ray.init()'."")

    _system_config = _system_config or {}
    if not isinstance(_system_config, dict):
        raise TypeError(""The _system_config must be a dict."")

    global _global_node
    if redis_address is None:
        # In this case, we need to start a new cluster.
        ray_params = ray.parameter.RayParams(
            redis_address=redis_address,
            node_ip_address=node_ip_address,
            raylet_ip_address=raylet_ip_address,
            object_ref_seed=None,
            driver_mode=driver_mode,
            redirect_worker_output=None,
            redirect_output=None,
            num_cpus=num_cpus,
            num_gpus=num_gpus,
            resources=resources,
            num_redis_shards=None,
            redis_max_clients=None,
            redis_password=_redis_password,
            plasma_directory=_plasma_directory,
            huge_pages=None,
            include_dashboard=include_dashboard,
            dashboard_host=dashboard_host,
            dashboard_port=dashboard_port,
            memory=_memory,
            object_store_memory=object_store_memory,
            redis_max_memory=_redis_max_memory,
            plasma_store_socket_name=None,
            temp_dir=_temp_dir,
            java_worker_options=_java_worker_options,
            start_initial_python_workers_for_first_job=True,
            _system_config=_system_config,
            lru_evict=_lru_evict,
            enable_object_reconstruction=_enable_object_reconstruction,
            metrics_export_port=_metrics_export_port)
        # Start the Ray processes. We set shutdown_at_exit=False because we
        # shutdown the node in the ray.shutdown call that happens in the atexit
        # handler. We still spawn a reaper process in case the atexit handler
        # isn't called.
        _global_node = ray.node.Node(
            head=True,
            shutdown_at_exit=False,
            spawn_reaper=True,
            ray_params=ray_params)
    else:
        # In this case, we are connecting to an existing cluster.
        if num_cpus is not None or num_gpus is not None:
            raise ValueError(
                ""When connecting to an existing cluster, num_cpus ""
                ""and num_gpus must not be provided."")
        if resources is not None:
            raise ValueError(""When connecting to an existing cluster, ""
                             ""resources must not be provided."")
        if object_store_memory is not None:
            raise ValueError(""When connecting to an existing cluster, ""
                             ""object_store_memory must not be provided."")
        if _system_config is not None and len(_system_config) != 0:
            raise ValueError(""When connecting to an existing cluster, ""
                             ""_system_config must not be provided."")
        if _lru_evict:
            raise ValueError(""When connecting to an existing cluster, ""
                             ""_lru_evict must not be provided."")
        if _enable_object_reconstruction:
            raise ValueError(
                ""When connecting to an existing cluster, ""
                ""_enable_object_reconstruction must not be provided."")

        # In this case, we only need to connect the node.
        ray_params = ray.parameter.RayParams(
            node_ip_address=node_ip_address,
            raylet_ip_address=raylet_ip_address,
            redis_address=redis_address,
            redis_password=_redis_password,
            object_ref_seed=None,
            temp_dir=_temp_dir,
            _system_config=_system_config,
            lru_evict=_lru_evict,
            enable_object_reconstruction=_enable_object_reconstruction,
            metrics_export_port=_metrics_export_port)
        _global_node = ray.node.Node(
            ray_params,
            head=False,
            shutdown_at_exit=False,
            spawn_reaper=False,
            connect_only=True)

    connect(
        _global_node,
        mode=driver_mode,
        log_to_driver=log_to_driver,
        worker=global_worker,
        driver_object_store_memory=_driver_object_store_memory,
        job_id=None,
        job_config=job_config)

    for hook in _post_init_hooks:
        hook()

    node_id = global_worker.core_worker.get_current_node_id()
    return dict(_global_node.address_info, node_id=node_id.hex())"
"    def shutdown(self) -> None:
        """"""Completely shut down the connected Serve instance.

        Shuts down all processes and deletes all state associated with the
        instance.
        """"""
        if (not self._shutdown) and ray.is_initialized():
            ray.get(self._controller.shutdown.remote())
            ray.kill(self._controller, no_restart=True)

            # Wait for the named actor entry gets removed as well.
            started = time.time()
            while True:
                try:
                    ray.get_actor(self._controller_name)
                    if time.time() - started > 5:
                        logger.warning(
                            ""Waited 5s for Serve to shutdown gracefully but ""
                            ""the controller is still not cleaned up. ""
                            ""You can ignore this warning if you are shutting ""
                            ""down the Ray cluster."")
                        break
                except ValueError:  # actor name is removed
                    break

            self._shutdown = True","    def shutdown(self) -> None:
        """"""Completely shut down the connected Serve instance.

        Shuts down all processes and deletes all state associated with the
        instance.
        """"""
        if not self._shutdown:
            ray.get(self._controller.shutdown.remote())
            ray.kill(self._controller, no_restart=True)
            self._shutdown = True"
"    def __init__(
            self,
            obs_space: gym.spaces.Space,
            action_space: gym.spaces.Space,
            config: TrainerConfigDict,
            loss_fn: Callable[[
                Policy, ModelV2, Type[TFActionDistribution], SampleBatch
            ], TensorType],
            *,
            stats_fn: Optional[Callable[[Policy, SampleBatch], Dict[
                str, TensorType]]] = None,
            grad_stats_fn: Optional[Callable[[
                Policy, SampleBatch, ModelGradients
            ], Dict[str, TensorType]]] = None,
            before_loss_init: Optional[Callable[[
                Policy, gym.spaces.Space, gym.spaces.Space, TrainerConfigDict
            ], None]] = None,
            make_model: Optional[Callable[[
                Policy, gym.spaces.Space, gym.spaces.Space, TrainerConfigDict
            ], ModelV2]] = None,
            action_sampler_fn: Optional[Callable[[
                TensorType, List[TensorType]
            ], Tuple[TensorType, TensorType]]] = None,
            action_distribution_fn: Optional[Callable[[
                Policy, ModelV2, TensorType, TensorType, TensorType
            ], Tuple[TensorType, type, List[TensorType]]]] = None,
            existing_inputs: Optional[Dict[str, ""tf1.placeholder""]] = None,
            existing_model: Optional[ModelV2] = None,
            get_batch_divisibility_req: Optional[Callable[[Policy],
                                                          int]] = None,
            obs_include_prev_action_reward: bool = True):
        """"""Initialize a dynamic TF policy.

        Args:
            observation_space (gym.spaces.Space): Observation space of the
                policy.
            action_space (gym.spaces.Space): Action space of the policy.
            config (TrainerConfigDict): Policy-specific configuration data.
            loss_fn (Callable[[Policy, ModelV2, Type[TFActionDistribution],
                SampleBatch], TensorType]): Function that returns a loss tensor
                for the policy graph.
            stats_fn (Optional[Callable[[Policy, SampleBatch],
                Dict[str, TensorType]]]): Optional function that returns a dict
                of TF fetches given the policy and batch input tensors.
            grad_stats_fn (Optional[Callable[[Policy, SampleBatch,
                ModelGradients], Dict[str, TensorType]]]):
                Optional function that returns a dict of TF fetches given the
                policy, sample batch, and loss gradient tensors.
            before_loss_init (Optional[Callable[
                [Policy, gym.spaces.Space, gym.spaces.Space,
                TrainerConfigDict], None]]): Optional function to run prior to
                loss init that takes the same arguments as __init__.
            make_model (Optional[Callable[[Policy, gym.spaces.Space,
                gym.spaces.Space, TrainerConfigDict], ModelV2]]): Optional
                function that returns a ModelV2 object given
                policy, obs_space, action_space, and policy config.
                All policy variables should be created in this function. If not
                specified, a default model will be created.
            action_sampler_fn (Optional[Callable[[Policy, ModelV2, Dict[
                str, TensorType], TensorType, TensorType], Tuple[TensorType,
                TensorType]]]): A callable returning a sampled action and its
                log-likelihood given Policy, ModelV2, input_dict, explore,
                timestep, and is_training.
            action_distribution_fn (Optional[Callable[[Policy, ModelV2,
                Dict[str, TensorType], TensorType, TensorType],
                Tuple[TensorType, type, List[TensorType]]]]): A callable
                returning distribution inputs (parameters), a dist-class to
                generate an action distribution object from, and
                internal-state outputs (or an empty list if not applicable).
                Note: No Exploration hooks have to be called from within
                `action_distribution_fn`. It's should only perform a simple
                forward pass through some model.
                If None, pass inputs through `self.model()` to get distribution
                inputs.
                The callable takes as inputs: Policy, ModelV2, input_dict,
                explore, timestep, is_training.
            existing_inputs (Optional[Dict[str, tf1.placeholder]]): When
                copying a policy, this specifies an existing dict of
                placeholders to use instead of defining new ones.
            existing_model (Optional[ModelV2]): When copying a policy, this
                specifies an existing model to clone and share weights with.
            get_batch_divisibility_req (Optional[Callable[[Policy], int]]):
                Optional callable that returns the divisibility requirement for
                sample batches. If None, will assume a value of 1.
            obs_include_prev_action_reward (bool): Whether to include the
                previous action and reward in the model input (default: True).
        """"""
        self.observation_space = obs_space
        self.action_space = action_space
        self.config = config
        self.framework = ""tf""
        self._loss_fn = loss_fn
        self._stats_fn = stats_fn
        self._grad_stats_fn = grad_stats_fn
        self._obs_include_prev_action_reward = obs_include_prev_action_reward

        dist_class = dist_inputs = None
        if action_sampler_fn or action_distribution_fn:
            if not make_model:
                raise ValueError(
                    ""`make_model` is required if `action_sampler_fn` OR ""
                    ""`action_distribution_fn` is given"")
        else:
            dist_class, logit_dim = ModelCatalog.get_action_dist(
                action_space, self.config[""model""])

        # Setup self.model.
        if existing_model:
            self.model = existing_model
        elif make_model:
            self.model = make_model(self, obs_space, action_space, config)
        else:
            self.model = ModelCatalog.get_model_v2(
                obs_space=obs_space,
                action_space=action_space,
                num_outputs=logit_dim,
                model_config=self.config[""model""],
                framework=""tf"")
        # Auto-update model's inference view requirements, if recurrent.
        self._update_model_inference_view_requirements_from_init_state()

        if existing_inputs:
            self._state_inputs = [
                v for k, v in existing_inputs.items()
                if k.startswith(""state_in_"")
            ]
            if self._state_inputs:
                self._seq_lens = existing_inputs[""seq_lens""]
        else:
            if self.config[""_use_trajectory_view_api""]:
                self._state_inputs = [
                    tf1.placeholder(
                        shape=(None, ) + vr.space.shape, dtype=vr.space.dtype)
                    for k, vr in
                    self.model.inference_view_requirements.items()
                    if k[:9] == ""state_in_""
                ]
            else:
                self._state_inputs = [
                    tf1.placeholder(shape=(None, ) + s.shape, dtype=s.dtype)
                    for s in self.model.get_initial_state()
                ]

        # Use default settings.
        # Add NEXT_OBS, STATE_IN_0.., and others.
        self.view_requirements = self._get_default_view_requirements()
        # Combine view_requirements for Model and Policy.
        self.view_requirements.update(self.model.inference_view_requirements)

        # Setup standard placeholders.
        if existing_inputs is not None:
            timestep = existing_inputs[""timestep""]
            explore = existing_inputs[""is_exploring""]
            self._input_dict, self._dummy_batch = \\
                self._get_input_dict_and_dummy_batch(
                    self.view_requirements, existing_inputs)
        else:
            action_ph = ModelCatalog.get_action_placeholder(action_space)
            prev_action_ph = ModelCatalog.get_action_placeholder(
                action_space, ""prev_action"")
            if self.config[""_use_trajectory_view_api""]:
                self._input_dict, self._dummy_batch = \\
                    self._get_input_dict_and_dummy_batch(
                        self.view_requirements,
                        {SampleBatch.ACTIONS: action_ph,
                         SampleBatch.PREV_ACTIONS: prev_action_ph})
            else:
                self._input_dict = {
                    SampleBatch.CUR_OBS: tf1.placeholder(
                        tf.float32,
                        shape=[None] + list(obs_space.shape),
                        name=""observation"")
                }
                self._input_dict[SampleBatch.ACTIONS] = action_ph
                if self._obs_include_prev_action_reward:
                    self._input_dict.update({
                        SampleBatch.PREV_ACTIONS: prev_action_ph,
                        SampleBatch.PREV_REWARDS: tf1.placeholder(
                            tf.float32, [None], name=""prev_reward""),
                    })
            # Placeholder for (sampling steps) timestep (int).
            timestep = tf1.placeholder_with_default(
                tf.zeros((), dtype=tf.int64), (), name=""timestep"")
            # Placeholder for `is_exploring` flag.
            explore = tf1.placeholder_with_default(
                True, (), name=""is_exploring"")

        # Placeholder for RNN time-chunk valid lengths.
        self._seq_lens = tf1.placeholder(
            dtype=tf.int32, shape=[None], name=""seq_lens"")
        # Placeholder for `is_training` flag.
        self._input_dict[""is_training""] = self._get_is_training_placeholder()

        # Create the Exploration object to use for this Policy.
        self.exploration = self._create_exploration()

        # Fully customized action generation (e.g., custom policy).
        if action_sampler_fn:
            sampled_action, sampled_action_logp = action_sampler_fn(
                self,
                self.model,
                obs_batch=self._input_dict[SampleBatch.CUR_OBS],
                state_batches=self._state_inputs,
                seq_lens=self._seq_lens,
                prev_action_batch=self._input_dict.get(
                    SampleBatch.PREV_ACTIONS),
                prev_reward_batch=self._input_dict.get(
                    SampleBatch.PREV_REWARDS),
                explore=explore,
                is_training=self._input_dict[""is_training""])
        else:
            # Distribution generation is customized, e.g., DQN, DDPG.
            if action_distribution_fn:
                dist_inputs, dist_class, self._state_out = \\
                    action_distribution_fn(
                        self, self.model,
                        obs_batch=self._input_dict[SampleBatch.CUR_OBS],
                        state_batches=self._state_inputs,
                        seq_lens=self._seq_lens,
                        prev_action_batch=self._input_dict.get(
                            SampleBatch.PREV_ACTIONS),
                        prev_reward_batch=self._input_dict.get(
                            SampleBatch.PREV_REWARDS),
                        explore=explore,
                        is_training=self._input_dict[""is_training""])
            # Default distribution generation behavior:
            # Pass through model. E.g., PG, PPO.
            else:
                dist_inputs, self._state_out = self.model(
                    self._input_dict, self._state_inputs, self._seq_lens)

            action_dist = dist_class(dist_inputs, self.model)

            # Using exploration to get final action (e.g. via sampling).
            sampled_action, sampled_action_logp = \\
                self.exploration.get_exploration_action(
                    action_distribution=action_dist,
                    timestep=timestep,
                    explore=explore)

        # Phase 1 init.
        sess = tf1.get_default_session() or tf1.Session()

        batch_divisibility_req = get_batch_divisibility_req(self) if \\
            callable(get_batch_divisibility_req) else \\
            (get_batch_divisibility_req or 1)

        super().__init__(
            observation_space=obs_space,
            action_space=action_space,
            config=config,
            sess=sess,
            obs_input=self._input_dict[SampleBatch.OBS],
            action_input=self._input_dict[SampleBatch.ACTIONS],
            sampled_action=sampled_action,
            sampled_action_logp=sampled_action_logp,
            dist_inputs=dist_inputs,
            dist_class=dist_class,
            loss=None,  # dynamically initialized on run
            loss_inputs=[],
            model=self.model,
            state_inputs=self._state_inputs,
            state_outputs=self._state_out,
            prev_action_input=self._input_dict.get(SampleBatch.PREV_ACTIONS),
            prev_reward_input=self._input_dict.get(SampleBatch.PREV_REWARDS),
            seq_lens=self._seq_lens,
            max_seq_len=config[""model""][""max_seq_len""],
            batch_divisibility_req=batch_divisibility_req,
            explore=explore,
            timestep=timestep)

        # Phase 2 init.
        if before_loss_init is not None:
            before_loss_init(self, obs_space, action_space, config)

        # Loss initialization and model/postprocessing test calls.
        if not existing_inputs:
            self._initialize_loss_from_dummy_batch(
                auto_remove_unneeded_view_reqs=True)","    def __init__(
            self,
            obs_space: gym.spaces.Space,
            action_space: gym.spaces.Space,
            config: TrainerConfigDict,
            loss_fn: Callable[[
                Policy, ModelV2, Type[TFActionDistribution], SampleBatch
            ], TensorType],
            *,
            stats_fn: Optional[Callable[[Policy, SampleBatch], Dict[
                str, TensorType]]] = None,
            grad_stats_fn: Optional[Callable[[
                Policy, SampleBatch, ModelGradients
            ], Dict[str, TensorType]]] = None,
            before_loss_init: Optional[Callable[[
                Policy, gym.spaces.Space, gym.spaces.Space, TrainerConfigDict
            ], None]] = None,
            make_model: Optional[Callable[[
                Policy, gym.spaces.Space, gym.spaces.Space, TrainerConfigDict
            ], ModelV2]] = None,
            action_sampler_fn: Optional[Callable[[
                TensorType, List[TensorType]
            ], Tuple[TensorType, TensorType]]] = None,
            action_distribution_fn: Optional[Callable[[
                Policy, ModelV2, TensorType, TensorType, TensorType
            ], Tuple[TensorType, type, List[TensorType]]]] = None,
            existing_inputs: Optional[Dict[str, ""tf1.placeholder""]] = None,
            existing_model: Optional[ModelV2] = None,
            get_batch_divisibility_req: Optional[Callable[[Policy],
                                                          int]] = None,
            obs_include_prev_action_reward: bool = True):
        """"""Initialize a dynamic TF policy.

        Args:
            observation_space (gym.spaces.Space): Observation space of the
                policy.
            action_space (gym.spaces.Space): Action space of the policy.
            config (TrainerConfigDict): Policy-specific configuration data.
            loss_fn (Callable[[Policy, ModelV2, Type[TFActionDistribution],
                SampleBatch], TensorType]): Function that returns a loss tensor
                for the policy graph.
            stats_fn (Optional[Callable[[Policy, SampleBatch],
                Dict[str, TensorType]]]): Optional function that returns a dict
                of TF fetches given the policy and batch input tensors.
            grad_stats_fn (Optional[Callable[[Policy, SampleBatch,
                ModelGradients], Dict[str, TensorType]]]):
                Optional function that returns a dict of TF fetches given the
                policy, sample batch, and loss gradient tensors.
            before_loss_init (Optional[Callable[
                [Policy, gym.spaces.Space, gym.spaces.Space,
                TrainerConfigDict], None]]): Optional function to run prior to
                loss init that takes the same arguments as __init__.
            make_model (Optional[Callable[[Policy, gym.spaces.Space,
                gym.spaces.Space, TrainerConfigDict], ModelV2]]): Optional
                function that returns a ModelV2 object given
                policy, obs_space, action_space, and policy config.
                All policy variables should be created in this function. If not
                specified, a default model will be created.
            action_sampler_fn (Optional[Callable[[Policy, ModelV2, Dict[
                str, TensorType], TensorType, TensorType], Tuple[TensorType,
                TensorType]]]): A callable returning a sampled action and its
                log-likelihood given Policy, ModelV2, input_dict, explore,
                timestep, and is_training.
            action_distribution_fn (Optional[Callable[[Policy, ModelV2,
                Dict[str, TensorType], TensorType, TensorType],
                Tuple[TensorType, type, List[TensorType]]]]): A callable
                returning distribution inputs (parameters), a dist-class to
                generate an action distribution object from, and
                internal-state outputs (or an empty list if not applicable).
                Note: No Exploration hooks have to be called from within
                `action_distribution_fn`. It's should only perform a simple
                forward pass through some model.
                If None, pass inputs through `self.model()` to get distribution
                inputs.
                The callable takes as inputs: Policy, ModelV2, input_dict,
                explore, timestep, is_training.
            existing_inputs (Optional[Dict[str, tf1.placeholder]]): When
                copying a policy, this specifies an existing dict of
                placeholders to use instead of defining new ones.
            existing_model (Optional[ModelV2]): When copying a policy, this
                specifies an existing model to clone and share weights with.
            get_batch_divisibility_req (Optional[Callable[[Policy], int]]):
                Optional callable that returns the divisibility requirement for
                sample batches. If None, will assume a value of 1.
            obs_include_prev_action_reward (bool): Whether to include the
                previous action and reward in the model input (default: True).
        """"""
        self.observation_space = obs_space
        self.action_space = action_space
        self.config = config
        self.framework = ""tf""
        self._loss_fn = loss_fn
        self._stats_fn = stats_fn
        self._grad_stats_fn = grad_stats_fn
        self._obs_include_prev_action_reward = obs_include_prev_action_reward

        dist_class = dist_inputs = None
        if action_sampler_fn or action_distribution_fn:
            if not make_model:
                raise ValueError(
                    ""`make_model` is required if `action_sampler_fn` OR ""
                    ""`action_distribution_fn` is given"")
        else:
            dist_class, logit_dim = ModelCatalog.get_action_dist(
                action_space, self.config[""model""])

        # Setup self.model.
        if existing_model:
            self.model = existing_model
        elif make_model:
            self.model = make_model(self, obs_space, action_space, config)
        else:
            self.model = ModelCatalog.get_model_v2(
                obs_space=obs_space,
                action_space=action_space,
                num_outputs=logit_dim,
                model_config=self.config[""model""],
                framework=""tf"")
        # Auto-update model's inference view requirements, if recurrent.
        self._update_model_inference_view_requirements_from_init_state()

        if existing_inputs:
            self._state_inputs = [
                v for k, v in existing_inputs.items()
                if k.startswith(""state_in_"")
            ]
            if self._state_inputs:
                self._seq_lens = existing_inputs[""seq_lens""]
        else:
            if self.config[""_use_trajectory_view_api""]:
                self._state_inputs = [
                    tf1.placeholder(
                        shape=(None, ) + vr.space.shape, dtype=vr.space.dtype)
                    for k, vr in
                    self.model.inference_view_requirements.items()
                    if k[:9] == ""state_in_""
                ]
            else:
                self._state_inputs = [
                    tf1.placeholder(shape=(None, ) + s.shape, dtype=s.dtype)
                    for s in self.model.get_initial_state()
                ]

        # Use default settings.
        # Add NEXT_OBS, STATE_IN_0.., and others.
        self.view_requirements = self._get_default_view_requirements()
        # Combine view_requirements for Model and Policy.
        self.view_requirements.update(self.model.inference_view_requirements)

        # Setup standard placeholders.
        if existing_inputs is not None:
            timestep = existing_inputs[""timestep""]
            explore = existing_inputs[""is_exploring""]
            self._input_dict, self._dummy_batch = \\
                self._get_input_dict_and_dummy_batch(
                    self.view_requirements, existing_inputs)
        else:
            action_ph = ModelCatalog.get_action_placeholder(action_space)
            prev_action_ph = ModelCatalog.get_action_placeholder(
                action_space, ""prev_action"")
            if self.config[""_use_trajectory_view_api""]:
                self._input_dict, self._dummy_batch = \\
                    self._get_input_dict_and_dummy_batch(
                        self.view_requirements,
                        {SampleBatch.ACTIONS: action_ph,
                         SampleBatch.PREV_ACTIONS: prev_action_ph})
            else:
                self._input_dict = {
                    SampleBatch.CUR_OBS: tf1.placeholder(
                        tf.float32,
                        shape=[None] + list(obs_space.shape),
                        name=""observation"")
                }
                self._input_dict[SampleBatch.ACTIONS] = action_ph
                if self._obs_include_prev_action_reward:
                    self._input_dict.update({
                        SampleBatch.PREV_ACTIONS: prev_action_ph,
                        SampleBatch.PREV_REWARDS: tf1.placeholder(
                            tf.float32, [None], name=""prev_reward""),
                    })
            # Placeholder for (sampling steps) timestep (int).
            timestep = tf1.placeholder(tf.int64, (), name=""timestep"")
            # Placeholder for `is_exploring` flag.
            explore = tf1.placeholder_with_default(
                True, (), name=""is_exploring"")

        # Placeholder for RNN time-chunk valid lengths.
        self._seq_lens = tf1.placeholder(
            dtype=tf.int32, shape=[None], name=""seq_lens"")
        # Placeholder for `is_training` flag.
        self._input_dict[""is_training""] = self._get_is_training_placeholder()

        # Create the Exploration object to use for this Policy.
        self.exploration = self._create_exploration()

        # Fully customized action generation (e.g., custom policy).
        if action_sampler_fn:
            sampled_action, sampled_action_logp = action_sampler_fn(
                self,
                self.model,
                obs_batch=self._input_dict[SampleBatch.CUR_OBS],
                state_batches=self._state_inputs,
                seq_lens=self._seq_lens,
                prev_action_batch=self._input_dict.get(
                    SampleBatch.PREV_ACTIONS),
                prev_reward_batch=self._input_dict.get(
                    SampleBatch.PREV_REWARDS),
                explore=explore,
                is_training=self._input_dict[""is_training""])
        else:
            # Distribution generation is customized, e.g., DQN, DDPG.
            if action_distribution_fn:
                dist_inputs, dist_class, self._state_out = \\
                    action_distribution_fn(
                        self, self.model,
                        obs_batch=self._input_dict[SampleBatch.CUR_OBS],
                        state_batches=self._state_inputs,
                        seq_lens=self._seq_lens,
                        prev_action_batch=self._input_dict.get(
                            SampleBatch.PREV_ACTIONS),
                        prev_reward_batch=self._input_dict.get(
                            SampleBatch.PREV_REWARDS),
                        explore=explore,
                        is_training=self._input_dict[""is_training""])
            # Default distribution generation behavior:
            # Pass through model. E.g., PG, PPO.
            else:
                dist_inputs, self._state_out = self.model(
                    self._input_dict, self._state_inputs, self._seq_lens)

            action_dist = dist_class(dist_inputs, self.model)

            # Using exploration to get final action (e.g. via sampling).
            sampled_action, sampled_action_logp = \\
                self.exploration.get_exploration_action(
                    action_distribution=action_dist,
                    timestep=timestep,
                    explore=explore)

        # Phase 1 init.
        sess = tf1.get_default_session() or tf1.Session()

        batch_divisibility_req = get_batch_divisibility_req(self) if \\
            callable(get_batch_divisibility_req) else \\
            (get_batch_divisibility_req or 1)

        super().__init__(
            observation_space=obs_space,
            action_space=action_space,
            config=config,
            sess=sess,
            obs_input=self._input_dict[SampleBatch.OBS],
            action_input=self._input_dict[SampleBatch.ACTIONS],
            sampled_action=sampled_action,
            sampled_action_logp=sampled_action_logp,
            dist_inputs=dist_inputs,
            dist_class=dist_class,
            loss=None,  # dynamically initialized on run
            loss_inputs=[],
            model=self.model,
            state_inputs=self._state_inputs,
            state_outputs=self._state_out,
            prev_action_input=self._input_dict.get(SampleBatch.PREV_ACTIONS),
            prev_reward_input=self._input_dict.get(SampleBatch.PREV_REWARDS),
            seq_lens=self._seq_lens,
            max_seq_len=config[""model""][""max_seq_len""],
            batch_divisibility_req=batch_divisibility_req,
            explore=explore,
            timestep=timestep)

        # Phase 2 init.
        if before_loss_init is not None:
            before_loss_init(self, obs_space, action_space, config)

        # Loss initialization and model/postprocessing test calls.
        if not existing_inputs:
            self._initialize_loss_from_dummy_batch(
                auto_remove_unneeded_view_reqs=True)"
"    def __init__(self,
                 observation_space: gym.spaces.Space,
                 action_space: gym.spaces.Space,
                 config: TrainerConfigDict,
                 sess: ""tf1.Session"",
                 obs_input: TensorType,
                 sampled_action: TensorType,
                 loss: TensorType,
                 loss_inputs: List[Tuple[str, TensorType]],
                 model: ModelV2 = None,
                 sampled_action_logp: Optional[TensorType] = None,
                 action_input: Optional[TensorType] = None,
                 log_likelihood: Optional[TensorType] = None,
                 dist_inputs: Optional[TensorType] = None,
                 dist_class: Optional[type] = None,
                 state_inputs: Optional[List[TensorType]] = None,
                 state_outputs: Optional[List[TensorType]] = None,
                 prev_action_input: Optional[TensorType] = None,
                 prev_reward_input: Optional[TensorType] = None,
                 seq_lens: Optional[TensorType] = None,
                 max_seq_len: int = 20,
                 batch_divisibility_req: int = 1,
                 update_ops: List[TensorType] = None,
                 explore: Optional[TensorType] = None,
                 timestep: Optional[TensorType] = None):
        """"""Initializes a Policy object.

        Args:
            observation_space (gym.spaces.Space): Observation space of the env.
            action_space (gym.spaces.Space): Action space of the env.
            config (TrainerConfigDict): The Policy config dict.
            sess (tf1.Session): The TensorFlow session to use.
            obs_input (TensorType): Input placeholder for observations, of
                shape [BATCH_SIZE, obs...].
            sampled_action (TensorType): Tensor for sampling an action, of
                shape [BATCH_SIZE, action...]
            loss (TensorType): Scalar policy loss output tensor.
            loss_inputs (List[Tuple[str, TensorType]]): A (name, placeholder)
                tuple for each loss input argument. Each placeholder name must
                correspond to a SampleBatch column key returned by
                postprocess_trajectory(), and has shape [BATCH_SIZE, data...].
                These keys will be read from postprocessed sample batches and
                fed into the specified placeholders during loss computation.
            model (ModelV2): used to integrate custom losses and
                stats from user-defined RLlib models.
            sampled_action_logp (Optional[TensorType]): log probability of the
                sampled action.
            action_input (Optional[TensorType]): Input placeholder for actions
                for logp/log-likelihood calculations.
            log_likelihood (Optional[TensorType]): Tensor to calculate the
                log_likelihood (given action_input and obs_input).
            dist_class (Optional[type]): An optional ActionDistribution class
                to use for generating a dist object from distribution inputs.
            dist_inputs (Optional[TensorType]): Tensor to calculate the
                distribution inputs/parameters.
            state_inputs (Optional[List[TensorType]]): List of RNN state input
                Tensors.
            state_outputs (Optional[List[TensorType]]): List of RNN state
                output Tensors.
            prev_action_input (Optional[TensorType]): placeholder for previous
                actions.
            prev_reward_input (Optional[TensorType]): placeholder for previous
                rewards.
            seq_lens (Optional[TensorType]): Placeholder for RNN sequence
                lengths, of shape [NUM_SEQUENCES].
                Note that NUM_SEQUENCES << BATCH_SIZE. See
                policy/rnn_sequencing.py for more information.
            max_seq_len (int): Max sequence length for LSTM training.
            batch_divisibility_req (int): pad all agent experiences batches to
                multiples of this value. This only has an effect if not using
                a LSTM model.
            update_ops (List[TensorType]): override the batchnorm update ops to
                run when applying gradients. Otherwise we run all update ops
                found in the current variable scope.
            explore (Optional[TensorType]): Placeholder for `explore` parameter
                into call to Exploration.get_exploration_action.
            timestep (Optional[TensorType]): Placeholder for the global
                sampling timestep.
        """"""
        self.framework = ""tf""
        super().__init__(observation_space, action_space, config)
        # Disable env-info placeholder.
        if SampleBatch.INFOS in self.view_requirements:
            self.view_requirements[SampleBatch.INFOS].used_for_training = False

        assert model is None or isinstance(model, ModelV2), \\
            ""Model classes for TFPolicy other than `ModelV2` not allowed! "" \\
            ""You passed in {}."".format(model)
        self.model = model
        # Auto-update model's inference view requirements, if recurrent.
        if self.model is not None:
            self._update_model_inference_view_requirements_from_init_state()

        self.exploration = self._create_exploration()
        self._sess = sess
        self._obs_input = obs_input
        self._prev_action_input = prev_action_input
        self._prev_reward_input = prev_reward_input
        self._sampled_action = sampled_action
        self._is_training = self._get_is_training_placeholder()
        self._is_exploring = explore if explore is not None else \\
            tf1.placeholder_with_default(True, (), name=""is_exploring"")
        self._sampled_action_logp = sampled_action_logp
        self._sampled_action_prob = (tf.math.exp(self._sampled_action_logp)
                                     if self._sampled_action_logp is not None
                                     else None)
        self._action_input = action_input  # For logp calculations.
        self._dist_inputs = dist_inputs
        self.dist_class = dist_class

        self._state_inputs = state_inputs or []
        self._state_outputs = state_outputs or []
        self._seq_lens = seq_lens
        self._max_seq_len = max_seq_len
        if len(self._state_inputs) != len(self._state_outputs):
            raise ValueError(
                ""Number of state input and output tensors must match, got: ""
                ""{} vs {}"".format(self._state_inputs, self._state_outputs))
        if len(self.get_initial_state()) != len(self._state_inputs):
            raise ValueError(
                ""Length of initial state must match number of state inputs, ""
                ""got: {} vs {}"".format(self.get_initial_state(),
                                       self._state_inputs))
        if self._state_inputs and self._seq_lens is None:
            raise ValueError(
                ""seq_lens tensor must be given if state inputs are defined"")

        self._batch_divisibility_req = batch_divisibility_req
        self._update_ops = update_ops
        self._apply_op = None
        self._stats_fetches = {}
        self._timestep = timestep if timestep is not None else \\
            tf1.placeholder_with_default(
                tf.zeros((), dtype=tf.int64), (), name=""timestep"")

        self._optimizer = None
        self._grads_and_vars = None
        self._grads = None
        # Policy tf-variables (weights), whose values to get/set via
        # get_weights/set_weights.
        self._variables = None
        # Local optimizer's tf-variables (e.g. state vars for Adam).
        # Will be stored alongside `self._variables` when checkpointing.
        self._optimizer_variables = None

        # The loss tf-op.
        self._loss = None
        # A batch dict passed into loss function as input.
        self._loss_input_dict = {}
        if loss is not None:
            self._initialize_loss(loss, loss_inputs)

        # The log-likelihood calculator op.
        self._log_likelihood = log_likelihood
        if self._log_likelihood is None and self._dist_inputs is not None and \\
                self.dist_class is not None:
            self._log_likelihood = self.dist_class(
                self._dist_inputs, self.model).logp(self._action_input)","    def __init__(self,
                 observation_space: gym.spaces.Space,
                 action_space: gym.spaces.Space,
                 config: TrainerConfigDict,
                 sess: ""tf1.Session"",
                 obs_input: TensorType,
                 sampled_action: TensorType,
                 loss: TensorType,
                 loss_inputs: List[Tuple[str, TensorType]],
                 model: ModelV2 = None,
                 sampled_action_logp: Optional[TensorType] = None,
                 action_input: Optional[TensorType] = None,
                 log_likelihood: Optional[TensorType] = None,
                 dist_inputs: Optional[TensorType] = None,
                 dist_class: Optional[type] = None,
                 state_inputs: Optional[List[TensorType]] = None,
                 state_outputs: Optional[List[TensorType]] = None,
                 prev_action_input: Optional[TensorType] = None,
                 prev_reward_input: Optional[TensorType] = None,
                 seq_lens: Optional[TensorType] = None,
                 max_seq_len: int = 20,
                 batch_divisibility_req: int = 1,
                 update_ops: List[TensorType] = None,
                 explore: Optional[TensorType] = None,
                 timestep: Optional[TensorType] = None):
        """"""Initializes a Policy object.

        Args:
            observation_space (gym.spaces.Space): Observation space of the env.
            action_space (gym.spaces.Space): Action space of the env.
            config (TrainerConfigDict): The Policy config dict.
            sess (tf1.Session): The TensorFlow session to use.
            obs_input (TensorType): Input placeholder for observations, of
                shape [BATCH_SIZE, obs...].
            sampled_action (TensorType): Tensor for sampling an action, of
                shape [BATCH_SIZE, action...]
            loss (TensorType): Scalar policy loss output tensor.
            loss_inputs (List[Tuple[str, TensorType]]): A (name, placeholder)
                tuple for each loss input argument. Each placeholder name must
                correspond to a SampleBatch column key returned by
                postprocess_trajectory(), and has shape [BATCH_SIZE, data...].
                These keys will be read from postprocessed sample batches and
                fed into the specified placeholders during loss computation.
            model (ModelV2): used to integrate custom losses and
                stats from user-defined RLlib models.
            sampled_action_logp (Optional[TensorType]): log probability of the
                sampled action.
            action_input (Optional[TensorType]): Input placeholder for actions
                for logp/log-likelihood calculations.
            log_likelihood (Optional[TensorType]): Tensor to calculate the
                log_likelihood (given action_input and obs_input).
            dist_class (Optional[type]): An optional ActionDistribution class
                to use for generating a dist object from distribution inputs.
            dist_inputs (Optional[TensorType]): Tensor to calculate the
                distribution inputs/parameters.
            state_inputs (Optional[List[TensorType]]): List of RNN state input
                Tensors.
            state_outputs (Optional[List[TensorType]]): List of RNN state
                output Tensors.
            prev_action_input (Optional[TensorType]): placeholder for previous
                actions.
            prev_reward_input (Optional[TensorType]): placeholder for previous
                rewards.
            seq_lens (Optional[TensorType]): Placeholder for RNN sequence
                lengths, of shape [NUM_SEQUENCES].
                Note that NUM_SEQUENCES << BATCH_SIZE. See
                policy/rnn_sequencing.py for more information.
            max_seq_len (int): Max sequence length for LSTM training.
            batch_divisibility_req (int): pad all agent experiences batches to
                multiples of this value. This only has an effect if not using
                a LSTM model.
            update_ops (List[TensorType]): override the batchnorm update ops to
                run when applying gradients. Otherwise we run all update ops
                found in the current variable scope.
            explore (Optional[TensorType]): Placeholder for `explore` parameter
                into call to Exploration.get_exploration_action.
            timestep (Optional[TensorType]): Placeholder for the global
                sampling timestep.
        """"""
        self.framework = ""tf""
        super().__init__(observation_space, action_space, config)
        # Disable env-info placeholder.
        if SampleBatch.INFOS in self.view_requirements:
            self.view_requirements[SampleBatch.INFOS].used_for_training = False

        assert model is None or isinstance(model, ModelV2), \\
            ""Model classes for TFPolicy other than `ModelV2` not allowed! "" \\
            ""You passed in {}."".format(model)
        self.model = model
        # Auto-update model's inference view requirements, if recurrent.
        if self.model is not None:
            self._update_model_inference_view_requirements_from_init_state()

        self.exploration = self._create_exploration()
        self._sess = sess
        self._obs_input = obs_input
        self._prev_action_input = prev_action_input
        self._prev_reward_input = prev_reward_input
        self._sampled_action = sampled_action
        self._is_training = self._get_is_training_placeholder()
        self._is_exploring = explore if explore is not None else \\
            tf1.placeholder_with_default(True, (), name=""is_exploring"")
        self._sampled_action_logp = sampled_action_logp
        self._sampled_action_prob = (tf.math.exp(self._sampled_action_logp)
                                     if self._sampled_action_logp is not None
                                     else None)
        self._action_input = action_input  # For logp calculations.
        self._dist_inputs = dist_inputs
        self.dist_class = dist_class

        self._state_inputs = state_inputs or []
        self._state_outputs = state_outputs or []
        self._seq_lens = seq_lens
        self._max_seq_len = max_seq_len
        if len(self._state_inputs) != len(self._state_outputs):
            raise ValueError(
                ""Number of state input and output tensors must match, got: ""
                ""{} vs {}"".format(self._state_inputs, self._state_outputs))
        if len(self.get_initial_state()) != len(self._state_inputs):
            raise ValueError(
                ""Length of initial state must match number of state inputs, ""
                ""got: {} vs {}"".format(self.get_initial_state(),
                                       self._state_inputs))
        if self._state_inputs and self._seq_lens is None:
            raise ValueError(
                ""seq_lens tensor must be given if state inputs are defined"")

        self._batch_divisibility_req = batch_divisibility_req
        self._update_ops = update_ops
        self._apply_op = None
        self._stats_fetches = {}
        self._timestep = timestep if timestep is not None else \\
            tf1.placeholder(tf.int64, (), name=""timestep"")

        self._optimizer = None
        self._grads_and_vars = None
        self._grads = None
        # Policy tf-variables (weights), whose values to get/set via
        # get_weights/set_weights.
        self._variables = None
        # Local optimizer's tf-variables (e.g. state vars for Adam).
        # Will be stored alongside `self._variables` when checkpointing.
        self._optimizer_variables = None

        # The loss tf-op.
        self._loss = None
        # A batch dict passed into loss function as input.
        self._loss_input_dict = {}
        if loss is not None:
            self._initialize_loss(loss, loss_inputs)

        # The log-likelihood calculator op.
        self._log_likelihood = log_likelihood
        if self._log_likelihood is None and self._dist_inputs is not None and \\
                self.dist_class is not None:
            self._log_likelihood = self.dist_class(
                self._dist_inputs, self.model).logp(self._action_input)"
"def try_import_tf(error=False):
    """"""Tries importing tf and returns the module (or None).

    Args:
        error (bool): Whether to raise an error if tf cannot be imported.

    Returns:
        Tuple:
            - tf1.x module (either from tf2.x.compat.v1 OR as tf1.x).
            - tf module (resulting from `import tensorflow`).
                Either tf1.x or 2.x.
            - The actually installed tf version as int: 1 or 2.

    Raises:
        ImportError: If error=True and tf is not installed.
    """"""
    # Make sure, these are reset after each test case
    # that uses them: del os.environ[""RLLIB_TEST_NO_TF_IMPORT""]
    if ""RLLIB_TEST_NO_TF_IMPORT"" in os.environ:
        logger.warning(""Not importing TensorFlow for test purposes"")
        return None, None, None

    if ""TF_CPP_MIN_LOG_LEVEL"" not in os.environ:
        os.environ[""TF_CPP_MIN_LOG_LEVEL""] = ""3""

    # Try to reuse already imported tf module. This will avoid going through
    # the initial import steps below and thereby switching off v2_behavior
    # (switching off v2 behavior twice breaks all-framework tests for eager).
    was_imported = False
    if ""tensorflow"" in sys.modules:
        tf_module = sys.modules[""tensorflow""]
        was_imported = True

    else:
        try:
            import tensorflow as tf_module
        except ImportError as e:
            if error:
                raise e
            return None, None, None

    # Try ""reducing"" tf to tf.compat.v1.
    try:
        tf1_module = tf_module.compat.v1
        if not was_imported:
            tf1_module.disable_v2_behavior()
            tf1_module.enable_resource_variables()
    # No compat.v1 -> return tf as is.
    except AttributeError:
        tf1_module = tf_module

    if not hasattr(tf_module, ""__version__""):
        version = 1  # sphinx doc gen
    else:
        version = 2 if ""2."" in tf_module.__version__[:2] else 1

    return tf1_module, tf_module, version","def try_import_tf(error=False):
    """"""Tries importing tf and returns the module (or None).

    Args:
        error (bool): Whether to raise an error if tf cannot be imported.

    Returns:
        Tuple:
            - tf1.x module (either from tf2.x.compat.v1 OR as tf1.x).
            - tf module (resulting from `import tensorflow`).
                Either tf1.x or 2.x.
            - The actually installed tf version as int: 1 or 2.

    Raises:
        ImportError: If error=True and tf is not installed.
    """"""
    # Make sure, these are reset after each test case
    # that uses them: del os.environ[""RLLIB_TEST_NO_TF_IMPORT""]
    if ""RLLIB_TEST_NO_TF_IMPORT"" in os.environ:
        logger.warning(""Not importing TensorFlow for test purposes"")
        return None, None, None

    if ""TF_CPP_MIN_LOG_LEVEL"" not in os.environ:
        os.environ[""TF_CPP_MIN_LOG_LEVEL""] = ""3""

    # Try to reuse already imported tf module. This will avoid going through
    # the initial import steps below and thereby switching off v2_behavior
    # (switching off v2 behavior twice breaks all-framework tests for eager).
    was_imported = False
    if ""tensorflow"" in sys.modules:
        tf_module = sys.modules[""tensorflow""]
        was_imported = True

    else:
        try:
            import tensorflow as tf_module
        except ImportError as e:
            if error:
                raise e
            return None, None, None

    # Try ""reducing"" tf to tf.compat.v1.
    try:
        tf1_module = tf_module.compat.v1
        if not was_imported:
            tf1_module.disable_v2_behavior()
    # No compat.v1 -> return tf as is.
    except AttributeError:
        tf1_module = tf_module

    if not hasattr(tf_module, ""__version__""):
        version = 1  # sphinx doc gen
    else:
        version = 2 if ""2."" in tf_module.__version__[:2] else 1

    return tf1_module, tf_module, version"
"def build_trainer(
        name: str,
        *,
        default_config: Optional[TrainerConfigDict] = None,
        validate_config: Optional[Callable[[TrainerConfigDict], None]] = None,
        default_policy: Optional[Type[Policy]] = None,
        get_policy_class: Optional[Callable[[TrainerConfigDict], Optional[Type[
            Policy]]]] = None,
        validate_env: Optional[Callable[[EnvType, EnvContext], None]] = None,
        before_init: Optional[Callable[[Trainer], None]] = None,
        after_init: Optional[Callable[[Trainer], None]] = None,
        before_evaluate_fn: Optional[Callable[[Trainer], None]] = None,
        mixins: Optional[List[type]] = None,
        execution_plan: Optional[Callable[[
            WorkerSet, TrainerConfigDict
        ], Iterable[ResultDict]]] = default_execution_plan) -> Type[Trainer]:
    """"""Helper function for defining a custom trainer.

    Functions will be run in this order to initialize the trainer:
        1. Config setup: validate_config, get_policy
        2. Worker setup: before_init, execution_plan
        3. Post setup: after_init

    Args:
        name (str): name of the trainer (e.g., ""PPO"")
        default_config (Optional[TrainerConfigDict]): The default config dict
            of the algorithm, otherwise uses the Trainer default config.
        validate_config (Optional[Callable[[TrainerConfigDict], None]]):
            Optional callable that takes the config to check for correctness.
            It may mutate the config as needed.
        default_policy (Optional[Type[Policy]]): The default Policy class to
            use if `get_policy_class` returns None.
        get_policy_class (Optional[Callable[
            TrainerConfigDict, Optional[Type[Policy]]]]): Optional callable
            that takes a config and returns the policy class or None. If None
            is returned, will use `default_policy` (which must be provided
            then).
        validate_env (Optional[Callable[[EnvType, EnvContext], None]]):
            Optional callable to validate the generated environment (only
            on worker=0).
        before_init (Optional[Callable[[Trainer], None]]): Optional callable to
            run before anything is constructed inside Trainer (Workers with
            Policies, execution plan, etc..). Takes the Trainer instance as
            argument.
        after_init (Optional[Callable[[Trainer], None]]): Optional callable to
            run at the end of trainer init (after all Workers and the exec.
            plan have been constructed). Takes the Trainer instance as
            argument.
        before_evaluate_fn (Optional[Callable[[Trainer], None]]): Callback to
            run before evaluation. This takes the trainer instance as argument.
        mixins (list): list of any class mixins for the returned trainer class.
            These mixins will be applied in order and will have higher
            precedence than the Trainer class.
        execution_plan (Optional[Callable[[WorkerSet, TrainerConfigDict],
            Iterable[ResultDict]]]): Optional callable that sets up the
            distributed execution workflow.

    Returns:
        Type[Trainer]: A Trainer sub-class configured by the specified args.
    """"""

    original_kwargs = locals().copy()
    base = add_mixins(Trainer, mixins)

    class trainer_cls(base):
        _name = name
        _default_config = default_config or COMMON_CONFIG
        _policy_class = default_policy

        def __init__(self, config=None, env=None, logger_creator=None):
            Trainer.__init__(self, config, env, logger_creator)

        def _init(self, config: TrainerConfigDict,
                  env_creator: Callable[[EnvConfigDict], EnvType]):
            # Validate config via custom validation function.
            if validate_config:
                validate_config(config)

            # No `get_policy_class` function.
            if get_policy_class is None:
                # Default_policy must be provided (unless in multi-agent mode,
                # where each policy can have its own default policy class.
                if not config[""multiagent""][""policies""]:
                    assert default_policy is not None
                self._policy_class = default_policy
            # Query the function for a class to use.
            else:
                self._policy_class = get_policy_class(config)
                # If None returned, use default policy (must be provided).
                if self._policy_class is None:
                    assert default_policy is not None
                    self._policy_class = default_policy

            if before_init:
                before_init(self)

            # Creating all workers (excluding evaluation workers).
            self.workers = self._make_workers(
                env_creator=env_creator,
                validate_env=validate_env,
                policy_class=self._policy_class,
                config=config,
                num_workers=self.config[""num_workers""])
            self.execution_plan = execution_plan
            self.train_exec_impl = execution_plan(self.workers, config)

            if after_init:
                after_init(self)

        @override(Trainer)
        def step(self):
            res = next(self.train_exec_impl)
            return res

        @override(Trainer)
        def _before_evaluate(self):
            if before_evaluate_fn:
                before_evaluate_fn(self)

        @override(Trainer)
        def __getstate__(self):
            state = Trainer.__getstate__(self)
            state[""train_exec_impl""] = (
                self.train_exec_impl.shared_metrics.get().save())
            return state

        @override(Trainer)
        def __setstate__(self, state):
            Trainer.__setstate__(self, state)
            self.train_exec_impl.shared_metrics.get().restore(
                state[""train_exec_impl""])

        @staticmethod
        @override(Trainer)
        def with_updates(**overrides) -> Type[Trainer]:
            """"""Build a copy of this trainer class with the specified overrides.

            Keyword Args:
                overrides (dict): use this to override any of the arguments
                    originally passed to build_trainer() for this policy.

            Returns:
                Type[Trainer]: A the Trainer sub-class using `original_kwargs`
                    and `overrides`.

            Examples:
                >>> MyClass = SomeOtherClass.with_updates({""name"": ""Mine""})
                >>> issubclass(MyClass, SomeOtherClass)
                ... False
                >>> issubclass(MyClass, Trainer)
                ... True
            """"""
            return build_trainer(**dict(original_kwargs, **overrides))

    trainer_cls.__name__ = name
    trainer_cls.__qualname__ = name
    return trainer_cls","def build_trainer(
        name: str,
        *,
        default_config: Optional[TrainerConfigDict] = None,
        validate_config: Optional[Callable[[TrainerConfigDict], None]] = None,
        default_policy: Optional[Type[Policy]] = None,
        get_policy_class: Optional[Callable[[TrainerConfigDict], Optional[Type[
            Policy]]]] = None,
        validate_env: Optional[Callable[[EnvType, EnvContext], None]] = None,
        before_init: Optional[Callable[[Trainer], None]] = None,
        after_init: Optional[Callable[[Trainer], None]] = None,
        before_evaluate_fn: Optional[Callable[[Trainer], None]] = None,
        mixins: Optional[List[type]] = None,
        execution_plan: Optional[Callable[[
            WorkerSet, TrainerConfigDict
        ], Iterable[ResultDict]]] = default_execution_plan) -> Type[Trainer]:
    """"""Helper function for defining a custom trainer.

    Functions will be run in this order to initialize the trainer:
        1. Config setup: validate_config, get_policy
        2. Worker setup: before_init, execution_plan
        3. Post setup: after_init

    Args:
        name (str): name of the trainer (e.g., ""PPO"")
        default_config (Optional[TrainerConfigDict]): The default config dict
            of the algorithm, otherwise uses the Trainer default config.
        validate_config (Optional[Callable[[TrainerConfigDict], None]]):
            Optional callable that takes the config to check for correctness.
            It may mutate the config as needed.
        default_policy (Optional[Type[Policy]]): The default Policy class to
            use.
        get_policy_class (Optional[Callable[
            TrainerConfigDict, Optional[Type[Policy]]]]): Optional callable
            that takes a config and returns the policy class or None. If None
            is returned, will use `default_policy` (which must be provided
            then).
        validate_env (Optional[Callable[[EnvType, EnvContext], None]]):
            Optional callable to validate the generated environment (only
            on worker=0).
        before_init (Optional[Callable[[Trainer], None]]): Optional callable to
            run before anything is constructed inside Trainer (Workers with
            Policies, execution plan, etc..). Takes the Trainer instance as
            argument.
        after_init (Optional[Callable[[Trainer], None]]): Optional callable to
            run at the end of trainer init (after all Workers and the exec.
            plan have been constructed). Takes the Trainer instance as
            argument.
        before_evaluate_fn (Optional[Callable[[Trainer], None]]): Callback to
            run before evaluation. This takes the trainer instance as argument.
        mixins (list): list of any class mixins for the returned trainer class.
            These mixins will be applied in order and will have higher
            precedence than the Trainer class.
        execution_plan (Optional[Callable[[WorkerSet, TrainerConfigDict],
            Iterable[ResultDict]]]): Optional callable that sets up the
            distributed execution workflow.

    Returns:
        Type[Trainer]: A Trainer sub-class configured by the specified args.
    """"""

    original_kwargs = locals().copy()
    base = add_mixins(Trainer, mixins)

    class trainer_cls(base):
        _name = name
        _default_config = default_config or COMMON_CONFIG
        _policy_class = default_policy

        def __init__(self, config=None, env=None, logger_creator=None):
            Trainer.__init__(self, config, env, logger_creator)

        def _init(self, config: TrainerConfigDict,
                  env_creator: Callable[[EnvConfigDict], EnvType]):
            # Validate config via custom validation function.
            if validate_config:
                validate_config(config)

            # No `get_policy_class` function.
            if get_policy_class is None:
                # Default_policy must be provided (unless in multi-agent mode,
                # where each policy can have its own default policy class.
                if not config[""multiagent""][""policies""]:
                    assert default_policy is not None
                self._policy_class = default_policy
            # Query the function for a class to use.
            else:
                self._policy_class = get_policy_class(config)
                # If None returned, use default policy (must be provided).
                if self._policy_class is None:
                    assert default_policy is not None
                    self._policy_class = default_policy

            if before_init:
                before_init(self)

            # Creating all workers (excluding evaluation workers).
            self.workers = self._make_workers(
                env_creator=env_creator,
                validate_env=validate_env,
                policy_class=self._policy_class,
                config=config,
                num_workers=self.config[""num_workers""])
            self.execution_plan = execution_plan
            self.train_exec_impl = execution_plan(self.workers, config)

            if after_init:
                after_init(self)

        @override(Trainer)
        def step(self):
            res = next(self.train_exec_impl)
            return res

        @override(Trainer)
        def _before_evaluate(self):
            if before_evaluate_fn:
                before_evaluate_fn(self)

        @override(Trainer)
        def __getstate__(self):
            state = Trainer.__getstate__(self)
            state[""train_exec_impl""] = (
                self.train_exec_impl.shared_metrics.get().save())
            return state

        @override(Trainer)
        def __setstate__(self, state):
            Trainer.__setstate__(self, state)
            self.train_exec_impl.shared_metrics.get().restore(
                state[""train_exec_impl""])

        @staticmethod
        @override(Trainer)
        def with_updates(**overrides) -> Type[Trainer]:
            """"""Build a copy of this trainer class with the specified overrides.

            Keyword Args:
                overrides (dict): use this to override any of the arguments
                    originally passed to build_trainer() for this policy.

            Returns:
                Type[Trainer]: A the Trainer sub-class using `original_kwargs`
                    and `overrides`.

            Examples:
                >>> MyClass = SomeOtherClass.with_updates({""name"": ""Mine""})
                >>> issubclass(MyClass, SomeOtherClass)
                ... False
                >>> issubclass(MyClass, Trainer)
                ... True
            """"""
            return build_trainer(**dict(original_kwargs, **overrides))

    trainer_cls.__name__ = name
    trainer_cls.__qualname__ = name
    return trainer_cls"
"    def __init__(self,
                 *,
                 env_creator: Optional[Callable[[EnvContext], EnvType]] = None,
                 validate_env: Optional[Callable[[EnvType], None]] = None,
                 policy_class: Optional[Type[Policy]] = None,
                 trainer_config: Optional[TrainerConfigDict] = None,
                 num_workers: int = 0,
                 logdir: Optional[str] = None,
                 _setup: bool = True):
        """"""Create a new WorkerSet and initialize its workers.

        Args:
            env_creator (Optional[Callable[[EnvContext], EnvType]]): Function
                that returns env given env config.
            validate_env (Optional[Callable[[EnvType], None]]): Optional
                callable to validate the generated environment (only on
                worker=0).
            policy (Optional[Type[Policy]]): A rllib.policy.Policy class.
            trainer_config (Optional[TrainerConfigDict]): Optional dict that
                extends the common config of the Trainer class.
            num_workers (int): Number of remote rollout workers to create.
            logdir (Optional[str]): Optional logging directory for workers.
            _setup (bool): Whether to setup workers. This is only for testing.
        """"""

        if not trainer_config:
            from ray.rllib.agents.trainer import COMMON_CONFIG
            trainer_config = COMMON_CONFIG

        self._env_creator = env_creator
        self._policy_class = policy_class
        self._remote_config = trainer_config
        self._logdir = logdir

        if _setup:
            self._local_config = merge_dicts(
                trainer_config,
                {""tf_session_args"": trainer_config[""local_tf_session_args""]})

            # Create a number of remote workers.
            self._remote_workers = []
            self.add_workers(num_workers)

            # If num_workers > 0, get the action_spaces and observation_spaces
            # to not be forced to create an Env on the driver.
            if self._remote_workers:
                remote_spaces = ray.get(self.remote_workers(
                )[0].foreach_policy.remote(
                    lambda p, pid: (pid, p.observation_space, p.action_space)))
                spaces = {
                    e[0]: (getattr(e[1], ""original_space"", e[1]), e[2])
                    for e in remote_spaces
                }
            else:
                spaces = None

            # Always create a local worker.
            self._local_worker = self._make_worker(
                cls=RolloutWorker,
                env_creator=env_creator,
                validate_env=validate_env,
                policy_cls=self._policy_class,
                worker_index=0,
                num_workers=num_workers,
                config=self._local_config,
                spaces=spaces,
            )","    def __init__(self,
                 *,
                 env_creator: Optional[Callable[[EnvContext], EnvType]] = None,
                 validate_env: Optional[Callable[[EnvType], None]] = None,
                 policy_class: Optional[Type[Policy]] = None,
                 trainer_config: Optional[TrainerConfigDict] = None,
                 num_workers: int = 0,
                 logdir: Optional[str] = None,
                 _setup: bool = True):
        """"""Create a new WorkerSet and initialize its workers.

        Args:
            env_creator (Optional[Callable[[EnvContext], EnvType]]): Function
                that returns env given env config.
            validate_env (Optional[Callable[[EnvType], None]]): Optional
                callable to validate the generated environment (only on
                worker=0).
            policy (Optional[Type[Policy]]): A rllib.policy.Policy class.
            trainer_config (Optional[TrainerConfigDict]): Optional dict that
                extends the common config of the Trainer class.
            num_workers (int): Number of remote rollout workers to create.
            logdir (Optional[str]): Optional logging directory for workers.
            _setup (bool): Whether to setup workers. This is only for testing.
        """"""

        if not trainer_config:
            from ray.rllib.agents.trainer import COMMON_CONFIG
            trainer_config = COMMON_CONFIG

        self._env_creator = env_creator
        self._policy_class = policy_class
        self._remote_config = trainer_config
        self._logdir = logdir

        if _setup:
            self._local_config = merge_dicts(
                trainer_config,
                {""tf_session_args"": trainer_config[""local_tf_session_args""]})

            # Create a number of remote workers.
            self._remote_workers = []
            self.add_workers(num_workers)

            # If num_workers > 0, get the action_spaces and observation_spaces
            # to not be forced to create an Env on the driver.
            if self._remote_workers:
                remote_spaces = ray.get(self.remote_workers(
                )[0].foreach_policy.remote(
                    lambda p, pid: (pid, p.observation_space, p.action_space)))
                spaces = {e[0]: (e[1], e[2]) for e in remote_spaces}
            else:
                spaces = None

            # Always create a local worker.
            self._local_worker = self._make_worker(
                cls=RolloutWorker,
                env_creator=env_creator,
                validate_env=validate_env,
                policy_cls=self._policy_class,
                worker_index=0,
                num_workers=num_workers,
                config=self._local_config,
                spaces=spaces,
            )"
"def memory_summary():
    """"""Returns a formatted string describing memory usage in the cluster.""""""

    import grpc
    from ray.core.generated import node_manager_pb2
    from ray.core.generated import node_manager_pb2_grpc

    # We can ask any Raylet for the global memory info.
    raylet = ray.nodes()[0]
    raylet_address = ""{}:{}"".format(raylet[""NodeManagerAddress""],
                                    ray.nodes()[0][""NodeManagerPort""])
    channel = grpc.insecure_channel(
        raylet_address,
        options=[
            (""grpc.max_send_message_length"", MAX_MESSAGE_LENGTH),
            (""grpc.max_receive_message_length"", MAX_MESSAGE_LENGTH),
        ],
    )
    stub = node_manager_pb2_grpc.NodeManagerServiceStub(channel)
    reply = stub.FormatGlobalMemoryInfo(
        node_manager_pb2.FormatGlobalMemoryInfoRequest(), timeout=30.0)
    return reply.memory_summary","def memory_summary():
    """"""Returns a formatted string describing memory usage in the cluster.""""""

    import grpc
    from ray.core.generated import node_manager_pb2
    from ray.core.generated import node_manager_pb2_grpc

    # We can ask any Raylet for the global memory info.
    raylet = ray.nodes()[0]
    raylet_address = ""{}:{}"".format(raylet[""NodeManagerAddress""],
                                    ray.nodes()[0][""NodeManagerPort""])
    channel = grpc.insecure_channel(raylet_address)
    stub = node_manager_pb2_grpc.NodeManagerServiceStub(channel)
    reply = stub.FormatGlobalMemoryInfo(
        node_manager_pb2.FormatGlobalMemoryInfoRequest(), timeout=30.0)
    return reply.memory_summary"
"def custom_excepthook(type, value, tb):
    # If this is a driver, push the exception to GCS worker table.
    if global_worker.mode == SCRIPT_MODE:
        error_message = """".join(traceback.format_tb(tb))
        worker_id = global_worker.worker_id
        worker_type = ray.gcs_utils.DRIVER
        worker_info = {""exception"": error_message}

        ray.state.state._check_connected()
        ray.state.state.add_worker(worker_id, worker_type, worker_info)
    # Call the normal excepthook.
    normal_excepthook(type, value, tb)","def custom_excepthook(type, value, tb):
    # If this is a driver, push the exception to GCS worker table.
    if global_worker.mode == SCRIPT_MODE:
        error_message = """".join(traceback.format_tb(tb))
        worker_id = global_worker.worker_id
        worker_type = ray.gcs_utils.DRIVER
        worker_info = {""exception"": error_message}

        ray.state.state.add_worker(worker_id, worker_type, worker_info)
    # Call the normal excepthook.
    normal_excepthook(type, value, tb)"
"    def __init__(self,
                 sync_up_template,
                 sync_down_template,
                 delete_template=noop_template):
        """"""Syncs between two directories with the given command.

        Arguments:
            sync_up_template (str): A runnable string template; needs to
                include replacement fields '{source}' and '{target}'.
            sync_down_template (str): A runnable string template; needs to
                include replacement fields '{source}' and '{target}'.
            delete_template (Optional[str]): A runnable string template; needs
                to include replacement field '{target}'. Noop by default.
        """"""
        self._validate_sync_string(sync_up_template)
        self._validate_sync_string(sync_down_template)
        self.sync_up_template = sync_up_template
        self.sync_down_template = sync_down_template
        self.delete_template = delete_template
        self.logfile = None
        self._closed = False
        self.cmd_process = None","    def __init__(self,
                 sync_up_template,
                 sync_down_template,
                 delete_template=noop_template):
        """"""Syncs between two directories with the given command.

        Arguments:
            sync_up_template (str): A runnable string template; needs to
                include replacement fields '{source}' and '{target}'.
            sync_down_template (str): A runnable string template; needs to
                include replacement fields '{source}' and '{target}'.
            delete_template (Optional[str]): A runnable string template; needs
                to include replacement field '{target}'. Noop by default.
        """"""
        self._validate_sync_string(sync_up_template)
        self._validate_sync_string(sync_down_template)
        self.sync_up_template = sync_up_template
        self.sync_down_template = sync_down_template
        self.delete_template = delete_template
        self.logfile = None
        self.cmd_process = None"
"    def set_logdir(self, logdir):
        """"""Sets the directory to log sync execution output in.

        Args:
            logdir (str): Log directory.
        """"""
        self.logfile = tempfile.NamedTemporaryFile(
            prefix=""log_sync_out"", dir=logdir, suffix="".log"", delete=False)
        self._closed = False","    def set_logdir(self, logdir):
        """"""Sets the directory to log sync execution output in.

        Args:
            logdir (str): Log directory.
        """"""
        self.logfile = tempfile.NamedTemporaryFile(
            prefix=""log_sync_out"", dir=logdir, suffix="".log"", delete=False)"
"    def delete(self, target):
        if self.is_running:
            logger.warning(""Last sync client cmd still in progress, skipping."")
            return False
        final_cmd = self.delete_template.format(target=quote(target))
        logger.debug(""Running delete: {}"".format(final_cmd))
        self.cmd_process = subprocess.Popen(
            final_cmd,
            shell=True,
            stderr=subprocess.PIPE,
            stdout=self._get_logfile())
        return True","    def delete(self, target):
        if self.is_running:
            logger.warning(""Last sync client cmd still in progress, skipping."")
            return False
        final_cmd = self.delete_template.format(target=quote(target))
        logger.debug(""Running delete: {}"".format(final_cmd))
        self.cmd_process = subprocess.Popen(
            final_cmd, shell=True, stderr=subprocess.PIPE, stdout=self.logfile)
        return True"
"    def _execute(self, sync_template, source, target):
        """"""Executes sync_template on source and target.""""""
        if self.is_running:
            logger.warning(""Last sync client cmd still in progress, skipping."")
            return False
        final_cmd = sync_template.format(
            source=quote(source), target=quote(target))
        logger.debug(""Running sync: {}"".format(final_cmd))
        self.cmd_process = subprocess.Popen(
            final_cmd,
            shell=True,
            stderr=subprocess.PIPE,
            stdout=self._get_logfile())
        return True","    def _execute(self, sync_template, source, target):
        """"""Executes sync_template on source and target.""""""
        if self.is_running:
            logger.warning(""Last sync client cmd still in progress, skipping."")
            return False
        final_cmd = sync_template.format(
            source=quote(source), target=quote(target))
        logger.debug(""Running sync: {}"".format(final_cmd))
        self.cmd_process = subprocess.Popen(
            final_cmd, shell=True, stderr=subprocess.PIPE, stdout=self.logfile)
        return True"
"    def on_trial_complete(self, iteration: int, trials: List[""Trial""],
                          trial: ""Trial"", **info):
        trial_syncer = self._get_trial_syncer(trial)
        if NODE_IP in trial.last_result:
            trainable_ip = trial.last_result[NODE_IP]
        else:
            trainable_ip = ray.get(trial.runner.get_current_ip.remote())
        trial_syncer.set_worker_ip(trainable_ip)
        trial_syncer.sync_down_if_needed()
        trial_syncer.close()","    def on_trial_complete(self, iteration: int, trials: List[""Trial""],
                          trial: ""Trial"", **info):
        trial_syncer = self._get_trial_syncer(trial)
        if NODE_IP in trial.last_result:
            trainable_ip = trial.last_result[NODE_IP]
        else:
            trainable_ip = ray.get(trial.runner.get_current_ip.remote())
        trial_syncer.set_worker_ip(trainable_ip)
        trial_syncer.sync_down_if_needed()"
"def mock_storage_client():
    """"""Mocks storage client that treats a local dir as durable storage.""""""
    client = get_sync_client(LOCAL_SYNC_TEMPLATE, LOCAL_DELETE_TEMPLATE)
    path = os.path.join(ray.utils.get_user_temp_dir(),
                        f""mock-client-{uuid.uuid4().hex[:4]}"")
    os.makedirs(path, exist_ok=True)
    client.set_logdir(path)
    return client","def mock_storage_client():
    """"""Mocks storage client that treats a local dir as durable storage.""""""
    return get_sync_client(LOCAL_SYNC_TEMPLATE, LOCAL_DELETE_TEMPLATE)"
"    async def _do_long_poll(self):
        while True:
            try:
                updates: Dict[str, UpdatedObject] = await self._poll_once()
                self._update(updates)
                logger.debug(f""LongPollerClient received udpates: {updates}"")
                for key, updated_object in updates.items():
                    # NOTE(simon):
                    # This blocks the loop from doing another poll. Consider
                    # use loop.create_task here or poll first then call the
                    # callbacks.
                    callback = self.key_listeners[key]
                    await callback(updated_object.object_snapshot)
            except ray.exceptions.RayActorError:
                # This can happen during shutdown where the controller is
                # intentionally killed, the client should just gracefully
                # exit.
                logger.debug(""LongPollerClient failed to connect to host. ""
                             ""Shutting down."")
                break","    async def _do_long_poll(self):
        while True:
            updates: Dict[str, UpdatedObject] = await self._poll_once()
            self._update(updates)
            logger.debug(f""LongPollerClient received updates: {updates}"")
            for key, updated_object in updates.items():
                # NOTE(simon): This blocks the loop from doing another poll.
                # Consider use loop.create_task here or poll first then call
                # the callbacks.
                callback = self.key_listeners[key]
                await callback(updated_object.object_snapshot)"
"    async def _get_actor(actor):
        actor = dict(actor)
        worker_id = actor[""address""][""workerId""]
        core_worker_stats = DataSource.core_worker_stats.get(worker_id, {})
        actor_constructor = core_worker_stats.get(""actorTitle"",
                                                  ""Unknown actor constructor"")
        actor[""actorConstructor""] = actor_constructor
        actor.update(core_worker_stats)

        # TODO(fyrestone): remove this, give a link from actor
        # info to worker info in front-end.
        node_id = actor[""address""][""rayletId""]
        pid = core_worker_stats.get(""pid"")
        node_physical_stats = DataSource.node_physical_stats.get(node_id, {})
        actor_process_stats = None
        actor_process_gpu_stats = None
        if pid:
            for process_stats in node_physical_stats.get(""workers"", []):
                if process_stats[""pid""] == pid:
                    actor_process_stats = process_stats
                    break

            for gpu_stats in node_physical_stats.get(""gpus"", []):
                for process in gpu_stats.get(""processes"", []):
                    if process[""pid""] == pid:
                        actor_process_gpu_stats = gpu_stats
                        break
                if actor_process_gpu_stats is not None:
                    break

        actor[""gpus""] = actor_process_gpu_stats
        actor[""processStats""] = actor_process_stats

        return actor","    async def _get_actor(actor):
        actor = dict(actor)
        worker_id = actor[""address""][""workerId""]
        core_worker_stats = DataSource.core_worker_stats.get(worker_id, {})
        actor_constructor = core_worker_stats.get(""actorTitle"",
                                                  ""Unknown actor constructor"")
        actor[""actorConstructor""] = actor_constructor
        actor.update(core_worker_stats)

        # TODO(fyrestone): remove this, give a link from actor
        # info to worker info in front-end.
        node_id = actor[""address""][""rayletId""]
        pid = core_worker_stats.get(""pid"")
        node_physical_stats = DataSource.node_physical_stats.get(node_id, {})
        actor_process_stats = None
        actor_process_gpu_stats = None
        if pid:
            for process_stats in node_physical_stats.get(""workers""):
                if process_stats[""pid""] == pid:
                    actor_process_stats = process_stats
                    break

            for gpu_stats in node_physical_stats.get(""gpus""):
                for process in gpu_stats.get(""processes"", []):
                    if process[""pid""] == pid:
                        actor_process_gpu_stats = gpu_stats
                        break
                if actor_process_gpu_stats is not None:
                    break

        actor[""gpus""] = actor_process_gpu_stats
        actor[""processStats""] = actor_process_stats

        return actor"
"def get_address_info_from_redis_helper(redis_address,
                                       node_ip_address,
                                       redis_password=None):
    redis_ip_address, redis_port = redis_address.split("":"")
    # Get node table from global state accessor.
    global_state = ray.state.GlobalState()
    global_state._initialize_global_state(redis_address, redis_password)
    client_table = global_state.node_table()
    if len(client_table) == 0:
        raise RuntimeError(
            ""Redis has started but no raylets have registered yet."")

    relevant_client = None
    for client_info in client_table:
        client_node_ip_address = client_info[""NodeManagerAddress""]
        if (client_node_ip_address == node_ip_address
                or (client_node_ip_address == ""127.0.0.1""
                    and redis_ip_address == get_node_ip_address())
                or client_node_ip_address == redis_ip_address):
            relevant_client = client_info
            break
    if relevant_client is None:
        raise RuntimeError(
            f""This node has an IP address of {node_ip_address}, and Ray ""
            ""expects this IP address to be either the Redis address or one of""
            f"" the Raylet addresses. Connected to Redis at {redis_address} and""
            "" found raylets at ""
            f""{', '.join(c['NodeManagerAddress'] for c in client_table)} but ""
            f""none of these match this node's IP {node_ip_address}. Are any of""
            "" these actually a different IP address for the same node?""
            ""You might need to provide --node-ip-address to specify the IP ""
            ""address that the head should use when sending to this node."")

    return {
        ""object_store_address"": relevant_client[""ObjectStoreSocketName""],
        ""raylet_socket_name"": relevant_client[""RayletSocketName""],
        ""node_manager_port"": relevant_client[""NodeManagerPort""],
    }","def get_address_info_from_redis_helper(redis_address,
                                       node_ip_address,
                                       redis_password=None):
    redis_ip_address, redis_port = redis_address.split("":"")
    # Get node table from global state accessor.
    global_state = ray.state.GlobalState()
    global_state._initialize_global_state(redis_address, redis_password)
    client_table = global_state.node_table()
    if len(client_table) == 0:
        raise RuntimeError(
            ""Redis has started but no raylets have registered yet."")

    relevant_client = None
    for client_info in client_table:
        client_node_ip_address = client_info[""NodeManagerAddress""]
        if (client_node_ip_address == node_ip_address
                or (client_node_ip_address == ""127.0.0.1""
                    and redis_ip_address == get_node_ip_address())):
            relevant_client = client_info
            break
    if relevant_client is None:
        raise RuntimeError(
            f""This node has an IP address of {node_ip_address}, and Ray ""
            ""expects this IP address to be either the Redis address or one of""
            f"" the Raylet addresses. Connected to Redis at {redis_address} and""
            "" found raylets at ""
            f""{', '.join(c['NodeManagerAddress'] for c in client_table)} but ""
            f""none of these match this node's IP {node_ip_address}. Are any of""
            "" these actually a different IP address for the same node?""
            ""You might need to provide --node-ip-address to specify the IP ""
            ""address that the head should use when sending to this node."")

    return {
        ""object_store_address"": relevant_client[""ObjectStoreSocketName""],
        ""raylet_socket_name"": relevant_client[""RayletSocketName""],
        ""node_manager_port"": relevant_client[""NodeManagerPort""],
    }"
"    def __init__(self):
        self.indent_level = 0

        self._verbosity = 0
        self._verbosity_overriden = False
        self._color_mode = ""auto""
        self._log_style = ""record""
        self.pretty = False
        self.interactive = False

        # store whatever colorful has detected for future use if
        # the color ouput is toggled (colorful detects # of supported colors,
        # so it has some non-trivial logic to determine this)
        self._autodetected_cf_colormode = cf.colorful.colormode
        self.set_format()","    def __init__(self):
        self.indent_level = 0

        self._verbosity = 0
        self._color_mode = ""auto""
        self._log_style = ""record""
        self.pretty = False
        self.interactive = False

        # store whatever colorful has detected for future use if
        # the color ouput is toggled (colorful detects # of supported colors,
        # so it has some non-trivial logic to determine this)
        self._autodetected_cf_colormode = cf.colorful.colormode
        self.set_format()"
"    def verbosity(self):
        if self._verbosity_overriden:
            return self._verbosity
        elif not self.pretty:
            return 999
        return self._verbosity","    def verbosity(self):
        if not self.pretty:
            return 999
        return self._verbosity"
"    def _set_verbosity(self, x):
        self._verbosity = x
        self._verbosity_overriden = True","    def _set_verbosity(self, x):
        self._verbosity = x"
"def rsync(config_file: str,
          source: Optional[str],
          target: Optional[str],
          override_cluster_name: Optional[str],
          down: bool,
          ip_address: Optional[str] = None,
          use_internal_ip: bool = False,
          no_config_cache: bool = False,
          all_nodes: bool = False,
          _runner: ModuleType = subprocess) -> None:
    """"""Rsyncs files.

    Arguments:
        config_file: path to the cluster yaml
        source: source dir
        target: target dir
        override_cluster_name: set the name of the cluster
        down: whether we're syncing remote -> local
        ip_address (str): Address of node. Raise Exception
            if both ip_address and 'all_nodes' are provided.
        use_internal_ip (bool): Whether the provided ip_address is
            public or private.
        all_nodes: whether to sync worker nodes in addition to the head node
    """"""
    if bool(source) != bool(target):
        cli_logger.abort(
            ""Expected either both a source and a target, or neither."")

    assert bool(source) == bool(target), (
        ""Must either provide both or neither source and target."")

    if ip_address and all_nodes:
        cli_logger.abort(""Cannot provide both ip_address and 'all_nodes'."")

    config = yaml.safe_load(open(config_file).read())
    if override_cluster_name is not None:
        config[""cluster_name""] = override_cluster_name
    config = _bootstrap_config(config, no_config_cache=no_config_cache)

    is_file_mount = False
    if source and target:
        for remote_mount in config.get(""file_mounts"", {}).keys():
            if (source if down else target).startswith(remote_mount):
                is_file_mount = True
                break

    provider = _get_node_provider(config[""provider""], config[""cluster_name""])

    def rsync_to_node(node_id, is_head_node):
        updater = NodeUpdaterThread(
            node_id=node_id,
            provider_config=config[""provider""],
            provider=provider,
            auth_config=config[""auth""],
            cluster_name=config[""cluster_name""],
            file_mounts=config[""file_mounts""],
            initialization_commands=[],
            setup_commands=[],
            ray_start_commands=[],
            runtime_hash="""",
            use_internal_ip=use_internal_ip,
            process_runner=_runner,
            file_mounts_contents_hash="""",
            is_head_node=is_head_node,
            rsync_options={
                ""rsync_exclude"": config.get(""rsync_exclude""),
                ""rsync_filter"": config.get(""rsync_filter"")
            },
            docker_config=config.get(""docker""))
        if down:
            rsync = updater.rsync_down
        else:
            rsync = updater.rsync_up

        if source and target:
            # print rsync progress for single file rsync
            if cli_logger.verbosity > 0:
                cmd_output_util.set_output_redirected(False)
                set_rsync_silent(False)
            rsync(source, target, is_file_mount)
        else:
            updater.sync_file_mounts(rsync)

    nodes = []
    head_node = _get_head_node(
        config, config_file, override_cluster_name, create_if_needed=False)
    if ip_address:
        nodes = [
            provider.get_node_id(ip_address, use_internal_ip=use_internal_ip)
        ]
    else:
        nodes = [head_node]
        if all_nodes:
            nodes.extend(_get_worker_nodes(config, override_cluster_name))

    for node_id in nodes:
        rsync_to_node(node_id, is_head_node=(node_id == head_node))","def rsync(config_file: str,
          source: Optional[str],
          target: Optional[str],
          override_cluster_name: Optional[str],
          down: bool,
          ip_address: Optional[str] = None,
          use_internal_ip: bool = False,
          no_config_cache: bool = False,
          all_nodes: bool = False,
          _runner: ModuleType = subprocess) -> None:
    """"""Rsyncs files.

    Arguments:
        config_file: path to the cluster yaml
        source: source dir
        target: target dir
        override_cluster_name: set the name of the cluster
        down: whether we're syncing remote -> local
        ip_address (str): Address of node. Raise Exception
            if both ip_address and 'all_nodes' are provided.
        use_internal_ip (bool): Whether the provided ip_address is
            public or private.
        all_nodes: whether to sync worker nodes in addition to the head node
    """"""
    if bool(source) != bool(target):
        cli_logger.abort(
            ""Expected either both a source and a target, or neither."")

    assert bool(source) == bool(target), (
        ""Must either provide both or neither source and target."")

    if ip_address and all_nodes:
        cli_logger.abort(""Cannot provide both ip_address and 'all_nodes'."")

    config = yaml.safe_load(open(config_file).read())
    if override_cluster_name is not None:
        config[""cluster_name""] = override_cluster_name
    config = _bootstrap_config(config, no_config_cache=no_config_cache)

    is_file_mount = False
    if source and target:
        for remote_mount in config.get(""file_mounts"", {}).keys():
            if (source if down else target).startswith(remote_mount):
                is_file_mount = True
                break

    provider = _get_node_provider(config[""provider""], config[""cluster_name""])

    def rsync_to_node(node_id, is_head_node):
        updater = NodeUpdaterThread(
            node_id=node_id,
            provider_config=config[""provider""],
            provider=provider,
            auth_config=config[""auth""],
            cluster_name=config[""cluster_name""],
            file_mounts=config[""file_mounts""],
            initialization_commands=[],
            setup_commands=[],
            ray_start_commands=[],
            runtime_hash="""",
            use_internal_ip=use_internal_ip,
            process_runner=_runner,
            file_mounts_contents_hash="""",
            is_head_node=is_head_node,
            rsync_options={
                ""rsync_exclude"": config.get(""rsync_exclude""),
                ""rsync_filter"": config.get(""rsync_filter"")
            },
            docker_config=config.get(""docker""))
        if down:
            rsync = updater.rsync_down
        else:
            rsync = updater.rsync_up

        if source and target:
            # print rsync progress for single file rsync
            cmd_output_util.set_output_redirected(False)
            set_rsync_silent(False)
            rsync(source, target, is_file_mount)
        else:
            updater.sync_file_mounts(rsync)

    nodes = []
    head_node = _get_head_node(
        config, config_file, override_cluster_name, create_if_needed=False)
    if ip_address:
        nodes = [
            provider.get_node_id(ip_address, use_internal_ip=use_internal_ip)
        ]
    else:
        nodes = [head_node]
        if all_nodes:
            nodes.extend(_get_worker_nodes(config, override_cluster_name))

    for node_id in nodes:
        rsync_to_node(node_id, is_head_node=(node_id == head_node))"
"    def rsync_to_node(node_id, is_head_node):
        updater = NodeUpdaterThread(
            node_id=node_id,
            provider_config=config[""provider""],
            provider=provider,
            auth_config=config[""auth""],
            cluster_name=config[""cluster_name""],
            file_mounts=config[""file_mounts""],
            initialization_commands=[],
            setup_commands=[],
            ray_start_commands=[],
            runtime_hash="""",
            use_internal_ip=use_internal_ip,
            process_runner=_runner,
            file_mounts_contents_hash="""",
            is_head_node=is_head_node,
            rsync_options={
                ""rsync_exclude"": config.get(""rsync_exclude""),
                ""rsync_filter"": config.get(""rsync_filter"")
            },
            docker_config=config.get(""docker""))
        if down:
            rsync = updater.rsync_down
        else:
            rsync = updater.rsync_up

        if source and target:
            # print rsync progress for single file rsync
            if cli_logger.verbosity > 0:
                cmd_output_util.set_output_redirected(False)
                set_rsync_silent(False)
            rsync(source, target, is_file_mount)
        else:
            updater.sync_file_mounts(rsync)","    def rsync_to_node(node_id, is_head_node):
        updater = NodeUpdaterThread(
            node_id=node_id,
            provider_config=config[""provider""],
            provider=provider,
            auth_config=config[""auth""],
            cluster_name=config[""cluster_name""],
            file_mounts=config[""file_mounts""],
            initialization_commands=[],
            setup_commands=[],
            ray_start_commands=[],
            runtime_hash="""",
            use_internal_ip=use_internal_ip,
            process_runner=_runner,
            file_mounts_contents_hash="""",
            is_head_node=is_head_node,
            rsync_options={
                ""rsync_exclude"": config.get(""rsync_exclude""),
                ""rsync_filter"": config.get(""rsync_filter"")
            },
            docker_config=config.get(""docker""))
        if down:
            rsync = updater.rsync_down
        else:
            rsync = updater.rsync_up

        if source and target:
            # print rsync progress for single file rsync
            cmd_output_util.set_output_redirected(False)
            set_rsync_silent(False)
            rsync(source, target, is_file_mount)
        else:
            updater.sync_file_mounts(rsync)"
"    def __init__(self,
                 local_dir: str,
                 remote_dir: str,
                 sync_client: Optional[SyncClient] = None):
        configure_logging(
            log_style=""record"",
            verbosity=env_integer(""TUNE_SYNCER_VERBOSITY"", 0))
        self.local_ip = services.get_node_ip_address()
        self.worker_ip = None

        sync_client = sync_client or DockerSyncClient()
        sync_client.configure(self._cluster_config_file)

        super(NodeSyncer, self).__init__(local_dir, remote_dir, sync_client)","    def __init__(self,
                 local_dir: str,
                 remote_dir: str,
                 sync_client: Optional[SyncClient] = None):
        self.local_ip = services.get_node_ip_address()
        self.worker_ip = None

        sync_client = sync_client or DockerSyncClient()
        sync_client.configure(self._cluster_config_file)

        super(NodeSyncer, self).__init__(local_dir, remote_dir, sync_client)"
"    def reset_trial(self,
                    trial,
                    new_config,
                    new_experiment_tag,
                    logger_creator=None):
        """"""Tries to invoke `Trainable.reset()` to reset trial.

        Args:
            trial (Trial): Trial to be reset.
            new_config (dict): New configuration for Trial trainable.
            new_experiment_tag (str): New experiment name for trial.
            logger_creator (Optional[Callable[[Dict], Logger]]): Function
                that instantiates a logger on the actor process.

        Returns:
            True if `reset_config` is successful else False.
        """"""
        trial.set_experiment_tag(new_experiment_tag)
        trial.set_config(new_config)
        trainable = trial.runner
        with self._change_working_directory(trial):
            with warn_if_slow(""reset""):
                try:
                    reset_val = ray.get(
                        trainable.reset.remote(new_config, logger_creator),
                        timeout=DEFAULT_GET_TIMEOUT)
                except GetTimeoutError:
                    logger.exception(""Trial %s: reset timed out."", trial)
                    return False
        return reset_val","    def reset_trial(self,
                    trial,
                    new_config,
                    new_experiment_tag,
                    logger_creator=None):
        """"""Tries to invoke `Trainable.reset()` to reset trial.

        Args:
            trial (Trial): Trial to be reset.
            new_config (dict): New configuration for Trial trainable.
            new_experiment_tag (str): New experiment name for trial.
            logger_creator (Callable[[Dict], Logger]): A function that
                instantiates a logger on the actor process.

        Returns:
            True if `reset_config` is successful else False.
        """"""
        trial.set_experiment_tag(new_experiment_tag)
        trial.set_config(new_config)
        trainable = trial.runner
        with self._change_working_directory(trial):
            with warn_if_slow(""reset""):
                try:
                    reset_val = ray.get(
                        trainable.reset.remote(new_config, logger_creator),
                        timeout=DEFAULT_GET_TIMEOUT)
                except GetTimeoutError:
                    logger.exception(""Trial %s: reset timed out."", trial)
                    return False
        return reset_val"
"    def reset(self, new_config, logger_creator=None):
        """"""Resets trial for use with new config.

        Subclasses should override reset_config() to actually
        reset actor behavior for the new config.""""""
        self.config = new_config

        self._result_logger.flush()
        self._result_logger.close()

        if logger_creator:
            logger.debug(""Logger reset."")
            self._create_logger(new_config.copy(), logger_creator)
        else:
            logger.debug(""Did not reset logger. Got: ""
                         f""trainable.reset(logger_creator={logger_creator})."")

        stdout_file = new_config.pop(STDOUT_FILE, None)
        stderr_file = new_config.pop(STDERR_FILE, None)

        self._close_logfiles()
        self._open_logfiles(stdout_file, stderr_file)

        success = self.reset_config(new_config)
        if not success:
            return False

        # Reset attributes. Will be overwritten by `restore` if a checkpoint
        # is provided.
        self._iteration = 0
        self._time_total = 0.0
        self._timesteps_total = None
        self._episodes_total = None
        self._time_since_restore = 0.0
        self._timesteps_since_restore = 0
        self._iterations_since_restore = 0
        self._restored = False

        return True","    def reset(self, new_config, logger_creator=None):
        """"""Resets trial for use with new config.

        Subclasses should override reset_config() to actually
        reset actor behavior for the new config.""""""
        self.config = new_config

        self._result_logger.flush()
        self._result_logger.close()

        self._create_logger(new_config.copy(), logger_creator)

        stdout_file = new_config.pop(STDOUT_FILE, None)
        stderr_file = new_config.pop(STDERR_FILE, None)

        self._close_logfiles()
        self._open_logfiles(stdout_file, stderr_file)

        success = self.reset_config(new_config)
        if not success:
            return False

        # Reset attributes. Will be overwritten by `restore` if a checkpoint
        # is provided.
        self._iteration = 0
        self._time_total = 0.0
        self._timesteps_total = None
        self._episodes_total = None
        self._time_since_restore = 0.0
        self._timesteps_since_restore = 0
        self._iterations_since_restore = 0
        self._restored = False

        return True"
"def _bootstrap_config(config: Dict[str, Any],
                      no_config_cache: bool = False) -> Dict[str, Any]:
    config = prepare_config(config)

    hasher = hashlib.sha1()
    hasher.update(json.dumps([config], sort_keys=True).encode(""utf-8""))
    cache_key = os.path.join(tempfile.gettempdir(),
                             ""ray-config-{}"".format(hasher.hexdigest()))

    if os.path.exists(cache_key) and not no_config_cache:
        config_cache = json.loads(open(cache_key).read())
        if config_cache.get(""_version"", -1) == CONFIG_CACHE_VERSION:
            # todo: is it fine to re-resolve? afaik it should be.
            # we can have migrations otherwise or something
            # but this seems overcomplicated given that resolving is
            # relatively cheap
            try_reload_log_state(config_cache[""config""][""provider""],
                                 config_cache.get(""provider_log_info""))

            if log_once(""_printed_cached_config_warning""):
                cli_logger.verbose_warning(
                    ""Loaded cached provider configuration ""
                    ""from "" + cf.bold(""{}""), cache_key)
                if cli_logger.verbosity == 0:
                    cli_logger.warning(""Loaded cached provider configuration"")
                cli_logger.warning(
                    ""If you experience issues with ""
                    ""the cloud provider, try re-running ""
                    ""the command with {}."", cf.bold(""--no-config-cache""))

            return config_cache[""config""]
        else:
            cli_logger.warning(
                ""Found cached cluster config ""
                ""but the version "" + cf.bold(""{}"") + "" ""
                ""(expected "" + cf.bold(""{}"") + "") does not match.\\n""
                ""This is normal if cluster launcher was updated.\\n""
                ""Config will be re-resolved."",
                config_cache.get(""_version"", ""none""), CONFIG_CACHE_VERSION)

    importer = _NODE_PROVIDERS.get(config[""provider""][""type""])
    if not importer:
        raise NotImplementedError(""Unsupported provider {}"".format(
            config[""provider""]))

    provider_cls = importer(config[""provider""])

    cli_logger.print(""Checking {} environment settings"",
                     _PROVIDER_PRETTY_NAMES.get(config[""provider""][""type""]))
    try:
        config = provider_cls.fillout_available_node_types_resources(config)
    except Exception as exc:
        if cli_logger.verbosity > 2:
            logger.exception(""Failed to autodetect node resources."")
        else:
            cli_logger.warning(
                f""Failed to autodetect node resources: {str(exc)}. ""
                ""You can see full stack trace with higher verbosity."")

    # NOTE: if `resources` field is missing, validate_config for non-AWS will
    # fail (the schema error will ask the user to manually fill the resources)
    # as we currently support autofilling resources for AWS instances only.
    validate_config(config)
    resolved_config = provider_cls.bootstrap_config(config)

    if not no_config_cache:
        with open(cache_key, ""w"") as f:
            config_cache = {
                ""_version"": CONFIG_CACHE_VERSION,
                ""provider_log_info"": try_get_log_state(config[""provider""]),
                ""config"": resolved_config
            }
            f.write(json.dumps(config_cache))
    return resolved_config","def _bootstrap_config(config: Dict[str, Any],
                      no_config_cache: bool = False) -> Dict[str, Any]:
    config = prepare_config(config)

    hasher = hashlib.sha1()
    hasher.update(json.dumps([config], sort_keys=True).encode(""utf-8""))
    cache_key = os.path.join(tempfile.gettempdir(),
                             ""ray-config-{}"".format(hasher.hexdigest()))

    if os.path.exists(cache_key) and not no_config_cache:
        config_cache = json.loads(open(cache_key).read())
        if config_cache.get(""_version"", -1) == CONFIG_CACHE_VERSION:
            # todo: is it fine to re-resolve? afaik it should be.
            # we can have migrations otherwise or something
            # but this seems overcomplicated given that resolving is
            # relatively cheap
            try_reload_log_state(config_cache[""config""][""provider""],
                                 config_cache.get(""provider_log_info""))

            if log_once(""_printed_cached_config_warning""):
                cli_logger.verbose_warning(
                    ""Loaded cached provider configuration ""
                    ""from "" + cf.bold(""{}""), cache_key)
                if cli_logger.verbosity == 0:
                    cli_logger.warning(""Loaded cached provider configuration"")
                cli_logger.warning(
                    ""If you experience issues with ""
                    ""the cloud provider, try re-running ""
                    ""the command with {}."", cf.bold(""--no-config-cache""))

            return config_cache[""config""]
        else:
            cli_logger.warning(
                ""Found cached cluster config ""
                ""but the version "" + cf.bold(""{}"") + "" ""
                ""(expected "" + cf.bold(""{}"") + "") does not match.\\n""
                ""This is normal if cluster launcher was updated.\\n""
                ""Config will be re-resolved."",
                config_cache.get(""_version"", ""none""), CONFIG_CACHE_VERSION)

    importer = _NODE_PROVIDERS.get(config[""provider""][""type""])
    if not importer:
        raise NotImplementedError(""Unsupported provider {}"".format(
            config[""provider""]))

    provider_cls = importer(config[""provider""])

    cli_logger.print(""Checking {} environment settings"",
                     _PROVIDER_PRETTY_NAMES.get(config[""provider""][""type""]))

    config = provider_cls.fillout_available_node_types_resources(config)

    # NOTE: if `resources` field is missing, validate_config for non-AWS will
    # fail (the schema error will ask the user to manually fill the resources)
    # as we currently support autofilling resources for AWS instances only.
    validate_config(config)
    resolved_config = provider_cls.bootstrap_config(config)

    if not no_config_cache:
        with open(cache_key, ""w"") as f:
            config_cache = {
                ""_version"": CONFIG_CACHE_VERSION,
                ""provider_log_info"": try_get_log_state(config[""provider""]),
                ""config"": resolved_config
            }
            f.write(json.dumps(config_cache))
    return resolved_config"
"    async def get_node_workers(cls, node_id):
        workers = []
        node_ip = DataSource.node_id_to_ip[node_id]
        node_logs = DataSource.ip_and_pid_to_logs.get(node_ip, {})
        node_errs = DataSource.ip_and_pid_to_errors.get(node_ip, {})
        node_physical_stats = DataSource.node_physical_stats.get(node_id, {})
        node_stats = DataSource.node_stats.get(node_id, {})
        # Merge coreWorkerStats (node stats) to workers (node physical stats)
        pid_to_worker_stats = {}
        pid_to_language = {}
        pid_to_job_id = {}
        for core_worker_stats in node_stats.get(""coreWorkersStats"", []):
            pid = core_worker_stats[""pid""]
            pid_to_worker_stats.setdefault(pid, []).append(core_worker_stats)
            pid_to_language[pid] = core_worker_stats[""language""]
            pid_to_job_id[pid] = core_worker_stats[""jobId""]
        for worker in node_physical_stats.get(""workers"", []):
            worker = dict(worker)
            pid = worker[""pid""]
            worker[""logCount""] = len(node_logs.get(str(pid), []))
            worker[""errorCount""] = len(node_errs.get(str(pid), []))
            worker[""coreWorkerStats""] = pid_to_worker_stats.get(pid, [])
            worker[""language""] = pid_to_language.get(
                pid, dashboard_consts.DEFAULT_LANGUAGE)
            worker[""jobId""] = pid_to_job_id.get(
                pid, dashboard_consts.DEFAULT_JOB_ID)

            await GlobalSignals.worker_info_fetched.send(node_id, worker)

            workers.append(worker)
        return workers","    async def get_node_workers(cls, node_id):
        workers = []
        node_ip = DataSource.node_id_to_ip[node_id]
        node_logs = DataSource.ip_and_pid_to_logs.get(node_ip, {})
        logger.error(node_logs)
        node_errs = DataSource.ip_and_pid_to_errors.get(node_ip, {})
        logger.error(node_errs)
        node_physical_stats = DataSource.node_physical_stats.get(node_id, {})
        node_stats = DataSource.node_stats.get(node_id, {})
        # Merge coreWorkerStats (node stats) to workers (node physical stats)
        pid_to_worker_stats = {}
        pid_to_language = {}
        pid_to_job_id = {}
        for core_worker_stats in node_stats.get(""coreWorkersStats"", []):
            pid = core_worker_stats[""pid""]
            pid_to_worker_stats.setdefault(pid, []).append(core_worker_stats)
            pid_to_language[pid] = core_worker_stats[""language""]
            pid_to_job_id[pid] = core_worker_stats[""jobId""]
        for worker in node_physical_stats.get(""workers"", []):
            worker = dict(worker)
            pid = worker[""pid""]
            logger.error(f""pid={pid}"")
            worker[""logCount""] = len(node_logs.get(str(pid), []))
            worker[""errorCount""] = len(node_errs.get(str(pid), []))
            worker[""coreWorkerStats""] = pid_to_worker_stats.get(pid, [])
            worker[""language""] = pid_to_language.get(
                pid, dashboard_consts.DEFAULT_LANGUAGE)
            worker[""jobId""] = pid_to_job_id.get(
                pid, dashboard_consts.DEFAULT_JOB_ID)

            await GlobalSignals.worker_info_fetched.send(node_id, worker)

            workers.append(worker)
        return workers"
"    async def get_node_info(cls, node_id):
        node_physical_stats = dict(
            DataSource.node_physical_stats.get(node_id, {}))
        node_stats = dict(DataSource.node_stats.get(node_id, {}))
        node = DataSource.nodes.get(node_id, {})
        node_ip = DataSource.node_id_to_ip.get(node_id)
        # Merge node log count information into the payload
        log_info = DataSource.ip_and_pid_to_logs.get(node_ip, {})
        node_log_count = 0
        for entries in log_info.values():
            node_log_count += len(entries)
        error_info = DataSource.ip_and_pid_to_errors.get(node_ip, {})
        node_err_count = 0
        for entries in error_info.values():
            node_err_count += len(entries)

        node_stats.pop(""coreWorkersStats"", None)

        view_data = node_stats.get(""viewData"", [])
        ray_stats = cls._extract_view_data(
            view_data,
            {""object_store_used_memory"", ""object_store_available_memory""})

        node_info = node_physical_stats
        # Merge node stats to node physical stats under raylet
        node_info[""raylet""] = node_stats
        node_info[""raylet""].update(ray_stats)

        # Merge GcsNodeInfo to node physical stats
        node_info[""raylet""].update(node)
        # Merge actors to node physical stats
        node_info[""actors""] = await cls.get_node_actors(node_id)
        # Update workers to node physical stats
        node_info[""workers""] = DataSource.node_workers.get(node_id, [])
        node_info[""logCount""] = node_log_count
        node_info[""errorCount""] = node_err_count
        await GlobalSignals.node_info_fetched.send(node_info)

        return node_info","    async def get_node_info(cls, node_id):
        node_physical_stats = dict(
            DataSource.node_physical_stats.get(node_id, {}))
        node_stats = dict(DataSource.node_stats.get(node_id, {}))
        node = DataSource.nodes.get(node_id, {})
        node_ip = DataSource.node_id_to_ip.get(node_id)
        # Merge node log count information into the payload
        log_info = DataSource.ip_and_pid_to_logs.get(node_ip, {})
        node_log_count = 0
        for entries in log_info.values():
            node_log_count += len(entries)
        error_info = DataSource.ip_and_pid_to_errors.get(node_ip, {})
        node_err_count = 0
        for entries in error_info.values():
            node_err_count += len(entries)

        node_stats.pop(""coreWorkersStats"", None)

        ray_stats = cls._extract_view_data(
            node_stats[""viewData""],
            {""object_store_used_memory"", ""object_store_available_memory""})

        node_info = node_physical_stats
        # Merge node stats to node physical stats under raylet
        node_info[""raylet""] = node_stats
        node_info[""raylet""].update(ray_stats)

        # Merge GcsNodeInfo to node physical stats
        node_info[""raylet""].update(node)
        # Merge actors to node physical stats
        node_info[""actors""] = await cls.get_node_actors(node_id)
        # Update workers to node physical stats
        node_info[""workers""] = DataSource.node_workers.get(node_id, [])
        node_info[""logCount""] = node_log_count
        node_info[""errorCount""] = node_err_count
        await GlobalSignals.node_info_fetched.send(node_info)

        return node_info"
"def unflatten_dict(dt, delimiter=""/""):
    """"""Unflatten dict. Does not support unflattening lists.""""""
    dict_type = type(dt)
    out = dict_type()
    for key, val in dt.items():
        path = key.split(delimiter)
        item = out
        for k in path[:-1]:
            item = item.setdefault(k, dict_type())
        item[path[-1]] = val
    return out","def unflatten_dict(dt, delimiter=""/""):
    """"""Unflatten dict. Does not support unflattening lists.""""""
    out = defaultdict(dict)
    for key, val in dt.items():
        path = key.split(delimiter)
        item = out
        for k in path[:-1]:
            item = item[k]
        item[path[-1]] = val
    return dict(out)"
"def find_redis_address(address=None):
    pids = psutil.pids()
    redis_addresses = set()
    for pid in pids:
        try:
            proc = psutil.Process(pid)
            # HACK: Workaround for UNIX idiosyncrasy
            # Normally, cmdline() is supposed to return the argument list.
            # But it in some cases (such as when setproctitle is called),
            # an arbitrary string resembling a command-line is stored in
            # the first argument.
            # Explanation: https://unix.stackexchange.com/a/432681
            # More info: https://github.com/giampaolo/psutil/issues/1179
            cmdline = proc.cmdline()
            # NOTE(kfstorm): To support Windows, we can't use
            # `os.path.basename(cmdline[0]) == ""raylet""` here.
            if len(cmdline) > 0 and ""raylet"" in os.path.basename(cmdline[0]):
                for arglist in cmdline:
                    # Given we're merely seeking --redis-address, we just split
                    # every argument on spaces for now.
                    for arg in arglist.split("" ""):
                        # TODO(ekl): Find a robust solution for locating Redis.
                        if arg.startswith(""--redis-address=""):
                            proc_addr = arg.split(""="")[1]
                            if address is not None and address != proc_addr:
                                continue
                            redis_addresses.add(proc_addr)
        except psutil.AccessDenied:
            pass
        except psutil.NoSuchProcess:
            pass
    return redis_addresses","def find_redis_address(address=None):
    pids = psutil.pids()
    redis_addresses = set()
    for pid in pids:
        try:
            proc = psutil.Process(pid)
            # HACK: Workaround for UNIX idiosyncrasy
            # Normally, cmdline() is supposed to return the argument list.
            # But it in some cases (such as when setproctitle is called),
            # an arbitrary string resembling a command-line is stored in
            # the first argument.
            # Explanation: https://unix.stackexchange.com/a/432681
            # More info: https://github.com/giampaolo/psutil/issues/1179
            for arglist in proc.cmdline():
                # Given we're merely seeking --redis-address, we just split
                # every argument on spaces for now.
                for arg in arglist.split("" ""):
                    # TODO(ekl): Find a robust solution for locating Redis.
                    if arg.startswith(""--redis-address=""):
                        proc_addr = arg.split(""="")[1]
                        if address is not None and address != proc_addr:
                            continue
                        redis_addresses.add(proc_addr)
        except psutil.AccessDenied:
            pass
        except psutil.NoSuchProcess:
            pass
    return redis_addresses"
"    def _run_helper(self,
                    final_cmd,
                    with_output=False,
                    exit_on_fail=False,
                    silent=False):
        """"""Run a command that was already setup with SSH and `bash` settings.

        Args:
            cmd (List[str]):
                Full command to run. Should include SSH options and other
                processing that we do.
            with_output (bool):
                If `with_output` is `True`, command stdout and stderr
                will be captured and returned.
            exit_on_fail (bool):
                If `exit_on_fail` is `True`, the process will exit
                if the command fails (exits with a code other than 0).

        Raises:
            ProcessRunnerError if using new log style and disabled
                login shells.
            click.ClickException if using login shells.
        """"""
        try:
            # For now, if the output is needed we just skip the new logic.
            # In the future we could update the new logic to support
            # capturing output, but it is probably not needed.
            if not cli_logger.old_style and not with_output:
                return run_cmd_redirected(
                    final_cmd,
                    process_runner=self.process_runner,
                    silent=silent,
                    use_login_shells=is_using_login_shells())
            if with_output:
                return self.process_runner.check_output(final_cmd)
            else:
                return self.process_runner.check_call(final_cmd)
        except subprocess.CalledProcessError as e:
            joined_cmd = "" "".join(final_cmd)
            if not cli_logger.old_style and not is_using_login_shells():
                raise ProcessRunnerError(
                    ""Command failed"",
                    ""ssh_command_failed"",
                    code=e.returncode,
                    command=joined_cmd)

            if exit_on_fail:
                raise click.ClickException(
                    ""Command failed:\\n\\n  {}\\n"".format(joined_cmd)) from None
            else:
                fail_msg = ""SSH command failed.""
                if is_output_redirected():
                    fail_msg += "" See above for the output from the failure.""
                raise click.ClickException(fail_msg) from None","    def _run_helper(self,
                    final_cmd,
                    with_output=False,
                    exit_on_fail=False,
                    silent=False):
        """"""Run a command that was already setup with SSH and `bash` settings.

        Args:
            cmd (List[str]):
                Full command to run. Should include SSH options and other
                processing that we do.
            with_output (bool):
                If `with_output` is `True`, command stdout and stderr
                will be captured and returned.
            exit_on_fail (bool):
                If `exit_on_fail` is `True`, the process will exit
                if the command fails (exits with a code other than 0).

        Raises:
            ProcessRunnerError if using new log style and disabled
                login shells.
            click.ClickException if using login shells.
        """"""
        try:
            # For now, if the output is needed we just skip the new logic.
            # In the future we could update the new logic to support
            # capturing output, but it is probably not needed.
            if not cli_logger.old_style and not with_output:
                return run_cmd_redirected(
                    final_cmd,
                    process_runner=self.process_runner,
                    silent=silent,
                    use_login_shells=is_using_login_shells())
            if with_output:
                return self.process_runner.check_output(final_cmd)
            else:
                return self.process_runner.check_call(final_cmd)
        except subprocess.CalledProcessError as e:
            quoted_cmd = "" "".join(final_cmd[:-1] + [quote(final_cmd[-1])])
            if not cli_logger.old_style and not is_using_login_shells():
                raise ProcessRunnerError(
                    ""Command failed"",
                    ""ssh_command_failed"",
                    code=e.returncode,
                    command=quoted_cmd)

            if exit_on_fail:
                raise click.ClickException(
                    ""Command failed:\\n\\n  {}\\n"".format(quoted_cmd)) from None
            else:
                fail_msg = ""SSH command failed.""
                if is_output_redirected():
                    fail_msg += "" See above for the output from the failure.""
                raise click.ClickException(fail_msg) from None"
"    def __init__(self,
                 space: Optional[Union[Dict, List[Dict]]] = None,
                 metric: Optional[str] = None,
                 mode: Optional[str] = None,
                 parameter_constraints: Optional[List] = None,
                 outcome_constraints: Optional[List] = None,
                 ax_client: Optional[AxClient] = None,
                 use_early_stopped_trials: Optional[bool] = None,
                 max_concurrent: Optional[int] = None):
        assert ax is not None, ""Ax must be installed!""
        if mode:
            assert mode in [""min"", ""max""], ""`mode` must be 'min' or 'max'.""

        super(AxSearch, self).__init__(
            metric=metric,
            mode=mode,
            max_concurrent=max_concurrent,
            use_early_stopped_trials=use_early_stopped_trials)

        self._ax = ax_client

        if isinstance(space, dict) and space:
            resolved_vars, domain_vars, grid_vars = parse_spec_vars(space)
            if domain_vars or grid_vars:
                logger.warning(
                    UNRESOLVED_SEARCH_SPACE.format(
                        par=""space"", cls=type(self)))
                space = self.convert_search_space(space)

        self._space = space
        self._parameter_constraints = parameter_constraints
        self._outcome_constraints = outcome_constraints

        self.max_concurrent = max_concurrent

        self._objective_name = metric
        self._parameters = []
        self._live_trial_mapping = {}

        if self._ax or self._space:
            self.setup_experiment()","    def __init__(self,
                 space: Optional[List[Dict]] = None,
                 metric: Optional[str] = None,
                 mode: Optional[str] = None,
                 parameter_constraints: Optional[List] = None,
                 outcome_constraints: Optional[List] = None,
                 ax_client: Optional[AxClient] = None,
                 use_early_stopped_trials: Optional[bool] = None,
                 max_concurrent: Optional[int] = None):
        assert ax is not None, ""Ax must be installed!""
        if mode:
            assert mode in [""min"", ""max""], ""`mode` must be 'min' or 'max'.""

        super(AxSearch, self).__init__(
            metric=metric,
            mode=mode,
            max_concurrent=max_concurrent,
            use_early_stopped_trials=use_early_stopped_trials)

        self._ax = ax_client
        self._space = space
        self._parameter_constraints = parameter_constraints
        self._outcome_constraints = outcome_constraints

        self.max_concurrent = max_concurrent

        self._objective_name = metric
        self._parameters = []
        self._live_trial_mapping = {}

        if self._ax or self._space:
            self.setup_experiment()"
"    def __init__(self,
                 space: Optional[Dict] = None,
                 metric: Optional[str] = None,
                 mode: Optional[str] = None,
                 utility_kwargs: Optional[Dict] = None,
                 random_state: int = 42,
                 random_search_steps: int = 10,
                 verbose: int = 0,
                 patience: int = 5,
                 skip_duplicate: bool = True,
                 analysis: Optional[ExperimentAnalysis] = None,
                 max_concurrent: Optional[int] = None,
                 use_early_stopped_trials: Optional[bool] = None):
        """"""Instantiate new BayesOptSearch object.

        Args:
            space (dict): Continuous search space.
                Parameters will be sampled from
                this space which will be used to run trials.
            metric (str): The training result objective value attribute.
            mode (str): One of {min, max}. Determines whether objective is
                minimizing or maximizing the metric attribute.
            utility_kwargs (dict): Parameters to define the utility function.
                Must provide values for the keys `kind`, `kappa`, and `xi`.
            random_state (int): Used to initialize BayesOpt.
            random_search_steps (int): Number of initial random searches.
                This is necessary to avoid initial local overfitting
                of the Bayesian process.
            patience (int): Must be > 0. If the optimizer suggests a set of
                hyperparameters more than 'patience' times,
                then the whole experiment will stop.
            skip_duplicate (bool): If true, BayesOptSearch will not create
                a trial with a previously seen set of hyperparameters. By
                default, floating values will be reduced to a digit precision
                of 5. You can override this by setting
                ``searcher.repeat_float_precision``.
            analysis (ExperimentAnalysis): Optionally, the previous analysis
                to integrate.
            verbose (int): Sets verbosity level for BayesOpt packages.
            max_concurrent: Deprecated.
            use_early_stopped_trials: Deprecated.
        """"""
        assert byo is not None, (
            ""BayesOpt must be installed!. You can install BayesOpt with""
            "" the command: `pip install bayesian-optimization`."")
        if mode:
            assert mode in [""min"", ""max""], ""`mode` must be 'min' or 'max'.""
        self.max_concurrent = max_concurrent
        self._config_counter = defaultdict(int)
        self._patience = patience
        # int: Precision at which to hash values.
        self.repeat_float_precision = 5
        if self._patience <= 0:
            raise ValueError(""patience must be set to a value greater than 0!"")
        self._skip_duplicate = skip_duplicate
        super(BayesOptSearch, self).__init__(
            metric=metric,
            mode=mode,
            max_concurrent=max_concurrent,
            use_early_stopped_trials=use_early_stopped_trials)

        if utility_kwargs is None:
            # The defaults arguments are the same
            # as in the package BayesianOptimization
            utility_kwargs = dict(
                kind=""ucb"",
                kappa=2.576,
                xi=0.0,
            )

        if mode == ""max"":
            self._metric_op = 1.
        elif mode == ""min"":
            self._metric_op = -1.

        self._live_trial_mapping = {}
        self._buffered_trial_results = []
        self.random_search_trials = random_search_steps
        self._total_random_search_trials = 0

        self.utility = byo.UtilityFunction(**utility_kwargs)

        # Registering the provided analysis, if given
        if analysis is not None:
            self.register_analysis(analysis)

        if isinstance(space, dict) and space:
            resolved_vars, domain_vars, grid_vars = parse_spec_vars(space)
            if domain_vars or grid_vars:
                logger.warning(
                    UNRESOLVED_SEARCH_SPACE.format(
                        par=""space"", cls=type(self)))
                space = self.convert_search_space(space, join=True)

        self._space = space
        self._verbose = verbose
        self._random_state = random_state

        self.optimizer = None
        if space:
            self.setup_optimizer()","    def __init__(self,
                 space: Optional[Dict] = None,
                 metric: Optional[str] = None,
                 mode: Optional[str] = None,
                 utility_kwargs: Optional[Dict] = None,
                 random_state: int = 42,
                 random_search_steps: int = 10,
                 verbose: int = 0,
                 patience: int = 5,
                 skip_duplicate: bool = True,
                 analysis: Optional[ExperimentAnalysis] = None,
                 max_concurrent: Optional[int] = None,
                 use_early_stopped_trials: Optional[bool] = None):
        """"""Instantiate new BayesOptSearch object.

        Args:
            space (dict): Continuous search space.
                Parameters will be sampled from
                this space which will be used to run trials.
            metric (str): The training result objective value attribute.
            mode (str): One of {min, max}. Determines whether objective is
                minimizing or maximizing the metric attribute.
            utility_kwargs (dict): Parameters to define the utility function.
                Must provide values for the keys `kind`, `kappa`, and `xi`.
            random_state (int): Used to initialize BayesOpt.
            random_search_steps (int): Number of initial random searches.
                This is necessary to avoid initial local overfitting
                of the Bayesian process.
            patience (int): Must be > 0. If the optimizer suggests a set of
                hyperparameters more than 'patience' times,
                then the whole experiment will stop.
            skip_duplicate (bool): If true, BayesOptSearch will not create
                a trial with a previously seen set of hyperparameters. By
                default, floating values will be reduced to a digit precision
                of 5. You can override this by setting
                ``searcher.repeat_float_precision``.
            analysis (ExperimentAnalysis): Optionally, the previous analysis
                to integrate.
            verbose (int): Sets verbosity level for BayesOpt packages.
            max_concurrent: Deprecated.
            use_early_stopped_trials: Deprecated.
        """"""
        assert byo is not None, (
            ""BayesOpt must be installed!. You can install BayesOpt with""
            "" the command: `pip install bayesian-optimization`."")
        if mode:
            assert mode in [""min"", ""max""], ""`mode` must be 'min' or 'max'.""
        self.max_concurrent = max_concurrent
        self._config_counter = defaultdict(int)
        self._patience = patience
        # int: Precision at which to hash values.
        self.repeat_float_precision = 5
        if self._patience <= 0:
            raise ValueError(""patience must be set to a value greater than 0!"")
        self._skip_duplicate = skip_duplicate
        super(BayesOptSearch, self).__init__(
            metric=metric,
            mode=mode,
            max_concurrent=max_concurrent,
            use_early_stopped_trials=use_early_stopped_trials)

        if utility_kwargs is None:
            # The defaults arguments are the same
            # as in the package BayesianOptimization
            utility_kwargs = dict(
                kind=""ucb"",
                kappa=2.576,
                xi=0.0,
            )

        if mode == ""max"":
            self._metric_op = 1.
        elif mode == ""min"":
            self._metric_op = -1.

        self._live_trial_mapping = {}
        self._buffered_trial_results = []
        self.random_search_trials = random_search_steps
        self._total_random_search_trials = 0

        self.utility = byo.UtilityFunction(**utility_kwargs)

        # Registering the provided analysis, if given
        if analysis is not None:
            self.register_analysis(analysis)

        self._space = space
        self._verbose = verbose
        self._random_state = random_state

        self.optimizer = None
        if space:
            self.setup_optimizer()"
"    def convert_search_space(spec: Dict, join: bool = False) -> Dict:
        spec = flatten_dict(spec, prevent_delimiter=True)
        resolved_vars, domain_vars, grid_vars = parse_spec_vars(spec)

        if grid_vars:
            raise ValueError(
                ""Grid search parameters cannot be automatically converted ""
                ""to a BayesOpt search space."")

        def resolve_value(domain: Domain) -> Tuple[float, float]:
            sampler = domain.get_sampler()
            if isinstance(sampler, Quantized):
                logger.warning(
                    ""BayesOpt search does not support quantization. ""
                    ""Dropped quantization."")
                sampler = sampler.get_sampler()

            if isinstance(domain, Float):
                if domain.sampler is not None:
                    logger.warning(
                        ""BayesOpt does not support specific sampling methods. ""
                        ""The {} sampler will be dropped."".format(sampler))
                    return (domain.lower, domain.upper)

            raise ValueError(""BayesOpt does not support parameters of type ""
                             ""`{}`"".format(type(domain).__name__))

        # Parameter name is e.g. ""a/b/c"" for nested dicts
        bounds = {
            ""/"".join(path): resolve_value(domain)
            for path, domain in domain_vars
        }

        if join:
            spec.update(bounds)
            bounds = spec

        return bounds","    def convert_search_space(spec: Dict) -> Dict:
        spec = flatten_dict(spec, prevent_delimiter=True)
        resolved_vars, domain_vars, grid_vars = parse_spec_vars(spec)

        if grid_vars:
            raise ValueError(
                ""Grid search parameters cannot be automatically converted ""
                ""to a BayesOpt search space."")

        def resolve_value(domain: Domain) -> Tuple[float, float]:
            sampler = domain.get_sampler()
            if isinstance(sampler, Quantized):
                logger.warning(
                    ""BayesOpt search does not support quantization. ""
                    ""Dropped quantization."")
                sampler = sampler.get_sampler()

            if isinstance(domain, Float):
                if domain.sampler is not None:
                    logger.warning(
                        ""BayesOpt does not support specific sampling methods. ""
                        ""The {} sampler will be dropped."".format(sampler))
                    return (domain.lower, domain.upper)

            raise ValueError(""BayesOpt does not support parameters of type ""
                             ""`{}`"".format(type(domain).__name__))

        # Parameter name is e.g. ""a/b/c"" for nested dicts
        bounds = {
            ""/"".join(path): resolve_value(domain)
            for path, domain in domain_vars
        }

        return bounds"
"    def __init__(self,
                 space: Optional[Union[Dict,
                                       ConfigSpace.ConfigurationSpace]] = None,
                 bohb_config: Optional[Dict] = None,
                 max_concurrent: int = 10,
                 metric: Optional[str] = None,
                 mode: Optional[str] = None):
        from hpbandster.optimizers.config_generators.bohb import BOHB
        assert BOHB is not None, ""HpBandSter must be installed!""
        if mode:
            assert mode in [""min"", ""max""], ""`mode` must be 'min' or 'max'.""
        self._max_concurrent = max_concurrent
        self.trial_to_params = {}
        self.running = set()
        self.paused = set()
        self._metric = metric

        self._bohb_config = bohb_config

        if isinstance(space, dict) and space:
            resolved_vars, domain_vars, grid_vars = parse_spec_vars(space)
            if domain_vars or grid_vars:
                logger.warning(
                    UNRESOLVED_SEARCH_SPACE.format(
                        par=""space"", cls=type(self)))
                space = self.convert_search_space(space)

        self._space = space

        super(TuneBOHB, self).__init__(metric=self._metric, mode=mode)

        if self._space:
            self.setup_bohb()","    def __init__(self,
                 space: Optional[ConfigSpace.ConfigurationSpace] = None,
                 bohb_config: Optional[Dict] = None,
                 max_concurrent: int = 10,
                 metric: Optional[str] = None,
                 mode: Optional[str] = None):
        from hpbandster.optimizers.config_generators.bohb import BOHB
        assert BOHB is not None, ""HpBandSter must be installed!""
        if mode:
            assert mode in [""min"", ""max""], ""`mode` must be 'min' or 'max'.""
        self._max_concurrent = max_concurrent
        self.trial_to_params = {}
        self.running = set()
        self.paused = set()
        self._metric = metric

        self._bohb_config = bohb_config
        self._space = space

        super(TuneBOHB, self).__init__(metric=self._metric, mode=mode)

        if self._space:
            self.setup_bohb()"
"    def __init__(self,
                 optimizer: Optional[BlackboxOptimiser] = None,
                 domain: Optional[str] = None,
                 space: Optional[Union[Dict, List[Dict]]] = None,
                 metric: Optional[str] = None,
                 mode: Optional[str] = None,
                 points_to_evaluate: Optional[List[List]] = None,
                 evaluated_rewards: Optional[List] = None,
                 **kwargs):
        assert dragonfly is not None, """"""dragonfly must be installed!
            You can install Dragonfly with the command:
            `pip install dragonfly-opt`.""""""
        if mode:
            assert mode in [""min"", ""max""], ""`mode` must be 'min' or 'max'.""

        super(DragonflySearch, self).__init__(
            metric=metric, mode=mode, **kwargs)

        self._opt_arg = optimizer
        self._domain = domain

        if isinstance(space, dict) and space:
            resolved_vars, domain_vars, grid_vars = parse_spec_vars(space)
            if domain_vars or grid_vars:
                logger.warning(
                    UNRESOLVED_SEARCH_SPACE.format(
                        par=""space"", cls=type(self)))
                space = self.convert_search_space(space)

        self._space = space
        self._points_to_evaluate = points_to_evaluate
        self._evaluated_rewards = evaluated_rewards
        self._initial_points = []
        self._live_trial_mapping = {}

        self._opt = None
        if isinstance(optimizer, BlackboxOptimiser):
            if domain or space:
                raise ValueError(
                    ""If you pass an optimizer instance to dragonfly, do not ""
                    ""pass a `domain` or `space`."")
            self._opt = optimizer
            self.init_dragonfly()
        elif self._space:
            self.setup_dragonfly()","    def __init__(self,
                 optimizer: Optional[BlackboxOptimiser] = None,
                 domain: Optional[str] = None,
                 space: Optional[List[Dict]] = None,
                 metric: Optional[str] = None,
                 mode: Optional[str] = None,
                 points_to_evaluate: Optional[List[List]] = None,
                 evaluated_rewards: Optional[List] = None,
                 **kwargs):
        assert dragonfly is not None, """"""dragonfly must be installed!
            You can install Dragonfly with the command:
            `pip install dragonfly-opt`.""""""
        if mode:
            assert mode in [""min"", ""max""], ""`mode` must be 'min' or 'max'.""

        super(DragonflySearch, self).__init__(
            metric=metric, mode=mode, **kwargs)

        self._opt_arg = optimizer
        self._domain = domain
        self._space = space
        self._points_to_evaluate = points_to_evaluate
        self._evaluated_rewards = evaluated_rewards
        self._initial_points = []
        self._live_trial_mapping = {}

        self._opt = None
        if isinstance(optimizer, BlackboxOptimiser):
            if domain or space:
                raise ValueError(
                    ""If you pass an optimizer instance to dragonfly, do not ""
                    ""pass a `domain` or `space`."")
            self._opt = optimizer
            self.init_dragonfly()
        elif self._space:
            self.setup_dragonfly()"
"    def __init__(
            self,
            space: Optional[Dict] = None,
            metric: Optional[str] = None,
            mode: Optional[str] = None,
            points_to_evaluate: Optional[List[Dict]] = None,
            n_initial_points: int = 20,
            random_state_seed: Optional[int] = None,
            gamma: float = 0.25,
            max_concurrent: Optional[int] = None,
            use_early_stopped_trials: Optional[bool] = None,
    ):
        assert hpo is not None, (
            ""HyperOpt must be installed! Run `pip install hyperopt`."")
        if mode:
            assert mode in [""min"", ""max""], ""`mode` must be 'min' or 'max'.""
        from hyperopt.fmin import generate_trials_to_calculate
        super(HyperOptSearch, self).__init__(
            metric=metric,
            mode=mode,
            max_concurrent=max_concurrent,
            use_early_stopped_trials=use_early_stopped_trials)
        self.max_concurrent = max_concurrent
        # hyperopt internally minimizes, so ""max"" => -1
        if mode == ""max"":
            self.metric_op = -1.
        elif mode == ""min"":
            self.metric_op = 1.

        if n_initial_points is None:
            self.algo = hpo.tpe.suggest
        else:
            self.algo = partial(
                hpo.tpe.suggest, n_startup_jobs=n_initial_points)
        if gamma is not None:
            self.algo = partial(self.algo, gamma=gamma)
        if points_to_evaluate is None:
            self._hpopt_trials = hpo.Trials()
            self._points_to_evaluate = 0
        else:
            assert isinstance(points_to_evaluate, (list, tuple))
            self._hpopt_trials = generate_trials_to_calculate(
                points_to_evaluate)
            self._hpopt_trials.refresh()
            self._points_to_evaluate = len(points_to_evaluate)
        self._live_trial_mapping = {}
        if random_state_seed is None:
            self.rstate = np.random.RandomState()
        else:
            self.rstate = np.random.RandomState(random_state_seed)

        self.domain = None
        if isinstance(space, dict) and space:
            resolved_vars, domain_vars, grid_vars = parse_spec_vars(space)
            if domain_vars or grid_vars:
                logger.warning(
                    UNRESOLVED_SEARCH_SPACE.format(
                        par=""space"", cls=type(self)))
                space = self.convert_search_space(space)
            self.domain = hpo.Domain(lambda spc: spc, space)","    def __init__(
            self,
            space: Optional[Dict] = None,
            metric: Optional[str] = None,
            mode: Optional[str] = None,
            points_to_evaluate: Optional[List[Dict]] = None,
            n_initial_points: int = 20,
            random_state_seed: Optional[int] = None,
            gamma: float = 0.25,
            max_concurrent: Optional[int] = None,
            use_early_stopped_trials: Optional[bool] = None,
    ):
        assert hpo is not None, (
            ""HyperOpt must be installed! Run `pip install hyperopt`."")
        if mode:
            assert mode in [""min"", ""max""], ""`mode` must be 'min' or 'max'.""
        from hyperopt.fmin import generate_trials_to_calculate
        super(HyperOptSearch, self).__init__(
            metric=metric,
            mode=mode,
            max_concurrent=max_concurrent,
            use_early_stopped_trials=use_early_stopped_trials)
        self.max_concurrent = max_concurrent
        # hyperopt internally minimizes, so ""max"" => -1
        if mode == ""max"":
            self.metric_op = -1.
        elif mode == ""min"":
            self.metric_op = 1.

        if n_initial_points is None:
            self.algo = hpo.tpe.suggest
        else:
            self.algo = partial(
                hpo.tpe.suggest, n_startup_jobs=n_initial_points)
        if gamma is not None:
            self.algo = partial(self.algo, gamma=gamma)
        if points_to_evaluate is None:
            self._hpopt_trials = hpo.Trials()
            self._points_to_evaluate = 0
        else:
            assert isinstance(points_to_evaluate, (list, tuple))
            self._hpopt_trials = generate_trials_to_calculate(
                points_to_evaluate)
            self._hpopt_trials.refresh()
            self._points_to_evaluate = len(points_to_evaluate)
        self._live_trial_mapping = {}
        if random_state_seed is None:
            self.rstate = np.random.RandomState()
        else:
            self.rstate = np.random.RandomState(random_state_seed)

        self.domain = None
        if space:
            self.domain = hpo.Domain(lambda spc: spc, space)"
"    def __init__(self,
                 optimizer: Union[None, Optimizer, ConfiguredOptimizer] = None,
                 space: Optional[Union[Dict, Parameter]] = None,
                 metric: Optional[str] = None,
                 mode: Optional[str] = None,
                 max_concurrent: Optional[int] = None,
                 **kwargs):
        assert ng is not None, ""Nevergrad must be installed!""
        if mode:
            assert mode in [""min"", ""max""], ""`mode` must be 'min' or 'max'.""

        super(NevergradSearch, self).__init__(
            metric=metric, mode=mode, max_concurrent=max_concurrent, **kwargs)

        self._space = None
        self._opt_factory = None
        self._nevergrad_opt = None

        if isinstance(space, dict) and space:
            resolved_vars, domain_vars, grid_vars = parse_spec_vars(space)
            if domain_vars or grid_vars:
                logger.warning(
                    UNRESOLVED_SEARCH_SPACE.format(
                        par=""space"", cls=type(self)))
                space = self.convert_search_space(space)

        if isinstance(optimizer, Optimizer):
            if space is not None or isinstance(space, list):
                raise ValueError(
                    ""If you pass a configured optimizer to Nevergrad, either ""
                    ""pass a list of parameter names or None as the `space` ""
                    ""parameter."")
            self._parameters = space
            self._nevergrad_opt = optimizer
        elif isinstance(optimizer, ConfiguredOptimizer):
            self._opt_factory = optimizer
            self._parameters = None
            self._space = space
        else:
            raise ValueError(
                ""The `optimizer` argument passed to NevergradSearch must be ""
                ""either an `Optimizer` or a `ConfiguredOptimizer`."")

        self._live_trial_mapping = {}
        self.max_concurrent = max_concurrent

        if self._nevergrad_opt or self._space:
            self.setup_nevergrad()","    def __init__(self,
                 optimizer: Union[None, Optimizer, ConfiguredOptimizer] = None,
                 space: Optional[Parameter] = None,
                 metric: Optional[str] = None,
                 mode: Optional[str] = None,
                 max_concurrent: Optional[int] = None,
                 **kwargs):
        assert ng is not None, ""Nevergrad must be installed!""
        if mode:
            assert mode in [""min"", ""max""], ""`mode` must be 'min' or 'max'.""

        super(NevergradSearch, self).__init__(
            metric=metric, mode=mode, max_concurrent=max_concurrent, **kwargs)

        self._space = None
        self._opt_factory = None
        self._nevergrad_opt = None

        if isinstance(optimizer, Optimizer):
            if space is not None or isinstance(space, list):
                raise ValueError(
                    ""If you pass a configured optimizer to Nevergrad, either ""
                    ""pass a list of parameter names or None as the `space` ""
                    ""parameter."")
            self._parameters = space
            self._nevergrad_opt = optimizer
        elif isinstance(optimizer, ConfiguredOptimizer):
            self._opt_factory = optimizer
            self._parameters = None
            self._space = space
        else:
            raise ValueError(
                ""The `optimizer` argument passed to NevergradSearch must be ""
                ""either an `Optimizer` or a `ConfiguredOptimizer`."")

        self._live_trial_mapping = {}
        self.max_concurrent = max_concurrent

        if self._nevergrad_opt or self._space:
            self.setup_nevergrad()"
"    def __init__(self,
                 space: Optional[Union[Dict, List[Tuple]]] = None,
                 metric: Optional[str] = None,
                 mode: Optional[str] = None,
                 sampler: Optional[BaseSampler] = None):
        assert ot is not None, (
            ""Optuna must be installed! Run `pip install optuna`."")
        super(OptunaSearch, self).__init__(
            metric=metric,
            mode=mode,
            max_concurrent=None,
            use_early_stopped_trials=None)

        if isinstance(space, dict) and space:
            resolved_vars, domain_vars, grid_vars = parse_spec_vars(space)
            if domain_vars or grid_vars:
                logger.warning(
                    UNRESOLVED_SEARCH_SPACE.format(
                        par=""space"", cls=type(self)))
                space = self.convert_search_space(space)

        self._space = space

        self._study_name = ""optuna""  # Fixed study name for in-memory storage
        self._sampler = sampler or ot.samplers.TPESampler()
        assert isinstance(self._sampler, BaseSampler), \\
            ""You can only pass an instance of `optuna.samplers.BaseSampler` "" \\
            ""as a sampler to `OptunaSearcher`.""

        self._pruner = ot.pruners.NopPruner()
        self._storage = ot.storages.InMemoryStorage()

        self._ot_trials = {}
        self._ot_study = None
        if self._space:
            self.setup_study(mode)","    def __init__(self,
                 space: Optional[List[Tuple]] = None,
                 metric: Optional[str] = None,
                 mode: Optional[str] = None,
                 sampler: Optional[BaseSampler] = None):
        assert ot is not None, (
            ""Optuna must be installed! Run `pip install optuna`."")
        super(OptunaSearch, self).__init__(
            metric=metric,
            mode=mode,
            max_concurrent=None,
            use_early_stopped_trials=None)

        self._space = space

        self._study_name = ""optuna""  # Fixed study name for in-memory storage
        self._sampler = sampler or ot.samplers.TPESampler()
        assert isinstance(self._sampler, BaseSampler), \\
            ""You can only pass an instance of `optuna.samplers.BaseSampler` "" \\
            ""as a sampler to `OptunaSearcher`.""

        self._pruner = ot.pruners.NopPruner()
        self._storage = ot.storages.InMemoryStorage()

        self._ot_trials = {}
        self._ot_study = None
        if self._space:
            self.setup_study(mode)"
"    def __init__(self,
                 optimizer: Optional[sko.optimizer.Optimizer] = None,
                 space: Union[List[str], Dict[str, Union[Tuple, List]]] = None,
                 metric: Optional[str] = None,
                 mode: Optional[str] = None,
                 points_to_evaluate: Optional[List[List]] = None,
                 evaluated_rewards: Optional[List] = None,
                 max_concurrent: Optional[int] = None,
                 use_early_stopped_trials: Optional[bool] = None):
        assert sko is not None, """"""skopt must be installed!
            You can install Skopt with the command:
            `pip install scikit-optimize`.""""""

        if mode:
            assert mode in [""min"", ""max""], ""`mode` must be 'min' or 'max'.""
        self.max_concurrent = max_concurrent
        super(SkOptSearch, self).__init__(
            metric=metric,
            mode=mode,
            max_concurrent=max_concurrent,
            use_early_stopped_trials=use_early_stopped_trials)

        self._initial_points = []
        self._parameters = None
        self._parameter_names = None
        self._parameter_ranges = None

        if isinstance(space, dict) and space:
            resolved_vars, domain_vars, grid_vars = parse_spec_vars(space)
            if domain_vars or grid_vars:
                logger.warning(
                    UNRESOLVED_SEARCH_SPACE.format(
                        par=""space"", cls=type(self)))
                space = self.convert_search_space(space, join=True)

        self._space = space

        if self._space:
            if isinstance(optimizer, sko.Optimizer):
                if not isinstance(space, list):
                    raise ValueError(
                        ""You passed an optimizer instance to SkOpt. Your ""
                        ""`space` parameter should be a list of parameter""
                        ""names."")
                self._parameter_names = space
            else:
                self._parameter_names = list(space.keys())
                self._parameter_ranges = space.values()

        self._points_to_evaluate = points_to_evaluate
        self._evaluated_rewards = evaluated_rewards

        self._skopt_opt = optimizer
        if self._skopt_opt or self._space:
            self.setup_skopt()

        self._live_trial_mapping = {}","    def __init__(self,
                 optimizer: Optional[sko.optimizer.Optimizer] = None,
                 space: Union[List[str], Dict[str, Union[Tuple, List]]] = None,
                 metric: Optional[str] = None,
                 mode: Optional[str] = None,
                 points_to_evaluate: Optional[List[List]] = None,
                 evaluated_rewards: Optional[List] = None,
                 max_concurrent: Optional[int] = None,
                 use_early_stopped_trials: Optional[bool] = None):
        assert sko is not None, """"""skopt must be installed!
            You can install Skopt with the command:
            `pip install scikit-optimize`.""""""

        if mode:
            assert mode in [""min"", ""max""], ""`mode` must be 'min' or 'max'.""
        self.max_concurrent = max_concurrent
        super(SkOptSearch, self).__init__(
            metric=metric,
            mode=mode,
            max_concurrent=max_concurrent,
            use_early_stopped_trials=use_early_stopped_trials)

        self._initial_points = []
        self._parameters = None
        self._parameter_names = None
        self._parameter_ranges = None

        self._space = space

        if self._space:
            if isinstance(optimizer, sko.Optimizer):
                if not isinstance(space, list):
                    raise ValueError(
                        ""You passed an optimizer instance to SkOpt. Your ""
                        ""`space` parameter should be a list of parameter""
                        ""names."")
                self._parameter_names = space
            else:
                self._parameter_names = list(space.keys())
                self._parameter_ranges = space.values()

        self._points_to_evaluate = points_to_evaluate
        self._evaluated_rewards = evaluated_rewards

        self._skopt_opt = optimizer
        if self._skopt_opt or self._space:
            self.setup_skopt()

        self._live_trial_mapping = {}"
"    def convert_search_space(spec: Dict, join: bool = False) -> Dict:
        spec = flatten_dict(spec, prevent_delimiter=True)
        resolved_vars, domain_vars, grid_vars = parse_spec_vars(spec)

        if grid_vars:
            raise ValueError(
                ""Grid search parameters cannot be automatically converted ""
                ""to a SkOpt search space."")

        def resolve_value(domain: Domain) -> Union[Tuple, List]:
            sampler = domain.get_sampler()
            if isinstance(sampler, Quantized):
                logger.warning(""SkOpt search does not support quantization. ""
                               ""Dropped quantization."")
                sampler = sampler.get_sampler()

            if isinstance(domain, Float):
                if domain.sampler is not None:
                    logger.warning(
                        ""SkOpt does not support specific sampling methods.""
                        "" The {} sampler will be dropped."".format(sampler))
                return domain.lower, domain.upper

            if isinstance(domain, Integer):
                if domain.sampler is not None:
                    logger.warning(
                        ""SkOpt does not support specific sampling methods.""
                        "" The {} sampler will be dropped."".format(sampler))
                return domain.lower, domain.upper

            if isinstance(domain, Categorical):
                return domain.categories

            raise ValueError(""SkOpt does not support parameters of type ""
                             ""`{}`"".format(type(domain).__name__))

        # Parameter name is e.g. ""a/b/c"" for nested dicts
        space = {
            ""/"".join(path): resolve_value(domain)
            for path, domain in domain_vars
        }

        if join:
            spec.update(space)
            space = spec

        return space","    def convert_search_space(spec: Dict) -> Dict:
        spec = flatten_dict(spec, prevent_delimiter=True)
        resolved_vars, domain_vars, grid_vars = parse_spec_vars(spec)

        if grid_vars:
            raise ValueError(
                ""Grid search parameters cannot be automatically converted ""
                ""to a SkOpt search space."")

        def resolve_value(domain: Domain) -> Union[Tuple, List]:
            sampler = domain.get_sampler()
            if isinstance(sampler, Quantized):
                logger.warning(""SkOpt search does not support quantization. ""
                               ""Dropped quantization."")
                sampler = sampler.get_sampler()

            if isinstance(domain, Float):
                if domain.sampler is not None:
                    logger.warning(
                        ""SkOpt does not support specific sampling methods.""
                        "" The {} sampler will be dropped."".format(sampler))
                return domain.lower, domain.upper

            if isinstance(domain, Integer):
                if domain.sampler is not None:
                    logger.warning(
                        ""SkOpt does not support specific sampling methods.""
                        "" The {} sampler will be dropped."".format(sampler))
                return domain.lower, domain.upper

            if isinstance(domain, Categorical):
                return domain.categories

            raise ValueError(""SkOpt does not support parameters of type ""
                             ""`{}`"".format(type(domain).__name__))

        # Parameter name is e.g. ""a/b/c"" for nested dicts
        space = {
            ""/"".join(path): resolve_value(domain)
            for path, domain in domain_vars
        }

        return space"
"    def __init__(self,
                 algo: str = ""asracos"",
                 budget: Optional[int] = None,
                 dim_dict: Optional[Dict] = None,
                 metric: Optional[str] = None,
                 mode: Optional[str] = None,
                 **kwargs):
        assert zoopt is not None, ""ZOOpt not found - please install zoopt "" \\
                                  ""by `pip install -U zoopt`.""
        assert budget is not None, ""`budget` should not be None!""
        if mode:
            assert mode in [""min"", ""max""], ""`mode` must be 'min' or 'max'.""
        _algo = algo.lower()
        assert _algo in [""asracos"", ""sracos""
                         ], ""`algo` must be in ['asracos', 'sracos'] currently""

        self._algo = _algo

        if isinstance(dim_dict, dict) and dim_dict:
            resolved_vars, domain_vars, grid_vars = parse_spec_vars(dim_dict)
            if domain_vars or grid_vars:
                logger.warning(
                    UNRESOLVED_SEARCH_SPACE.format(
                        par=""dim_dict"", cls=type(self)))
                dim_dict = self.convert_search_space(dim_dict, join=True)

        self._dim_dict = dim_dict
        self._budget = budget

        self._metric = metric
        if mode == ""max"":
            self._metric_op = -1.
        elif mode == ""min"":
            self._metric_op = 1.
        self._live_trial_mapping = {}

        self._dim_keys = []
        self.solution_dict = {}
        self.best_solution_list = []
        self.optimizer = None

        self.kwargs = kwargs

        super(ZOOptSearch, self).__init__(metric=self._metric, mode=mode)

        if self._dim_dict:
            self.setup_zoopt()","    def __init__(self,
                 algo: str = ""asracos"",
                 budget: Optional[int] = None,
                 dim_dict: Optional[Dict] = None,
                 metric: Optional[str] = None,
                 mode: Optional[str] = None,
                 **kwargs):
        assert zoopt is not None, ""ZOOpt not found - please install zoopt "" \\
                                  ""by `pip install -U zoopt`.""
        assert budget is not None, ""`budget` should not be None!""
        if mode:
            assert mode in [""min"", ""max""], ""`mode` must be 'min' or 'max'.""
        _algo = algo.lower()
        assert _algo in [""asracos"", ""sracos""
                         ], ""`algo` must be in ['asracos', 'sracos'] currently""

        self._algo = _algo
        self._dim_dict = dim_dict
        self._budget = budget

        self._metric = metric
        if mode == ""max"":
            self._metric_op = -1.
        elif mode == ""min"":
            self._metric_op = 1.
        self._live_trial_mapping = {}

        self._dim_keys = []
        self.solution_dict = {}
        self.best_solution_list = []
        self.optimizer = None

        self.kwargs = kwargs

        super(ZOOptSearch, self).__init__(metric=self._metric, mode=mode)

        if self._dim_dict:
            self.setup_zoopt()"
"    def convert_search_space(spec: Dict,
                             join: bool = False) -> Dict[str, Tuple]:
        spec = copy.deepcopy(spec)
        resolved_vars, domain_vars, grid_vars = parse_spec_vars(spec)

        if not domain_vars and not grid_vars:
            return {}

        if grid_vars:
            raise ValueError(
                ""Grid search parameters cannot be automatically converted ""
                ""to a ZOOpt search space."")

        def resolve_value(domain: Domain) -> Tuple:
            quantize = None

            sampler = domain.get_sampler()
            if isinstance(sampler, Quantized):
                quantize = sampler.q
                sampler = sampler.sampler

            if isinstance(domain, Float):
                precision = quantize or 1e-12
                if isinstance(sampler, Uniform):
                    return (ValueType.CONTINUOUS, [domain.lower, domain.upper],
                            precision)

            elif isinstance(domain, Integer):
                if isinstance(sampler, Uniform):
                    return (ValueType.DISCRETE, [domain.lower, domain.upper],
                            True)

            elif isinstance(domain, Categorical):
                # Categorical variables would use ValueType.DISCRETE with
                # has_partial_order=False, however, currently we do not
                # keep track of category values and cannot automatically
                # translate back and forth between them.
                if isinstance(sampler, Uniform):
                    return (ValueType.GRID, domain.categories)

            raise ValueError(""ZOOpt does not support parameters of type ""
                             ""`{}` with samplers of type `{}`"".format(
                                 type(domain).__name__,
                                 type(domain.sampler).__name__))

        conv_spec = {
            ""/"".join(path): resolve_value(domain)
            for path, domain in domain_vars
        }

        if join:
            spec.update(conv_spec)
            conv_spec = spec

        return conv_spec","    def convert_search_space(spec: Dict) -> Dict[str, Tuple]:
        spec = copy.deepcopy(spec)
        resolved_vars, domain_vars, grid_vars = parse_spec_vars(spec)

        if not domain_vars and not grid_vars:
            return []

        if grid_vars:
            raise ValueError(
                ""Grid search parameters cannot be automatically converted ""
                ""to a ZOOpt search space."")

        def resolve_value(domain: Domain) -> Tuple:
            quantize = None

            sampler = domain.get_sampler()
            if isinstance(sampler, Quantized):
                quantize = sampler.q
                sampler = sampler.sampler

            if isinstance(domain, Float):
                precision = quantize or 1e-12
                if isinstance(sampler, Uniform):
                    return (ValueType.CONTINUOUS, [domain.lower, domain.upper],
                            precision)

            elif isinstance(domain, Integer):
                if isinstance(sampler, Uniform):
                    return (ValueType.DISCRETE, [domain.lower, domain.upper],
                            True)

            elif isinstance(domain, Categorical):
                # Categorical variables would use ValueType.DISCRETE with
                # has_partial_order=False, however, currently we do not
                # keep track of category values and cannot automatically
                # translate back and forth between them.
                if isinstance(sampler, Uniform):
                    return (ValueType.GRID, domain.categories)

            raise ValueError(""ZOOpt does not support parameters of type ""
                             ""`{}` with samplers of type `{}`"".format(
                                 type(domain).__name__,
                                 type(domain.sampler).__name__))

        spec = {
            ""/"".join(path): resolve_value(domain)
            for path, domain in domain_vars
        }

        return spec"
"def init(
        address=None,
        *,
        num_cpus=None,
        num_gpus=None,
        resources=None,
        object_store_memory=None,
        local_mode=False,
        ignore_reinit_error=False,
        include_dashboard=None,
        dashboard_host=ray_constants.DEFAULT_DASHBOARD_IP,
        dashboard_port=ray_constants.DEFAULT_DASHBOARD_PORT,
        job_config=None,
        configure_logging=True,
        logging_level=logging.INFO,
        logging_format=ray_constants.LOGGER_FORMAT,
        log_to_driver=True,
        # The following are unstable parameters and their use is discouraged.
        _enable_object_reconstruction=False,
        _redis_max_memory=None,
        _plasma_directory=None,
        _node_ip_address=ray_constants.NODE_DEFAULT_IP,
        _driver_object_store_memory=None,
        _memory=None,
        _redis_password=ray_constants.REDIS_DEFAULT_PASSWORD,
        _java_worker_options=None,
        _code_search_path=None,
        _temp_dir=None,
        _load_code_from_local=False,
        _lru_evict=False,
        _metrics_export_port=None,
        _object_spilling_config=None,
        _system_config=None):
    """"""
    Connect to an existing Ray cluster or start one and connect to it.

    This method handles two cases; either a Ray cluster already exists and we
    just attach this driver to it or we start all of the processes associated
    with a Ray cluster and attach to the newly started cluster.

    To start Ray and all of the relevant processes, use this as follows:

    .. code-block:: python

        ray.init()

    To connect to an existing Ray cluster, use this as follows (substituting
    in the appropriate address):

    .. code-block:: python

        ray.init(address=""123.45.67.89:6379"")

    You can also define an environment variable called `RAY_ADDRESS` in
    the same format as the `address` parameter to connect to an existing
    cluster with ray.init().

    Args:
        address (str): The address of the Ray cluster to connect to. If
            this address is not provided, then this command will start Redis,
            a raylet, a plasma store, a plasma manager, and some workers.
            It will also kill these processes when Python exits. If the driver
            is running on a node in a Ray cluster, using `auto` as the value
            tells the driver to detect the the cluster, removing the need to
            specify a specific node address.
        num_cpus (int): Number of CPUs the user wishes to assign to each
            raylet. By default, this is set based on virtual cores.
        num_gpus (int): Number of GPUs the user wishes to assign to each
            raylet. By default, this is set based on detected GPUs.
        resources: A dictionary mapping the names of custom resources to the
            quantities for them available.
        object_store_memory: The amount of memory (in bytes) to start the
            object store with. By default, this is automatically set based on
            available system memory.
        local_mode (bool): If true, the code will be executed serially. This
            is useful for debugging.
        ignore_reinit_error: If true, Ray suppresses errors from calling
            ray.init() a second time. Ray won't be restarted.
        include_dashboard: Boolean flag indicating whether or not to start the
            Ray dashboard, which displays the status of the Ray
            cluster. If this argument is None, then the UI will be started if
            the relevant dependencies are present.
        dashboard_host: The host to bind the dashboard server to. Can either be
            localhost (127.0.0.1) or 0.0.0.0 (available from all interfaces).
            By default, this is set to localhost to prevent access from
            external machines.
        dashboard_port: The port to bind the dashboard server to. Defaults to
            8265.
        job_config (ray.job_config.JobConfig): The job configuration.
        configure_logging: True (default) if configuration of logging is
            allowed here. Otherwise, the user may want to configure it
            separately.
        logging_level: Logging level, defaults to logging.INFO. Ignored unless
            ""configure_logging"" is true.
        logging_format: Logging format, defaults to string containing a
            timestamp, filename, line number, and message. See the source file
            ray_constants.py for details. Ignored unless ""configure_logging""
            is true.
        log_to_driver (bool): If true, the output from all of the worker
            processes on all nodes will be directed to the driver.
        _enable_object_reconstruction (bool): If True, when an object stored in
            the distributed plasma store is lost due to node failure, Ray will
            attempt to reconstruct the object by re-executing the task that
            created the object. Arguments to the task will be recursively
            reconstructed. If False, then ray.ObjectLostError will be
            thrown.
        _redis_max_memory: Redis max memory.
        _plasma_directory: Override the plasma mmap file directory.
        _node_ip_address (str): The IP address of the node that we are on.
        _driver_object_store_memory (int): Limit the amount of memory the
            driver can use in the object store for creating objects.
        _memory: Amount of reservable memory resource to create.
        _redis_password (str): Prevents external clients without the password
            from connecting to Redis if provided.
        _temp_dir (str): If provided, specifies the root temporary
            directory for the Ray process. Defaults to an OS-specific
            conventional location, e.g., ""/tmp/ray"".
        _load_code_from_local: Whether code should be loaded from a local
            module or from the GCS.
        _java_worker_options: Overwrite the options to start Java workers.
        _code_search_path (list): Java classpath or python import path.
        _lru_evict (bool): If True, when an object store is full, it will evict
            objects in LRU order to make more space and when under memory
            pressure, ray.ObjectLostError may be thrown. If False, then
            reference counting will be used to decide which objects are safe
            to evict and when under memory pressure, ray.ObjectStoreFullError
            may be thrown.
        _metrics_export_port(int): Port number Ray exposes system metrics
            through a Prometheus endpoint. It is currently under active
            development, and the API is subject to change.
        _object_spilling_config (str): The configuration json string for object
            spilling I/O worker.
        _system_config (str): JSON configuration for overriding
            RayConfig defaults. For testing purposes ONLY.

    Returns:
        Address information about the started processes.

    Raises:
        Exception: An exception is raised if an inappropriate combination of
            arguments is passed in.
    """"""

    # Try to increase the file descriptor limit, which is too low by
    # default for Ray: https://github.com/ray-project/ray/issues/11239
    try:
        import resource
        soft, hard = resource.getrlimit(resource.RLIMIT_NOFILE)
        if soft < hard:
            logger.debug(""Automatically increasing RLIMIT_NOFILE to max ""
                         ""value of {}"".format(hard))
            try:
                resource.setrlimit(resource.RLIMIT_NOFILE, (hard, hard))
            except ValueError:
                logger.debug(""Failed to raise limit."")
        soft, _ = resource.getrlimit(resource.RLIMIT_NOFILE)
        if soft < 4096:
            logger.warning(
                ""File descriptor limit {} is too low for production ""
                ""servers and may result in connection errors. ""
                ""At least 8192 is recommended. --- ""
                ""Fix with 'ulimit -n 8192'"".format(soft))
    except ImportError:
        logger.debug(""Could not import resource module (on Windows)"")
        pass

    if ""RAY_ADDRESS"" in os.environ:
        if address is None or address == ""auto"":
            address = os.environ[""RAY_ADDRESS""]
        else:
            raise RuntimeError(
                ""Cannot use both the RAY_ADDRESS environment variable and ""
                ""the address argument of ray.init simultaneously. If you ""
                ""use RAY_ADDRESS to connect to a specific Ray cluster, ""
                ""please call ray.init() or ray.init(address=\\""auto\\"") on the ""
                ""driver."")

    # Convert hostnames to numerical IP address.
    if _node_ip_address is not None:
        node_ip_address = services.address_to_ip(_node_ip_address)
    raylet_ip_address = node_ip_address

    if address:
        redis_address, _, _ = services.validate_redis_address(address)
    else:
        redis_address = None

    if configure_logging:
        setup_logger(logging_level, logging_format)

    if redis_address is not None:
        logger.info(
            f""Connecting to existing Ray cluster at address: {redis_address}"")

    if local_mode:
        driver_mode = LOCAL_MODE
    else:
        driver_mode = SCRIPT_MODE

    if global_worker.connected:
        if ignore_reinit_error:
            logger.error(""Calling ray.init() again after it has already been ""
                         ""called."")
            return
        else:
            raise RuntimeError(""Maybe you called ray.init twice by accident? ""
                               ""This error can be suppressed by passing in ""
                               ""'ignore_reinit_error=True' or by calling ""
                               ""'ray.shutdown()' prior to 'ray.init()'."")

    _system_config = _system_config or {}
    if not isinstance(_system_config, dict):
        raise TypeError(""The _system_config must be a dict."")

    global _global_node
    if redis_address is None:
        # In this case, we need to start a new cluster.
        ray_params = ray.parameter.RayParams(
            redis_address=redis_address,
            node_ip_address=node_ip_address,
            raylet_ip_address=raylet_ip_address,
            object_ref_seed=None,
            driver_mode=driver_mode,
            redirect_worker_output=None,
            redirect_output=None,
            num_cpus=num_cpus,
            num_gpus=num_gpus,
            resources=resources,
            num_redis_shards=None,
            redis_max_clients=None,
            redis_password=_redis_password,
            plasma_directory=_plasma_directory,
            huge_pages=None,
            include_dashboard=include_dashboard,
            dashboard_host=dashboard_host,
            dashboard_port=dashboard_port,
            memory=_memory,
            object_store_memory=object_store_memory,
            redis_max_memory=_redis_max_memory,
            plasma_store_socket_name=None,
            temp_dir=_temp_dir,
            load_code_from_local=_load_code_from_local,
            java_worker_options=_java_worker_options,
            code_search_path=_code_search_path,
            start_initial_python_workers_for_first_job=True,
            _system_config=_system_config,
            lru_evict=_lru_evict,
            enable_object_reconstruction=_enable_object_reconstruction,
            metrics_export_port=_metrics_export_port,
            object_spilling_config=_object_spilling_config)
        # Start the Ray processes. We set shutdown_at_exit=False because we
        # shutdown the node in the ray.shutdown call that happens in the atexit
        # handler. We still spawn a reaper process in case the atexit handler
        # isn't called.
        _global_node = ray.node.Node(
            head=True,
            shutdown_at_exit=False,
            spawn_reaper=True,
            ray_params=ray_params)
    else:
        # In this case, we are connecting to an existing cluster.
        if num_cpus is not None or num_gpus is not None:
            raise ValueError(
                ""When connecting to an existing cluster, num_cpus ""
                ""and num_gpus must not be provided."")
        if resources is not None:
            raise ValueError(""When connecting to an existing cluster, ""
                             ""resources must not be provided."")
        if object_store_memory is not None:
            raise ValueError(""When connecting to an existing cluster, ""
                             ""object_store_memory must not be provided."")
        if _system_config is not None and len(_system_config) != 0:
            raise ValueError(""When connecting to an existing cluster, ""
                             ""_system_config must not be provided."")
        if _lru_evict:
            raise ValueError(""When connecting to an existing cluster, ""
                             ""_lru_evict must not be provided."")
        if _enable_object_reconstruction:
            raise ValueError(
                ""When connecting to an existing cluster, ""
                ""_enable_object_reconstruction must not be provided."")

        # In this case, we only need to connect the node.
        ray_params = ray.parameter.RayParams(
            node_ip_address=node_ip_address,
            raylet_ip_address=raylet_ip_address,
            redis_address=redis_address,
            redis_password=_redis_password,
            object_ref_seed=None,
            temp_dir=_temp_dir,
            load_code_from_local=_load_code_from_local,
            _system_config=_system_config,
            lru_evict=_lru_evict,
            enable_object_reconstruction=_enable_object_reconstruction,
            metrics_export_port=_metrics_export_port)
        _global_node = ray.node.Node(
            ray_params,
            head=False,
            shutdown_at_exit=False,
            spawn_reaper=False,
            connect_only=True)

    connect(
        _global_node,
        mode=driver_mode,
        log_to_driver=log_to_driver,
        worker=global_worker,
        driver_object_store_memory=_driver_object_store_memory,
        job_id=None,
        job_config=job_config)

    for hook in _post_init_hooks:
        hook()

    node_id = global_worker.core_worker.get_current_node_id()
    return dict(_global_node.address_info, node_id=node_id.hex())","def init(
        address=None,
        *,
        num_cpus=None,
        num_gpus=None,
        resources=None,
        object_store_memory=None,
        local_mode=False,
        ignore_reinit_error=False,
        include_dashboard=None,
        dashboard_host=ray_constants.DEFAULT_DASHBOARD_IP,
        dashboard_port=ray_constants.DEFAULT_DASHBOARD_PORT,
        job_config=None,
        configure_logging=True,
        logging_level=logging.INFO,
        logging_format=ray_constants.LOGGER_FORMAT,
        log_to_driver=True,
        # The following are unstable parameters and their use is discouraged.
        _enable_object_reconstruction=False,
        _redis_max_memory=None,
        _plasma_directory=None,
        _node_ip_address=ray_constants.NODE_DEFAULT_IP,
        _driver_object_store_memory=None,
        _memory=None,
        _redis_password=ray_constants.REDIS_DEFAULT_PASSWORD,
        _java_worker_options=None,
        _code_search_path=None,
        _temp_dir=None,
        _load_code_from_local=False,
        _lru_evict=False,
        _metrics_export_port=None,
        _object_spilling_config=None,
        _system_config=None):
    """"""
    Connect to an existing Ray cluster or start one and connect to it.

    This method handles two cases; either a Ray cluster already exists and we
    just attach this driver to it or we start all of the processes associated
    with a Ray cluster and attach to the newly started cluster.

    To start Ray and all of the relevant processes, use this as follows:

    .. code-block:: python

        ray.init()

    To connect to an existing Ray cluster, use this as follows (substituting
    in the appropriate address):

    .. code-block:: python

        ray.init(address=""123.45.67.89:6379"")

    You can also define an environment variable called `RAY_ADDRESS` in
    the same format as the `address` parameter to connect to an existing
    cluster with ray.init().

    Args:
        address (str): The address of the Ray cluster to connect to. If
            this address is not provided, then this command will start Redis,
            a raylet, a plasma store, a plasma manager, and some workers.
            It will also kill these processes when Python exits. If the driver
            is running on a node in a Ray cluster, using `auto` as the value
            tells the driver to detect the the cluster, removing the need to
            specify a specific node address.
        num_cpus (int): Number of CPUs the user wishes to assign to each
            raylet. By default, this is set based on virtual cores.
        num_gpus (int): Number of GPUs the user wishes to assign to each
            raylet. By default, this is set based on detected GPUs.
        resources: A dictionary mapping the names of custom resources to the
            quantities for them available.
        object_store_memory: The amount of memory (in bytes) to start the
            object store with. By default, this is automatically set based on
            available system memory.
        local_mode (bool): If true, the code will be executed serially. This
            is useful for debugging.
        ignore_reinit_error: If true, Ray suppresses errors from calling
            ray.init() a second time. Ray won't be restarted.
        include_dashboard: Boolean flag indicating whether or not to start the
            Ray dashboard, which displays the status of the Ray
            cluster. If this argument is None, then the UI will be started if
            the relevant dependencies are present.
        dashboard_host: The host to bind the dashboard server to. Can either be
            localhost (127.0.0.1) or 0.0.0.0 (available from all interfaces).
            By default, this is set to localhost to prevent access from
            external machines.
        dashboard_port: The port to bind the dashboard server to. Defaults to
            8265.
        job_config (ray.job_config.JobConfig): The job configuration.
        configure_logging: True (default) if configuration of logging is
            allowed here. Otherwise, the user may want to configure it
            separately.
        logging_level: Logging level, defaults to logging.INFO. Ignored unless
            ""configure_logging"" is true.
        logging_format: Logging format, defaults to string containing a
            timestamp, filename, line number, and message. See the source file
            ray_constants.py for details. Ignored unless ""configure_logging""
            is true.
        log_to_driver (bool): If true, the output from all of the worker
            processes on all nodes will be directed to the driver.
        _enable_object_reconstruction (bool): If True, when an object stored in
            the distributed plasma store is lost due to node failure, Ray will
            attempt to reconstruct the object by re-executing the task that
            created the object. Arguments to the task will be recursively
            reconstructed. If False, then ray.ObjectLostError will be
            thrown.
        _redis_max_memory: Redis max memory.
        _plasma_directory: Override the plasma mmap file directory.
        _node_ip_address (str): The IP address of the node that we are on.
        _driver_object_store_memory (int): Limit the amount of memory the
            driver can use in the object store for creating objects.
        _memory: Amount of reservable memory resource to create.
        _redis_password (str): Prevents external clients without the password
            from connecting to Redis if provided.
        _temp_dir (str): If provided, specifies the root temporary
            directory for the Ray process. Defaults to an OS-specific
            conventional location, e.g., ""/tmp/ray"".
        _load_code_from_local: Whether code should be loaded from a local
            module or from the GCS.
        _java_worker_options: Overwrite the options to start Java workers.
        _code_search_path (list): Java classpath or python import path.
        _lru_evict (bool): If True, when an object store is full, it will evict
            objects in LRU order to make more space and when under memory
            pressure, ray.ObjectLostError may be thrown. If False, then
            reference counting will be used to decide which objects are safe
            to evict and when under memory pressure, ray.ObjectStoreFullError
            may be thrown.
        _metrics_export_port(int): Port number Ray exposes system metrics
            through a Prometheus endpoint. It is currently under active
            development, and the API is subject to change.
        _object_spilling_config (str): The configuration json string for object
            spilling I/O worker.
        _system_config (str): JSON configuration for overriding
            RayConfig defaults. For testing purposes ONLY.

    Returns:
        Address information about the started processes.

    Raises:
        Exception: An exception is raised if an inappropriate combination of
            arguments is passed in.
    """"""

    if ""RAY_ADDRESS"" in os.environ:
        if address is None or address == ""auto"":
            address = os.environ[""RAY_ADDRESS""]
        else:
            raise RuntimeError(
                ""Cannot use both the RAY_ADDRESS environment variable and ""
                ""the address argument of ray.init simultaneously. If you ""
                ""use RAY_ADDRESS to connect to a specific Ray cluster, ""
                ""please call ray.init() or ray.init(address=\\""auto\\"") on the ""
                ""driver."")

    # Convert hostnames to numerical IP address.
    if _node_ip_address is not None:
        node_ip_address = services.address_to_ip(_node_ip_address)
    raylet_ip_address = node_ip_address

    if address:
        redis_address, _, _ = services.validate_redis_address(address)
    else:
        redis_address = None

    if configure_logging:
        setup_logger(logging_level, logging_format)

    if redis_address is not None:
        logger.info(
            f""Connecting to existing Ray cluster at address: {redis_address}"")

    if local_mode:
        driver_mode = LOCAL_MODE
    else:
        driver_mode = SCRIPT_MODE

    if global_worker.connected:
        if ignore_reinit_error:
            logger.error(""Calling ray.init() again after it has already been ""
                         ""called."")
            return
        else:
            raise RuntimeError(""Maybe you called ray.init twice by accident? ""
                               ""This error can be suppressed by passing in ""
                               ""'ignore_reinit_error=True' or by calling ""
                               ""'ray.shutdown()' prior to 'ray.init()'."")

    _system_config = _system_config or {}
    if not isinstance(_system_config, dict):
        raise TypeError(""The _system_config must be a dict."")

    global _global_node
    if redis_address is None:
        # In this case, we need to start a new cluster.
        ray_params = ray.parameter.RayParams(
            redis_address=redis_address,
            node_ip_address=node_ip_address,
            raylet_ip_address=raylet_ip_address,
            object_ref_seed=None,
            driver_mode=driver_mode,
            redirect_worker_output=None,
            redirect_output=None,
            num_cpus=num_cpus,
            num_gpus=num_gpus,
            resources=resources,
            num_redis_shards=None,
            redis_max_clients=None,
            redis_password=_redis_password,
            plasma_directory=_plasma_directory,
            huge_pages=None,
            include_dashboard=include_dashboard,
            dashboard_host=dashboard_host,
            dashboard_port=dashboard_port,
            memory=_memory,
            object_store_memory=object_store_memory,
            redis_max_memory=_redis_max_memory,
            plasma_store_socket_name=None,
            temp_dir=_temp_dir,
            load_code_from_local=_load_code_from_local,
            java_worker_options=_java_worker_options,
            code_search_path=_code_search_path,
            start_initial_python_workers_for_first_job=True,
            _system_config=_system_config,
            lru_evict=_lru_evict,
            enable_object_reconstruction=_enable_object_reconstruction,
            metrics_export_port=_metrics_export_port,
            object_spilling_config=_object_spilling_config)
        # Start the Ray processes. We set shutdown_at_exit=False because we
        # shutdown the node in the ray.shutdown call that happens in the atexit
        # handler. We still spawn a reaper process in case the atexit handler
        # isn't called.
        _global_node = ray.node.Node(
            head=True,
            shutdown_at_exit=False,
            spawn_reaper=True,
            ray_params=ray_params)
    else:
        # In this case, we are connecting to an existing cluster.
        if num_cpus is not None or num_gpus is not None:
            raise ValueError(
                ""When connecting to an existing cluster, num_cpus ""
                ""and num_gpus must not be provided."")
        if resources is not None:
            raise ValueError(""When connecting to an existing cluster, ""
                             ""resources must not be provided."")
        if object_store_memory is not None:
            raise ValueError(""When connecting to an existing cluster, ""
                             ""object_store_memory must not be provided."")
        if _system_config is not None and len(_system_config) != 0:
            raise ValueError(""When connecting to an existing cluster, ""
                             ""_system_config must not be provided."")
        if _lru_evict:
            raise ValueError(""When connecting to an existing cluster, ""
                             ""_lru_evict must not be provided."")
        if _enable_object_reconstruction:
            raise ValueError(
                ""When connecting to an existing cluster, ""
                ""_enable_object_reconstruction must not be provided."")

        # In this case, we only need to connect the node.
        ray_params = ray.parameter.RayParams(
            node_ip_address=node_ip_address,
            raylet_ip_address=raylet_ip_address,
            redis_address=redis_address,
            redis_password=_redis_password,
            object_ref_seed=None,
            temp_dir=_temp_dir,
            load_code_from_local=_load_code_from_local,
            _system_config=_system_config,
            lru_evict=_lru_evict,
            enable_object_reconstruction=_enable_object_reconstruction,
            metrics_export_port=_metrics_export_port)
        _global_node = ray.node.Node(
            ray_params,
            head=False,
            shutdown_at_exit=False,
            spawn_reaper=False,
            connect_only=True)

    connect(
        _global_node,
        mode=driver_mode,
        log_to_driver=log_to_driver,
        worker=global_worker,
        driver_object_store_memory=_driver_object_store_memory,
        job_id=None,
        job_config=job_config)

    for hook in _post_init_hooks:
        hook()

    node_id = global_worker.core_worker.get_current_node_id()
    return dict(_global_node.address_info, node_id=node_id.hex())"
"def with_parameters(fn, **kwargs):
    """"""Wrapper for function trainables to pass arbitrary large data objects.

    This wrapper function will store all passed parameters in the Ray
    object store and retrieve them when calling the function. It can thus
    be used to pass arbitrary data, even datasets, to Tune trainable functions.

    This can also be used as an alternative to `functools.partial` to pass
    default arguments to trainables.

    Args:
        fn: function to wrap
        **kwargs: parameters to store in object store.


    .. code-block:: python

        from ray import tune

        def train(config, data=None):
            for sample in data:
                # ...
                tune.report(loss=loss)

        data = HugeDataset(download=True)

        tune.run(
            tune.with_parameters(train, data=data),
            #...
        )

    """"""
    if not callable(fn):
        raise ValueError(
            ""`tune.with_parameters()` only works with the function API. ""
            ""If you want to pass parameters to Trainable _classes_, consider ""
            ""passing them via the `config` parameter."")

    prefix = f""{str(fn)}_""
    for k, v in kwargs.items():
        parameter_registry.put(prefix + k, v)

    use_checkpoint = detect_checkpoint_function(fn)

    def inner(config, checkpoint_dir=None):
        fn_kwargs = {}
        if use_checkpoint:
            default = checkpoint_dir
            sig = inspect.signature(fn)
            if ""checkpoint_dir"" in sig.parameters:
                default = sig.parameters[""checkpoint_dir""].default \\
                          or default
            fn_kwargs[""checkpoint_dir""] = default

        for k in kwargs:
            fn_kwargs[k] = parameter_registry.get(prefix + k)
        fn(config, **fn_kwargs)

    # Use correct function signature if no `checkpoint_dir` parameter is set
    if not use_checkpoint:

        def _inner(config):
            inner(config, checkpoint_dir=None)

        return _inner

    return inner","def with_parameters(fn, **kwargs):
    """"""Wrapper for function trainables to pass arbitrary large data objects.

    This wrapper function will store all passed parameters in the Ray
    object store and retrieve them when calling the function. It can thus
    be used to pass arbitrary data, even datasets, to Tune trainable functions.

    This can also be used as an alternative to `functools.partial` to pass
    default arguments to trainables.

    Args:
        fn: function to wrap
        **kwargs: parameters to store in object store.


    .. code-block:: python

        from ray import tune

        def train(config, data=None):
            for sample in data:
                # ...
                tune.report(loss=loss)

        data = HugeDataset(download=True)

        tune.run(
            tune.with_parameters(train, data=data),
            #...
        )

    """"""
    prefix = f""{str(fn)}_""
    for k, v in kwargs.items():
        parameter_registry.put(prefix + k, v)

    use_checkpoint = detect_checkpoint_function(fn)

    def inner(config, checkpoint_dir=None):
        fn_kwargs = {}
        if use_checkpoint:
            default = checkpoint_dir
            sig = inspect.signature(fn)
            if ""checkpoint_dir"" in sig.parameters:
                default = sig.parameters[""checkpoint_dir""].default \\
                          or default
            fn_kwargs[""checkpoint_dir""] = default

        for k in kwargs:
            fn_kwargs[k] = parameter_registry.get(prefix + k)
        fn(config, **fn_kwargs)

    # Use correct function signature if no `checkpoint_dir` parameter is set
    if not use_checkpoint:

        def _inner(config):
            inner(config, checkpoint_dir=None)

        return _inner

    return inner"
"def wandb_mixin(func: Callable):
    """"""wandb_mixin

    Weights and biases (https://www.wandb.com/) is a tool for experiment
    tracking, model optimization, and dataset versioning. This Ray Tune
    Trainable mixin helps initializing the Wandb API for use with the
    ``Trainable`` class or with `@wandb_mixin` for the function API.

    For basic usage, just prepend your training function with the
    ``@wandb_mixin`` decorator:

    .. code-block:: python

        from ray.tune.integration.wandb import wandb_mixin

        @wandb_mixin
        def train_fn(config):
            wandb.log()


    Wandb configuration is done by passing a ``wandb`` key to
    the ``config`` parameter of ``tune.run()`` (see example below).

    The content of the ``wandb`` config entry is passed to ``wandb.init()``
    as keyword arguments. The exception are the following settings, which
    are used to configure the ``WandbTrainableMixin`` itself:

    Args:
        api_key_file (str): Path to file containing the Wandb API KEY. This
            file must be on all nodes if using the `wandb_mixin`.
        api_key (str): Wandb API Key. Alternative to setting `api_key_file`.

    Wandb's ``group``, ``run_id`` and ``run_name`` are automatically selected
    by Tune, but can be overwritten by filling out the respective configuration
    values.

    Please see here for all other valid configuration settings:
    https://docs.wandb.com/library/init

    Example:

    .. code-block:: python

        from ray import tune
        from ray.tune.integration.wandb import wandb_mixin

        @wandb_mixin
        def train_fn(config):
            for i in range(10):
                loss = self.config[""a""] + self.config[""b""]
                wandb.log({""loss"": loss})
            tune.report(loss=loss, done=True)

        tune.run(
            train_fn,
            config={
                # define search space here
                ""a"": tune.choice([1, 2, 3]),
                ""b"": tune.choice([4, 5, 6]),
                # wandb configuration
                ""wandb"": {
                    ""project"": ""Optimization_Project"",
                    ""api_key_file"": ""/path/to/file""
                }
            })

    """"""
    func.__mixins__ = (WandbTrainableMixin, )
    return func","def wandb_mixin(func: Callable):
    """"""wandb_mixin

    Weights and biases (https://www.wandb.com/) is a tool for experiment
    tracking, model optimization, and dataset versioning. This Ray Tune
    Trainable mixin helps initializing the Wandb API for use with the
    ``Trainable`` class or with `@wandb_mixin` for the function API.

    For basic usage, just prepend your training function with the
    ``@wandb_mixin`` decorator:

    .. code-block:: python

        from ray.tune.integration.wandb import wandb_mixin

        @wandb_mixin
        def train_fn(config):
            wandb.log()


    Wandb configuration is done by passing a ``wandb`` key to
    the ``config`` parameter of ``tune.run()`` (see example below).

    The content of the ``wandb`` config entry is passed to ``wandb.init()``
    as keyword arguments. The exception are the following settings, which
    are used to configure the ``WandbTrainableMixin`` itself:

    Args:
        api_key_file (str): Path to file containing the Wandb API KEY. This
            file must be on all nodes if using the `wandb_mixin`.
        api_key (str): Wandb API Key. Alternative to setting `api_key_file`.

    Wandb's ``group``, ``run_id`` and ``run_name`` are automatically selected
    by Tune, but can be overwritten by filling out the respective configuration
    values.

    Please see here for all other valid configuration settings:
    https://docs.wandb.com/library/init

    Example:

    .. code-block:: python

        from ray import tune
        from ray.tune.integration.wandb import wandb_mixin

        @wandb_mixin
        def train_fn(config):
            for i in range(10):
                loss = self.config[""a""] + self.config[""b""]
                wandb.log({""loss"": loss})
            tune.report(loss=loss, done=True)

        tune.run(
            train_fn,
            config={
                # define search space here
                ""a"": tune.choice([1, 2, 3]),
                ""b"": tune.choice([4, 5, 6]),
                # wandb configuration
                ""wandb"": {
                    ""project"": ""Optimization_Project"",
                    ""api_key_file"": ""/path/to/file""
                }
            })

    """"""
    func.__mixins__ = (WandbTrainableMixin, )
    func.__wandb_group__ = func.__name__
    return func"
"    def _init(self):
        config = self.config.copy()

        config.pop(""callbacks"", None)  # Remove callbacks

        try:
            if config.get(""logger_config"", {}).get(""wandb""):
                logger_config = config.pop(""logger_config"")
                wandb_config = logger_config.get(""wandb"").copy()
            else:
                wandb_config = config.pop(""wandb"").copy()
        except KeyError:
            raise ValueError(
                ""Wandb logger specified but no configuration has been passed. ""
                ""Make sure to include a `wandb` key in your `config` dict ""
                ""containing at least a `project` specification."")

        _set_api_key(wandb_config)

        exclude_results = self._exclude_results.copy()

        # Additional excludes
        additional_excludes = wandb_config.pop(""excludes"", [])
        exclude_results += additional_excludes

        # Log config keys on each result?
        log_config = wandb_config.pop(""log_config"", False)
        if not log_config:
            exclude_results += [""config""]

        # Fill trial ID and name
        trial_id = self.trial.trial_id if self.trial else None
        trial_name = str(self.trial) if self.trial else None

        # Project name for Wandb
        try:
            wandb_project = wandb_config.pop(""project"")
        except KeyError:
            raise ValueError(
                ""You need to specify a `project` in your wandb `config` dict."")

        # Grouping
        wandb_group = wandb_config.pop(
            ""group"", self.trial.trainable_name if self.trial else None)

        # remove unpickleable items!
        config = _clean_log(config)

        wandb_init_kwargs = dict(
            id=trial_id,
            name=trial_name,
            resume=True,
            reinit=True,
            allow_val_change=True,
            group=wandb_group,
            project=wandb_project,
            config=config)
        wandb_init_kwargs.update(wandb_config)

        self._queue = Queue()
        self._wandb = self._logger_process_cls(
            queue=self._queue,
            exclude=exclude_results,
            to_config=self._config_results,
            **wandb_init_kwargs)
        self._wandb.start()","    def _init(self):
        config = self.config.copy()

        config.pop(""callbacks"", None)  # Remove callbacks

        try:
            if config.get(""logger_config"", {}).get(""wandb""):
                logger_config = config.pop(""logger_config"")
                wandb_config = logger_config.get(""wandb"").copy()
            else:
                wandb_config = config.pop(""wandb"").copy()
        except KeyError:
            raise ValueError(
                ""Wandb logger specified but no configuration has been passed. ""
                ""Make sure to include a `wandb` key in your `config` dict ""
                ""containing at least a `project` specification."")

        _set_api_key(wandb_config)

        exclude_results = self._exclude_results.copy()

        # Additional excludes
        additional_excludes = wandb_config.pop(""excludes"", [])
        exclude_results += additional_excludes

        # Log config keys on each result?
        log_config = wandb_config.pop(""log_config"", False)
        if not log_config:
            exclude_results += [""config""]

        # Fill trial ID and name
        trial_id = self.trial.trial_id
        trial_name = str(self.trial)

        # Project name for Wandb
        try:
            wandb_project = wandb_config.pop(""project"")
        except KeyError:
            raise ValueError(
                ""You need to specify a `project` in your wandb `config` dict."")

        # Grouping
        wandb_group = wandb_config.pop(""group"", self.trial.trainable_name)

        # remove unpickleable items!
        config = _clean_log(config)

        wandb_init_kwargs = dict(
            id=trial_id,
            name=trial_name,
            resume=True,
            reinit=True,
            allow_val_change=True,
            group=wandb_group,
            project=wandb_project,
            config=config)
        wandb_init_kwargs.update(wandb_config)

        self._queue = Queue()
        self._wandb = self._logger_process_cls(
            queue=self._queue,
            exclude=exclude_results,
            to_config=self._config_results,
            **wandb_init_kwargs)
        self._wandb.start()"
"def Concurrently(ops: List[LocalIterator],
                 *,
                 mode=""round_robin"",
                 output_indexes=None,
                 round_robin_weights=None):
    """"""Operator that runs the given parent iterators concurrently.

    Args:
        mode (str): One of 'round_robin', 'async'. In 'round_robin' mode,
            we alternate between pulling items from each parent iterator in
            order deterministically. In 'async' mode, we pull from each parent
            iterator as fast as they are produced. This is non-deterministic.
        output_indexes (list): If specified, only output results from the
            given ops. For example, if ``output_indexes=[0]``, only results
            from the first op in ops will be returned.
        round_robin_weights (list): List of weights to use for round robin
            mode. For example, ``[2, 1]`` will cause the iterator to pull twice
            as many items from the first iterator as the second. ``[2, 1, *]``
            will cause as many items to be pulled as possible from the third
            iterator without blocking. This is only allowed in round robin
            mode.

    Examples:
        >>> sim_op = ParallelRollouts(...).for_each(...)
        >>> replay_op = LocalReplay(...).for_each(...)
        >>> combined_op = Concurrently([sim_op, replay_op], mode=""async"")
    """"""

    if len(ops) < 2:
        raise ValueError(""Should specify at least 2 ops."")
    if mode == ""round_robin"":
        deterministic = True
    elif mode == ""async"":
        deterministic = False
        if round_robin_weights:
            raise ValueError(
                ""round_robin_weights cannot be specified in async mode"")
    else:
        raise ValueError(""Unknown mode {}"".format(mode))
    if round_robin_weights and all(r == ""*"" for r in round_robin_weights):
        raise ValueError(""Cannot specify all round robin weights = *"")

    if output_indexes:
        for i in output_indexes:
            assert i in range(len(ops)), (""Index out of range"", i)

        def tag(op, i):
            return op.for_each(lambda x: (i, x))

        ops = [tag(op, i) for i, op in enumerate(ops)]

    output = ops[0].union(
        *ops[1:],
        deterministic=deterministic,
        round_robin_weights=round_robin_weights)

    if output_indexes:
        output = (output.filter(lambda tup: tup[0] in output_indexes)
                  .for_each(lambda tup: tup[1]))

    return output","def Concurrently(ops: List[LocalIterator],
                 *,
                 mode=""round_robin"",
                 output_indexes=None,
                 round_robin_weights=None):
    """"""Operator that runs the given parent iterators concurrently.

    Args:
        mode (str): One of {'round_robin', 'async'}.
            - In 'round_robin' mode, we alternate between pulling items from
              each parent iterator in order deterministically.
            - In 'async' mode, we pull from each parent iterator as fast as
              they are produced. This is non-deterministic.
        output_indexes (list): If specified, only output results from the
            given ops. For example, if output_indexes=[0], only results from
            the first op in ops will be returned.
        round_robin_weights (list): List of weights to use for round robin
            mode. For example, [2, 1] will cause the iterator to pull twice
            as many items from the first iterator as the second. [2, 1, *] will
            cause as many items to be pulled as possible from the third
            iterator without blocking. This is only allowed in round robin
            mode.

        >>> sim_op = ParallelRollouts(...).for_each(...)
        >>> replay_op = LocalReplay(...).for_each(...)
        >>> combined_op = Concurrently([sim_op, replay_op], mode=""async"")
    """"""

    if len(ops) < 2:
        raise ValueError(""Should specify at least 2 ops."")
    if mode == ""round_robin"":
        deterministic = True
    elif mode == ""async"":
        deterministic = False
        if round_robin_weights:
            raise ValueError(
                ""round_robin_weights cannot be specified in async mode"")
    else:
        raise ValueError(""Unknown mode {}"".format(mode))
    if round_robin_weights and all(r == ""*"" for r in round_robin_weights):
        raise ValueError(""Cannot specify all round robin weights = *"")

    if output_indexes:
        for i in output_indexes:
            assert i in range(len(ops)), (""Index out of range"", i)

        def tag(op, i):
            return op.for_each(lambda x: (i, x))

        ops = [tag(op, i) for i, op in enumerate(ops)]

    output = ops[0].union(
        *ops[1:],
        deterministic=deterministic,
        round_robin_weights=round_robin_weights)

    if output_indexes:
        output = (output.filter(lambda tup: tup[0] in output_indexes)
                  .for_each(lambda tup: tup[1]))

    return output"
"def ParallelRollouts(workers: WorkerSet, *, mode=""bulk_sync"",
                     num_async=1) -> LocalIterator[SampleBatch]:
    """"""Operator to collect experiences in parallel from rollout workers.

    If there are no remote workers, experiences will be collected serially from
    the local worker instance instead.

    Args:
        workers (WorkerSet): set of rollout workers to use.
        mode (str): One of 'async', 'bulk_sync', 'raw'. In 'async' mode,
            batches are returned as soon as they are computed by rollout
            workers with no order guarantees. In 'bulk_sync' mode, we collect
            one batch from each worker and concatenate them together into a
            large batch to return. In 'raw' mode, the ParallelIterator object
            is returned directly and the caller is responsible for implementing
            gather and updating the timesteps counter.
        num_async (int): In async mode, the max number of async
            requests in flight per actor.

    Returns:
        A local iterator over experiences collected in parallel.

    Examples:
        >>> rollouts = ParallelRollouts(workers, mode=""async"")
        >>> batch = next(rollouts)
        >>> print(batch.count)
        50  # config.rollout_fragment_length

        >>> rollouts = ParallelRollouts(workers, mode=""bulk_sync"")
        >>> batch = next(rollouts)
        >>> print(batch.count)
        200  # config.rollout_fragment_length * config.num_workers

    Updates the STEPS_SAMPLED_COUNTER counter in the local iterator context.
    """"""

    # Ensure workers are initially in sync.
    workers.sync_weights()

    def report_timesteps(batch):
        metrics = _get_shared_metrics()
        metrics.counters[STEPS_SAMPLED_COUNTER] += batch.count
        return batch

    if not workers.remote_workers():
        # Handle the serial sampling case.
        def sampler(_):
            while True:
                yield workers.local_worker().sample()

        return (LocalIterator(sampler, SharedMetrics())
                .for_each(report_timesteps))

    # Create a parallel iterator over generated experiences.
    rollouts = from_actors(workers.remote_workers())

    if mode == ""bulk_sync"":
        return rollouts \\
            .batch_across_shards() \\
            .for_each(lambda batches: SampleBatch.concat_samples(batches)) \\
            .for_each(report_timesteps)
    elif mode == ""async"":
        return rollouts.gather_async(
            num_async=num_async).for_each(report_timesteps)
    elif mode == ""raw"":
        return rollouts
    else:
        raise ValueError(""mode must be one of 'bulk_sync', 'async', 'raw', ""
                         ""got '{}'"".format(mode))","def ParallelRollouts(workers: WorkerSet, *, mode=""bulk_sync"",
                     num_async=1) -> LocalIterator[SampleBatch]:
    """"""Operator to collect experiences in parallel from rollout workers.

    If there are no remote workers, experiences will be collected serially from
    the local worker instance instead.

    Args:
        workers (WorkerSet): set of rollout workers to use.
        mode (str): One of {'async', 'bulk_sync', 'raw'}.
            - In 'async' mode, batches are returned as soon as they are
              computed by rollout workers with no order guarantees.
            - In 'bulk_sync' mode, we collect one batch from each worker
              and concatenate them together into a large batch to return.
            - In 'raw' mode, the ParallelIterator object is returned directly
              and the caller is responsible for implementing gather and
              updating the timesteps counter.
        num_async (int): In async mode, the max number of async
            requests in flight per actor.

    Returns:
        A local iterator over experiences collected in parallel.

    Examples:
        >>> rollouts = ParallelRollouts(workers, mode=""async"")
        >>> batch = next(rollouts)
        >>> print(batch.count)
        50  # config.rollout_fragment_length

        >>> rollouts = ParallelRollouts(workers, mode=""bulk_sync"")
        >>> batch = next(rollouts)
        >>> print(batch.count)
        200  # config.rollout_fragment_length * config.num_workers

    Updates the STEPS_SAMPLED_COUNTER counter in the local iterator context.
    """"""

    # Ensure workers are initially in sync.
    workers.sync_weights()

    def report_timesteps(batch):
        metrics = _get_shared_metrics()
        metrics.counters[STEPS_SAMPLED_COUNTER] += batch.count
        return batch

    if not workers.remote_workers():
        # Handle the serial sampling case.
        def sampler(_):
            while True:
                yield workers.local_worker().sample()

        return (LocalIterator(sampler, SharedMetrics())
                .for_each(report_timesteps))

    # Create a parallel iterator over generated experiences.
    rollouts = from_actors(workers.remote_workers())

    if mode == ""bulk_sync"":
        return rollouts \\
            .batch_across_shards() \\
            .for_each(lambda batches: SampleBatch.concat_samples(batches)) \\
            .for_each(report_timesteps)
    elif mode == ""async"":
        return rollouts.gather_async(
            num_async=num_async).for_each(report_timesteps)
    elif mode == ""raw"":
        return rollouts
    else:
        raise ValueError(""mode must be one of 'bulk_sync', 'async', 'raw', ""
                         ""got '{}'"".format(mode))"
"    def _get_node_specific_docker_config(self, node_id):
        if ""docker"" not in self.config:
            return {}
        docker_config = copy.deepcopy(self.config.get(""docker"", {}))
        node_specific_docker = self._get_node_type_specific_fields(
            node_id, ""docker"")
        docker_config.update(node_specific_docker)
        return docker_config","    def _get_node_specific_docker_config(self, node_id):
        docker_config = copy.deepcopy(self.config.get(""docker"", {}))
        node_specific_docker = self._get_node_type_specific_fields(
            node_id, ""docker"")
        docker_config.update(node_specific_docker)
        return docker_config"
"def _clean_log(obj):
    # Fixes https://github.com/ray-project/ray/issues/10631
    if isinstance(obj, dict):
        return {k: _clean_log(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [_clean_log(v) for v in obj]

    # Else
    try:
        pickle.dumps(obj)
        yaml.dump(
            obj,
            Dumper=yaml.SafeDumper,
            default_flow_style=False,
            allow_unicode=True,
            encoding=""utf-8"")
        return obj
    except Exception:
        # give up, similar to _SafeFallBackEncoder
        return str(obj)","def _clean_log(obj):
    # Fixes https://github.com/ray-project/ray/issues/10631
    if isinstance(obj, dict):
        return {k: _clean_log(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [_clean_log(v) for v in obj]

    # Else
    try:
        pickle.dumps(obj)
        return obj
    except Exception:
        # give up, similar to _SafeFallBackEncoder
        return str(obj)"
"    def _init(self):
        config = self.config.copy()

        config.pop(""callbacks"", None)  # Remove callbacks

        try:
            if config.get(""logger_config"", {}).get(""wandb""):
                logger_config = config.pop(""logger_config"")
                wandb_config = logger_config.get(""wandb"").copy()
            else:
                wandb_config = config.pop(""wandb"").copy()
        except KeyError:
            raise ValueError(
                ""Wandb logger specified but no configuration has been passed. ""
                ""Make sure to include a `wandb` key in your `config` dict ""
                ""containing at least a `project` specification."")

        _set_api_key(wandb_config)

        exclude_results = self._exclude_results.copy()

        # Additional excludes
        additional_excludes = wandb_config.pop(""excludes"", [])
        exclude_results += additional_excludes

        # Log config keys on each result?
        log_config = wandb_config.pop(""log_config"", False)
        if not log_config:
            exclude_results += [""config""]

        # Fill trial ID and name
        trial_id = self.trial.trial_id
        trial_name = str(self.trial)

        # Project name for Wandb
        try:
            wandb_project = wandb_config.pop(""project"")
        except KeyError:
            raise ValueError(
                ""You need to specify a `project` in your wandb `config` dict."")

        # Grouping
        wandb_group = wandb_config.pop(""group"", self.trial.trainable_name)

        wandb_init_kwargs = dict(
            id=trial_id,
            name=trial_name,
            resume=True,
            reinit=True,
            allow_val_change=True,
            group=wandb_group,
            project=wandb_project,
            config=config)
        wandb_init_kwargs.update(wandb_config)

        self._queue = Queue()
        self._wandb = self._logger_process_cls(
            queue=self._queue,
            exclude=exclude_results,
            to_config=self._config_results,
            **wandb_init_kwargs)
        self._wandb.start()","    def _init(self):
        config = self.config.copy()

        try:
            if config.get(""logger_config"", {}).get(""wandb""):
                logger_config = config.pop(""logger_config"")
                wandb_config = logger_config.get(""wandb"").copy()
            else:
                wandb_config = config.pop(""wandb"").copy()
        except KeyError:
            raise ValueError(
                ""Wandb logger specified but no configuration has been passed. ""
                ""Make sure to include a `wandb` key in your `config` dict ""
                ""containing at least a `project` specification."")

        _set_api_key(wandb_config)

        exclude_results = self._exclude_results.copy()

        # Additional excludes
        additional_excludes = wandb_config.pop(""excludes"", [])
        exclude_results += additional_excludes

        # Log config keys on each result?
        log_config = wandb_config.pop(""log_config"", False)
        if not log_config:
            exclude_results += [""config""]

        # Fill trial ID and name
        trial_id = self.trial.trial_id
        trial_name = str(self.trial)

        # Project name for Wandb
        try:
            wandb_project = wandb_config.pop(""project"")
        except KeyError:
            raise ValueError(
                ""You need to specify a `project` in your wandb `config` dict."")

        # Grouping
        wandb_group = wandb_config.pop(""group"", self.trial.trainable_name)

        wandb_init_kwargs = dict(
            id=trial_id,
            name=trial_name,
            resume=True,
            reinit=True,
            allow_val_change=True,
            group=wandb_group,
            project=wandb_project,
            config=config)
        wandb_init_kwargs.update(wandb_config)

        self._queue = Queue()
        self._wandb = self._logger_process_cls(
            queue=self._queue,
            exclude=exclude_results,
            to_config=self._config_results,
            **wandb_init_kwargs)
        self._wandb.start()"
"def _clean_log(obj):
    # Fixes https://github.com/ray-project/ray/issues/10631
    if isinstance(obj, dict):
        return {k: _clean_log(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [_clean_log(v) for v in obj]
    elif _is_allowed_type(obj):
        return obj

    # Else
    try:
        pickle.dumps(obj)
        yaml.dump(
            obj,
            Dumper=yaml.SafeDumper,
            default_flow_style=False,
            allow_unicode=True,
            encoding=""utf-8"")
        return obj
    except Exception:
        # give up, similar to _SafeFallBackEncoder
        fallback = str(obj)

        # Try to convert to int
        try:
            fallback = int(fallback)
            return fallback
        except ValueError:
            pass

        # Try to convert to float
        try:
            fallback = float(fallback)
            return fallback
        except ValueError:
            pass

        # Else, return string
        return fallback","def _clean_log(obj):
    # Fixes https://github.com/ray-project/ray/issues/10631
    if isinstance(obj, dict):
        return {k: _clean_log(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [_clean_log(v) for v in obj]

    # Else
    try:
        pickle.dumps(obj)
        yaml.dump(
            obj,
            Dumper=yaml.SafeDumper,
            default_flow_style=False,
            allow_unicode=True,
            encoding=""utf-8"")
        return obj
    except Exception:
        # give up, similar to _SafeFallBackEncoder
        return str(obj)"
"    def _handle_result(self, result):
        config_update = result.get(""config"", {}).copy()
        log = {}
        flat_result = flatten_dict(result, delimiter=""/"")

        for k, v in flat_result.items():
            if any(
                    k.startswith(item + ""/"") or k == item
                    for item in self._to_config):
                config_update[k] = v
            elif any(
                    k.startswith(item + ""/"") or k == item
                    for item in self._exclude):
                continue
            elif not _is_allowed_type(v):
                continue
            else:
                log[k] = v

        config_update.pop(""callbacks"", None)  # Remove callbacks
        return log, config_update","    def _handle_result(self, result):
        config_update = result.get(""config"", {}).copy()
        log = {}
        flat_result = flatten_dict(result, delimiter=""/"")

        for k, v in flat_result.items():
            if any(
                    k.startswith(item + ""/"") or k == item
                    for item in self._to_config):
                config_update[k] = v
            elif any(
                    k.startswith(item + ""/"") or k == item
                    for item in self._exclude):
                continue
            elif not isinstance(v, Number):
                continue
            else:
                log[k] = v

        config_update.pop(""callbacks"", None)  # Remove callbacks
        return log, config_update"
"def start(node_ip_address, redis_address, address, redis_port, port,
          num_redis_shards, redis_max_clients, redis_password,
          redis_shard_ports, object_manager_port, node_manager_port,
          gcs_server_port, min_worker_port, max_worker_port, memory,
          object_store_memory, redis_max_memory, num_cpus, num_gpus, resources,
          head, include_webui, webui_host, include_dashboard, dashboard_host,
          dashboard_port, block, plasma_directory, huge_pages,
          autoscaling_config, no_redirect_worker_output, no_redirect_output,
          plasma_store_socket_name, raylet_socket_name, temp_dir,
          java_worker_options, code_search_path, load_code_from_local,
          system_config, lru_evict, enable_object_reconstruction,
          metrics_export_port, log_style, log_color, verbose):
    """"""Start Ray processes manually on the local machine.""""""
    cli_logger.log_style = log_style
    cli_logger.color_mode = log_color
    cli_logger.verbosity = verbose
    cli_logger.detect_colors()

    if gcs_server_port and not head:
        raise ValueError(
            ""gcs_server_port can be only assigned when you specify --head."")

    if redis_address is not None:
        cli_logger.abort(""{} is deprecated. Use {} instead."",
                         cf.bold(""--redis-address""), cf.bold(""--address""))

        raise DeprecationWarning(""The --redis-address argument is ""
                                 ""deprecated. Please use --address instead."")
    if redis_port is not None:
        cli_logger.warning(""{} is being deprecated. Use {} instead."",
                           cf.bold(""--redis-port""), cf.bold(""--port""))
        cli_logger.old_warning(
            logger, ""The --redis-port argument will be deprecated soon. ""
            ""Please use --port instead."")
        if port is not None and port != redis_port:
            cli_logger.abort(
                ""Incompatible values for {} and {}. Use only {} instead."",
                cf.bold(""--port""), cf.bold(""--redis-port""), cf.bold(""--port""))

            raise ValueError(""Cannot specify both --port and --redis-port ""
                             ""as port is a rename of deprecated redis-port"")
    if include_webui is not None:
        cli_logger.warning(""{} is being deprecated. Use {} instead."",
                           cf.bold(""--include-webui""),
                           cf.bold(""--include-dashboard""))
        cli_logger.old_warning(
            logger, ""The --include-webui argument will be deprecated soon""
            ""Please use --include-dashboard instead."")
        if include_dashboard is not None:
            include_dashboard = include_webui

    dashboard_host_default = ""localhost""
    if webui_host != dashboard_host_default:
        cli_logger.warning(""{} is being deprecated. Use {} instead."",
                           cf.bold(""--webui-host""),
                           cf.bold(""--dashboard-host""))
        cli_logger.old_warning(
            logger, ""The --webui-host argument will be deprecated""
            "" soon. Please use --dashboard-host instead."")
        if webui_host != dashboard_host and dashboard_host != ""localhost"":
            cli_logger.abort(
                ""Incompatible values for {} and {}. Use only {} instead."",
                cf.bold(""--dashboard-host""), cf.bold(""--webui-host""),
                cf.bold(""--dashboard-host""))

            raise ValueError(
                ""Cannot specify both --webui-host and --dashboard-host,""
                "" please specify only the latter"")
        else:
            dashboard_host = webui_host

    # Convert hostnames to numerical IP address.
    if node_ip_address is not None:
        node_ip_address = services.address_to_ip(node_ip_address)

    if address is not None:
        (redis_address, redis_address_ip,
         redis_address_port) = services.validate_redis_address(address)

    try:
        resources = json.loads(resources)
    except Exception:
        cli_logger.error(""`{}` is not a valid JSON string."",
                         cf.bold(""--resources""))
        cli_logger.abort(
            ""Valid values look like this: `{}`"",
            cf.bold(""--resources='\\""CustomResource3\\"": 1, ""
                    ""\\""CustomResource2\\"": 2}'""))

        raise Exception(""Unable to parse the --resources argument using ""
                        ""json.loads. Try using a format like\\n\\n""
                        ""    --resources='{\\""CustomResource1\\"": 3, ""
                        ""\\""CustomReseource2\\"": 2}'"")

    redirect_worker_output = None if not no_redirect_worker_output else True
    redirect_output = None if not no_redirect_output else True
    ray_params = ray.parameter.RayParams(
        node_ip_address=node_ip_address,
        min_worker_port=min_worker_port,
        max_worker_port=max_worker_port,
        object_manager_port=object_manager_port,
        node_manager_port=node_manager_port,
        gcs_server_port=gcs_server_port,
        memory=memory,
        object_store_memory=object_store_memory,
        redis_password=redis_password,
        redirect_worker_output=redirect_worker_output,
        redirect_output=redirect_output,
        num_cpus=num_cpus,
        num_gpus=num_gpus,
        resources=resources,
        plasma_directory=plasma_directory,
        huge_pages=huge_pages,
        plasma_store_socket_name=plasma_store_socket_name,
        raylet_socket_name=raylet_socket_name,
        temp_dir=temp_dir,
        include_dashboard=include_dashboard,
        dashboard_host=dashboard_host,
        dashboard_port=dashboard_port,
        java_worker_options=java_worker_options,
        load_code_from_local=load_code_from_local,
        code_search_path=code_search_path,
        _system_config=system_config,
        lru_evict=lru_evict,
        enable_object_reconstruction=enable_object_reconstruction,
        metrics_export_port=metrics_export_port)
    if head:
        # Start Ray on the head node.
        if redis_shard_ports is not None:
            redis_shard_ports = redis_shard_ports.split("","")
            # Infer the number of Redis shards from the ports if the number is
            # not provided.
            if num_redis_shards is None:
                num_redis_shards = len(redis_shard_ports)
            # Check that the arguments match.
            if len(redis_shard_ports) != num_redis_shards:
                cli_logger.error(
                    ""`{}` must be a comma-separated list of ports, ""
                    ""with length equal to `{}` (which defaults to {})"",
                    cf.bold(""--redis-shard-ports""),
                    cf.bold(""--num-redis-shards""), cf.bold(""1""))
                cli_logger.abort(
                    ""Example: `{}`"",
                    cf.bold(""--num-redis-shards 3 ""
                            ""--redis_shard_ports 6380,6381,6382""))

                raise Exception(""If --redis-shard-ports is provided, it must ""
                                ""have the form '6380,6381,6382', and the ""
                                ""number of ports provided must equal ""
                                ""--num-redis-shards (which is 1 if not ""
                                ""provided)"")

        if redis_address is not None:
            cli_logger.abort(
                ""`{}` starts a new Redis server, `{}` should not be set."",
                cf.bold(""--head""), cf.bold(""--address""))

            raise Exception(""If --head is passed in, a Redis server will be ""
                            ""started, so a Redis address should not be ""
                            ""provided."")

        # Get the node IP address if one is not provided.
        ray_params.update_if_absent(
            node_ip_address=services.get_node_ip_address())
        cli_logger.labeled_value(""Local node IP"", ray_params.node_ip_address)
        cli_logger.old_info(logger, ""Using IP address {} for this node."",
                            ray_params.node_ip_address)
        ray_params.update_if_absent(
            redis_port=port or redis_port,
            redis_shard_ports=redis_shard_ports,
            redis_max_memory=redis_max_memory,
            num_redis_shards=num_redis_shards,
            redis_max_clients=redis_max_clients,
            autoscaling_config=autoscaling_config,
        )

        node = ray.node.Node(
            ray_params, head=True, shutdown_at_exit=block, spawn_reaper=block)
        redis_address = node.redis_address

        # this is a noop if new-style is not set, so the old logger calls
        # are still in place
        cli_logger.newline()
        startup_msg = ""Ray runtime started.""
        cli_logger.success(""-"" * len(startup_msg))
        cli_logger.success(startup_msg)
        cli_logger.success(""-"" * len(startup_msg))
        cli_logger.newline()
        with cli_logger.group(""Next steps""):
            cli_logger.print(
                ""To connect to this Ray runtime from another node, run"")
            cli_logger.print(
                cf.bold(""  ray start --address='{}'{}""), redis_address,
                f"" --redis-password='{redis_password}'""
                if redis_password else """")
            cli_logger.newline()
            cli_logger.print(""Alternatively, use the following Python code:"")
            with cli_logger.indented():
                with cf.with_style(""monokai"") as c:
                    cli_logger.print(""{} ray"", c.magenta(""import""))
                    cli_logger.print(
                        ""ray{}init(address{}{}{})"", c.magenta("".""),
                        c.magenta(""=""), c.yellow(""'auto'""),
                        "", redis_password{}{}"".format(
                            c.magenta(""=""),
                            c.yellow(""'"" + redis_password + ""'""))
                        if redis_password else """")
            cli_logger.newline()
            cli_logger.print(
                cf.underlined(""If connection fails, check your ""
                              ""firewall settings other ""
                              ""network configuration.""))
            cli_logger.newline()
            cli_logger.print(""To terminate the Ray runtime, run"")
            cli_logger.print(cf.bold(""  ray stop""))

        cli_logger.old_info(
            logger,
            ""\\nStarted Ray on this node. You can add additional nodes to ""
            ""the cluster by calling\\n\\n""
            ""    ray start --address='{}'{}\\n\\n""
            ""from the node you wish to add. You can connect a driver to the ""
            ""cluster from Python by running\\n\\n""
            ""    import ray\\n""
            ""    ray.init(address='auto'{})\\n\\n""
            ""If you have trouble connecting from a different machine, check ""
            ""that your firewall is configured properly. If you wish to ""
            ""terminate the processes that have been started, run\\n\\n""
            ""    ray stop"".format(
                redis_address, "" --redis-password='"" + redis_password + ""'""
                if redis_password else """",
                "", _redis_password='"" + redis_password + ""'""
                if redis_password else """"))
    else:
        # Start Ray on a non-head node.
        if not (redis_port is None and port is None):
            cli_logger.abort(""`{}/{}` should not be specified without `{}`."",
                             cf.bold(""--port""), cf.bold(""--redis-port""),
                             cf.bold(""--head""))

            raise Exception(
                ""If --head is not passed in, --port and --redis-port are not ""
                ""allowed."")
        if redis_shard_ports is not None:
            cli_logger.abort(""`{}` should not be specified without `{}`."",
                             cf.bold(""--redis-shard-ports""), cf.bold(""--head""))

            raise Exception(""If --head is not passed in, --redis-shard-ports ""
                            ""is not allowed."")
        if redis_address is None:
            cli_logger.abort(""`{}` is required unless starting with `{}`."",
                             cf.bold(""--address""), cf.bold(""--head""))

            raise Exception(""If --head is not passed in, --address must ""
                            ""be provided."")
        if num_redis_shards is not None:
            cli_logger.abort(""`{}` should not be specified without `{}`."",
                             cf.bold(""--num-redis-shards""), cf.bold(""--head""))

            raise Exception(""If --head is not passed in, --num-redis-shards ""
                            ""must not be provided."")
        if redis_max_clients is not None:
            cli_logger.abort(""`{}` should not be specified without `{}`."",
                             cf.bold(""--redis-max-clients""), cf.bold(""--head""))

            raise Exception(""If --head is not passed in, --redis-max-clients ""
                            ""must not be provided."")
        if include_webui:
            cli_logger.abort(""`{}` should not be specified without `{}`."",
                             cf.bold(""--include-web-ui""), cf.bold(""--head""))

            raise Exception(""If --head is not passed in, the --include-webui""
                            ""flag is not relevant."")
        if include_dashboard:
            cli_logger.abort(""`{}` should not be specified without `{}`."",
                             cf.bold(""--include-dashboard""), cf.bold(""--head""))

            raise ValueError(
                ""If --head is not passed in, the --include-dashboard""
                ""flag is not relevant."")

        # Wait for the Redis server to be started. And throw an exception if we
        # can't connect to it.
        services.wait_for_redis_to_start(
            redis_address_ip, redis_address_port, password=redis_password)

        # Create a Redis client.
        redis_client = services.create_redis_client(
            redis_address, password=redis_password)

        # Check that the version information on this node matches the version
        # information that the cluster was started with.
        services.check_version_info(redis_client)

        # Get the node IP address if one is not provided.
        ray_params.update_if_absent(
            node_ip_address=services.get_node_ip_address(redis_address))

        cli_logger.labeled_value(""Local node IP"", ray_params.node_ip_address)
        cli_logger.old_info(logger, ""Using IP address {} for this node."",
                            ray_params.node_ip_address)

        # Check that there aren't already Redis clients with the same IP
        # address connected with this Redis instance. This raises an exception
        # if the Redis server already has clients on this node.
        check_no_existing_redis_clients(ray_params.node_ip_address,
                                        redis_client)
        ray_params.update(redis_address=redis_address)
        node = ray.node.Node(
            ray_params, head=False, shutdown_at_exit=block, spawn_reaper=block)

        cli_logger.newline()
        startup_msg = ""Ray runtime started.""
        cli_logger.success(""-"" * len(startup_msg))
        cli_logger.success(startup_msg)
        cli_logger.success(""-"" * len(startup_msg))
        cli_logger.newline()
        cli_logger.print(""To terminate the Ray runtime, run"")
        cli_logger.print(cf.bold(""  ray stop""))

        cli_logger.old_info(
            logger, ""\\nStarted Ray on this node. If you wish to terminate the ""
            ""processes that have been started, run\\n\\n""
            ""    ray stop"")

    if block:
        cli_logger.newline()
        with cli_logger.group(cf.bold(""--block"")):
            cli_logger.print(
                ""This command will now block until terminated by a signal."")
            cli_logger.print(
                ""Runing subprocesses are monitored and a message will be ""
                ""printed if any of them terminate unexpectedly."")

        while True:
            time.sleep(1)
            deceased = node.dead_processes()
            if len(deceased) > 0:
                cli_logger.newline()
                cli_logger.error(""Some Ray subprcesses exited unexpectedly:"")
                cli_logger.old_error(logger,
                                     ""Ray processes died unexpectedly:"")

                with cli_logger.indented():
                    for process_type, process in deceased:
                        cli_logger.error(
                            ""{}"",
                            cf.bold(str(process_type)),
                            _tags={""exit code"": str(process.returncode)})
                        cli_logger.old_error(
                            logger, ""\\t{} died with exit code {}"".format(
                                process_type, process.returncode))

                # shutdown_at_exit will handle cleanup.
                cli_logger.newline()
                cli_logger.error(""Remaining processes will be killed."")
                cli_logger.old_error(
                    logger, ""Killing remaining processes and exiting..."")
                sys.exit(1)","def start(node_ip_address, redis_address, address, redis_port, port,
          num_redis_shards, redis_max_clients, redis_password,
          redis_shard_ports, object_manager_port, node_manager_port,
          gcs_server_port, min_worker_port, max_worker_port, memory,
          object_store_memory, redis_max_memory, num_cpus, num_gpus, resources,
          head, include_webui, webui_host, include_dashboard, dashboard_host,
          dashboard_port, block, plasma_directory, huge_pages,
          autoscaling_config, no_redirect_worker_output, no_redirect_output,
          plasma_store_socket_name, raylet_socket_name, temp_dir,
          java_worker_options, code_search_path, load_code_from_local,
          system_config, lru_evict, enable_object_reconstruction,
          metrics_export_port, log_style, log_color, verbose):
    """"""Start Ray processes manually on the local machine.""""""
    cli_logger.log_style = log_style
    cli_logger.color_mode = log_color
    cli_logger.verbosity = verbose
    cli_logger.detect_colors()

    if gcs_server_port and not head:
        raise ValueError(
            ""gcs_server_port can be only assigned when you specify --head."")

    if redis_address is not None:
        cli_logger.abort(""{} is deprecated. Use {} instead."",
                         cf.bold(""--redis-address""), cf.bold(""--address""))

        raise DeprecationWarning(""The --redis-address argument is ""
                                 ""deprecated. Please use --address instead."")
    if redis_port is not None:
        cli_logger.warning(""{} is being deprecated. Use {} instead."",
                           cf.bold(""--redis-port""), cf.bold(""--port""))
        cli_logger.old_warning(
            logger, ""The --redis-port argument will be deprecated soon. ""
            ""Please use --port instead."")
        if port is not None and port != redis_port:
            cli_logger.abort(
                ""Incompatible values for {} and {}. Use only {} instead."",
                cf.bold(""--port""), cf.bold(""--redis-port""), cf.bold(""--port""))

            raise ValueError(""Cannot specify both --port and --redis-port ""
                             ""as port is a rename of deprecated redis-port"")
    if include_webui is not None:
        cli_logger.warning(""{} is being deprecated. Use {} instead."",
                           cf.bold(""--include-webui""),
                           cf.bold(""--include-dashboard""))
        cli_logger.old_warning(
            logger, ""The --include-webui argument will be deprecated soon""
            ""Please use --include-dashboard instead."")
        if include_dashboard is not None:
            include_dashboard = include_webui

    dashboard_host_default = ""localhost""
    if webui_host != dashboard_host_default:
        cli_logger.warning(""{} is being deprecated. Use {} instead."",
                           cf.bold(""--webui-host""),
                           cf.bold(""--dashboard-host""))
        cli_logger.old_warning(
            logger, ""The --webui-host argument will be deprecated""
            "" soon. Please use --dashboard-host instead."")
        if webui_host != dashboard_host and dashboard_host != ""localhost"":
            cli_logger.abort(
                ""Incompatible values for {} and {}. Use only {} instead."",
                cf.bold(""--dashboard-host""), cf.bold(""--webui-host""),
                cf.bold(""--dashboard-host""))

            raise ValueError(
                ""Cannot specify both --webui-host and --dashboard-host,""
                "" please specify only the latter"")
        else:
            dashboard_host = webui_host

    # Convert hostnames to numerical IP address.
    if node_ip_address is not None:
        node_ip_address = services.address_to_ip(node_ip_address)

    if address is not None:
        (redis_address, redis_address_ip,
         redis_address_port) = services.validate_redis_address(address)

    try:
        resources = json.loads(resources)
    except Exception:
        cli_logger.error(""`{}` is not a valid JSON string."",
                         cf.bold(""--resources""))
        cli_logger.abort(
            ""Valid values look like this: `{}`"",
            cf.bold(""--resources='\\""CustomResource3\\"": 1, ""
                    ""\\""CustomResource2\\"": 2}'""))

        raise Exception(""Unable to parse the --resources argument using ""
                        ""json.loads. Try using a format like\\n\\n""
                        ""    --resources='{\\""CustomResource1\\"": 3, ""
                        ""\\""CustomReseource2\\"": 2}'"")

    redirect_worker_output = None if not no_redirect_worker_output else True
    redirect_output = None if not no_redirect_output else True
    ray_params = ray.parameter.RayParams(
        node_ip_address=node_ip_address,
        min_worker_port=min_worker_port,
        max_worker_port=max_worker_port,
        object_manager_port=object_manager_port,
        node_manager_port=node_manager_port,
        gcs_server_port=gcs_server_port,
        memory=memory,
        object_store_memory=object_store_memory,
        redis_password=redis_password,
        redirect_worker_output=redirect_worker_output,
        redirect_output=redirect_output,
        num_cpus=num_cpus,
        num_gpus=num_gpus,
        resources=resources,
        plasma_directory=plasma_directory,
        huge_pages=huge_pages,
        plasma_store_socket_name=plasma_store_socket_name,
        raylet_socket_name=raylet_socket_name,
        temp_dir=temp_dir,
        include_dashboard=include_dashboard,
        dashboard_host=dashboard_host,
        dashboard_port=dashboard_port,
        java_worker_options=java_worker_options,
        load_code_from_local=load_code_from_local,
        code_search_path=code_search_path,
        _system_config=system_config,
        lru_evict=lru_evict,
        enable_object_reconstruction=enable_object_reconstruction,
        metrics_export_port=metrics_export_port)
    if head:
        # Start Ray on the head node.
        if redis_shard_ports is not None:
            redis_shard_ports = redis_shard_ports.split("","")
            # Infer the number of Redis shards from the ports if the number is
            # not provided.
            if num_redis_shards is None:
                num_redis_shards = len(redis_shard_ports)
            # Check that the arguments match.
            if len(redis_shard_ports) != num_redis_shards:
                cli_logger.error(
                    ""`{}` must be a comma-separated list of ports, ""
                    ""with length equal to `{}` (which defaults to {})"",
                    cf.bold(""--redis-shard-ports""),
                    cf.bold(""--num-redis-shards""), cf.bold(""1""))
                cli_logger.abort(
                    ""Example: `{}`"",
                    cf.bold(""--num-redis-shards 3 ""
                            ""--redis_shard_ports 6380,6381,6382""))

                raise Exception(""If --redis-shard-ports is provided, it must ""
                                ""have the form '6380,6381,6382', and the ""
                                ""number of ports provided must equal ""
                                ""--num-redis-shards (which is 1 if not ""
                                ""provided)"")

        if redis_address is not None:
            cli_logger.abort(
                ""`{}` starts a new Redis server, `{}` should not be set."",
                cf.bold(""--head""), cf.bold(""--address""))

            raise Exception(""If --head is passed in, a Redis server will be ""
                            ""started, so a Redis address should not be ""
                            ""provided."")

        # Get the node IP address if one is not provided.
        ray_params.update_if_absent(
            node_ip_address=services.get_node_ip_address())
        cli_logger.labeled_value(""Local node IP"", ray_params.node_ip_address)
        cli_logger.old_info(logger, ""Using IP address {} for this node."",
                            ray_params.node_ip_address)
        ray_params.update_if_absent(
            redis_port=port or redis_port,
            redis_shard_ports=redis_shard_ports,
            redis_max_memory=redis_max_memory,
            num_redis_shards=num_redis_shards,
            redis_max_clients=redis_max_clients,
            autoscaling_config=autoscaling_config,
        )

        node = ray.node.Node(
            ray_params, head=True, shutdown_at_exit=block, spawn_reaper=block)
        redis_address = node.redis_address

        # this is a noop if new-style is not set, so the old logger calls
        # are still in place
        cli_logger.newline()
        startup_msg = ""Ray runtime started.""
        cli_logger.success(""-"" * len(startup_msg))
        cli_logger.success(startup_msg)
        cli_logger.success(""-"" * len(startup_msg))
        cli_logger.newline()
        with cli_logger.group(""Next steps""):
            cli_logger.print(
                ""To connect to this Ray runtime from another node, run"")
            cli_logger.print(
                cf.bold(""  ray start --address='{}'{}""), redis_address,
                f"" --redis-password='{redis_password}'""
                if redis_password else """")
            cli_logger.newline()
            cli_logger.print(""Alternatively, use the following Python code:"")
            with cli_logger.indented():
                with cf.with_style(""monokai"") as c:
                    cli_logger.print(""{} ray"", c.magenta(""import""))
                    cli_logger.print(
                        ""ray{}init(address{}{}{})"", c.magenta("".""),
                        c.magenta(""=""), c.yellow(""'auto'""),
                        "", redis_password{}{}"".format(
                            c.magenta(""=""),
                            c.yellow(""'"" + redis_password + ""'""))
                        if redis_password else """")
            cli_logger.newline()
            cli_logger.print(
                cf.underlined(""If connection fails, check your ""
                              ""firewall settings other ""
                              ""network configuration.""))
            cli_logger.newline()
            cli_logger.print(""To terminate the Ray runtime, run"")
            cli_logger.print(cf.bold(""  ray stop""))

        cli_logger.old_info(
            logger,
            ""\\nStarted Ray on this node. You can add additional nodes to ""
            ""the cluster by calling\\n\\n""
            ""    ray start --address='{}'{}\\n\\n""
            ""from the node you wish to add. You can connect a driver to the ""
            ""cluster from Python by running\\n\\n""
            ""    import ray\\n""
            ""    ray.init(address='auto'{})\\n\\n""
            ""If you have trouble connecting from a different machine, check ""
            ""that your firewall is configured properly. If you wish to ""
            ""terminate the processes that have been started, run\\n\\n""
            ""    ray stop"".format(
                redis_address, "" --redis-password='"" + redis_password + ""'""
                if redis_password else """",
                "", redis_password='"" + redis_password + ""'""
                if redis_password else """"))
    else:
        # Start Ray on a non-head node.
        if not (redis_port is None and port is None):
            cli_logger.abort(""`{}/{}` should not be specified without `{}`."",
                             cf.bold(""--port""), cf.bold(""--redis-port""),
                             cf.bold(""--head""))

            raise Exception(
                ""If --head is not passed in, --port and --redis-port are not ""
                ""allowed."")
        if redis_shard_ports is not None:
            cli_logger.abort(""`{}` should not be specified without `{}`."",
                             cf.bold(""--redis-shard-ports""), cf.bold(""--head""))

            raise Exception(""If --head is not passed in, --redis-shard-ports ""
                            ""is not allowed."")
        if redis_address is None:
            cli_logger.abort(""`{}` is required unless starting with `{}`."",
                             cf.bold(""--address""), cf.bold(""--head""))

            raise Exception(""If --head is not passed in, --address must ""
                            ""be provided."")
        if num_redis_shards is not None:
            cli_logger.abort(""`{}` should not be specified without `{}`."",
                             cf.bold(""--num-redis-shards""), cf.bold(""--head""))

            raise Exception(""If --head is not passed in, --num-redis-shards ""
                            ""must not be provided."")
        if redis_max_clients is not None:
            cli_logger.abort(""`{}` should not be specified without `{}`."",
                             cf.bold(""--redis-max-clients""), cf.bold(""--head""))

            raise Exception(""If --head is not passed in, --redis-max-clients ""
                            ""must not be provided."")
        if include_webui:
            cli_logger.abort(""`{}` should not be specified without `{}`."",
                             cf.bold(""--include-web-ui""), cf.bold(""--head""))

            raise Exception(""If --head is not passed in, the --include-webui""
                            ""flag is not relevant."")
        if include_dashboard:
            cli_logger.abort(""`{}` should not be specified without `{}`."",
                             cf.bold(""--include-dashboard""), cf.bold(""--head""))

            raise ValueError(
                ""If --head is not passed in, the --include-dashboard""
                ""flag is not relevant."")

        # Wait for the Redis server to be started. And throw an exception if we
        # can't connect to it.
        services.wait_for_redis_to_start(
            redis_address_ip, redis_address_port, password=redis_password)

        # Create a Redis client.
        redis_client = services.create_redis_client(
            redis_address, password=redis_password)

        # Check that the version information on this node matches the version
        # information that the cluster was started with.
        services.check_version_info(redis_client)

        # Get the node IP address if one is not provided.
        ray_params.update_if_absent(
            node_ip_address=services.get_node_ip_address(redis_address))

        cli_logger.labeled_value(""Local node IP"", ray_params.node_ip_address)
        cli_logger.old_info(logger, ""Using IP address {} for this node."",
                            ray_params.node_ip_address)

        # Check that there aren't already Redis clients with the same IP
        # address connected with this Redis instance. This raises an exception
        # if the Redis server already has clients on this node.
        check_no_existing_redis_clients(ray_params.node_ip_address,
                                        redis_client)
        ray_params.update(redis_address=redis_address)
        node = ray.node.Node(
            ray_params, head=False, shutdown_at_exit=block, spawn_reaper=block)

        cli_logger.newline()
        startup_msg = ""Ray runtime started.""
        cli_logger.success(""-"" * len(startup_msg))
        cli_logger.success(startup_msg)
        cli_logger.success(""-"" * len(startup_msg))
        cli_logger.newline()
        cli_logger.print(""To terminate the Ray runtime, run"")
        cli_logger.print(cf.bold(""  ray stop""))

        cli_logger.old_info(
            logger, ""\\nStarted Ray on this node. If you wish to terminate the ""
            ""processes that have been started, run\\n\\n""
            ""    ray stop"")

    if block:
        cli_logger.newline()
        with cli_logger.group(cf.bold(""--block"")):
            cli_logger.print(
                ""This command will now block until terminated by a signal."")
            cli_logger.print(
                ""Runing subprocesses are monitored and a message will be ""
                ""printed if any of them terminate unexpectedly."")

        while True:
            time.sleep(1)
            deceased = node.dead_processes()
            if len(deceased) > 0:
                cli_logger.newline()
                cli_logger.error(""Some Ray subprcesses exited unexpectedly:"")
                cli_logger.old_error(logger,
                                     ""Ray processes died unexpectedly:"")

                with cli_logger.indented():
                    for process_type, process in deceased:
                        cli_logger.error(
                            ""{}"",
                            cf.bold(str(process_type)),
                            _tags={""exit code"": str(process.returncode)})
                        cli_logger.old_error(
                            logger, ""\\t{} died with exit code {}"".format(
                                process_type, process.returncode))

                # shutdown_at_exit will handle cleanup.
                cli_logger.newline()
                cli_logger.error(""Remaining processes will be killed."")
                cli_logger.old_error(
                    logger, ""Killing remaining processes and exiting..."")
                sys.exit(1)"
"def memory(address, redis_password):
    """"""Print object references held in a Ray cluster.""""""
    if not address:
        address = services.find_redis_address_or_die()
    logger.info(f""Connecting to Ray instance at {address}."")
    ray.init(address=address, _redis_password=redis_password)
    print(ray.internal.internal_api.memory_summary())","def memory(address, redis_password):
    """"""Print object references held in a Ray cluster.""""""
    if not address:
        address = services.find_redis_address_or_die()
    logger.info(f""Connecting to Ray instance at {address}."")
    ray.init(address=address, redis_password=redis_password)
    print(ray.internal.internal_api.memory_summary())"
"    def choose_trial_to_run(self, trial_runner, allow_recurse=True):
        """"""Fair scheduling within iteration by completion percentage.

        List of trials not used since all trials are tracked as state
        of scheduler. If iteration is occupied (ie, no trials to run),
        then look into next iteration.
        """"""

        for hyperband in self._hyperbands:
            # band will have None entries if no resources
            # are to be allocated to that bracket.
            scrubbed = [b for b in hyperband if b is not None]
            for bracket in scrubbed:
                for trial in bracket.current_trials():
                    if (trial.status == Trial.PENDING
                            and trial_runner.has_resources(trial.resources)):
                        return trial
        # MAIN CHANGE HERE!
        if not any(t.status == Trial.RUNNING
                   for t in trial_runner.get_trials()):
            for hyperband in self._hyperbands:
                for bracket in hyperband:
                    if bracket and any(trial.status == Trial.PAUSED
                                       for trial in bracket.current_trials()):
                        # This will change the trial state
                        self._process_bracket(trial_runner, bracket)

                        # If there are pending trials now, suggest one.
                        # This is because there might be both PENDING and
                        # PAUSED trials now, and PAUSED trials will raise
                        # an error before the trial runner tries again.
                        if allow_recurse and any(
                                trial.status == Trial.PENDING
                                for trial in bracket.current_trials()):
                            return self.choose_trial_to_run(
                                trial_runner, allow_recurse=False)
        # MAIN CHANGE HERE!
        return None","    def choose_trial_to_run(self, trial_runner):
        """"""Fair scheduling within iteration by completion percentage.

        List of trials not used since all trials are tracked as state
        of scheduler. If iteration is occupied (ie, no trials to run),
        then look into next iteration.
        """"""

        for hyperband in self._hyperbands:
            # band will have None entries if no resources
            # are to be allocated to that bracket.
            scrubbed = [b for b in hyperband if b is not None]
            for bracket in scrubbed:
                for trial in bracket.current_trials():
                    if (trial.status == Trial.PENDING
                            and trial_runner.has_resources(trial.resources)):
                        return trial
        # MAIN CHANGE HERE!
        if not any(t.status == Trial.RUNNING
                   for t in trial_runner.get_trials()):
            for hyperband in self._hyperbands:
                for bracket in hyperband:
                    if bracket and any(trial.status == Trial.PAUSED
                                       for trial in bracket.current_trials()):
                        # This will change the trial state and let the
                        # trial runner retry.
                        self._process_bracket(trial_runner, bracket)
        # MAIN CHANGE HERE!
        return None"
"    def debug_string(self):
        """"""This provides a progress notification for the algorithm.

        For each bracket, the algorithm will output a string as follows:

            Bracket(Max Size (n)=5, Milestone (r)=33, completed=14.6%):
            {PENDING: 2, RUNNING: 3, TERMINATED: 2}

        ""Max Size"" indicates the max number of pending/running experiments
        set according to the Hyperband algorithm.

        ""Milestone"" indicates the iterations a trial will run for before
        the next halving will occur.

        ""Completed"" indicates an approximate progress metric. Some brackets,
        like ones that are unfilled, will not reach 100%.
        """"""
        out = ""Using HyperBand: ""
        out += ""num_stopped={} total_brackets={}"".format(
            self._num_stopped, sum(len(band) for band in self._hyperbands))
        for i, band in enumerate(self._hyperbands):
            out += ""\\nRound #{}:"".format(i)
            for bracket in band:
                if bracket:
                    out += ""\\n  {}"".format(bracket)
        return out","    def debug_string(self):
        """"""This provides a progress notification for the algorithm.

        For each bracket, the algorithm will output a string as follows:

            Bracket(Max Size (n)=5, Milestone (r)=33, completed=14.6%):
            {PENDING: 2, RUNNING: 3, TERMINATED: 2}

        ""Max Size"" indicates the max number of pending/running experiments
        set according to the Hyperband algorithm.

        ""Milestone"" indicates the iterations a trial will run for before
        the next halving will occur.

        ""Completed"" indicates an approximate progress metric. Some brackets,
        like ones that are unfilled, will not reach 100%.
        """"""
        out = ""Using HyperBand: ""
        out += ""num_stopped={} total_brackets={}"".format(
            self._num_stopped, sum(len(band) for band in self._hyperbands))
        for i, band in enumerate(self._hyperbands):
            out += ""\\nRound #{}:"".format(i)
            for bracket in band:
                out += ""\\n  {}"".format(bracket)
        return out"
"    def run_rsync_up(self, source, target):
        if target.startswith(""~""):
            target = ""/root"" + target[1:]

        try:
            self.process_runner.check_call([
                KUBECTL_RSYNC,
                ""-avz"",
                source,
                ""{}@{}:{}"".format(self.node_id, self.namespace, target),
            ])
        except Exception as e:
            logger.warning(self.log_prefix +
                           ""rsync failed: '{}'. Falling back to 'kubectl cp'""
                           .format(e))
            self.run_cp_up(source, target)","    def run_rsync_up(self, source, target):
        if target.startswith(""~""):
            target = ""/root"" + target[1:]

        try:
            self.process_runner.check_call([
                KUBECTL_RSYNC,
                ""-avz"",
                source,
                ""{}@{}:{}"".format(self.node_id, self.namespace, target),
            ])
        except Exception as e:
            logger.warning(self.log_prefix +
                           ""rsync failed: '{}'. Falling back to 'kubectl cp'""
                           .format(e))
            self.process_runner.check_call(self.kubectl + [
                ""cp"", source, ""{}/{}:{}"".format(self.namespace, self.node_id,
                                                target)
            ])"
"    def run_rsync_down(self, source, target):
        if target.startswith(""~""):
            target = ""/root"" + target[1:]

        try:
            self.process_runner.check_call([
                KUBECTL_RSYNC,
                ""-avz"",
                ""{}@{}:{}"".format(self.node_id, self.namespace, source),
                target,
            ])
        except Exception as e:
            logger.warning(self.log_prefix +
                           ""rsync failed: '{}'. Falling back to 'kubectl cp'""
                           .format(e))
            self.run_cp_down(source, target)","    def run_rsync_down(self, source, target):
        if target.startswith(""~""):
            target = ""/root"" + target[1:]

        try:
            self.process_runner.check_call([
                KUBECTL_RSYNC,
                ""-avz"",
                ""{}@{}:{}"".format(self.node_id, self.namespace, source),
                target,
            ])
        except Exception as e:
            logger.warning(self.log_prefix +
                           ""rsync failed: '{}'. Falling back to 'kubectl cp'""
                           .format(e))
            self.process_runner.check_call(self.kubectl + [
                ""cp"", ""{}/{}:{}"".format(self.namespace, self.node_id, source),
                target
            ])"
"def get_node_syncer(local_dir, remote_dir=None, sync_function=None):
    """"""Returns a NodeSyncer.

    Args:
        local_dir (str): Source directory for syncing.
        remote_dir (str): Target directory for syncing. If not provided, a
            noop Syncer is returned.
        sync_function (func|str|bool): Function for syncing the local_dir to
            remote_dir. If string, then it must be a string template for
            syncer to run. If True or not provided, it defaults rsync. If
            False, a noop Syncer is returned.
    """"""
    key = (local_dir, remote_dir)
    if key in _syncers:
        return _syncers[key]
    elif isclass(sync_function) and issubclass(sync_function, Syncer):
        _syncers[key] = sync_function(local_dir, remote_dir, None)
        return _syncers[key]
    elif not remote_dir or sync_function is False:
        sync_client = NOOP
    elif sync_function and sync_function is not True:
        sync_client = get_sync_client(sync_function)
    else:
        sync = log_sync_template()
        if sync:
            sync_client = CommandBasedClient(sync, sync)
            sync_client.set_logdir(local_dir)
        else:
            sync_client = NOOP

    _syncers[key] = NodeSyncer(local_dir, remote_dir, sync_client)
    return _syncers[key]","def get_node_syncer(local_dir, remote_dir=None, sync_function=None):
    """"""Returns a NodeSyncer.

    Args:
        local_dir (str): Source directory for syncing.
        remote_dir (str): Target directory for syncing. If not provided, a
            noop Syncer is returned.
        sync_function (func|str|bool): Function for syncing the local_dir to
            remote_dir. If string, then it must be a string template for
            syncer to run. If True or not provided, it defaults rsync. If
            False, a noop Syncer is returned.
    """"""
    key = (local_dir, remote_dir)
    if key in _syncers:
        return _syncers[key]
    elif not remote_dir or sync_function is False:
        sync_client = NOOP
    elif sync_function and sync_function is not True:
        sync_client = get_sync_client(sync_function)
    else:
        sync = log_sync_template()
        if sync:
            sync_client = CommandBasedClient(sync, sync)
            sync_client.set_logdir(local_dir)
        else:
            sync_client = NOOP

    _syncers[key] = NodeSyncer(local_dir, remote_dir, sync_client)
    return _syncers[key]"
"    def action_prob(self, batch: SampleBatchType) -> np.ndarray:
        """"""Returns the probs for the batch actions for the current policy.""""""

        num_state_inputs = 0
        for k in batch.keys():
            if k.startswith(""state_in_""):
                num_state_inputs += 1
        state_keys = [""state_in_{}"".format(i) for i in range(num_state_inputs)]
        log_likelihoods: TensorType = self.policy.compute_log_likelihoods(
            actions=batch[SampleBatch.ACTIONS],
            obs_batch=batch[SampleBatch.CUR_OBS],
            state_batches=[batch[k] for k in state_keys],
            prev_action_batch=batch.data.get(SampleBatch.PREV_ACTIONS),
            prev_reward_batch=batch.data.get(SampleBatch.PREV_REWARDS))
        return convert_to_numpy(log_likelihoods)","    def action_prob(self, batch: SampleBatchType) -> TensorType:
        """"""Returns the probs for the batch actions for the current policy.""""""

        num_state_inputs = 0
        for k in batch.keys():
            if k.startswith(""state_in_""):
                num_state_inputs += 1
        state_keys = [""state_in_{}"".format(i) for i in range(num_state_inputs)]
        log_likelihoods = self.policy.compute_log_likelihoods(
            actions=batch[SampleBatch.ACTIONS],
            obs_batch=batch[SampleBatch.CUR_OBS],
            state_batches=[batch[k] for k in state_keys],
            prev_action_batch=batch.data.get(SampleBatch.PREV_ACTIONS),
            prev_reward_batch=batch.data.get(SampleBatch.PREV_REWARDS))
        return log_likelihoods"
"    def run_rsync_up(self, source, target):
        # TODO(ilr) Expose this to before NodeUpdater::sync_file_mounts
        protected_path = target
        if target.find(""/root"") == 0:
            target = target.replace(""/root"", ""/tmp/root"")
        self.ssh_command_runner.run(
            f""mkdir -p {os.path.dirname(target.rstrip('/'))}"")
        self.ssh_command_runner.run_rsync_up(source, target)
        if self._check_container_status():
            self.ssh_command_runner.run(""docker cp {} {}:{}"".format(
                target, self.docker_name,
                self._docker_expand_user(protected_path)))","    def run_rsync_up(self, source, target):
        protected_path = target
        if target.find(""/root"") == 0:
            target = target.replace(""/root"", ""/tmp/root"")
        self.ssh_command_runner.run(
            f""mkdir -p {os.path.dirname(target.rstrip('/'))}"")
        self.ssh_command_runner.run_rsync_up(source, target)
        if self._check_container_status():
            self.ssh_command_runner.run(""docker cp {} {}:{}"".format(
                target, self.docker_name,
                self._docker_expand_user(protected_path)))"
"    def sync_file_mounts(self, sync_cmd, step_numbers=(0, 2)):
        # step_numbers is (# of previous steps, total steps)
        previous_steps, total_steps = step_numbers

        nolog_paths = []
        if cli_logger.verbosity == 0:
            nolog_paths = [
                ""~/ray_bootstrap_key.pem"", ""~/ray_bootstrap_config.yaml""
            ]

        def do_sync(remote_path, local_path, allow_non_existing_paths=False):
            if allow_non_existing_paths and not os.path.exists(local_path):
                # Ignore missing source files. In the future we should support
                # the --delete-missing-args command to delete files that have
                # been removed
                return

            assert os.path.exists(local_path), local_path

            if os.path.isdir(local_path):
                if not local_path.endswith(""/""):
                    local_path += ""/""
                if not remote_path.endswith(""/""):
                    remote_path += ""/""

            with LogTimer(self.log_prefix +
                          ""Synced {} to {}"".format(local_path, remote_path)):
                self.cmd_runner.run(
                    ""mkdir -p {}"".format(os.path.dirname(remote_path)),
                    run_env=""host"")
                sync_cmd(local_path, remote_path)

                if remote_path not in nolog_paths:
                    # todo: timed here?
                    cli_logger.print(""{} from {}"", cf.bold(remote_path),
                                     cf.bold(local_path))

        # Rsync file mounts
        with cli_logger.group(
                ""Processing file mounts"",
                _numbered=(""[]"", previous_steps + 1, total_steps)):
            for remote_path, local_path in self.file_mounts.items():
                do_sync(remote_path, local_path)

        if self.cluster_synced_files:
            with cli_logger.group(
                    ""Processing worker file mounts"",
                    _numbered=(""[]"", previous_steps + 2, total_steps)):
                for path in self.cluster_synced_files:
                    do_sync(path, path, allow_non_existing_paths=True)
        else:
            cli_logger.print(
                ""No worker file mounts to sync"",
                _numbered=(""[]"", previous_steps + 2, total_steps))","    def sync_file_mounts(self, sync_cmd, step_numbers=(0, 2)):
        # step_numbers is (# of previous steps, total steps)
        previous_steps, total_steps = step_numbers

        nolog_paths = []
        if cli_logger.verbosity == 0:
            nolog_paths = [
                ""~/ray_bootstrap_key.pem"", ""~/ray_bootstrap_config.yaml""
            ]

        def do_sync(remote_path, local_path, allow_non_existing_paths=False):
            if allow_non_existing_paths and not os.path.exists(local_path):
                # Ignore missing source files. In the future we should support
                # the --delete-missing-args command to delete files that have
                # been removed
                return

            assert os.path.exists(local_path), local_path

            if os.path.isdir(local_path):
                if not local_path.endswith(""/""):
                    local_path += ""/""
                if not remote_path.endswith(""/""):
                    remote_path += ""/""

            with LogTimer(self.log_prefix +
                          ""Synced {} to {}"".format(local_path, remote_path)):
                self.cmd_runner.run(""mkdir -p {}"".format(
                    os.path.dirname(remote_path)))
                sync_cmd(local_path, remote_path)

                if remote_path not in nolog_paths:
                    # todo: timed here?
                    cli_logger.print(""{} from {}"", cf.bold(remote_path),
                                     cf.bold(local_path))

        # Rsync file mounts
        with cli_logger.group(
                ""Processing file mounts"",
                _numbered=(""[]"", previous_steps + 1, total_steps)):
            for remote_path, local_path in self.file_mounts.items():
                do_sync(remote_path, local_path)

        if self.cluster_synced_files:
            with cli_logger.group(
                    ""Processing worker file mounts"",
                    _numbered=(""[]"", previous_steps + 2, total_steps)):
                for path in self.cluster_synced_files:
                    do_sync(path, path, allow_non_existing_paths=True)
        else:
            cli_logger.print(
                ""No worker file mounts to sync"",
                _numbered=(""[]"", previous_steps + 2, total_steps))"
"        def do_sync(remote_path, local_path, allow_non_existing_paths=False):
            if allow_non_existing_paths and not os.path.exists(local_path):
                # Ignore missing source files. In the future we should support
                # the --delete-missing-args command to delete files that have
                # been removed
                return

            assert os.path.exists(local_path), local_path

            if os.path.isdir(local_path):
                if not local_path.endswith(""/""):
                    local_path += ""/""
                if not remote_path.endswith(""/""):
                    remote_path += ""/""

            with LogTimer(self.log_prefix +
                          ""Synced {} to {}"".format(local_path, remote_path)):
                self.cmd_runner.run(
                    ""mkdir -p {}"".format(os.path.dirname(remote_path)),
                    run_env=""host"")
                sync_cmd(local_path, remote_path)

                if remote_path not in nolog_paths:
                    # todo: timed here?
                    cli_logger.print(""{} from {}"", cf.bold(remote_path),
                                     cf.bold(local_path))","        def do_sync(remote_path, local_path, allow_non_existing_paths=False):
            if allow_non_existing_paths and not os.path.exists(local_path):
                # Ignore missing source files. In the future we should support
                # the --delete-missing-args command to delete files that have
                # been removed
                return

            assert os.path.exists(local_path), local_path

            if os.path.isdir(local_path):
                if not local_path.endswith(""/""):
                    local_path += ""/""
                if not remote_path.endswith(""/""):
                    remote_path += ""/""

            with LogTimer(self.log_prefix +
                          ""Synced {} to {}"".format(local_path, remote_path)):
                self.cmd_runner.run(""mkdir -p {}"".format(
                    os.path.dirname(remote_path)))
                sync_cmd(local_path, remote_path)

                if remote_path not in nolog_paths:
                    # todo: timed here?
                    cli_logger.print(""{} from {}"", cf.bold(remote_path),
                                     cf.bold(local_path))"
"    def wait_ready(self, deadline):
        with cli_logger.group(
                ""Waiting for SSH to become available"", _numbered=(""[]"", 1, 6)):
            with LogTimer(self.log_prefix + ""Got remote shell""):
                cli_logger.old_info(logger, ""{}Waiting for remote shell..."",
                                    self.log_prefix)

                cli_logger.print(""Running `{}` as a test."", cf.bold(""uptime""))
                first_conn_refused_time = None
                while time.time() < deadline and \\
                        not self.provider.is_terminated(self.node_id):
                    try:
                        cli_logger.old_debug(logger,
                                             ""{}Waiting for remote shell..."",
                                             self.log_prefix)

                        self.cmd_runner.run(""uptime"", run_env=""host"")
                        cli_logger.old_debug(logger, ""Uptime succeeded."")
                        cli_logger.success(""Success."")
                        return True
                    except ProcessRunnerError as e:
                        first_conn_refused_time = \\
                            cmd_output_util.handle_ssh_fails(
                                e, first_conn_refused_time,
                                retry_interval=READY_CHECK_INTERVAL)
                        time.sleep(READY_CHECK_INTERVAL)
                    except Exception as e:
                        # TODO(maximsmol): we should not be ignoring
                        # exceptions if they get filtered properly
                        # (new style log + non-interactive shells)
                        #
                        # however threading this configuration state
                        # is a pain and I'm leaving it for later

                        retry_str = str(e)
                        if hasattr(e, ""cmd""):
                            retry_str = ""(Exit Status {}): {}"".format(
                                e.returncode, "" "".join(e.cmd))

                        cli_logger.print(
                            ""SSH still not available {}, ""
                            ""retrying in {} seconds."", cf.gray(retry_str),
                            cf.bold(str(READY_CHECK_INTERVAL)))
                        cli_logger.old_debug(logger,
                                             ""{}Node not up, retrying: {}"",
                                             self.log_prefix, retry_str)

                        time.sleep(READY_CHECK_INTERVAL)

        assert False, ""Unable to connect to node""","    def wait_ready(self, deadline):
        with cli_logger.group(
                ""Waiting for SSH to become available"", _numbered=(""[]"", 1, 6)):
            with LogTimer(self.log_prefix + ""Got remote shell""):
                cli_logger.old_info(logger, ""{}Waiting for remote shell..."",
                                    self.log_prefix)

                cli_logger.print(""Running `{}` as a test."", cf.bold(""uptime""))
                first_conn_refused_time = None
                while time.time() < deadline and \\
                        not self.provider.is_terminated(self.node_id):
                    try:
                        cli_logger.old_debug(logger,
                                             ""{}Waiting for remote shell..."",
                                             self.log_prefix)

                        self.cmd_runner.run(""uptime"")
                        cli_logger.old_debug(logger, ""Uptime succeeded."")
                        cli_logger.success(""Success."")
                        return True
                    except ProcessRunnerError as e:
                        first_conn_refused_time = \\
                            cmd_output_util.handle_ssh_fails(
                                e, first_conn_refused_time,
                                retry_interval=READY_CHECK_INTERVAL)
                        time.sleep(READY_CHECK_INTERVAL)
                    except Exception as e:
                        # TODO(maximsmol): we should not be ignoring
                        # exceptions if they get filtered properly
                        # (new style log + non-interactive shells)
                        #
                        # however threading this configuration state
                        # is a pain and I'm leaving it for later

                        retry_str = str(e)
                        if hasattr(e, ""cmd""):
                            retry_str = ""(Exit Status {}): {}"".format(
                                e.returncode, "" "".join(e.cmd))

                        cli_logger.print(
                            ""SSH still not available {}, ""
                            ""retrying in {} seconds."", cf.gray(retry_str),
                            cf.bold(str(READY_CHECK_INTERVAL)))
                        cli_logger.old_debug(logger,
                                             ""{}Node not up, retrying: {}"",
                                             self.log_prefix, retry_str)

                        time.sleep(READY_CHECK_INTERVAL)

        assert False, ""Unable to connect to node"""
"def create_or_update_cluster(config_file: str,
                             override_min_workers: Optional[int],
                             override_max_workers: Optional[int],
                             no_restart: bool,
                             restart_only: bool,
                             yes: bool,
                             override_cluster_name: Optional[str],
                             no_config_cache: bool,
                             dump_command_output: bool = True,
                             use_login_shells: bool = True) -> None:
    """"""Create or updates an autoscaling Ray cluster from a config json.""""""
    set_using_login_shells(use_login_shells)
    cmd_output_util.set_output_redirected(not dump_command_output)

    if use_login_shells:
        cli_logger.warning(
            ""Commands running under a login shell can produce more ""
            ""output than special processing can handle."")
        cli_logger.warning(
            ""Thus, the output from subcommands will be logged as is."")
        cli_logger.warning(
            ""Consider using {}, {}."", cf.bold(""--use-normal-shells""),
            cf.underlined(""if you tested your workflow and it is compatible""))
        cli_logger.newline()

    cli_logger.detect_colors()

    def handle_yaml_error(e):
        cli_logger.error(""Cluster config invalid\\n"")
        cli_logger.error(""Failed to load YAML file "" + cf.bold(""{}""),
                         config_file)
        cli_logger.newline()
        with cli_logger.verbatim_error_ctx(""PyYAML error:""):
            cli_logger.error(e)
        cli_logger.abort()

    try:
        config = yaml.safe_load(open(config_file).read())
    except FileNotFoundError:
        cli_logger.abort(
            ""Provided cluster configuration file ({}) does not exist"",
            cf.bold(config_file))
    except yaml.parser.ParserError as e:
        handle_yaml_error(e)
    except yaml.scanner.ScannerError as e:
        handle_yaml_error(e)

    # todo: validate file_mounts, ssh keys, etc.

    importer = NODE_PROVIDERS.get(config[""provider""][""type""])
    if not importer:
        cli_logger.abort(
            ""Unknown provider type "" + cf.bold(""{}"") + ""\\n""
            ""Available providers are: {}"", config[""provider""][""type""],
            cli_logger.render_list([
                k for k in NODE_PROVIDERS.keys()
                if NODE_PROVIDERS[k] is not None
            ]))
        raise NotImplementedError(""Unsupported provider {}"".format(
            config[""provider""]))

    cli_logger.success(""Cluster configuration valid\\n"")

    printed_overrides = False

    def handle_cli_override(key, override):
        if override is not None:
            if key in config:
                nonlocal printed_overrides
                printed_overrides = True
                cli_logger.warning(
                    ""`{}` override provided on the command line.\\n""
                    ""  Using "" + cf.bold(""{}"") + cf.dimmed(
                        "" [configuration file has "" + cf.bold(""{}"") + ""]""),
                    key, override, config[key])
            config[key] = override

    handle_cli_override(""min_workers"", override_min_workers)
    handle_cli_override(""max_workers"", override_max_workers)
    handle_cli_override(""cluster_name"", override_cluster_name)

    if printed_overrides:
        cli_logger.newline()

    cli_logger.labeled_value(""Cluster"", config[""cluster_name""])

    # disable the cli_logger here if needed
    # because it only supports aws
    if config[""provider""][""type""] != ""aws"":
        cli_logger.old_style = True
    cli_logger.newline()
    config = _bootstrap_config(config, no_config_cache)
    if config[""provider""][""type""] != ""aws"":
        cli_logger.old_style = False

    try_logging_config(config)
    get_or_create_head_node(config, config_file, no_restart, restart_only, yes,
                            override_cluster_name)","def create_or_update_cluster(
        config_file: str, override_min_workers: Optional[int],
        override_max_workers: Optional[int], no_restart: bool,
        restart_only: bool, yes: bool, override_cluster_name: Optional[str],
        no_config_cache: bool, dump_command_output: bool,
        use_login_shells: bool) -> None:
    """"""Create or updates an autoscaling Ray cluster from a config json.""""""
    set_using_login_shells(use_login_shells)
    cmd_output_util.set_output_redirected(not dump_command_output)

    if use_login_shells:
        cli_logger.warning(
            ""Commands running under a login shell can produce more ""
            ""output than special processing can handle."")
        cli_logger.warning(
            ""Thus, the output from subcommands will be logged as is."")
        cli_logger.warning(
            ""Consider using {}, {}."", cf.bold(""--use-normal-shells""),
            cf.underlined(""if you tested your workflow and it is compatible""))
        cli_logger.newline()

    cli_logger.detect_colors()

    def handle_yaml_error(e):
        cli_logger.error(""Cluster config invalid\\n"")
        cli_logger.error(""Failed to load YAML file "" + cf.bold(""{}""),
                         config_file)
        cli_logger.newline()
        with cli_logger.verbatim_error_ctx(""PyYAML error:""):
            cli_logger.error(e)
        cli_logger.abort()

    try:
        config = yaml.safe_load(open(config_file).read())
    except FileNotFoundError:
        cli_logger.abort(
            ""Provided cluster configuration file ({}) does not exist"",
            cf.bold(config_file))
    except yaml.parser.ParserError as e:
        handle_yaml_error(e)
    except yaml.scanner.ScannerError as e:
        handle_yaml_error(e)

    # todo: validate file_mounts, ssh keys, etc.

    importer = NODE_PROVIDERS.get(config[""provider""][""type""])
    if not importer:
        cli_logger.abort(
            ""Unknown provider type "" + cf.bold(""{}"") + ""\\n""
            ""Available providers are: {}"", config[""provider""][""type""],
            cli_logger.render_list([
                k for k in NODE_PROVIDERS.keys()
                if NODE_PROVIDERS[k] is not None
            ]))
        raise NotImplementedError(""Unsupported provider {}"".format(
            config[""provider""]))

    cli_logger.success(""Cluster configuration valid\\n"")

    printed_overrides = False

    def handle_cli_override(key, override):
        if override is not None:
            if key in config:
                nonlocal printed_overrides
                printed_overrides = True
                cli_logger.warning(
                    ""`{}` override provided on the command line.\\n""
                    ""  Using "" + cf.bold(""{}"") + cf.dimmed(
                        "" [configuration file has "" + cf.bold(""{}"") + ""]""),
                    key, override, config[key])
            config[key] = override

    handle_cli_override(""min_workers"", override_min_workers)
    handle_cli_override(""max_workers"", override_max_workers)
    handle_cli_override(""cluster_name"", override_cluster_name)

    if printed_overrides:
        cli_logger.newline()

    cli_logger.labeled_value(""Cluster"", config[""cluster_name""])

    # disable the cli_logger here if needed
    # because it only supports aws
    if config[""provider""][""type""] != ""aws"":
        cli_logger.old_style = True
    cli_logger.newline()
    config = _bootstrap_config(config, no_config_cache)
    if config[""provider""][""type""] != ""aws"":
        cli_logger.old_style = False

    try_logging_config(config)
    get_or_create_head_node(config, config_file, no_restart, restart_only, yes,
                            override_cluster_name)"
"def submit(cluster_config_file, screen, tmux, stop, start, cluster_name,
           port_forward, script, args, script_args, log_new_style, log_color,
           verbose):
    """"""Uploads and runs a script on the specified cluster.

    The script is automatically synced to the following location:

        os.path.join(""~"", os.path.basename(script))

    Example:
        >>> ray submit [CLUSTER.YAML] experiment.py -- --smoke-test
    """"""
    cli_logger.old_style = not log_new_style
    cli_logger.color_mode = log_color
    cli_logger.verbosity = verbose

    set_output_redirected(False)

    cli_logger.doassert(not (screen and tmux),
                        ""`{}` and `{}` are incompatible."", cf.bold(""--screen""),
                        cf.bold(""--tmux""))
    cli_logger.doassert(
        not (script_args and args),
        ""`{0}` and `{1}` are incompatible. Use only `{1}`.\\n""
        ""Example: `{2}`"", cf.bold(""--args""), cf.bold(""-- <args ...>""),
        cf.bold(""ray submit script.py -- --arg=123 --flag""))

    assert not (screen and tmux), ""Can specify only one of `screen` or `tmux`.""
    assert not (script_args and args), ""Use -- --arg1 --arg2 for script args.""

    if args:
        cli_logger.warning(
            ""`{}` is deprecated and will be removed in the future."",
            cf.bold(""--args""))
        cli_logger.warning(""Use `{}` instead. Example: `{}`."",
                           cf.bold(""-- <args ...>""),
                           cf.bold(""ray submit script.py -- --arg=123 --flag""))
        cli_logger.newline()
        cli_logger.old_warning(
            logger,
            ""ray submit [yaml] [script.py] --args=... is deprecated and ""
            ""will be removed in a future version of Ray. Use ""
            ""`ray submit [yaml] script.py -- --arg1 --arg2` instead."")

    if start:
        create_or_update_cluster(
            config_file=cluster_config_file,
            override_min_workers=None,
            override_max_workers=None,
            no_restart=False,
            restart_only=False,
            yes=True,
            override_cluster_name=cluster_name,
            no_config_cache=False,
            dump_command_output=True,
            use_login_shells=True)
    target = os.path.basename(script)
    target = os.path.join(""~"", target)
    rsync(cluster_config_file, script, target, cluster_name, down=False)

    command_parts = [""python"", target]
    if script_args:
        command_parts += list(script_args)
    elif args is not None:
        command_parts += [args]

    port_forward = [(port, port) for port in list(port_forward)]
    cmd = "" "".join(command_parts)
    exec_cluster(
        cluster_config_file,
        cmd=cmd,
        run_env=""docker"",
        screen=screen,
        tmux=tmux,
        stop=stop,
        start=False,
        override_cluster_name=cluster_name,
        port_forward=port_forward)","def submit(cluster_config_file, screen, tmux, stop, start, cluster_name,
           port_forward, script, args, script_args, log_new_style, log_color,
           verbose):
    """"""Uploads and runs a script on the specified cluster.

    The script is automatically synced to the following location:

        os.path.join(""~"", os.path.basename(script))

    Example:
        >>> ray submit [CLUSTER.YAML] experiment.py -- --smoke-test
    """"""
    cli_logger.old_style = not log_new_style
    cli_logger.color_mode = log_color
    cli_logger.verbosity = verbose

    set_output_redirected(False)

    cli_logger.doassert(not (screen and tmux),
                        ""`{}` and `{}` are incompatible."", cf.bold(""--screen""),
                        cf.bold(""--tmux""))
    cli_logger.doassert(
        not (script_args and args),
        ""`{0}` and `{1}` are incompatible. Use only `{1}`.\\n""
        ""Example: `{2}`"", cf.bold(""--args""), cf.bold(""-- <args ...>""),
        cf.bold(""ray submit script.py -- --arg=123 --flag""))

    assert not (screen and tmux), ""Can specify only one of `screen` or `tmux`.""
    assert not (script_args and args), ""Use -- --arg1 --arg2 for script args.""

    if args:
        cli_logger.warning(
            ""`{}` is deprecated and will be removed in the future."",
            cf.bold(""--args""))
        cli_logger.warning(""Use `{}` instead. Example: `{}`."",
                           cf.bold(""-- <args ...>""),
                           cf.bold(""ray submit script.py -- --arg=123 --flag""))
        cli_logger.newline()
        cli_logger.old_warning(
            logger,
            ""ray submit [yaml] [script.py] --args=... is deprecated and ""
            ""will be removed in a future version of Ray. Use ""
            ""`ray submit [yaml] script.py -- --arg1 --arg2` instead."")

    if start:
        create_or_update_cluster(cluster_config_file, None, None, False, False,
                                 True, cluster_name, False)
    target = os.path.basename(script)
    target = os.path.join(""~"", target)
    rsync(cluster_config_file, script, target, cluster_name, down=False)

    command_parts = [""python"", target]
    if script_args:
        command_parts += list(script_args)
    elif args is not None:
        command_parts += [args]

    port_forward = [(port, port) for port in list(port_forward)]
    cmd = "" "".join(command_parts)
    exec_cluster(
        cluster_config_file,
        cmd=cmd,
        run_env=""docker"",
        screen=screen,
        tmux=tmux,
        stop=stop,
        start=False,
        override_cluster_name=cluster_name,
        port_forward=port_forward)"
