index,before_merge,codeT5_secure_before_score,codeT5_optimize_before_score,T5_secure_before_score,T5_optimize_before_score
0,"def cmdb_get_mainline_object_topo(request, bk_biz_id, bk_supplier_account=''):
    """"""
    @summary: 获取配置平台业务拓扑模型
    @param request:
    @param bk_biz_id:
    @param bk_supplier_account:
    @return:
    """"""
    kwargs = {
        'bk_biz_id': bk_biz_id,
        'bk_supplier_account': bk_supplier_account,
    }
    client = get_client_by_request(request)
    cc_result = client.cc.get_mainline_object_topo(kwargs)
    if not cc_result['result']:
        message = handle_api_error(_(u""配置平台(CMDB)""),
                                   'cc.get_mainline_object_topo',
                                   kwargs,
                                   cc_result['message'])
        return {'result': cc_result['result'], 'code': cc_result['code'], 'message': message}
    data = cc_result['data']
    for bk_obj in data:
        if bk_obj['bk_obj_id'] == 'host':
            bk_obj['bk_obj_name'] = 'IP'
    result = {'result': cc_result['result'], 'code': cc_result['code'], 'data': cc_result['data']}
    return JsonResponse(result)",0.7832898286371102,0.7313493273475662,0.7131451308674834,0.957353046536347
1,"def cc_search_object_attribute(request, obj_id, biz_cc_id, supplier_account):
    """"""
    @summary: 获取对象自定义属性
    @param request:
    @param biz_cc_id:
    @return:
    """"""
    client = get_client_by_request(request)
    kwargs = {
        'bk_obj_id': obj_id,
        'bk_supplier_account': supplier_account
    }
    cc_result = client.cc.search_object_attribute(kwargs)
    if not cc_result['result']:
        message = handle_api_error('cc', 'cc.search_object_attribute', kwargs, cc_result['message'])
        logger.error(message)
        result = {
            'result': False,
            'data': [],
            'message': message
        }
        return JsonResponse(result)

    obj_property = []
    for item in cc_result['data']:
        if item['editable']:
            obj_property.append({
                'value': item['bk_property_id'],
                'text': item['bk_property_name']
            })

    return JsonResponse({'result': True, 'data': obj_property})",0.7958452584667938,0.8045668837957962,0.9720129010368984,0.3082115634137763
2,"def cc_search_create_object_attribute(request, obj_id, biz_cc_id, supplier_account):
    client = get_client_by_request(request)
    kwargs = {
        'bk_obj_id': obj_id,
        'bk_supplier_account': supplier_account
    }
    cc_result = client.cc.search_object_attribute(kwargs)
    if not cc_result['result']:
        message = handle_api_error('cc', 'cc.search_object_attribute', kwargs, cc_result['message'])
        logger.error(message)
        result = {
            'result': False,
            'data': [],
            'message': message
        }
        return JsonResponse(result)

    obj_property = []
    for item in cc_result['data']:
        if item['editable']:
            prop_dict = {
                'tag_code': item['bk_property_id'],
                'type': ""input"",
                'attrs': {
                    'name': item['bk_property_name'],
                    'editable': 'true',
                },
            }
            if item['bk_property_id'] in ['bk_set_name']:
                prop_dict[""attrs""][""validation""] = [
                    {
                        ""type"": ""required""
                    }
                ]
            obj_property.append(prop_dict)

    return JsonResponse({'result': True, 'data': obj_property})",0.9701514917007692,0.8318274579238776,0.7215416835465073,0.9777834629424744
3,"def cc_search_topo(request, obj_id, category, biz_cc_id, supplier_account):
    """"""
    @summary: 查询对象拓扑
    @param request:
    @param biz_cc_id:
    @return:
    """"""
    client = get_client_by_request(request)
    kwargs = {
        'bk_biz_id': biz_cc_id,
        'bk_supplier_account': supplier_account
    }
    cc_result = client.cc.search_biz_inst_topo(kwargs)
    if not cc_result['result']:
        message = handle_api_error('cc', 'cc.search_biz_inst_topo', kwargs, cc_result['message'])
        logger.error(message)
        result = {
            'result': False,
            'data': [],
            'message': message
        }
        return JsonResponse(result)

    if category in [""normal"", ""prev"", ""picker""]:
        cc_topo = cc_format_topo_data(cc_result['data'], obj_id, category)
    else:
        cc_topo = []

    return JsonResponse({'result': True, 'data': cc_topo})",0.781099404686565,0.7173508616353851,0.7376549796812725,0.8990918742551253
4,"def job_get_script_list(request, biz_cc_id):
    """"""
    查询业务脚本列表
    :param request:
    :param biz_cc_id:
    :return:
    """"""
    # 查询脚本列表
    client = get_client_by_request(request)
    script_type = request.GET.get('type')
    kwargs = {
        'bk_biz_id': biz_cc_id,
        'is_public': True if script_type == 'public' else False
    }
    script_result = client.job.get_script_list(kwargs)

    if not script_result['result']:
        message = handle_api_error('cc', 'job.get_script_list', kwargs, script_result['message'])
        logger.error(message)
        result = {
            'result': False,
            'message': message
        }
        return JsonResponse(result)

    script_dict = {}
    for script in script_result['data']['data']:
        script_dict.setdefault(script['name'], []).append(script['id'])

    version_data = []
    for name, version in script_dict.items():
        version_data.append({
            ""text"": name,
            ""value"": max(version)
        })

    return JsonResponse({'result': True, 'data': version_data})",0.9713406625429728,0.6705210957574562,0.8921357164023518,0.970176505092188
5,"def job_get_job_tasks_by_biz(request, biz_cc_id):
    client = get_client_by_request(request)
    job_result = client.job.get_job_list({'bk_biz_id': biz_cc_id})
    if not job_result['result']:
        message = _(u""查询作业平台(JOB)的作业模板[app_id=%s]接口job.get_task返回失败: %s"") % (
            biz_cc_id, job_result['message'])
        logger.error(message)
        result = {
            'result': False,
            'data': [],
            'message': message
        }
        return JsonResponse(result)
    task_list = []
    for task in job_result['data']:
        task_list.append({
            'value': task['bk_job_id'],
            'text': task['name'],
        })
    return JsonResponse({'result': True, 'data': task_list})",0.9909255550608124,0.8251715617782741,0.6714756457016864,0.9239983297073558
6,"def job_get_job_task_detail(request, biz_cc_id, task_id):
    client = get_client_by_request(request)
    job_result = client.job.get_job_detail({'bk_biz_id': biz_cc_id,
                                            'bk_job_id': task_id})
    if not job_result['result']:
        message = _(u""查询作业平台(JOB)的作业模板详情[app_id=%s]接口job.get_task_detail返回失败: %s"") % (
            biz_cc_id, job_result['message'])
        logger.error(message)
        result = {
            'result': False,
            'data': [],
            'message': message
        }
        return JsonResponse(result)

    job_step_type_name = {
        1: _(u""脚本""),
        2: _(u""文件""),
        4: u""SQL""
    }
    task_detail = job_result['data']
    global_var = []
    steps = []
    for var in task_detail.get('global_vars', []):
        # 1-字符串, 2-IP, 3-索引数组, 4-关联数组
        if var['type'] in [JOB_VAR_TYPE_STR, JOB_VAR_TYPE_IP, JOB_VAR_TYPE_ARRAY]:
            value = var.get('value', '')
        else:
            value = ['{plat_id}:{ip}'.format(plat_id=ip_item['bk_cloud_id'], ip=ip_item['ip'])
                     for ip_item in var.get('ip_list', [])]
        global_var.append({
            'id': var['id'],
            # 全局变量类型：1:云参, 2:上下文参数，3:IP
            'category': var.get('category', 1),
            'name': var['name'],
            'type': var['type'],
            'value': value,
            'description': var['description']
        })
    for info in task_detail.get('steps', []):
        # 1-执行脚本, 2-传文件, 4-传SQL
        steps.append({
            'stepId': info['step_id'],
            'name': info['name'],
            'scriptParams': info.get('script_param', ''),
            'account': info.get('account', ''),
            'ipList': '',
            'type': info['type'],
            'type_name': job_step_type_name.get(info['type'], info['type'])
        })
    return JsonResponse({'result': True, 'data': {'global_var': global_var, 'steps': steps}})",0.9833693090547544,0.9327619165475872,0.7074136050413318,0.987230081527972
7,"def get_bk_user(request):
    bkuser = None
    if request.weixin_user and not isinstance(request.weixin_user, AnonymousUser):
        try:
            user_property = UserProperty.objects.get(key='wx_userid', value=request.weixin_user.userid)
            bkuser = user_property.user
        except UserProperty.DoesNotExist:
            bkuser = None
    return bkuser or AnonymousUser()",0.9443486000129632,0.8325440639711157,0.6804073867094043,0.5540621579099914
8,"    def fit(self, dataset: Dataset):
        """"""Calculates statistics for this workflow on the input dataset

        Parameters
        -----------
        dataset: Dataset
            The input dataset to calculate statistics for. If there is a train/test split this
            data should be the training dataset only.
        """"""
        self._clear_worker_cache()
        ddf = dataset.to_ddf(columns=self._input_columns())

        # Get a dictionary mapping all StatOperators we need to fit to a set of any dependant
        # StatOperators (having StatOperators that depend on the output of other StatOperators
        # means that will have multiple phases in the fit cycle here)
        stat_ops = {op: _get_stat_ops(op.parents) for op in _get_stat_ops([self.column_group])}

        while stat_ops:
            # get all the StatOperators that we can currently call fit on (no outstanding
            # dependencies)
            current_phase = [op for op, dependencies in stat_ops.items() if not dependencies]
            if not current_phase:
                # this shouldn't happen, but lets not infinite loop just in case
                raise RuntimeError(""failed to find dependency-free StatOperator to fit"")

            stats, ops = [], []
            for column_group in current_phase:
                # apply transforms necessary for the inputs to the current column group, ignoring
                # the transforms from the statop itself
                transformed_ddf = _transform_ddf(ddf, column_group.parents)

                op = column_group.op
                try:
                    stats.append(op.fit(column_group.input_column_names, transformed_ddf))
                    ops.append(op)
                except Exception:
                    LOG.exception(""Failed to fit operator %s"", column_group.op)
                    raise

            if self.client:
                results = [r.result() for r in self.client.compute(stats)]
            else:
                results = dask.compute(stats, scheduler=""synchronous"")[0]

            for computed_stats, op in zip(results, ops):
                op.fit_finalize(computed_stats)

            # Remove all the operators we processed in this phase, and remove
            # from the dependencies of other ops too
            for stat_op in current_phase:
                stat_ops.pop(stat_op)
            for dependencies in stat_ops.values():
                dependencies.difference_update(current_phase)

        # hack: store input/output dtypes here. We should have complete dtype
        # information for each operator (like we do for column names), but as
        # an interim solution this gets us what we need.
        input_dtypes = dataset.to_ddf().dtypes
        self.input_dtypes = dict(zip(input_dtypes.index, input_dtypes))
        output_dtypes = self.transform(dataset).to_ddf().head(1).dtypes
        self.output_dtypes = dict(zip(output_dtypes.index, output_dtypes))",0.8725388668037075,0.8725388668037075,0.7675380858029047,0.8149987861164258
9,"def main(args):
    """"""Multi-GPU Criteo/DLRM Preprocessing Benchmark

    This benchmark is designed to measure the time required to preprocess
    the Criteo (1TB) dataset for Facebook’s DLRM model.  The user must specify
    the path of the raw dataset (using the `--data-path` flag), as well as the
    output directory for all temporary/final data (using the `--out-path` flag)

    Example Usage
    -------------

    python dask-nvtabular-criteo-benchmark.py
                        --data-path /path/to/criteo_parquet --out-path /out/dir/`


    Dataset Requirements (Parquet)
    ------------------------------

    This benchmark is designed with a parquet-formatted dataset in mind.
    While a CSV-formatted dataset can be processed by NVTabular, converting
    to parquet will yield significantly better performance.  To convert your
    dataset, try using the `optimize_criteo.ipynb` notebook (also located
    in `NVTabular/examples/`)

    For a detailed parameter overview see `NVTabular/examples/MultiGPUBench.md`
    """"""

    # Input
    data_path = args.data_path
    freq_limit = args.freq_limit
    out_files_per_proc = args.out_files_per_proc
    high_card_columns = args.high_cards.split("","")
    dashboard_port = args.dashboard_port
    if args.protocol == ""ucx"":
        UCX_TLS = os.environ.get(""UCX_TLS"", ""tcp,cuda_copy,cuda_ipc,sockcm"")
        os.environ[""UCX_TLS""] = UCX_TLS

    # Cleanup output directory
    BASE_DIR = args.out_path
    dask_workdir = os.path.join(BASE_DIR, ""workdir"")
    output_path = os.path.join(BASE_DIR, ""output"")
    stats_path = os.path.join(BASE_DIR, ""stats"")
    if not os.path.isdir(BASE_DIR):
        os.mkdir(BASE_DIR)
    for dir_path in (dask_workdir, output_path, stats_path):
        if os.path.isdir(dir_path):
            shutil.rmtree(dir_path)
        os.mkdir(dir_path)

    # Use Criteo dataset by default (for now)
    cont_names = (
        args.cont_names.split("","") if args.cont_names else [""I"" + str(x) for x in range(1, 14)]
    )
    cat_names = (
        args.cat_names.split("","") if args.cat_names else [""C"" + str(x) for x in range(1, 27)]
    )
    label_name = [""label""]

    # Specify Categorify/GroupbyStatistics options
    tree_width = {}
    cat_cache = {}
    for col in cat_names:
        if col in high_card_columns:
            tree_width[col] = args.tree_width
            cat_cache[col] = args.cat_cache_high
        else:
            tree_width[col] = 1
            cat_cache[col] = args.cat_cache_low

    # Use total device size to calculate args.device_limit_frac
    device_size = device_mem_size(kind=""total"")
    device_limit = int(args.device_limit_frac * device_size)
    device_pool_size = int(args.device_pool_frac * device_size)
    part_size = int(args.part_mem_frac * device_size)

    # Parse shuffle option
    shuffle = None
    if args.shuffle == ""PER_WORKER"":
        shuffle = nvt_io.Shuffle.PER_WORKER
    elif args.shuffle == ""PER_PARTITION"":
        shuffle = nvt_io.Shuffle.PER_PARTITION

    # Check if any device memory is already occupied
    for dev in args.devices.split("",""):
        fmem = _pynvml_mem_size(kind=""free"", index=int(dev))
        used = (device_size - fmem) / 1e9
        if used > 1.0:
            warnings.warn(f""BEWARE - {used} GB is already occupied on device {int(dev)}!"")

    # Setup LocalCUDACluster
    if args.protocol == ""tcp"":
        cluster = LocalCUDACluster(
            protocol=args.protocol,
            n_workers=args.n_workers,
            CUDA_VISIBLE_DEVICES=args.devices,
            device_memory_limit=device_limit,
            local_directory=dask_workdir,
            dashboard_address="":"" + dashboard_port,
        )
    else:
        cluster = LocalCUDACluster(
            protocol=args.protocol,
            n_workers=args.n_workers,
            CUDA_VISIBLE_DEVICES=args.devices,
            enable_nvlink=True,
            device_memory_limit=device_limit,
            local_directory=dask_workdir,
            dashboard_address="":"" + dashboard_port,
        )
    client = Client(cluster)

    # Setup RMM pool
    if args.device_pool_frac > 0.01:
        setup_rmm_pool(client, device_pool_size)

    # Define Dask NVTabular ""Workflow""
    if args.normalize:
        cont_features = cont_names >> ops.FillMissing() >> ops.Normalize()
    else:
        cont_features = cont_names >> ops.FillMissing() >> ops.Clip(min_value=0) >> ops.LogOp()

    cat_features = cat_names >> ops.Categorify(
        out_path=stats_path,
        tree_width=tree_width,
        cat_cache=cat_cache,
        freq_threshold=freq_limit,
        search_sorted=not freq_limit,
        on_host=not args.cats_on_device,
    )
    processor = Workflow(cat_features + cont_features + label_name, client=client)

    dataset = Dataset(data_path, ""parquet"", part_size=part_size)

    # Execute the dask graph
    runtime = time.time()

    processor.fit(dataset)

    if args.profile is not None:
        with performance_report(filename=args.profile):
            processor.transform(dataset).to_parquet(
                output_path=output_path,
                num_threads=args.num_io_threads,
                shuffle=shuffle,
                out_files_per_proc=out_files_per_proc,
            )
    else:
        processor.transform(dataset).to_parquet(
            output_path=output_path,
            num_threads=args.num_io_threads,
            shuffle=shuffle,
            out_files_per_proc=out_files_per_proc,
        )
    runtime = time.time() - runtime

    print(""\\nDask-NVTabular DLRM/Criteo benchmark"")
    print(""--------------------------------------"")
    print(f""partition size     | {part_size}"")
    print(f""protocol           | {args.protocol}"")
    print(f""device(s)          | {args.devices}"")
    print(f""rmm-pool-frac      | {(args.device_pool_frac)}"")
    print(f""out-files-per-proc | {args.out_files_per_proc}"")
    print(f""num_io_threads     | {args.num_io_threads}"")
    print(f""shuffle            | {args.shuffle}"")
    print(f""cats-on-device     | {args.cats_on_device}"")
    print(""======================================"")
    print(f""Runtime[s]         | {runtime}"")
    print(""======================================\\n"")

    client.close()",0.3618927684999274,0.4258207416105459,0.4258207416105459,0.4258207416105459
10,"    def __init__(self, out_dir, **kwargs):
        super().__init__(out_dir, **kwargs)
        self.data_paths = []
        self.data_writers = []
        self.data_bios = []
        self._lock = threading.RLock()
        self.pwriter = self._pwriter
        self.pwriter_kwargs = {}",0.7735281905624953,0.6730897663037786,0.8149695356293876,0.6877004948144134
11,"    def _append_writer(self, path, schema=None, add_args=None, add_kwargs=None):
        # Add additional args and kwargs
        _args = add_args or []
        _kwargs = tlz.merge(self.pwriter_kwargs, add_kwargs or {})

        if self.bytes_io:
            bio = BytesIO()
            self.data_bios.append(bio)
            self.data_writers.append(self.pwriter(bio, *_args, **_kwargs))
        else:
            self.data_writers.append(self.pwriter(path, *_args, **_kwargs))",0.914061489790294,0.8402266848923912,0.7816090668341766,0.7236193567903786
12,"    def _close_writers(self):
        md_dict = {}
        for writer, path in zip(self.data_writers, self.data_paths):
            fn = path.split(self.fs.sep)[-1]
            md_dict[fn] = writer.close(metadata_file_path=fn)
        return md_dict",0.7145282042775756,0.4445759911679247,0.9251916084357528,0.6911482315917052
13,"def fetch_table_data(
    table_cache, path, cache=""disk"", cats_only=False, reader=None, columns=None, **kwargs
):
    """"""Utility to retrieve a cudf DataFrame from a cache (and add the
    DataFrame to a cache if the element is missing).  Note that `cats_only=True`
    results in optimized logic for the `Categorify` transformation.
    """"""
    table = table_cache.get(path, None)
    if table and not isinstance(table, cudf.DataFrame):
        if not cats_only:
            return cudf.io.read_parquet(table, index=False)
        df = cudf.io.read_parquet(table, index=False, columns=columns)
        df.index.name = ""labels""
        df.reset_index(drop=False, inplace=True)
        return df

    reader = reader or cudf.io.read_parquet
    if table is None:
        if cache in (""device"", ""disk""):
            table = reader(path, index=False, columns=columns, **kwargs)
        elif cache == ""host"":
            if reader == cudf.io.read_parquet:
                # If the file is already in parquet format,
                # we can just move the same bytes to host memory
                with open(path, ""rb"") as f:
                    table_cache[path] = BytesIO(f.read())
                table = reader(table_cache[path], index=False, columns=columns, **kwargs)
            else:
                # Otherwise, we should convert the format to parquet
                table = reader(path, index=False, columns=columns, **kwargs)
                table_cache[path] = BytesIO()
                table.to_parquet(table_cache[path])
        if cats_only:
            table.index.name = ""labels""
            table.reset_index(drop=False, inplace=True)
        if cache == ""device"":
            table_cache[path] = table.copy(deep=False)
    return table",0.9961533406831286,0.7806660668485258,0.9961533406831286,0.8892173288280523
14,"def _chunkwise_moments(df):
    df2 = cudf.DataFrame()
    for col in df.columns:
        df2[col] = df[col].astype(""float64"").pow(2)
    vals = {
        ""df-count"": df.count().to_frame().transpose(),
        ""df-sum"": df.sum().to_frame().transpose(),
        ""df2-sum"": df2.sum().to_frame().transpose(),
    }
    # NOTE: Perhaps we should convert to pandas here
    # (since we know the results should be small)?
    del df2
    return vals",0.7308362026855654,0.5363988443475343,0.9093640707662568,0.6689633887957152
15,"    def to_ddf(self, columns=None):
        return dask_cudf.read_parquet(
            self.paths,
            columns=columns,
            index=False,
            gather_statistics=False,
            split_row_groups=self.row_groups_per_part,
            storage_options=self.storage_options,
        )",0.9603282561260568,0.7318484429127751,0.5946825651117897,0.6331478500425876
16,"    def get_ddf(self):
        if self.ddf is None:
            raise ValueError(""No dask_cudf frame available."")
        elif isinstance(self.ddf, Dataset):
            columns = self.columns_ctx[""all""][""base""]
            return self.ddf.to_ddf(columns=columns, shuffle=self._shuffle_parts)
        return self.ddf",0.9140159816641276,0.7715089606808251,0.7223790877474482,0.5328962461082575
17,"    def add_data(self, gdf):
        # Populate columns idxs
        if not self.col_idx:
            for i, x in enumerate(gdf.columns.values):
                self.col_idx[str(x)] = i

        # list columns in cudf don't currently support chunked writing in parquet.
        # hack around this by just writing a single file with this partition
        # this restriction can be removed once cudf supports chunked writing
        # in parquet
        if any(is_list_dtype(gdf[col].dtype) for col in gdf.columns):
            self._write_table(gdf, 0, True)
            return

        # Generate `ind` array to map each row to an output file.
        # This approach is certainly more optimized for shuffling
        # than it is for non-shuffling, but using a single code
        # path is probably worth the (possible) minor overhead.
        nrows = gdf.shape[0]
        typ = np.min_scalar_type(nrows * 2)
        if self.shuffle:
            ind = cp.random.choice(cp.arange(self.num_out_files, dtype=typ), nrows)
        else:
            ind = cp.arange(nrows, dtype=typ)
            cp.floor_divide(ind, math.ceil(nrows / self.num_out_files), out=ind)
        for x, group in enumerate(
            gdf.scatter_by_map(ind, map_size=self.num_out_files, keep_index=False)
        ):
            self.num_samples[x] += len(group)
            if self.num_threads > 1:
                self.queue.put((x, group))
            else:
                self._write_table(x, group)

        # wait for all writes to finish before exiting
        # (so that we aren't using memory)
        if self.num_threads > 1:
            self.queue.join()",0.9365589690503864,0.8380674862893169,0.8730895259404731,0.8119031394328231
18,"    def __init__(
        self,
        paths,
        part_size,
        storage_options,
        row_groups_per_part=None,
        legacy=False,
        batch_size=None,
    ):
        # TODO: Improve dask_cudf.read_parquet performance so that
        # this class can be slimmed down.
        super().__init__(paths, part_size, storage_options)
        self.batch_size = batch_size
        self._metadata, self._base = self.metadata
        self._pieces = None
        if row_groups_per_part is None:
            file_path = self._metadata.row_group(0).column(0).file_path
            path0 = (
                self.fs.sep.join([self._base, file_path])
                if file_path != """"
                else self._base  # This is a single file
            )

        if row_groups_per_part is None:
            rg_byte_size_0 = (
                cudf.io.read_parquet(path0, row_groups=0, row_group=0)
                .memory_usage(deep=True, index=True)
                .sum()
            )
            row_groups_per_part = self.part_size / rg_byte_size_0
            if row_groups_per_part < 1.0:
                warnings.warn(
                    f""Row group size {rg_byte_size_0} is bigger than requested part_size ""
                    f""{self.part_size}""
                )
                row_groups_per_part = 1.0

        self.row_groups_per_part = int(row_groups_per_part)

        assert self.row_groups_per_part > 0",0.9238751764540796,0.968254578483663,0.9241733217097582,0.7413554059602987
19,"    def __init__(self, *args, **kwargs):
        super().__init__(*args)
        self._meta = {}
        self.names = kwargs.pop(""names"", None)
        self.csv_kwargs = kwargs
        # CSV reader needs a list of files
        # (Assume flat directory structure if this is a dir)
        if len(self.paths) == 1 and self.fs.isdir(self.paths[0]):
            self.paths = self.fs.glob(self.fs.sep.join([self.paths[0], ""*""]))",0.797005699149262,0.7679453830975157,0.8914656070092244,0.7400160582739024
20,"    def to_ddf(self, columns=None):
        return dask_cudf.read_csv(
            self.paths, names=self.names, chunksize=self.part_size, **self.csv_kwargs
        )[columns]",0.9188290502777836,0.6008813908631505,0.8743222919702069,0.4734656231703142
21,"    def _predict(self, X):
        """"""Collect results from clf.predict calls.""""""
        return np.asarray([clf.predict(X) for clf in self.clfs_]).T",0.4,0.5503041711365013,0.723102448833079,0.95945068632967
22,"    def transform(
        self,
        xx: Any,
        yy: Any,
        zz: Any = None,
        tt: Any = None,
        radians: bool = False,
        errcheck: bool = False,
        direction: Union[TransformDirection, str] = TransformDirection.FORWARD,
    ) -> Any:
        """"""
        Transform points between two coordinate systems.

        .. versionadded:: 2.1.1 errcheck
        .. versionadded:: 2.2.0 direction

        Parameters
        ----------
        xx: scalar or array (numpy or python)
            Input x coordinate(s).
        yy: scalar or array (numpy or python)
            Input y coordinate(s).
        zz: scalar or array (numpy or python), optional
            Input z coordinate(s).
        tt: scalar or array (numpy or python), optional
            Input time coordinate(s).
        radians: boolean, optional
            If True, will expect input data to be in radians and will return radians
            if the projection is geographic. Default is False (degrees). Ignored for
            pipeline transformations.
        errcheck: boolean, optional (default False)
            If True an exception is raised if the transformation is invalid.
            By default errcheck=False and an invalid transformation
            returns ``inf`` and no exception is raised.
        direction: pyproj.enums.TransformDirection, optional
            The direction of the transform.
            Default is :attr:`pyproj.enums.TransformDirection.FORWARD`.


        Example:

        >>> from pyproj import Transformer
        >>> transformer = Transformer.from_crs(""epsg:4326"", ""epsg:3857"")
        >>> x3, y3 = transformer.transform(33, 98)
        >>> ""%.3f  %.3f"" % (x3, y3)
        '10909310.098  3895303.963'
        >>> pipeline_str = (
        ...     ""+proj=pipeline +step +proj=longlat +ellps=WGS84 ""
        ...     ""+step +proj=unitconvert +xy_in=rad +xy_out=deg""
        ... )
        >>> pipe_trans = Transformer.from_pipeline(pipeline_str)
        >>> xt, yt = pipe_trans.transform(2.1, 0.001)
        >>> ""%.3f  %.3f"" % (xt, yt)
        '120.321  0.057'
        >>> transproj = Transformer.from_crs(
        ...     {""proj"":'geocent', ""ellps"":'WGS84', ""datum"":'WGS84'},
        ...     ""EPSG:4326"",
        ...     always_xy=True,
        ... )
        >>> xpj, ypj, zpj = transproj.transform(
        ...     -2704026.010,
        ...     -4253051.810,
        ...     3895878.820,
        ...     radians=True,
        ... )
        >>> ""%.3f %.3f %.3f"" % (xpj, ypj, zpj)
        '-2.137 0.661 -20.531'
        >>> transprojr = Transformer.from_crs(
        ...     ""EPSG:4326"",
        ...     {""proj"":'geocent', ""ellps"":'WGS84', ""datum"":'WGS84'},
        ...     always_xy=True,
        ... )
        >>> xpjr, ypjr, zpjr = transprojr.transform(xpj, ypj, zpj, radians=True)
        >>> ""%.3f %.3f %.3f"" % (xpjr, ypjr, zpjr)
        '-2704026.010 -4253051.810 3895878.820'
        >>> transformer = Transformer.from_proj(""epsg:4326"", 4326, skip_equivalent=True)
        >>> xeq, yeq = transformer.transform(33, 98)
        >>> ""%.0f  %.0f"" % (xeq, yeq)
        '33  98'

        """"""
        # process inputs, making copies that support buffer API.
        inx, xisfloat, xislist, xistuple = _copytobuffer(xx)
        iny, yisfloat, yislist, yistuple = _copytobuffer(yy)
        if zz is not None:
            inz, zisfloat, zislist, zistuple = _copytobuffer(zz)
        else:
            inz = None
        if tt is not None:
            intime, tisfloat, tislist, tistuple = _copytobuffer(tt)
        else:
            intime = None
        # call pj_transform.  inx,iny,inz buffers modified in place.
        self._transformer._transform(
            inx,
            iny,
            inz=inz,
            intime=intime,
            direction=direction,
            radians=radians,
            errcheck=errcheck,
        )
        # if inputs were lists, tuples or floats, convert back.
        outx = _convertback(xisfloat, xislist, xistuple, inx)
        outy = _convertback(yisfloat, yislist, xistuple, iny)
        return_data = (outx, outy)
        if inz is not None:
            return_data += (  # type: ignore
                _convertback(zisfloat, zislist, zistuple, inz),
            )
        if intime is not None:
            return_data += (  # type: ignore
                _convertback(tisfloat, tislist, tistuple, intime),
            )
        return return_data",0.2433676109700459,0.2433676109700459,0.2433676109700459,0.242307400381362
23,"    def itransform(
        self,
        points: Any,
        switch: bool = False,
        time_3rd: bool = False,
        radians: bool = False,
        errcheck: bool = False,
        direction: Union[TransformDirection, str] = TransformDirection.FORWARD,
    ) -> Iterator[Iterable]:
        """"""
        Iterator/generator version of the function pyproj.Transformer.transform.

        .. versionadded:: 2.1.1 errcheck
        .. versionadded:: 2.2.0 direction

        Parameters
        ----------
        points: list
            List of point tuples.
        switch: boolean, optional
            If True x, y or lon,lat coordinates of points are switched to y, x
            or lat, lon. Default is False.
        time_3rd: boolean, optional
            If the input coordinates are 3 dimensional and the 3rd dimension is time.
        radians: boolean, optional
            If True, will expect input data to be in radians and will return radians
            if the projection is geographic. Default is False (degrees). Ignored for
            pipeline transformations.
        errcheck: boolean, optional (default False)
            If True an exception is raised if the transformation is invalid.
            By default errcheck=False and an invalid transformation
            returns ``inf`` and no exception is raised.
        direction: pyproj.enums.TransformDirection, optional
            The direction of the transform.
            Default is :attr:`pyproj.enums.TransformDirection.FORWARD`.


        Example:

        >>> from pyproj import Transformer
        >>> transformer = Transformer.from_crs(4326, 2100)
        >>> points = [(22.95, 40.63), (22.81, 40.53), (23.51, 40.86)]
        >>> for pt in transformer.itransform(points): '{:.3f} {:.3f}'.format(*pt)
        '2221638.801 2637034.372'
        '2212924.125 2619851.898'
        '2238294.779 2703763.736'
        >>> pipeline_str = (
        ...     ""+proj=pipeline +step +proj=longlat +ellps=WGS84 ""
        ...     ""+step +proj=unitconvert +xy_in=rad +xy_out=deg""
        ... )
        >>> pipe_trans = Transformer.from_pipeline(pipeline_str)
        >>> for pt in pipe_trans.itransform([(2.1, 0.001)]):
        ...     '{:.3f} {:.3f}'.format(*pt)
        '120.321 0.057'
        >>> transproj = Transformer.from_crs(
        ...     {""proj"":'geocent', ""ellps"":'WGS84', ""datum"":'WGS84'},
        ...     ""EPSG:4326"",
        ...     always_xy=True,
        ... )
        >>> for pt in transproj.itransform(
        ...     [(-2704026.010, -4253051.810, 3895878.820)],
        ...     radians=True,
        ... ):
        ...     '{:.3f} {:.3f} {:.3f}'.format(*pt)
        '-2.137 0.661 -20.531'
        >>> transprojr = Transformer.from_crs(
        ...     ""EPSG:4326"",
        ...     {""proj"":'geocent', ""ellps"":'WGS84', ""datum"":'WGS84'},
        ...     always_xy=True,
        ... )
        >>> for pt in transprojr.itransform(
        ...     [(-2.137, 0.661, -20.531)],
        ...     radians=True
        ... ):
        ...     '{:.3f} {:.3f} {:.3f}'.format(*pt)
        '-2704214.394 -4254414.478 3894270.731'
        >>> transproj_eq = Transformer.from_proj(
        ...     'EPSG:4326',
        ...     '+proj=longlat +datum=WGS84 +no_defs +type=crs',
        ...     always_xy=True,
        ...     skip_equivalent=True
        ... )
        >>> for pt in transproj_eq.itransform([(-2.137, 0.661)]):
        ...     '{:.3f} {:.3f}'.format(*pt)
        '-2.137 0.661'

        """"""
        it = iter(points)  # point iterator
        # get first point to check stride
        try:
            fst_pt = next(it)
        except StopIteration:
            raise ValueError(""iterable must contain at least one point"")

        stride = len(fst_pt)
        if stride not in (2, 3, 4):
            raise ValueError(""points can contain up to 4 coordinates"")

        if time_3rd and stride != 3:
            raise ValueError(""'time_3rd' is only valid for 3 coordinates."")

        # create a coordinate sequence generator etc. x1,y1,z1,x2,y2,z2,....
        # chain so the generator returns the first point that was already acquired
        coord_gen = chain(fst_pt, (coords[c] for coords in it for c in range(stride)))

        while True:
            # create a temporary buffer storage for
            # the next 64 points (64*stride*8 bytes)
            buff = array(""d"", islice(coord_gen, 0, 64 * stride))
            if len(buff) == 0:
                break

            self._transformer._transform_sequence(
                stride,
                buff,
                switch=switch,
                direction=direction,
                time_3rd=time_3rd,
                radians=radians,
                errcheck=errcheck,
            )

            for pt in zip(*([iter(buff)] * stride)):
                yield pt",0.2306201161798424,0.2306201161798424,0.2306201161798424,0.5134925440711006
24,"    def from_user_input(value: str) -> ""CRS"":
        """"""
        Initialize a CRS class instance with:
          - PROJ string
          - Dictionary of PROJ parameters
          - PROJ keyword arguments for parameters
          - JSON string with PROJ parameters
          - CRS WKT string
          - An authority string [i.e. 'epsg:4326']
          - An EPSG integer code [i.e. 4326]
          - A tuple of (""auth_name"": ""auth_code"") [i.e ('epsg', '4326')]
          - An object with a `to_wkt` method.
          - A :class:`pyproj.crs.CRS` class

        Parameters
        ----------
        value : obj
            A Python int, dict, or str.

        Returns
        -------
        CRS
        """"""
        if isinstance(value, CRS):
            return value
        return CRS(value)",0.7496697341933072,0.8863863523119624,0.8869327988221103,0.7172492705443272
25,"    def __init__(
        self,
        name: str = ""undefined"",
        datum: Any = ""urn:ogc:def:datum:EPSG::6326"",
        ellipsoidal_cs: Any = Ellipsoidal2DCS(),
    ) -> None:
        """"""
        Parameters
        ----------
        name: str, optional
            Name of the CRS. Default is undefined.
        datum: Any, optional
            Anything accepted by :meth:`pyproj.crs.Datum.from_user_input` or
            a :class:`pyproj.crs.datum.CustomDatum`.
        ellipsoidal_cs: Any, optional
            Input to create an Ellipsoidal Coordinate System.
            Anything accepted by :meth:`pyproj.crs.CoordinateSystem.from_user_input`
            or an Ellipsoidal Coordinate System created from :ref:`coordinate_system`.
        """"""
        geographic_crs_json = {
            ""$schema"": ""https://proj.org/schemas/v0.2/projjson.schema.json"",
            ""type"": ""GeographicCRS"",
            ""name"": name,
            ""datum"": Datum.from_user_input(datum).to_json_dict(),
            ""coordinate_system"": CoordinateSystem.from_user_input(
                ellipsoidal_cs
            ).to_json_dict(),
        }
        super().__init__(geographic_crs_json)",0.8001341293879104,0.7457151233998782,0.8722841949052802,0.778349319976455
26,"    def __init__(
        self,
        base_crs: Any,
        conversion: Any,
        ellipsoidal_cs: Any = Ellipsoidal2DCS(),
        name: str = ""undefined"",
    ) -> None:
        """"""
        Parameters
        ----------
        base_crs: Any
            Input to create the Geodetic CRS, a :class:`GeographicCRS` or
            anything accepted by :meth:`pyproj.crs.CRS.from_user_input`.
        conversion: Any
            Anything accepted by :meth:`pyproj.crs.CoordinateSystem.from_user_input`
            or a conversion from :ref:`coordinate_operation`.
        ellipsoidal_cs: Any, optional
            Input to create an Ellipsoidal Coordinate System.
            Anything accepted by :meth:`pyproj.crs.CoordinateSystem.from_user_input`
            or an Ellipsoidal Coordinate System created from :ref:`coordinate_system`.
        name: str, optional
            Name of the CRS. Default is undefined.
        """"""
        derived_geographic_crs_json = {
            ""$schema"": ""https://proj.org/schemas/v0.2/projjson.schema.json"",
            ""type"": ""DerivedGeographicCRS"",
            ""name"": name,
            ""base_crs"": CRS.from_user_input(base_crs).to_json_dict(),
            ""conversion"": CoordinateOperation.from_user_input(
                conversion
            ).to_json_dict(),
            ""coordinate_system"": CoordinateSystem.from_user_input(
                ellipsoidal_cs
            ).to_json_dict(),
        }
        super().__init__(derived_geographic_crs_json)",0.9380021978675702,0.763076559651819,0.919058339327367,0.8598002654978579
27,"    def __init__(
        self,
        conversion: Any,
        name: str = ""undefined"",
        cartesian_cs: Any = Cartesian2DCS(),
        geodetic_crs: Any = GeographicCRS(),
    ) -> None:
        """"""
        Parameters
        ----------
        conversion: Any
            Anything accepted by :meth:`pyproj.crs.CoordinateSystem.from_user_input`
            or a conversion from :ref:`coordinate_operation`.
        name: str, optional
            The name of the Projected CRS. Default is undefined.
        cartesian_cs: Any, optional
            Input to create a Cartesian Coordinate System.
            Anything accepted by :meth:`pyproj.crs.CoordinateSystem.from_user_input`
            or :class:`pyproj.crs.coordinate_system.Cartesian2DCS`.
        geodetic_crs: Any, optional
            Input to create the Geodetic CRS, a :class:`GeographicCRS` or
            anything accepted by :meth:`pyproj.crs.CRS.from_user_input`.
        """"""
        proj_crs_json = {
            ""$schema"": ""https://proj.org/schemas/v0.2/projjson.schema.json"",
            ""type"": ""ProjectedCRS"",
            ""name"": name,
            ""base_crs"": CRS.from_user_input(geodetic_crs).to_json_dict(),
            ""conversion"": CoordinateOperation.from_user_input(
                conversion
            ).to_json_dict(),
            ""coordinate_system"": CoordinateSystem.from_user_input(
                cartesian_cs
            ).to_json_dict(),
        }
        super().__init__(proj_crs_json)",0.8084491917792769,0.7502374235326356,0.8641009033855815,0.8853671795757128
28,"    def __init__(
        self,
        name: str,
        datum: Any,
        vertical_cs: Any = VerticalCS(),
        geoid_model: str = None,
    ) -> None:
        """"""
        Parameters
        ----------
        name: str
            The name of the Vertical CRS (e.g. NAVD88 height).
        datum: Any
            Anything accepted by :meth:`pyproj.crs.Datum.from_user_input`
        vertical_cs: Any, optional
            Input to create a Vertical Coordinate System accepted by
            :meth:`pyproj.crs.CoordinateSystem.from_user_input`
            or :class:`pyproj.crs.coordinate_system.VerticalCS`
        geoid_model: str, optional
            The name of the GEOID Model (e.g. GEOID12B).
        """"""
        vert_crs_json = {
            ""$schema"": ""https://proj.org/schemas/v0.2/projjson.schema.json"",
            ""type"": ""VerticalCRS"",
            ""name"": name,
            ""datum"": Datum.from_user_input(datum).to_json_dict(),
            ""coordinate_system"": CoordinateSystem.from_user_input(
                vertical_cs
            ).to_json_dict(),
        }
        if geoid_model is not None:
            vert_crs_json[""geoid_model""] = {""name"": geoid_model}

        super().__init__(vert_crs_json)",0.8072549963497817,0.7275633821570824,0.6356155320148646,0.9839651557872734
29,"def set_data_dir(proj_data_dir):
    """"""
    Set the data directory for PROJ to use.

    Parameters
    ----------
    proj_data_dir: str
        The path to rhe PROJ data directory.
    """"""
    global _USER_PROJ_DATA
    _USER_PROJ_DATA = proj_data_dir
    # reset search paths
    from pyproj._datadir import PYPROJ_CONTEXT

    PYPROJ_CONTEXT.set_search_paths()",0.8489514116937501,0.8704613587646972,0.8259692357406481,0.8127246003969075
30,"def set_data_dir(proj_data_dir):
    """"""
    Set the data directory for PROJ to use.

    Parameters
    ----------
    proj_data_dir: str
        The path to rhe PROJ data directory.
    """"""
    global _USER_PROJ_DATA
    global _VALIDATED_PROJ_DATA
    _USER_PROJ_DATA = proj_data_dir
    # set to none to re-validate
    _VALIDATED_PROJ_DATA = None",0.844085765943255,0.824894234217063,0.7400919840051536,0.3816202148891068
31,"def get_data_dir():
    """"""
    The order of preference for the data directory is:

    1. The one set by pyproj.datadir.set_data_dir (if exists & valid)
    2. The internal proj directory (if exists & valid)
    3. The directory in PROJ_LIB (if exists & valid)
    4. The directory on the PATH (if exists & valid)

    Returns
    -------
    str: The valid data directory.

    """"""
    # to avoid re-validating
    global _VALIDATED_PROJ_DATA
    if _VALIDATED_PROJ_DATA is not None:
        return _VALIDATED_PROJ_DATA

    global _USER_PROJ_DATA
    internal_datadir = os.path.join(
        os.path.dirname(os.path.abspath(__file__)), ""proj_dir"", ""share"", ""proj""
    )
    proj_lib_dirs = os.environ.get(""PROJ_LIB"", """")

    def valid_data_dir(potential_data_dir):
        if potential_data_dir is not None and os.path.exists(
            os.path.join(potential_data_dir, ""proj.db"")
        ):
            return True
        return False

    def valid_data_dirs(potential_data_dirs):
        if potential_data_dirs is None:
            return False
        for proj_data_dir in potential_data_dirs.split(os.pathsep):
            if valid_data_dir(proj_data_dir):
                return True
                break
        return None

    if valid_data_dirs(_USER_PROJ_DATA):
        _VALIDATED_PROJ_DATA = _USER_PROJ_DATA
    elif valid_data_dir(internal_datadir):
        _VALIDATED_PROJ_DATA = internal_datadir
    elif valid_data_dirs(proj_lib_dirs):
        _VALIDATED_PROJ_DATA = proj_lib_dirs
    else:
        proj_exe = find_executable(""proj"")
        if proj_exe is not None:
            system_proj_dir = os.path.join(
                os.path.dirname(os.path.dirname(proj_exe)), ""share"", ""proj""
            )
            if valid_data_dir(system_proj_dir):
                _VALIDATED_PROJ_DATA = system_proj_dir

    if _VALIDATED_PROJ_DATA is None:
        raise DataDirError(
            ""Valid PROJ data directory not found. ""
            ""Either set the path using the environmental variable PROJ_LIB or ""
            ""with `pyproj.datadir.set_data_dir`.""
        )
    return _VALIDATED_PROJ_DATA",0.9725457934024008,0.9627176670938032,0.9529384261521988,0.9529384261521988
32,"    def from_proj(proj_from, proj_to, skip_equivalent=False, always_xy=False):
        """"""Make a Transformer from a :obj:`~pyproj.proj.Proj` or input used to create one.

        Parameters
        ----------
        proj_from: :obj:`~pyproj.proj.Proj` or input used to create one
            Projection of input data.
        proj_to: :obj:`~pyproj.proj.Proj` or input used to create one
            Projection of output data.
        skip_equivalent: bool, optional
            If true, will skip the transformation operation if input and output 
            projections are equivalent. Default is false.
        always_xy: bool, optional
            If true, the transform method will accept as input and return as output
            coordinates using the traditional GIS order, that is longitude, latitude
            for geographic CRS and easting, northing for most projected CRS.
            Default is false.

        Returns
        -------
        :obj:`~Transformer`

        """"""
        if not isinstance(proj_from, Proj):
            proj_from = Proj(proj_from)
        if not isinstance(proj_to, Proj):
            proj_to = Proj(proj_to)

        transformer = Transformer()
        transformer._transformer = _Transformer.from_crs(
            proj_from.crs,
            proj_to.crs,
            skip_equivalent=skip_equivalent,
            always_xy=always_xy,
        )
        return transformer",0.6670097003109317,0.5127965165912155,0.6288351846655068,0.7222459181850637
33,"    def from_crs(crs_from, crs_to, skip_equivalent=False, always_xy=False):
        """"""Make a Transformer from a :obj:`~pyproj.crs.CRS` or input used to create one.

        Parameters
        ----------
        crs_from: ~pyproj.crs.CRS or input used to create one
            Projection of input data.
        crs_to: ~pyproj.crs.CRS or input used to create one
            Projection of output data.
        skip_equivalent: bool, optional
            If true, will skip the transformation operation if input and output
            projections are equivalent. Default is false.
        always_xy: bool, optional
            If true, the transform method will accept as input and return as output
            coordinates using the traditional GIS order, that is longitude, latitude
            for geographic CRS and easting, northing for most projected CRS.
            Default is false.

        Returns
        -------
        :obj:`~Transformer`

        """"""
        transformer = Transformer()
        transformer._transformer = _Transformer.from_crs(
            CRS.from_user_input(crs_from),
            CRS.from_user_input(crs_to),
            skip_equivalent=skip_equivalent,
            always_xy=always_xy,
        )
        return transformer",0.9823262120648144,0.7365910685712891,0.7964739948349266,0.7745869169953301
34,"    def from_pipeline(proj_pipeline):
        """"""Make a Transformer from a PROJ pipeline string.

        https://proj4.org/operations/pipeline.html

        Parameters
        ----------
        proj_pipeline: str
            Projection pipeline string.

        Returns
        -------
        ~Transformer

        """"""
        transformer = Transformer()
        transformer._transformer = _Transformer.from_pipeline(cstrencode(proj_pipeline))
        return transformer",0.7375948837138722,0.8787173877759473,0.7572822545409782,0.7194147086227287
35,"def _dict2string(projparams):
    # convert a dict to a proj4 string.
    pjargs = []
    for key, value in projparams.items():
        # the towgs84 as list
        if isinstance(value, (list, tuple)):
            value = "","".join([str(val) for val in value])
        # issue 183 (+ no_rot)
        if value is None or value is True:
            pjargs.append(""+"" + key + "" "")
        elif value is False:
            pass
        else:
            pjargs.append(""+"" + key + ""="" + str(value) + "" "")
    return """".join(pjargs)",0.7617546814317384,0.5113100112622013,0.7129004387790016,0.7517451275654017
36,"    def __init__(self, projparams=None, preserve_units=True, **kwargs):
        """"""
        initialize a Proj class instance.

        See the proj documentation (https://github.com/OSGeo/proj.4/wiki)
        for more information about projection parameters.

        Parameters
        ----------
        projparams: int, str, dict, pyproj.CRS
            A proj.4 or WKT string, proj.4 dict, EPSG integer, or a pyproj.CRS instnace.
        preserve_units: bool
            If false, will ensure +units=m.
        **kwargs:
            proj.4 projection parameters.


        Example usage:

        >>> from pyproj import Proj
        >>> p = Proj(proj='utm',zone=10,ellps='WGS84', preserve_units=False) # use kwargs
        >>> x,y = p(-120.108, 34.36116666)
        >>> 'x=%9.3f y=%11.3f' % (x,y)
        'x=765975.641 y=3805993.134'
        >>> 'lon=%8.3f lat=%5.3f' % p(x,y,inverse=True)
        'lon=-120.108 lat=34.361'
        >>> # do 3 cities at a time in a tuple (Fresno, LA, SF)
        >>> lons = (-119.72,-118.40,-122.38)
        >>> lats = (36.77, 33.93, 37.62 )
        >>> x,y = p(lons, lats)
        >>> 'x: %9.3f %9.3f %9.3f' % x
        'x: 792763.863 925321.537 554714.301'
        >>> 'y: %9.3f %9.3f %9.3f' % y
        'y: 4074377.617 3763936.941 4163835.303'
        >>> lons, lats = p(x, y, inverse=True) # inverse transform
        >>> 'lons: %8.3f %8.3f %8.3f' % lons
        'lons: -119.720 -118.400 -122.380'
        >>> 'lats: %8.3f %8.3f %8.3f' % lats
        'lats:   36.770   33.930   37.620'
        >>> p2 = Proj('+proj=utm +zone=10 +ellps=WGS84', preserve_units=False) # use proj4 string
        >>> x,y = p2(-120.108, 34.36116666)
        >>> 'x=%9.3f y=%11.3f' % (x,y)
        'x=765975.641 y=3805993.134'
        >>> p = Proj(init=""epsg:32667"", preserve_units=False)
        >>> 'x=%12.3f y=%12.3f (meters)' % p(-114.057222, 51.045)
        'x=-1783506.250 y= 6193827.033 (meters)'
        >>> p = Proj(""+init=epsg:32667"")
        >>> 'x=%12.3f y=%12.3f (feet)' % p(-114.057222, 51.045)
        'x=-5851386.754 y=20320914.191 (feet)'
        >>> # test data with radian inputs
        >>> p1 = Proj(init=""epsg:4214"")
        >>> x1, y1 = p1(116.366, 39.867)
        >>> '{:.3f} {:.3f}'.format(x1, y1)
        '2.031 0.696'
        >>> x2, y2 = p1(x1, y1, inverse=True)
        >>> '{:.3f} {:.3f}'.format(x2, y2)
        '116.366 39.867'
        """"""
        self.crs = CRS.from_user_input(projparams if projparams is not None else kwargs)
        # make sure units are meters if preserve_units is False.
        if not preserve_units and ""foot"" in self.crs.axis_info[0].unit_name:
            projstring = self.crs.to_proj4(4)
            projstring = re.sub(r""\\s\\+units=[\\w-]+"", """", projstring)
            projstring += "" +units=m""
            self.crs = CRS(projstring)
        super(Proj, self).__init__(
            cstrencode(self.crs.to_proj4().replace(""+type=crs"", """").strip())
        )",0.3061854273342831,0.1905558366356405,0.1905558366356405,0.1766032385658607
37,"def Kuf_conv_patch(feat, kern, Xnew):
    Xp = kern.get_patches(Xnew)  # [N, num_patches, patch_len]
    bigKzx = kern.base_kernel.K(feat.Z, Xp)  # [M, N, P] -- thanks to broadcasting of kernels
    Kzx = tf.reduce_sum(bigKzx * kern.weights if hasattr(kern, ""weights"") else bigKzx, [2])
    return Kzx / kern.num_patches",0.7512870778449472,0.6921886030526243,0.938552086374722,0.7693406784563797
38,"def Kuu_kernel_inducingpoints(inducing_variable: InducingPoints, kernel: Kernel, *, jitter=0.0):
    Kzz = kernel(inducing_variable.Z)
    Kzz += jitter * tf.eye(len(inducing_variable), dtype=Kzz.dtype)
    return Kzz",0.80528341206673,0.7685174322540038,0.8734585513618947,0.7312621201561522
39,"def Kuu_sqexp_multiscale(inducing_variable: Multiscale, kernel: SquaredExponential, *, jitter=0.0):
    Zmu, Zlen = kernel.slice(inducing_variable.Z, inducing_variable.scales)
    idlengthscales2 = tf.square(kernel.lengthscales + Zlen)
    sc = tf.sqrt(
        idlengthscales2[None, ...] + idlengthscales2[:, None, ...] - kernel.lengthscales ** 2
    )
    d = inducing_variable._cust_square_dist(Zmu, Zmu, sc)
    Kzz = kernel.variance * tf.exp(-d / 2) * tf.reduce_prod(kernel.lengthscales / sc, 2)
    Kzz += jitter * tf.eye(len(inducing_variable), dtype=Kzz.dtype)
    return Kzz",0.7997942652223178,0.8078921093241087,0.4,0.639181150886266
40,"def Kuu_conv_patch(feat, kern, jitter=0.0):
    return kern.base_kernel.K(feat.Z) + jitter * tf.eye(len(feat), dtype=default_float())",0.7474644970933795,0.8199203361375197,0.4,0.8759187313755759
41,"def _Kuu(
    inducing_variable: FallbackSeparateIndependentInducingVariables,
    kernel: Union[SeparateIndependent, LinearCoregionalization],
    *,
    jitter=0.0,
):
    Kmms = [Kuu(f, k) for f, k in zip(inducing_variable.inducing_variable_list, kernel.kernels)]
    Kmm = tf.stack(Kmms, axis=0)  # [L, M, M]
    jittermat = tf.eye(len(inducing_variable), dtype=Kmm.dtype)[None, :, :] * jitter
    return Kmm + jittermat",0.6896429499061594,0.8130275962474349,0.4,0.9165070158183604
42,"    def __init__(self, Z: TensorData, name: Optional[str] = None):
        """"""
        :param Z: the initial positions of the inducing points, size [M, D]
        """"""
        super().__init__(name=name)
        self.Z = Parameter(Z, dtype=default_float())",0.8162649756980955,0.801451735372606,0.9299347148469902,0.8331510852609685
43,"    def __len__(self) -> int:
        return self.Z.shape[0]",0.8638543467584857,0.8708511148930844,0.8726538792483141,0.8768354383759972
44,"    def __len__(self) -> int:
        return len(self.inducing_variable)",0.8094248142293723,0.6207284442432303,0.8623098683842313,0.7926807607725496
45,"    def __len__(self) -> int:
        return len(self.inducing_variable_list[0])",0.6089261839339075,0.5837192746834787,0.7867723367857719,0.589418520885776
46,"    def __init__(
        self,
        distribution_class: Type[tfp.distributions.Distribution] = tfp.distributions.Normal,
        scale_transform: tfp.bijectors.Bijector = positive(base=""exp""),
        **kwargs,
    ):
        """"""
        :param distribution_class: distribution class parameterized by `loc` and `scale`
            as first and second argument, respectively.
        :param scale_transform: callable/bijector applied to the latent
            function modelling the scale to ensure its positivity.
            Typically, `tf.exp` or `tf.softplus`, but can be any function f: R -> R^+.
        """"""

        def conditional_distribution(Fs) -> tfp.distributions.Distribution:
            tf.debugging.assert_equal(tf.shape(Fs)[-1], 2)
            loc = Fs[..., :1]
            scale = scale_transform(Fs[..., 1:])
            return distribution_class(loc, scale)

        super().__init__(
            latent_dim=2, conditional_distribution=conditional_distribution, **kwargs,
        )",0.8427701463097438,0.9908483757831038,0.8846920197064847,0.9908483757831038
47,"        def conditional_distribution(Fs) -> tfp.distributions.Distribution:
            tf.debugging.assert_equal(tf.shape(Fs)[-1], 2)
            loc = Fs[..., :1]
            scale = scale_transform(Fs[..., 1:])
            return distribution_class(loc, scale)",0.6199407062855333,0.8974808343438507,0.9751880714210752,0.9751880714210752
48,"    def elbo(self) -> tf.Tensor:
        """"""
        Construct a tensorflow function to compute the bound on the marginal
        likelihood.
        """"""
        Y_data = self.data

        pX = DiagonalGaussian(self.X_data_mean, self.X_data_var)

        num_inducing = len(self.inducing_variable)
        psi0 = tf.reduce_sum(expectation(pX, self.kernel))
        psi1 = expectation(pX, (self.kernel, self.inducing_variable))
        psi2 = tf.reduce_sum(
            expectation(
                pX, (self.kernel, self.inducing_variable), (self.kernel, self.inducing_variable)
            ),
            axis=0,
        )
        cov_uu = covariances.Kuu(self.inducing_variable, self.kernel, jitter=default_jitter())
        L = tf.linalg.cholesky(cov_uu)
        sigma2 = self.likelihood.variance
        sigma = tf.sqrt(sigma2)

        # Compute intermediate matrices
        A = tf.linalg.triangular_solve(L, tf.transpose(psi1), lower=True) / sigma
        tmp = tf.linalg.triangular_solve(L, psi2, lower=True)
        AAT = tf.linalg.triangular_solve(L, tf.transpose(tmp), lower=True) / sigma2
        B = AAT + tf.eye(num_inducing, dtype=default_float())
        LB = tf.linalg.cholesky(B)
        log_det_B = 2.0 * tf.reduce_sum(tf.math.log(tf.linalg.diag_part(LB)))
        c = tf.linalg.triangular_solve(LB, tf.linalg.matmul(A, Y_data), lower=True) / sigma

        # KL[q(x) || p(x)]
        dX_data_var = (
            self.X_data_var
            if self.X_data_var.shape.ndims == 2
            else tf.linalg.diag_part(self.X_data_var)
        )
        NQ = to_default_float(tf.size(self.X_data_mean))
        D = to_default_float(tf.shape(Y_data)[1])
        KL = -0.5 * tf.reduce_sum(tf.math.log(dX_data_var))
        KL += 0.5 * tf.reduce_sum(tf.math.log(self.X_prior_var))
        KL -= 0.5 * NQ
        KL += 0.5 * tf.reduce_sum(
            (tf.square(self.X_data_mean - self.X_prior_mean) + dX_data_var) / self.X_prior_var
        )

        # compute log marginal bound
        ND = to_default_float(tf.size(Y_data))
        bound = -0.5 * ND * tf.math.log(2 * np.pi * sigma2)
        bound += -0.5 * D * log_det_B
        bound += -0.5 * tf.reduce_sum(tf.square(Y_data)) / sigma2
        bound += 0.5 * tf.reduce_sum(tf.square(c))
        bound += -0.5 * D * (tf.reduce_sum(psi0) / sigma2 - tf.reduce_sum(tf.linalg.diag_part(AAT)))
        bound -= KL
        return bound",0.988288768352608,0.8864172113837154,0.9976257755419016,0.9976257755419016
49,"    def predict_f(
        self, Xnew: InputData, full_cov: bool = False, full_output_cov: bool = False
    ) -> MeanAndVariance:
        """"""
        Compute the mean and variance of the latent function at some new points.
        Note that this is very similar to the SGPR prediction, for which
        there are notes in the SGPR notebook.

        Note: This model does not allow full output covariances.

        :param Xnew: points at which to predict
        """"""
        if full_output_cov:
            raise NotImplementedError

        pX = DiagonalGaussian(self.X_data_mean, self.X_data_var)

        Y_data = self.data
        num_inducing = len(self.inducing_variable)
        psi1 = expectation(pX, (self.kernel, self.inducing_variable))
        psi2 = tf.reduce_sum(
            expectation(
                pX, (self.kernel, self.inducing_variable), (self.kernel, self.inducing_variable)
            ),
            axis=0,
        )
        jitter = default_jitter()
        Kus = covariances.Kuf(self.inducing_variable, self.kernel, Xnew)
        sigma2 = self.likelihood.variance
        sigma = tf.sqrt(sigma2)
        L = tf.linalg.cholesky(covariances.Kuu(self.inducing_variable, self.kernel, jitter=jitter))

        A = tf.linalg.triangular_solve(L, tf.transpose(psi1), lower=True) / sigma
        tmp = tf.linalg.triangular_solve(L, psi2, lower=True)
        AAT = tf.linalg.triangular_solve(L, tf.transpose(tmp), lower=True) / sigma2
        B = AAT + tf.eye(num_inducing, dtype=default_float())
        LB = tf.linalg.cholesky(B)
        c = tf.linalg.triangular_solve(LB, tf.linalg.matmul(A, Y_data), lower=True) / sigma
        tmp1 = tf.linalg.triangular_solve(L, Kus, lower=True)
        tmp2 = tf.linalg.triangular_solve(LB, tmp1, lower=True)
        mean = tf.linalg.matmul(tmp2, c, transpose_a=True)
        if full_cov:
            var = (
                self.kernel(Xnew)
                + tf.linalg.matmul(tmp2, tmp2, transpose_a=True)
                - tf.linalg.matmul(tmp1, tmp1, transpose_a=True)
            )
            shape = tf.stack([1, 1, tf.shape(Y_data)[1]])
            var = tf.tile(tf.expand_dims(var, 2), shape)
        else:
            var = (
                self.kernel(Xnew, full_cov=False)
                + tf.reduce_sum(tf.square(tmp2), axis=0)
                - tf.reduce_sum(tf.square(tmp1), axis=0)
            )
            shape = tf.stack([1, tf.shape(Y_data)[1]])
            var = tf.tile(tf.expand_dims(var, 1), shape)
        return mean + self.mean_function(Xnew), var",0.890197693443075,0.9919213225543984,0.8564894965398475,0.8675660293004643
50,"    def __init__(
        self,
        data: RegressionData,
        kernel: Kernel,
        likelihood: Likelihood,
        mean_function: Optional[MeanFunction] = None,
        num_latent_gps: Optional[int] = None,
        inducing_variable: Optional[InducingPoints] = None,
    ):
        """"""
        data is a tuple of X, Y with X, a data matrix, size [N, D] and Y, a data matrix, size [N, R]
        Z is a data matrix, of inducing inputs, size [M, D]
        kernel, likelihood, mean_function are appropriate GPflow objects
        """"""
        if num_latent_gps is None:
            num_latent_gps = self.calc_num_latent_gps_from_data(data, kernel, likelihood)
        super().__init__(kernel, likelihood, mean_function, num_latent_gps=num_latent_gps)
        self.data = data_input_to_tensor(data)
        self.num_data = data[0].shape[0]
        self.inducing_variable = inducingpoint_wrapper(inducing_variable)
        self.V = Parameter(np.zeros((len(self.inducing_variable), self.num_latent_gps)))
        self.V.prior = tfp.distributions.Normal(
            loc=to_default_float(0.0), scale=to_default_float(1.0)
        )",0.8495687360652541,0.8473209105485922,0.9193394430344178,0.9193394430344178
51,"    def upper_bound(self) -> tf.Tensor:
        """"""
        Upper bound for the sparse GP regression marginal likelihood.  Note that
        the same inducing points are used for calculating the upper bound, as are
        used for computing the likelihood approximation. This may not lead to the
        best upper bound. The upper bound can be tightened by optimising Z, just
        like the lower bound. This is especially important in FITC, as FITC is
        known to produce poor inducing point locations. An optimisable upper bound
        can be found in https://github.com/markvdw/gp_upper.

        The key reference is

        ::

          @misc{titsias_2014,
            title={Variational Inference for Gaussian and Determinantal Point Processes},
            url={http://www2.aueb.gr/users/mtitsias/papers/titsiasNipsVar14.pdf},
            publisher={Workshop on Advances in Variational Inference (NIPS 2014)},
            author={Titsias, Michalis K.},
            year={2014},
            month={Dec}
          }

        The key quantity, the trace term, can be computed via

        >>> _, v = conditionals.conditional(X, model.inducing_variable.Z, model.kernel,
        ...                                 np.zeros((len(model.inducing_variable), 1)))

        which computes each individual element of the trace term.
        """"""
        X_data, Y_data = self.data
        num_data = to_default_float(tf.shape(Y_data)[0])

        Kdiag = self.kernel(X_data, full_cov=False)
        kuu = Kuu(self.inducing_variable, self.kernel, jitter=default_jitter())
        kuf = Kuf(self.inducing_variable, self.kernel, X_data)

        I = tf.eye(tf.shape(kuu)[0], dtype=default_float())

        L = tf.linalg.cholesky(kuu)
        A = tf.linalg.triangular_solve(L, kuf, lower=True)
        AAT = tf.linalg.matmul(A, A, transpose_b=True)
        B = I + AAT / self.likelihood.variance
        LB = tf.linalg.cholesky(B)

        # Using the Trace bound, from Titsias' presentation
        c = tf.reduce_sum(Kdiag) - tf.reduce_sum(tf.square(A))

        # Alternative bound on max eigenval:
        corrected_noise = self.likelihood.variance + c

        const = -0.5 * num_data * tf.math.log(2 * np.pi * self.likelihood.variance)
        logdet = -tf.reduce_sum(tf.math.log(tf.linalg.diag_part(LB)))

        err = Y_data - self.mean_function(X_data)
        LC = tf.linalg.cholesky(I + AAT / corrected_noise)
        v = tf.linalg.triangular_solve(LC, tf.linalg.matmul(A, err) / corrected_noise, lower=True)
        quad = -0.5 * tf.reduce_sum(tf.square(err)) / corrected_noise + 0.5 * tf.reduce_sum(
            tf.square(v)
        )

        return const + logdet + quad",0.9502238061737456,0.832449326234151,0.8262538700900892,0.8787073640164456
52,"    def elbo(self) -> tf.Tensor:
        """"""
        Construct a tensorflow function to compute the bound on the marginal
        likelihood. For a derivation of the terms in here, see the associated
        SGPR notebook.
        """"""
        X_data, Y_data = self.data

        num_inducing = len(self.inducing_variable)
        num_data = to_default_float(tf.shape(Y_data)[0])
        output_dim = to_default_float(tf.shape(Y_data)[1])

        err = Y_data - self.mean_function(X_data)
        Kdiag = self.kernel(X_data, full_cov=False)
        kuf = Kuf(self.inducing_variable, self.kernel, X_data)
        kuu = Kuu(self.inducing_variable, self.kernel, jitter=default_jitter())
        L = tf.linalg.cholesky(kuu)
        sigma = tf.sqrt(self.likelihood.variance)

        # Compute intermediate matrices
        A = tf.linalg.triangular_solve(L, kuf, lower=True) / sigma
        AAT = tf.linalg.matmul(A, A, transpose_b=True)
        B = AAT + tf.eye(num_inducing, dtype=default_float())
        LB = tf.linalg.cholesky(B)
        Aerr = tf.linalg.matmul(A, err)
        c = tf.linalg.triangular_solve(LB, Aerr, lower=True) / sigma

        # compute log marginal bound
        bound = -0.5 * num_data * output_dim * np.log(2 * np.pi)
        bound += tf.negative(output_dim) * tf.reduce_sum(tf.math.log(tf.linalg.diag_part(LB)))
        bound -= 0.5 * num_data * output_dim * tf.math.log(self.likelihood.variance)
        bound += -0.5 * tf.reduce_sum(tf.square(err)) / self.likelihood.variance
        bound += 0.5 * tf.reduce_sum(tf.square(c))
        bound += -0.5 * output_dim * tf.reduce_sum(Kdiag) / self.likelihood.variance
        bound += 0.5 * output_dim * tf.reduce_sum(tf.linalg.diag_part(AAT))

        return bound",0.9635595508277628,0.809166164526897,0.8428656437214099,0.8126564465998928
53,"    def predict_f(self, Xnew: InputData, full_cov=False, full_output_cov=False) -> MeanAndVariance:
        """"""
        Compute the mean and variance of the latent function at some new points
        Xnew. For a derivation of the terms in here, see the associated SGPR
        notebook.
        """"""
        X_data, Y_data = self.data
        num_inducing = len(self.inducing_variable)
        err = Y_data - self.mean_function(X_data)
        kuf = Kuf(self.inducing_variable, self.kernel, X_data)
        kuu = Kuu(self.inducing_variable, self.kernel, jitter=default_jitter())
        Kus = Kuf(self.inducing_variable, self.kernel, Xnew)
        sigma = tf.sqrt(self.likelihood.variance)
        L = tf.linalg.cholesky(kuu)
        A = tf.linalg.triangular_solve(L, kuf, lower=True) / sigma
        B = tf.linalg.matmul(A, A, transpose_b=True) + tf.eye(num_inducing, dtype=default_float())
        LB = tf.linalg.cholesky(B)
        Aerr = tf.linalg.matmul(A, err)
        c = tf.linalg.triangular_solve(LB, Aerr, lower=True) / sigma
        tmp1 = tf.linalg.triangular_solve(L, Kus, lower=True)
        tmp2 = tf.linalg.triangular_solve(LB, tmp1, lower=True)
        mean = tf.linalg.matmul(tmp2, c, transpose_a=True)
        if full_cov:
            var = (
                self.kernel(Xnew)
                + tf.linalg.matmul(tmp2, tmp2, transpose_a=True)
                - tf.linalg.matmul(tmp1, tmp1, transpose_a=True)
            )
            var = tf.tile(var[None, ...], [self.num_latent_gps, 1, 1])  # [P, N, N]
        else:
            var = (
                self.kernel(Xnew, full_cov=False)
                + tf.reduce_sum(tf.square(tmp2), 0)
                - tf.reduce_sum(tf.square(tmp1), 0)
            )
            var = tf.tile(var[:, None], [1, self.num_latent_gps])
        return mean + self.mean_function(Xnew), var",0.9966451005838244,0.9084089178174706,0.8579045073444757,0.9966451005838244
54,"    def common_terms(self):
        X_data, Y_data = self.data
        num_inducing = len(self.inducing_variable)
        err = Y_data - self.mean_function(X_data)  # size [N, R]
        Kdiag = self.kernel(X_data, full_cov=False)
        kuf = Kuf(self.inducing_variable, self.kernel, X_data)
        kuu = Kuu(self.inducing_variable, self.kernel, jitter=default_jitter())

        Luu = tf.linalg.cholesky(kuu)  # => Luu Luu^T = kuu
        V = tf.linalg.triangular_solve(Luu, kuf)  # => V^T V = Qff = kuf^T kuu^-1 kuf

        diagQff = tf.reduce_sum(tf.square(V), 0)
        nu = Kdiag - diagQff + self.likelihood.variance

        B = tf.eye(num_inducing, dtype=default_float()) + tf.linalg.matmul(
            V / nu, V, transpose_b=True
        )
        L = tf.linalg.cholesky(B)
        beta = err / tf.expand_dims(nu, 1)  # size [N, R]
        alpha = tf.linalg.matmul(V, beta)  # size [N, R]

        gamma = tf.linalg.triangular_solve(L, alpha, lower=True)  # size [N, R]

        return err, nu, Luu, L, alpha, beta, gamma",0.95744052798318,0.9872023056350488,0.9850681550283245,0.9858678160421762
55,"    def __init__(
        self,
        kernel,
        likelihood,
        inducing_variable,
        *,
        mean_function=None,
        num_latent_gps: int = 1,
        q_diag: bool = False,
        q_mu=None,
        q_sqrt=None,
        whiten: bool = True,
        num_data=None,
    ):
        """"""
        - kernel, likelihood, inducing_variables, mean_function are appropriate
          GPflow objects
        - num_latent_gps is the number of latent processes to use, defaults to 1
        - q_diag is a boolean. If True, the covariance is approximated by a
          diagonal matrix.
        - whiten is a boolean. If True, we use the whitened representation of
          the inducing points.
        - num_data is the total number of observations, defaults to X.shape[0]
          (relevant when feeding in external minibatches)
        """"""
        # init the super class, accept args
        super().__init__(kernel, likelihood, mean_function, num_latent_gps)
        self.num_data = num_data
        self.q_diag = q_diag
        self.whiten = whiten
        self.inducing_variable = inducingpoint_wrapper(inducing_variable)

        # init variational parameters
        num_inducing = len(self.inducing_variable)
        self._init_variational_parameters(num_inducing, q_mu, q_sqrt, q_diag)",0.9361826280406222,0.6360085797320061,0.9358707986770162,0.7992602099636912
56,"def ndiagquad(funcs, H: int, Fmu, Fvar, logspace: bool = False, **Ys):
    """"""
    Computes N Gaussian expectation integrals of one or more functions
    using Gauss-Hermite quadrature. The Gaussians must be independent.

    The means and variances of the Gaussians are specified by Fmu and Fvar.
    The N-integrals are assumed to be taken wrt the last dimensions of Fmu, Fvar.

    :param funcs: the integrand(s):
        Callable or Iterable of Callables that operates elementwise
    :param H: number of Gauss-Hermite quadrature points
    :param Fmu: array/tensor or `Din`-tuple/list thereof
    :param Fvar: array/tensor or `Din`-tuple/list thereof
    :param logspace: if True, funcs are the log-integrands and this calculates
        the log-expectation of exp(funcs)
    :param **Ys: arrays/tensors; deterministic arguments to be passed by name

    Fmu, Fvar, Ys should all have same shape, with overall size `N`
    :return: shape is the same as that of the first Fmu
    """"""
    n_gh = H
    if isinstance(Fmu, (tuple, list)):
        dim = len(Fmu)
        shape = tf.shape(Fmu[0])
        Fmu = tf.stack(Fmu, axis=-1)
        Fvar = tf.stack(Fvar, axis=-1)
    else:
        dim = 1
        shape = tf.shape(Fmu)

    Fmu = tf.reshape(Fmu, (-1, dim))
    Fvar = tf.reshape(Fvar, (-1, dim))

    Ys = {Yname: tf.reshape(Y, (-1, 1)) for Yname, Y in Ys.items()}

    def wrapper(old_fun):
        def new_fun(X, **Ys):
            Xs = tf.unstack(X, axis=-1)
            fun_eval = old_fun(*Xs, **Ys)
            if tf.rank(fun_eval) < tf.rank(X):
                fun_eval = tf.expand_dims(fun_eval, axis=-1)
            return fun_eval

        return new_fun

    if isinstance(funcs, Iterable):
        funcs = [wrapper(f) for f in funcs]
    else:
        funcs = wrapper(funcs)

    quadrature = NDiagGHQuadrature(dim, n_gh)
    if logspace:
        result = quadrature.logspace(funcs, Fmu, Fvar, **Ys)
    else:
        result = quadrature(funcs, Fmu, Fvar, **Ys)

    if isinstance(result, list):
        result = [tf.reshape(r, shape) for r in result]
    else:
        result = tf.reshape(result, shape)

    return result",0.956407035127131,0.9734162679519872,0.953317427251084,0.9446915312335497
57,"    def wrapper(old_fun):
        def new_fun(X, **Ys):
            Xs = tf.unstack(X, axis=-1)
            fun_eval = old_fun(*Xs, **Ys)
            if tf.rank(fun_eval) < tf.rank(X):
                fun_eval = tf.expand_dims(fun_eval, axis=-1)
            return fun_eval

        return new_fun",0.7089182805651844,0.7453697821037382,0.4,0.7109130082653067
58,"        def new_fun(X, **Ys):
            Xs = tf.unstack(X, axis=-1)
            fun_eval = old_fun(*Xs, **Ys)
            if tf.rank(fun_eval) < tf.rank(X):
                fun_eval = tf.expand_dims(fun_eval, axis=-1)
            return fun_eval",0.6231434396937103,0.6687538545528496,0.4,0.979242531240839
59,"    def __init__(
        self,
        data: OutputData,
        latent_dim: int,
        X_data_mean: Optional[tf.Tensor] = None,
        kernel: Optional[Kernel] = None,
        mean_function: Optional[MeanFunction] = None,
    ):
        """"""
        Initialise GPLVM object. This method only works with a Gaussian likelihood.

        :param data: y data matrix, size N (number of points) x D (dimensions)
        :param latent_dim: the number of latent dimensions (Q)
        :param X_data_mean: latent positions ([N, Q]), for the initialisation of the latent space.
        :param kernel: kernel specification, by default Squared Exponential
        :param mean_function: mean function, by default None.
        """"""
        if X_data_mean is None:
            X_data_mean = pca_reduce(data, latent_dim)

        num_latent_gps = X_data_mean.shape[1]
        if num_latent_gps != latent_dim:
            msg = ""Passed in number of latent {0} does not match initial X {1}.""
            raise ValueError(msg.format(latent_dim, num_latent_gps))

        if mean_function is None:
            mean_function = Zero()

        if kernel is None:
            kernel = kernels.SquaredExponential(lengthscales=tf.ones((latent_dim,)))

        if data.shape[1] < num_latent_gps:
            raise ValueError(""More latent dimensions than observed."")

        gpr_data = (Parameter(X_data_mean), data)
        super().__init__(gpr_data, kernel, mean_function=mean_function)",0.8585192011594964,0.9940798932725512,0.7923323858996634,0.7826752554865094
60,"    def __init__(
        self,
        data: OutputData,
        X_data_mean: tf.Tensor,
        X_data_var: tf.Tensor,
        kernel: Kernel,
        num_inducing_variables: Optional[int] = None,
        inducing_variable=None,
        X_prior_mean=None,
        X_prior_var=None,
    ):
        """"""
        Initialise Bayesian GPLVM object. This method only works with a Gaussian likelihood.

        :param data: data matrix, size N (number of points) x D (dimensions)
        :param X_data_mean: initial latent positions, size N (number of points) x Q (latent dimensions).
        :param X_data_var: variance of latent positions ([N, Q]), for the initialisation of the latent space.
        :param kernel: kernel specification, by default Squared Exponential
        :param num_inducing_variables: number of inducing points, M
        :param inducing_variable: matrix of inducing points, size M (inducing points) x Q (latent dimensions). By default
            random permutation of X_data_mean.
        :param X_prior_mean: prior mean used in KL term of bound. By default 0. Same size as X_data_mean.
        :param X_prior_var: prior variance used in KL term of bound. By default 1.
        """"""
        num_data, num_latent_gps = X_data_mean.shape
        super().__init__(kernel, likelihoods.Gaussian(), num_latent_gps=num_latent_gps)
        self.data = data
        assert X_data_var.ndim == 2

        self.X_data_mean = Parameter(X_data_mean)
        self.X_data_var = Parameter(X_data_var, transform=positive())

        self.num_data = num_data
        self.output_dim = data.shape[-1]

        assert np.all(X_data_mean.shape == X_data_var.shape)
        assert X_data_mean.shape[0] == data.shape[0], ""X mean and Y must be same size.""
        assert X_data_var.shape[0] == data.shape[0], ""X var and Y must be same size.""

        if (inducing_variable is None) == (num_inducing_variables is None):
            raise ValueError(
                ""BayesianGPLVM needs exactly one of `inducing_variable` and `num_inducing_variables`""
            )

        if inducing_variable is None:
            # By default we initialize by subset of initial latent points
            # Note that tf.random.shuffle returns a copy, it does not shuffle in-place
            Z = tf.random.shuffle(X_data_mean)[:num_inducing_variables]
            inducing_variable = InducingPoints(Z)

        self.inducing_variable = inducingpoint_wrapper(inducing_variable)

        assert X_data_mean.shape[1] == self.num_latent_gps

        # deal with parameters for the prior mean variance of X
        if X_prior_mean is None:
            X_prior_mean = tf.zeros((self.num_data, self.num_latent_gps), dtype=default_float())
        if X_prior_var is None:
            X_prior_var = tf.ones((self.num_data, self.num_latent_gps))

        self.X_prior_mean = tf.convert_to_tensor(np.atleast_1d(X_prior_mean), dtype=default_float())
        self.X_prior_var = tf.convert_to_tensor(np.atleast_1d(X_prior_var), dtype=default_float())

        assert self.X_prior_mean.shape[0] == self.num_data
        assert self.X_prior_mean.shape[1] == self.num_latent_gps
        assert self.X_prior_var.shape[0] == self.num_data
        assert self.X_prior_var.shape[1] == self.num_latent_gps",0.9334870884510982,0.8534085130084166,0.980057694647414,0.9890508129129462
61,"    def __init__(
        self,
        data: RegressionData,
        kernel: Kernel,
        mean_function: Optional[MeanFunction] = None,
        noise_variance: float = 1.0,
    ):
        likelihood = gpflow.likelihoods.Gaussian(noise_variance)
        _, Y_data = data
        super().__init__(kernel, likelihood, mean_function, num_latent_gps=Y_data.shape[-1])
        self.data = data",0.8507221223980284,0.9103631928086128,0.9826226704938622,0.7174790641770727
62,"    def __init__(
        self,
        data: RegressionData,
        kernel: Kernel,
        likelihood: Likelihood,
        mean_function: Optional[MeanFunction] = None,
        num_latent_gps: Optional[int] = None,
        inducing_variable: Optional[InducingPoints] = None,
    ):
        """"""
        data is a tuple of X, Y with X, a data matrix, size [N, D] and Y, a data matrix, size [N, R]
        Z is a data matrix, of inducing inputs, size [M, D]
        kernel, likelihood, mean_function are appropriate GPflow objects
        """"""
        if num_latent_gps is None:
            num_latent_gps = self.calc_num_latent_gps_from_data(data, kernel, likelihood)
        super().__init__(kernel, likelihood, mean_function, num_latent_gps=num_latent_gps)
        self.data = data
        self.num_data = data[0].shape[0]
        self.inducing_variable = inducingpoint_wrapper(inducing_variable)
        self.V = Parameter(np.zeros((len(self.inducing_variable), self.num_latent_gps)))
        self.V.prior = tfp.distributions.Normal(
            loc=to_default_float(0.0), scale=to_default_float(1.0)
        )",0.922796596391548,0.8794789861588332,0.8942556552782456,0.8937559685253423
63,"    def __init__(
        self,
        data: RegressionData,
        kernel: Kernel,
        inducing_variable: InducingPoints,
        *,
        mean_function: Optional[MeanFunction] = None,
        num_latent_gps: Optional[int] = None,
        noise_variance: float = 1.0,
    ):
        """"""
        `data`:  a tuple of (X, Y), where the inputs X has shape [N, D]
            and the outputs Y has shape [N, R].
        `inducing_variable`:  an InducingPoints instance or a matrix of
            the pseudo inputs Z, of shape [M, D].
        `kernel`, `mean_function` are appropriate GPflow objects

        This method only works with a Gaussian likelihood, its variance is
        initialized to `noise_variance`.
        """"""
        likelihood = likelihoods.Gaussian(noise_variance)
        X_data, Y_data = data
        num_latent_gps = Y_data.shape[-1] if num_latent_gps is None else num_latent_gps
        super().__init__(kernel, likelihood, mean_function, num_latent_gps=num_latent_gps)

        self.data = data
        self.num_data = X_data.shape[0]

        self.inducing_variable = inducingpoint_wrapper(inducing_variable)",0.8283991306124476,0.8164762388827214,0.9355907345005772,0.728321221853818
64,"    def __init__(
        self,
        data: RegressionData,
        kernel: Kernel,
        likelihood: Likelihood,
        mean_function: Optional[MeanFunction] = None,
        num_latent_gps: Optional[int] = None,
    ):
        """"""
        data = (X, Y) contains the input points [N, D] and the observations [N, P]
        kernel, likelihood, mean_function are appropriate GPflow objects
        """"""
        if num_latent_gps is None:
            num_latent_gps = self.calc_num_latent_gps_from_data(data, kernel, likelihood)
        super().__init__(kernel, likelihood, mean_function, num_latent_gps)

        X_data, Y_data = data
        num_data = X_data.shape[0]
        self.num_data = num_data
        self.data = data

        self.q_mu = Parameter(np.zeros((num_data, self.num_latent_gps)))
        q_sqrt = np.array([np.eye(num_data) for _ in range(self.num_latent_gps)])
        self.q_sqrt = Parameter(q_sqrt, transform=triangular())",0.7603761143136049,0.7639440574147984,0.743163065525509,0.9802099266605048
65,"    def __init__(
        self,
        data: RegressionData,
        kernel: Kernel,
        likelihood: Likelihood,
        mean_function: Optional[MeanFunction] = None,
        num_latent_gps: Optional[int] = None,
    ):
        """"""
        data = (X, Y) contains the input points [N, D] and the observations [N, P]
        kernel, likelihood, mean_function are appropriate GPflow objects
        """"""
        if num_latent_gps is None:
            num_latent_gps = self.calc_num_latent_gps_from_data(data, kernel, likelihood)
        super().__init__(kernel, likelihood, mean_function, num_latent_gps)

        X_data, Y_data = data
        self.data = data
        self.num_data = X_data.shape[0]
        self.q_alpha = Parameter(np.zeros((self.num_data, self.num_latent_gps)))
        self.q_lambda = Parameter(
            np.ones((self.num_data, self.num_latent_gps)), transform=gpflow.utilities.positive()
        )",0.7648211615140377,0.7882531249872895,0.7939859304320651,0.8364030538962766
66,"    def __init__(
        self,
        data: RegressionData,
        kernel: Kernel,
        likelihood: Likelihood,
        mean_function: Optional[MeanFunction] = None,
        num_latent_gps: Optional[int] = None,
    ):
        """"""
        data is a tuple of X, Y with X, a data matrix, size [N, D] and Y, a data matrix, size [N, R]
        kernel, likelihood, mean_function are appropriate GPflow objects

        This is a vanilla implementation of a GP with a non-Gaussian
        likelihood. The latent function values are represented by centered
        (whitened) variables, so

            v ~ N(0, I)
            f = Lv + m(x)

        with

            L L^T = K

        """"""
        if num_latent_gps is None:
            num_latent_gps = self.calc_num_latent_gps_from_data(data, kernel, likelihood)
        super().__init__(kernel, likelihood, mean_function, num_latent_gps)
        self.data = data
        self.num_data = data[0].shape[0]
        self.V = Parameter(np.zeros((self.num_data, self.num_latent_gps)))
        self.V.prior = tfp.distributions.Normal(
            loc=to_default_float(0.0), scale=to_default_float(1.0)
        )",0.9192189105064956,0.8762246869117529,0.7824559069515548,0.9924007493450732
67,"    def K(self, X: tf.Tensor, X2: Optional[tf.Tensor] = None) -> tf.Tensor:
        sig_X = self._sigmoids(X)  # N x 1 x Ncp
        sig_X2 = self._sigmoids(X2) if X2 is not None else sig_X

        # `starters` are the sigmoids going from 0 -> 1, whilst `stoppers` go
        # from 1 -> 0, dimensions are N x N x Ncp
        starters = sig_X * tf.transpose(sig_X2, perm=(1, 0, 2))
        stoppers = (1 - sig_X) * tf.transpose((1 - sig_X2), perm=(1, 0, 2))

        # prepend `starters` with ones and append ones to `stoppers` since the
        # first kernel has no start and the last kernel has no end
        N = tf.shape(X)[0]
        ones = tf.ones((N, N, 1), dtype=X.dtype)
        starters = tf.concat([ones, starters], axis=2)
        stoppers = tf.concat([stoppers, ones], axis=2)

        # now combine with the underlying kernels
        kernel_stack = tf.stack([k(X, X2) for k in self.kernels], axis=2)
        return tf.reduce_sum(kernel_stack * starters * stoppers, axis=2)",0.9942854082865468,0.7993436143333476,0.7719205545718235,0.7693629423508075
68,"def autoflow(*af_args, **af_kwargs):
    def autoflow_wrapper(method):
        @functools.wraps(method)
        def runnable(obj, *args, **kwargs):
            if not isinstance(obj, Node):
                raise GPflowError(
                    'AutoFlow works only with node-like objects.')
            if obj.is_built_coherence(obj.graph) is Build.NO:
                raise GPflowError('Not built with ""{graph}"".'.format(graph=obj.graph))
            name = method.__name__
            store = AutoFlow.get_autoflow(obj, name)
            session = kwargs.pop('session', None)
            session = obj.enquire_session(session=session)
            if not store:
                scope_name = _name_scope_name(obj, name)
                with session.graph.as_default(), tf.name_scope(scope_name):
                    _setup_storage(store, *af_args, **af_kwargs)
                    _build_method(method, obj, store)
            return _session_run(session, obj, store, *args, **kwargs)
        return runnable
    return autoflow_wrapper",0.9246576895601442,0.9931616990913544,0.9931616990913544,0.991820659232164
69,"    def autoflow_wrapper(method):
        @functools.wraps(method)
        def runnable(obj, *args, **kwargs):
            if not isinstance(obj, Node):
                raise GPflowError(
                    'AutoFlow works only with node-like objects.')
            if obj.is_built_coherence(obj.graph) is Build.NO:
                raise GPflowError('Not built with ""{graph}"".'.format(graph=obj.graph))
            name = method.__name__
            store = AutoFlow.get_autoflow(obj, name)
            session = kwargs.pop('session', None)
            session = obj.enquire_session(session=session)
            if not store:
                scope_name = _name_scope_name(obj, name)
                with session.graph.as_default(), tf.name_scope(scope_name):
                    _setup_storage(store, *af_args, **af_kwargs)
                    _build_method(method, obj, store)
            return _session_run(session, obj, store, *args, **kwargs)
        return runnable",0.8820674483661863,0.9926905422351328,0.9623151787901244,0.9608860945787248
70,"        def runnable(obj, *args, **kwargs):
            if not isinstance(obj, Node):
                raise GPflowError(
                    'AutoFlow works only with node-like objects.')
            if obj.is_built_coherence(obj.graph) is Build.NO:
                raise GPflowError('Not built with ""{graph}"".'.format(graph=obj.graph))
            name = method.__name__
            store = AutoFlow.get_autoflow(obj, name)
            session = kwargs.pop('session', None)
            session = obj.enquire_session(session=session)
            if not store:
                scope_name = _name_scope_name(obj, name)
                with session.graph.as_default(), tf.name_scope(scope_name):
                    _setup_storage(store, *af_args, **af_kwargs)
                    _build_method(method, obj, store)
            return _session_run(session, obj, store, *args, **kwargs)",0.9099217519486804,0.8779143137951761,0.8604820333857239,0.7659611259941902
71,"def initialize_variables(variables=None, session=None, force=False, **run_kwargs):
    session = tf.get_default_session() if session is None else session
    if variables is None:
        initializer = tf.global_variables_initializer()
    else:
        if force:
            initializer = tf.variables_initializer(variables)
        else:
            uninitialized = tf.report_uninitialized_variables(var_list=variables)
            def uninitialized_names():
                for uv in session.run(uninitialized):
                    yield uv.decode('utf-8')
                    # if isinstance(uv, bytes):
                    #     yield uv.decode('utf-8')
                    # elif isinstance(uv, str):
                    #     yield uv
                    # else:
                    #     msg = 'Unknown output type ""{}"" from `tf.report_uninitialized_variables`'
                    #     raise ValueError(msg.format(type(uv)))
            names = set(uninitialized_names())
            vars_for_init = [v for v in variables if v.name.split(':')[0] in names]
            initializer = tf.variables_initializer(vars_for_init)
    session.run(initializer, **run_kwargs)",0.6789357308122747,0.5763520536231405,0.6416651840990288,0.6515099734560135
72,"    def _clear(self):
        self._reset_name()
        self._initial_value_tensor = None
        self._dataholder_tensor = None",0.6457583253022845,0.4368566291812717,0.3597579282030105,0.5171587072182152
73,"    def _build(self):
        self._dataholder_tensor = self._build_parameter()  # pylint: disable=W0201",0.4,0.450734611815856,0.500022240299931,0.4708325364070425
74,"    def _init_parameter_defaults(self):
        self._initial_value_tensor = None
        self._dataholder_tensor = None",0.2306564161460151,0.1985807074737667,0.455836006331346,0.5680790543233218
75,"    def initializables(self):
        if self._externally_defined:
            return None
        return [self.parameter_tensor]",0.8934542852431553,0.7682333969567181,0.8494904584470001,0.5833858513388062
76,"    def read_value(self, session=None):
        if session is not None:
            if not isinstance(session, tf.Session):
                raise ValueError('TensorFlow session expected as session argument.')
            is_built = self.is_built_coherence(session.graph)
            if is_built is Build.YES:
                return self._read_parameter_tensor(session)
        elif self._externally_defined:
            raise GPflowError('Externally defined parameter requires session.')
        return self._value",0.702137575875516,0.5962142710720868,0.9362641020933056,0.9137244309519268
77,"    def _clear(self):
        self._reset_name()
        self._externally_defined = False   # pylint: disable=W0201
        self._initial_value_tensor = None  # pylint: disable=W0201
        self._unconstrained_tensor = None  # pylint: disable=W0201
        self._constrained_tensor = None    # pylint: disable=W0201
        self._prior_tensor = None          # pylint: disable=W0201",0.3868465319866215,0.3868465319866215,0.6044787362100682,0.4757279193796092
78,"    def _build(self):
        unconstrained = self._build_parameter()
        constrained = self._build_constrained(unconstrained)
        prior = self._build_prior(unconstrained, constrained)
        self._unconstrained_tensor = unconstrained  # pylint: disable=W0201
        self._constrained_tensor = constrained      # pylint: disable=W0201
        self._prior_tensor = prior                  # pylint: disable=W0201",0.5844802574743986,0.5307012298079625,0.7515908365978528,0.3890887400101397
79,"    def _build_parameter(self):
        if self._externally_defined:
            self._check_tensor_trainable(self.parameter_tensor)
            return self.parameter_tensor

        name = self._parameter_name()
        tensor = misc.get_variable_by_name(name)
        if tensor is not None:
            raise GPflowError('Tensor with name ""{name}"" already exists, {tensor}.'
                              .format(name=name, tensor=tensor))

        value = self._apply_transform(self._value)
        shape = value.shape if self.fixed_shape else None
        init = tf.placeholder(self.dtype, shape=shape, name='initial_unconstrained_value')
        self._initial_value_tensor = init
        if self.fixed_shape:
            return tf.get_variable(name, initializer=init, trainable=self.trainable)
        return tf.get_variable(name, initializer=init,
                               validate_shape=False,
                               trainable=self.trainable)",0.9445435780381432,0.9160177810669056,0.8544985535241647,0.8507532238271862
80,"    def _init_parameter_defaults(self):
        self._initial_value_tensor = None
        self._unconstrained_tensor = None
        self._prior_tensor = None
        self._constrained_tensor = None",0.5235028749711658,0.1628125019308407,0.1646720478833566,0.165378721291652
81,"    def minimize(self, model, session=None, var_list=None, feed_dict=None,
                 maxiter=1000, initialize=True, anchor=True, **kwargs):
        """"""
        Minimizes objective function of the model.

        :param model: GPflow model with objective tensor.
        :param session: Session where optimization will be run.
        :param var_list: List of extra variables which should be trained during optimization.
        :param feed_dict: Feed dictionary of tensors passed to session run method.
        :param maxiter: Number of run interation.
        :param initialize: If `True` model parameters will be re-initialized even if they were
            initialized before for gotten session.
        :param anchor: If `True` trained variable values computed during optimization at
            particular session will be synchronized with internal parameter values.
        :param kwargs: This is a dictionary of extra parameters for session run method.
        """"""

        if model is None or not isinstance(model, Model):
            raise ValueError('Unknown type passed for optimization.')

        session = model.enquire_session(session)

        self._model = model
        objective = model.objective

        with session.graph.as_default():
            full_var_list = self._gen_var_list(model, var_list)

            # Create optimizer variables before initialization.
            self._minimize_operation = self.optimizer.minimize(
                objective, var_list=full_var_list, **kwargs)

            model.initialize(session=session, force=initialize)
            self._initialize_optimizer(session, full_var_list)

            feed_dict = self._gen_feed_dict(model, feed_dict)
            for _i in range(maxiter):
                session.run(self.minimize_operation, feed_dict=feed_dict)

        if anchor:
            model.anchor(session)",0.9041437073754004,0.94547201534011,0.9941826065935164,0.9229111638722562
82,"    def prob_is_largest(self, Y, mu, var, gh_x, gh_w):
        # work out what the mean and variance is of the indicated latent function.
        oh_on = tf.cast(tf.one_hot(tf.reshape(Y, (-1,)), self.num_classes, 1., 0.), tf.float64)
        mu_selected = tf.reduce_sum(oh_on * mu, 1)
        var_selected = tf.reduce_sum(oh_on * var, 1)

        # generate Gauss Hermite grid
        X = tf.reshape(mu_selected, (-1, 1)) + gh_x * tf.reshape(tf.sqrt(tf.clip_by_value(2. * var_selected, 1e-10, np.inf)), (-1, 1))

        # compute the CDF of the Gaussian between the latent functions and the grid (including the selected function)
        dist = (tf.expand_dims(X, 1) - tf.expand_dims(mu, 2)) / tf.expand_dims(tf.sqrt(tf.clip_by_value(var, 1e-10, np.inf)), 2)
        cdfs = 0.5 * (1.0 + tf.erf(dist/np.sqrt(2.0)))

        cdfs = cdfs * (1-2e-4) + 1e-4

        # blank out all the distances on the selected latent function
        oh_off = tf.cast(tf.one_hot(tf.reshape(Y, (-1,)), self.num_classes, 0., 1.), tf.float64)
        cdfs = cdfs * tf.expand_dims(oh_off, 2) + tf.expand_dims(oh_on, 2)

        # take the product over the latent functions, and the sum over the GH grid.
        return tf.matmul(tf.reduce_prod(cdfs, 1), tf.reshape(gh_w/np.sqrt(np.pi), (-1, 1)))",0.78975033053818,0.8113933681750598,0.4,0.7987204706857616
83,"    def __call__(self, tf_method):
        @wraps(tf_method)
        def runnable(instance, *np_args):
            graph_name = '_' + tf_method.__name__ + '_graph'
            if not hasattr(instance, graph_name):
                instance._compile()
                self.tf_args = [tf.placeholder(*a) for a in self.tf_arg_tuples]
                with instance.tf_mode():
                    graph = tf_method(instance, *self.tf_args)
                setattr(instance, graph_name, graph)
            feed_dict = dict(zip(self.tf_args, np_args))
            feed_dict[instance._free_vars] = instance.get_free_state()
            graph = getattr(instance, graph_name)
            return instance._session.run(graph, feed_dict=feed_dict)
        return runnable",0.7459400766881281,0.7459425080319124,0.991051344970492,0.9065258045891926
84,"        def runnable(instance, *np_args):
            graph_name = '_' + tf_method.__name__ + '_graph'
            if not hasattr(instance, graph_name):
                instance._compile()
                self.tf_args = [tf.placeholder(*a) for a in self.tf_arg_tuples]
                with instance.tf_mode():
                    graph = tf_method(instance, *self.tf_args)
                setattr(instance, graph_name, graph)
            feed_dict = dict(zip(self.tf_args, np_args))
            feed_dict[instance._free_vars] = instance.get_free_state()
            graph = getattr(instance, graph_name)
            return instance._session.run(graph, feed_dict=feed_dict)",0.7772140166239296,0.9123718190642102,0.9899678749658885,0.6323895633759731
85,"    def browse(self, uri):
        logger.debug(""Browsing files at: %s"", uri)
        result = []
        local_path = path.uri_to_path(uri)

        if str(local_path) == ""root"":
            return list(self._get_media_dirs_refs())

        if not self._is_in_basedir(local_path):
            logger.warning(
                ""Rejected attempt to browse path (%s) outside dirs defined ""
                ""in file/media_dirs config."",
                uri,
            )
            return []

        for dir_entry in local_path.iterdir():
            child_path = dir_entry.resolve()
            uri = path.path_to_uri(child_path)

            if not self._show_dotfiles and dir_entry.name.startswith("".""):
                continue

            if (
                self._excluded_file_extensions
                and dir_entry.suffix in self._excluded_file_extensions
            ):
                continue

            if child_path.is_symlink() and not self._follow_symlinks:
                logger.debug(""Ignoring symlink: %s"", uri)
                continue

            if not self._is_in_basedir(child_path):
                logger.debug(""Ignoring symlink to outside base dir: %s"", uri)
                continue

            if child_path.is_dir():
                result.append(
                    models.Ref.directory(name=dir_entry.name, uri=uri)
                )
            elif child_path.is_file():
                result.append(models.Ref.track(name=dir_entry.name, uri=uri))

        def order(item):
            return (item.type != models.Ref.DIRECTORY, item.name)

        result.sort(key=order)

        return result",0.7697062872727596,0.7448687043416055,0.9857374171532658,0.8291241474276325
86,"    def on_error(self, error, debug):
        error_msg = str(error).decode()
        debug_msg = debug.decode()
        gst_logger.debug(
            ""Got ERROR bus message: error=%r debug=%r"", error_msg, debug_msg
        )
        gst_logger.error(""GStreamer error: %s"", error_msg)
        # TODO: is this needed?
        self._audio.stop_playback()",0.571073591290471,0.4908805446666699,0.9805302554982214,0.7262786337083136
87,"    def on_warning(self, error, debug):
        error_msg = str(error).decode()
        debug_msg = debug.decode()
        gst_logger.warning(""GStreamer warning: %s"", error_msg)
        gst_logger.debug(
            ""Got WARNING bus message: error=%r debug=%r"", error_msg, debug_msg
        )",0.9764342992373236,0.5697353371547761,0.7510884601666199,0.6131490644205317
88,"def _unwrap_stream(uri, timeout, scanner, requests_session):
    """"""
    Get a stream URI from a playlist URI, ``uri``.

    Unwraps nested playlists until something that's not a playlist is found or
    the ``timeout`` is reached.
    """"""

    original_uri = uri
    seen_uris = set()
    deadline = time.time() + timeout

    while time.time() < deadline:
        if uri in seen_uris:
            logger.info(
                'Unwrapping stream from URI (%s) failed: '
                'playlist referenced itself', uri)
            return None, None
        else:
            seen_uris.add(uri)

        logger.debug('Unwrapping stream from URI: %s', uri)

        try:
            scan_timeout = deadline - time.time()
            if scan_timeout < 0:
                logger.info(
                    'Unwrapping stream from URI (%s) failed: '
                    'timed out in %sms', uri, timeout)
                return None, None
            scan_result = scanner.scan(uri, timeout=scan_timeout)
        except exceptions.ScannerError as exc:
            logger.debug('GStreamer failed scanning URI (%s): %s', uri, exc)
            scan_result = None

        if scan_result is not None:
            if scan_result.playable or (
                not scan_result.mime.startswith('text/') and
                not scan_result.mime.startswith('application/')
            ):
                logger.debug(
                    'Unwrapped potential %s stream: %s', scan_result.mime, uri)
                return uri, scan_result

        download_timeout = deadline - time.time()
        if download_timeout < 0:
            logger.info(
                'Unwrapping stream from URI (%s) failed: timed out in %sms',
                uri, timeout)
            return None, None
        content = http.download(
            requests_session, uri, timeout=download_timeout / 1000)

        if content is None:
            logger.info(
                'Unwrapping stream from URI (%s) failed: '
                'error downloading URI %s', original_uri, uri)
            return None, None

        uris = playlists.parse(content)
        if not uris:
            logger.debug(
                'Failed parsing URI (%s) as playlist; found potential stream.',
                uri)
            return uri, None

        # TODO Test streams and return first that seems to be playable
        logger.debug(
            'Parsed playlist (%s) and found new URI: %s', uri, uris[0])
        uri = urllib.parse.urljoin(uri, uris[0])",0.879493794563727,0.8051721100044753,0.7557217696107134,0.7837886295368683
89,"def listplaylist(context, name):
    """"""
    *musicpd.org, stored playlists section:*

        ``listplaylist {NAME}``

        Lists the files in the playlist ``NAME.m3u``.

    Output format::

        file: relative/path/to/file1.flac
        file: relative/path/to/file2.ogg
        file: relative/path/to/file3.mp3
    """"""
    playlist = _get_playlist(context, name)
    return ['file: %s' % t.uri for t in playlist.tracks]",0.6583954643098917,0.7701798851056731,0.660168485327068,0.6212655696407632
90,"def track_to_mpd_format(track, position=None, stream_title=None):
    """"""
    Format track for output to MPD client.

    :param track: the track
    :type track: :class:`mopidy.models.Track` or :class:`mopidy.models.TlTrack`
    :param position: track's position in playlist
    :type position: integer
    :param stream_title: the current streams title
    :type position: string
    :rtype: list of two-tuples
    """"""
    if isinstance(track, TlTrack):
        (tlid, track) = track
    else:
        (tlid, track) = (None, track)

    if not track.uri:
        logger.warning('Ignoring track without uri')
        return []

    result = [
        ('file', track.uri),
        ('Time', track.length and (track.length // 1000) or 0),
        ('Artist', concat_multi_values(track.artists, 'name')),
        ('Album', track.album and track.album.name or ''),
    ]

    if stream_title is not None:
        result.append(('Title', stream_title))
        if track.name:
            result.append(('Name', track.name))
    else:
        result.append(('Title', track.name or ''))

    if track.date:
        result.append(('Date', track.date))

    if track.album is not None and track.album.num_tracks is not None:
        result.append(('Track', '%d/%d' % (
            track.track_no or 0, track.album.num_tracks)))
    else:
        result.append(('Track', track.track_no or 0))
    if position is not None and tlid is not None:
        result.append(('Pos', position))
        result.append(('Id', tlid))
    if track.album is not None and track.album.musicbrainz_id is not None:
        result.append(('MUSICBRAINZ_ALBUMID', track.album.musicbrainz_id))

    if track.album is not None and track.album.artists:
        result.append(
            ('AlbumArtist', concat_multi_values(track.album.artists, 'name')))
        musicbrainz_ids = concat_multi_values(
            track.album.artists, 'musicbrainz_id')
        if musicbrainz_ids:
            result.append(('MUSICBRAINZ_ALBUMARTISTID', musicbrainz_ids))

    if track.artists:
        musicbrainz_ids = concat_multi_values(track.artists, 'musicbrainz_id')
        if musicbrainz_ids:
            result.append(('MUSICBRAINZ_ARTISTID', musicbrainz_ids))

    if track.composers:
        result.append(
            ('Composer', concat_multi_values(track.composers, 'name')))

    if track.performers:
        result.append(
            ('Performer', concat_multi_values(track.performers, 'name')))

    if track.genre:
        result.append(('Genre', track.genre))

    if track.disc_no:
        result.append(('Disc', track.disc_no))

    if track.last_modified:
        datestring = datetime.datetime.utcfromtimestamp(
            track.last_modified // 1000).isoformat()
        result.append(('Last-Modified', datestring + 'Z'))

    if track.musicbrainz_id is not None:
        result.append(('MUSICBRAINZ_TRACKID', track.musicbrainz_id))

    if track.album and track.album.uri:
        result.append(('X-AlbumUri', track.album.uri))
    if track.album and track.album.images:
        images = ';'.join(i for i in track.album.images if i != '')
        result.append(('X-AlbumImage', images))

    result = [element for element in result if _has_value(*element)]

    return result",0.7013823587133805,0.8391559497023031,0.8676397566081,0.8676397566081
91,"def _get_user_dirs(xdg_config_dir):
    """"""Returns a dict of XDG dirs read from
    ``$XDG_CONFIG_HOME/user-dirs.dirs``.

    This is used at import time for most users of :mod:`mopidy`. By rolling our
    own implementation instead of using :meth:`glib.get_user_special_dir` we
    make it possible for many extensions to run their test suites, which are
    importing parts of :mod:`mopidy`, in a virtualenv with global site-packages
    disabled, and thus no :mod:`glib` available.
    """"""

    dirs_file = os.path.join(xdg_config_dir, b'user-dirs.dirs')

    if not os.path.exists(dirs_file):
        return {}

    with open(dirs_file, 'rb') as fh:
        data = fh.read().decode('utf-8')

    data = '[XDG_USER_DIRS]\\n' + data
    data = data.replace('$HOME', os.path.expanduser('~'))
    data = data.replace('""', '')

    config = configparser.RawConfigParser()
    config.readfp(io.StringIO(data))

    return {
        k.upper(): os.path.abspath(v)
        for k, v in config.items('XDG_USER_DIRS') if v is not None}",0.9852382115171002,0.7763164276642912,0.9283352969306884,0.7435096351020238
92,"    def validate(self, value):
        return compat.intern(str(super(Identifier, self).validate(value)))",0.5836777140856042,0.5846093300877666,0.708602700759901,0.9057105604647898
93,"    def on_stream_start(self):
        gst_logger.debug('Got STREAM_START bus message')
        uri = self._audio._pending_uri
        logger.debug('Audio event: stream_changed(uri=%r)', uri)
        AudioListener.send('stream_changed', uri=uri)

        # Emit any postponed tags that we got after about-to-finish.
        tags, self._audio._pending_tags = self._audio._pending_tags, None
        self._audio._tags = tags

        if tags:
            logger.debug('Audio event: tags_changed(tags=%r)', tags.keys())
            AudioListener.send('tags_changed', tags=tags.keys())",0.7536892422672471,0.951648455080183,0.9879044048319338,0.7462288175505751
94,"    def on_playbin_state_changed(self, old_state, new_state, pending_state):
        gst_logger.debug(
            'Got STATE_CHANGED bus message: old=%s new=%s pending=%s',
            old_state.value_name, new_state.value_name,
            pending_state.value_name)

        if new_state == Gst.State.READY and pending_state == Gst.State.NULL:
            # XXX: We're not called on the last state change when going down to
            # NULL, so we rewrite the second to last call to get the expected
            # behavior.
            new_state = Gst.State.NULL
            pending_state = Gst.State.VOID_PENDING

        if pending_state != Gst.State.VOID_PENDING:
            return  # Ignore intermediate state changes

        if new_state == Gst.State.READY:
            return  # Ignore READY state as it's GStreamer specific

        new_state = _GST_STATE_MAPPING[new_state]
        old_state, self._audio.state = self._audio.state, new_state

        target_state = _GST_STATE_MAPPING[self._audio._target_state]
        if target_state == new_state:
            target_state = None

        logger.debug('Audio event: state_changed(old_state=%s, new_state=%s, '
                     'target_state=%s)', old_state, new_state, target_state)
        AudioListener.send('state_changed', old_state=old_state,
                           new_state=new_state, target_state=target_state)
        if new_state == PlaybackState.STOPPED:
            logger.debug('Audio event: stream_changed(uri=None)')
            AudioListener.send('stream_changed', uri=None)

        if 'GST_DEBUG_DUMP_DOT_DIR' in os.environ:
            Gst.debug_bin_to_dot_file(
                self._audio._playbin, Gst.DebugGraphDetails.ALL, 'mopidy')",0.9947053917427854,0.9947053917427854,0.9833784220869788,0.9947053917427854
95,"    def playlist_uri_from_name(self, name):
        """"""
        Helper function to retrieve a playlist URI from its unique MPD name.
        """"""
        if not self._uri_from_name:
            self.refresh_playlists_mapping()
        return self._uri_from_name.get(name)",0.8382953719643677,0.8010130214328081,0.8727983606564136,0.6141437703071778
96,"def _get_library(args, config):
    libraries = dict((l.name, l) for l in args.registry['local:library'])
    library_name = config['local']['library']

    if library_name not in libraries:
        logger.warning('Local library %s not found', library_name)
        return 1

    logger.debug('Using %s as the local library', library_name)
    return libraries[library_name](config)",0.6993460790825955,0.5679790035549717,0.7354352254427672,0.9432670862781126
97,"    def run(self, args, config):
        library = _get_library(args, config)
        prompt = '\\nAre you sure you want to clear the library? [y/N] '

        if compat.input(prompt).lower() != 'y':
            print('Clearing library aborted.')
            return 0

        if library.clear():
            print('Library successfully cleared.')
            return 0

        print('Unable to clear library.')
        return 1",0.6707035219746531,0.8276155631958743,0.9858753754599436,0.6337863304471948
98,"    def run(self, args, config):
        media_dir = config['local']['media_dir']
        scan_timeout = config['local']['scan_timeout']
        flush_threshold = config['local']['scan_flush_threshold']
        excluded_file_extensions = config['local']['excluded_file_extensions']
        excluded_file_extensions = tuple(
            bytes(file_ext.lower()) for file_ext in excluded_file_extensions)

        library = _get_library(args, config)

        file_mtimes, file_errors = path.find_mtimes(
            media_dir, follow=config['local']['scan_follow_symlinks'])

        logger.info('Found %d files in media_dir.', len(file_mtimes))

        if file_errors:
            logger.warning('Encountered %d errors while scanning media_dir.',
                           len(file_errors))
        for name in file_errors:
            logger.debug('Scan error %r for %r', file_errors[name], name)

        num_tracks = library.load()
        logger.info('Checking %d tracks from library.', num_tracks)

        uris_to_update = set()
        uris_to_remove = set()
        uris_in_library = set()

        for track in library.begin():
            abspath = translator.local_track_uri_to_path(track.uri, media_dir)
            mtime = file_mtimes.get(abspath)
            if mtime is None:
                logger.debug('Missing file %s', track.uri)
                uris_to_remove.add(track.uri)
            elif mtime > track.last_modified or args.force:
                uris_to_update.add(track.uri)
            uris_in_library.add(track.uri)

        logger.info('Removing %d missing tracks.', len(uris_to_remove))
        for uri in uris_to_remove:
            library.remove(uri)

        for abspath in file_mtimes:
            relpath = os.path.relpath(abspath, media_dir)
            uri = translator.path_to_local_track_uri(relpath)

            if b'/.' in relpath:
                logger.debug('Skipped %s: Hidden directory/file.', uri)
            elif relpath.lower().endswith(excluded_file_extensions):
                logger.debug('Skipped %s: File extension excluded.', uri)
            elif uri not in uris_in_library:
                uris_to_update.add(uri)

        logger.info(
            'Found %d tracks which need to be updated.', len(uris_to_update))
        logger.info('Scanning...')

        uris_to_update = sorted(uris_to_update, key=lambda v: v.lower())
        uris_to_update = uris_to_update[:args.limit]

        scanner = scan.Scanner(scan_timeout)
        progress = _Progress(flush_threshold, len(uris_to_update))

        for uri in uris_to_update:
            try:
                relpath = translator.local_track_uri_to_path(uri, media_dir)
                file_uri = path.path_to_uri(os.path.join(media_dir, relpath))
                result = scanner.scan(file_uri)
                tags, duration = result.tags, result.duration
                if not result.playable:
                    logger.warning('Failed %s: No audio found in file.', uri)
                elif duration < MIN_DURATION_MS:
                    logger.warning('Failed %s: Track shorter than %dms',
                                   uri, MIN_DURATION_MS)
                else:
                    mtime = file_mtimes.get(os.path.join(media_dir, relpath))
                    track = utils.convert_tags_to_track(tags).replace(
                        uri=uri, length=duration, last_modified=mtime)
                    if library.add_supports_tags_and_duration:
                        library.add(track, tags=tags, duration=duration)
                    else:
                        library.add(track)
                    logger.debug('Added %s', track.uri)
            except exceptions.ScannerError as error:
                logger.warning('Failed %s: %s', uri, error)

            if progress.increment():
                progress.log()
                if library.flush():
                    logger.debug('Progress flushed.')

        progress.log()
        library.close()
        logger.info('Done scanning.')
        return 0",0.7104267026304043,0.6736273307655544,0.6002758656707887,0.5892900647730053
99,"def parse_urilist(data):
    result = []
    for line in data.splitlines():
        if not line.strip() or line.startswith('#'):
            continue
        try:
            validation.check_uri(line)
        except ValueError:
            return []
        result.append(line)
    return result",0.7742285538669267,0.9815625043655288,0.8446943025868388,0.6340929761543412
100,"    def send(self, data):
        """"""Send data to client, return any unsent data.""""""
        try:
            sent = self.sock.send(data)
            return data[sent:]
        except socket.error as e:
            if e.errno in (errno.EWOULDBLOCK, errno.EINTR):
                return data
            self.stop('Unexpected client error: %s' % e)
            return b''",0.9834041381587282,0.6000225202596334,0.5972193841161586,0.8469946890369242
101,"def _find_worker(relative, follow, done, work, results, errors):
    """"""Worker thread for collecting stat() results.

    :param str relative: directory to make results relative to
    :param bool follow: if symlinks should be followed
    :param threading.Event done: event indicating that all work has been done
    :param queue.Queue work: queue of paths to process
    :param dict results: shared dictionary for storing all the stat() results
    :param dict errors: shared dictionary for storing any per path errors
    """"""
    while not done.is_set():
        try:
            entry, parents = work.get(block=False)
        except queue.Empty:
            continue

        if relative:
            path = os.path.relpath(entry, relative)
        else:
            path = entry

        try:
            if follow:
                st = os.stat(entry)
            else:
                st = os.lstat(entry)

            if (st.st_dev, st.st_ino) in parents:
                errors[path] = exceptions.FindError('Sym/hardlink loop found.')
                continue

            parents = parents + [(st.st_dev, st.st_ino)]
            if stat.S_ISDIR(st.st_mode):
                for e in os.listdir(entry):
                    work.put((os.path.join(entry, e), parents))
            elif stat.S_ISREG(st.st_mode):
                results[path] = st
            elif stat.S_ISLNK(st.st_mode):
                errors[path] = exceptions.FindError('Not following symlinks.')
            else:
                errors[path] = exceptions.FindError('Not a file or directory.')

        except OSError as e:
            errors[path] = exceptions.FindError(e.strerror, e.errno)
        finally:
            work.task_done()",0.960747536702258,0.776359656795376,0.901283144071686,0.718774260937402
102,"    def push(self, buffer_):
        if buffer_ is None:
            gst_logger.debug('Sending appsrc end-of-stream event.')
            return self._source.emit('end-of-stream') == gst.FLOW_OK
        else:
            return self._source.emit('push-buffer', buffer_) == gst.FLOW_OK",0.4053837655249784,0.6488397670176809,0.7309371990863168,0.4942868611077857
103,"def find_exact(tracks, query=None, uris=None):
    # TODO Only return results within URI roots given by ``uris``

    if query is None:
        query = {}

    _validate_query(query)

    for (field, values) in query.items():
        if not hasattr(values, '__iter__'):
            values = [values]
        # FIXME this is bound to be slow for large libraries
        for value in values:
            if field == 'track_no':
                q = _convert_to_int(value)
            else:
                q = value.strip()

            uri_filter = lambda t: q == t.uri
            track_name_filter = lambda t: q == t.name
            album_filter = lambda t: q == getattr(t, 'album', Album()).name
            artist_filter = lambda t: filter(
                lambda a: q == a.name, t.artists)
            albumartist_filter = lambda t: any([
                q == a.name
                for a in getattr(t.album, 'artists', [])])
            composer_filter = lambda t: any([
                q == a.name
                for a in getattr(t, 'composers', [])])
            performer_filter = lambda t: any([
                q == a.name
                for a in getattr(t, 'performers', [])])
            track_no_filter = lambda t: q == t.track_no
            genre_filter = lambda t: t.genre and q == t.genre
            date_filter = lambda t: q == t.date
            comment_filter = lambda t: q == t.comment
            any_filter = lambda t: (
                uri_filter(t) or
                track_name_filter(t) or
                album_filter(t) or
                artist_filter(t) or
                albumartist_filter(t) or
                composer_filter(t) or
                performer_filter(t) or
                track_no_filter(t) or
                genre_filter(t) or
                date_filter(t) or
                comment_filter(t))

            if field == 'uri':
                tracks = filter(uri_filter, tracks)
            elif field == 'track_name':
                tracks = filter(track_name_filter, tracks)
            elif field == 'album':
                tracks = filter(album_filter, tracks)
            elif field == 'artist':
                tracks = filter(artist_filter, tracks)
            elif field == 'albumartist':
                tracks = filter(albumartist_filter, tracks)
            elif field == 'composer':
                tracks = filter(composer_filter, tracks)
            elif field == 'performer':
                tracks = filter(performer_filter, tracks)
            elif field == 'track_no':
                tracks = filter(track_no_filter, tracks)
            elif field == 'genre':
                tracks = filter(genre_filter, tracks)
            elif field == 'date':
                tracks = filter(date_filter, tracks)
            elif field == 'comment':
                tracks = filter(comment_filter, tracks)
            elif field == 'any':
                tracks = filter(any_filter, tracks)
            else:
                raise LookupError('Invalid lookup field: %s' % field)

    # TODO: add local:search:<query>
    return SearchResult(uri='local:search', tracks=tracks)",0.6284346417169784,0.3329857678381371,0.395463397551508,0.2973447058495475
104,"def validate_extension(extension):
    """"""Verify extension's dependencies and environment.

    :param extensions: an extension to check
    :returns: if extension should be run
    """"""

    logger.debug('Validating extension: %s', extension.ext_name)

    if extension.ext_name != extension.entry_point.name:
        logger.warning(
            'Disabled extension %(ep)s: entry point name (%(ep)s) '
            'does not match extension name (%(ext)s)',
            {'ep': extension.entry_point.name, 'ext': extension.ext_name})
        return False

    try:
        extension.entry_point.require()
    except pkg_resources.DistributionNotFound as ex:
        logger.info(
            'Disabled extension %s: Dependency %s not found',
            extension.ext_name, ex)
        return False
    except pkg_resources.VersionConflict as ex:
        found, required = ex.args
        logger.info(
            'Disabled extension %s: %s required, but found %s at %s',
            extension.ext_name, required, found, found.location)
        return False

    try:
        extension.validate_environment()
    except exceptions.ExtensionError as ex:
        logger.info(
            'Disabled extension %s: %s', extension.ext_name, ex.message)
        return False

    return True",0.7536312351322159,0.6705904891547678,0.906661532045464,0.9603476431902642
105,"    def recv_callback(self, fd, flags):
        if flags & (gobject.IO_ERR | gobject.IO_HUP):
            self.stop('Bad client flags: %s' % flags)
            return True

        try:
            data = self.sock.recv(4096)
        except socket.error as e:
            if e.errno not in (errno.EWOULDBLOCK, errno.EINTR):
                self.stop('Unexpected client error: %s' % e)
            return True

        if not data:
            self.actor_ref.tell({'close': True})
            self.disable_recv()
            return True

        try:
            self.actor_ref.tell({'received': data})
        except pykka.ActorDeadError:
            self.stop('Actor is dead.')

        return True",0.830281144415036,0.7061509559651842,0.9915799249576248,0.8096046884560332
106,"    def publish(self):
        if not dbus:
            logger.debug('Zeroconf publish failed: dbus not installed.')
            return False

        try:
            bus = dbus.SystemBus()
        except dbus.exceptions.DBusException as e:
            logger.debug('Zeroconf publish failed: %s', e)
            return False

        if not bus.name_has_owner('org.freedesktop.Avahi'):
            logger.debug('Zeroconf publish failed: Avahi service not running.')
            return False

        server = dbus.Interface(bus.get_object('org.freedesktop.Avahi', '/'),
                                'org.freedesktop.Avahi.Server')

        self.group = dbus.Interface(
            bus.get_object('org.freedesktop.Avahi', server.EntryGroupNew()),
            'org.freedesktop.Avahi.EntryGroup')

        text = [_convert_text_to_dbus_bytes(t) for t in self.text]
        self.group.AddService(_AVAHI_IF_UNSPEC, _AVAHI_PROTO_UNSPEC,
                              dbus.UInt32(_AVAHI_PUBLISHFLAGS_NONE),
                              self.name, self.stype, self.domain, self.host,
                              dbus.UInt16(self.port), text)

        self.group.Commit()
        return True",0.7406577793167499,0.7507941197835074,0.75051160057936,0.6265595637138333
107,"def parse_options():
    parser = optparse.OptionParser(
        version='Mopidy %s' % versioning.get_version())
    parser.add_option(
        '--help-gst',
        action='store_true', dest='help_gst',
        help='show GStreamer help options')
    parser.add_option(
        '-i', '--interactive',
        action='store_true', dest='interactive',
        help='ask interactively for required settings which are missing')
    parser.add_option(
        '-q', '--quiet',
        action='store_const', const=0, dest='verbosity_level',
        help='less output (warning level)')
    parser.add_option(
        '-v', '--verbose',
        action='count', default=1, dest='verbosity_level',
        help='more output (debug level)')
    parser.add_option(
        '--save-debug-log',
        action='store_true', dest='save_debug_log',
        help='save debug log to ""./mopidy.log""')
    parser.add_option(
        '--list-settings',
        action='callback',
        callback=settings_utils.list_settings_optparse_callback,
        help='list current settings')
    parser.add_option(
        '--list-deps',
        action='callback', callback=deps.list_deps_optparse_callback,
        help='list dependencies and their versions')
    parser.add_option(
        '--debug-thread',
        action='store_true', dest='debug_thread',
        help='run background thread that dumps tracebacks on SIGUSR1')
    return parser.parse_args(args=mopidy_args)[0]",0.9146903182431562,0.9674770595677282,0.9540433115297184,0.8684509936802518
108,"def _convert_mpd_data(data, tracks, music_dir):
    if not data:
        return

    track_kwargs = {}
    album_kwargs = {}
    artist_kwargs = {}
    albumartist_kwargs = {}

    if 'track' in data:
        if '/' in data['track']:
            album_kwargs['num_tracks'] = int(data['track'].split('/')[1])
            track_kwargs['track_no'] = int(data['track'].split('/')[0])
        else:
            track_kwargs['track_no'] = int(data['track'])

    if 'artist' in data:
        artist_kwargs['name'] = data['artist']
        albumartist_kwargs['name'] = data['artist']

    if 'albumartist' in data:
        albumartist_kwargs['name'] = data['albumartist']

    if 'album' in data:
        album_kwargs['name'] = data['album']

    if 'title' in data:
        track_kwargs['name'] = data['title']

    if 'date' in data:
        track_kwargs['date'] = data['date']

    if 'musicbrainz_trackid' in data:
        track_kwargs['musicbrainz_id'] = data['musicbrainz_trackid']

    if 'musicbrainz_albumid' in data:
        album_kwargs['musicbrainz_id'] = data['musicbrainz_albumid']

    if 'musicbrainz_artistid' in data:
        artist_kwargs['musicbrainz_id'] = data['musicbrainz_artistid']

    if 'musicbrainz_albumartistid' in data:
        albumartist_kwargs['musicbrainz_id'] = (
            data['musicbrainz_albumartistid'])

    if data['file'][0] == '/':
        path = data['file'][1:]
    else:
        path = data['file']
    path = urllib.unquote(path)

    if artist_kwargs:
        artist = Artist(**artist_kwargs)
        track_kwargs['artists'] = [artist]

    if albumartist_kwargs:
        albumartist = Artist(**albumartist_kwargs)
        album_kwargs['artists'] = [albumartist]

    if album_kwargs:
        album = Album(**album_kwargs)
        track_kwargs['album'] = album

    track_kwargs['uri'] = path_to_uri(music_dir, path)
    track_kwargs['length'] = int(data.get('time', 0)) * 1000

    track = Track(**track_kwargs)
    tracks.add(track)",0.6086396122276989,0.6199085583010637,0.6918833856780507,0.6575740752433593
109,"def translator(data):
    albumartist_kwargs = {}
    album_kwargs = {}
    artist_kwargs = {}
    track_kwargs = {}

    def _retrieve(source_key, target_key, target):
        if source_key in data:
            target[target_key] = data[source_key]

    _retrieve(gst.TAG_ALBUM, 'name', album_kwargs)
    _retrieve(gst.TAG_TRACK_COUNT, 'num_tracks', album_kwargs)
    _retrieve(gst.TAG_ARTIST, 'name', artist_kwargs)

    if gst.TAG_DATE in data and data[gst.TAG_DATE]:
        date = data[gst.TAG_DATE]
        try:
            date = datetime.date(date.year, date.month, date.day)
        except ValueError:
            pass  # Ignore invalid dates
        else:
            track_kwargs['date'] = date.isoformat()

    _retrieve(gst.TAG_TITLE, 'name', track_kwargs)
    _retrieve(gst.TAG_TRACK_NUMBER, 'track_no', track_kwargs)

    # Following keys don't seem to have TAG_* constant.
    _retrieve('album-artist', 'name', albumartist_kwargs)
    _retrieve('musicbrainz-trackid', 'musicbrainz_id', track_kwargs)
    _retrieve('musicbrainz-artistid', 'musicbrainz_id', artist_kwargs)
    _retrieve('musicbrainz-albumid', 'musicbrainz_id', album_kwargs)
    _retrieve(
        'musicbrainz-albumartistid', 'musicbrainz_id', albumartist_kwargs)

    if albumartist_kwargs:
        album_kwargs['artists'] = [Artist(**albumartist_kwargs)]

    track_kwargs['uri'] = data['uri']
    track_kwargs['length'] = data[gst.TAG_DURATION]
    track_kwargs['album'] = Album(**album_kwargs)
    track_kwargs['artists'] = [Artist(**artist_kwargs)]

    return Track(**track_kwargs)",0.5222249647534799,0.6634765305222843,0.9072071338419616,0.7595583381431003
110,"    def _retrieve(source_key, target_key, target):
        if source_key in data:
            target[target_key] = data[source_key]",0.4,0.589304900571862,0.4,0.6812479655614744
111,"def translator(data):
    albumartist_kwargs = {}
    album_kwargs = {}
    artist_kwargs = {}
    track_kwargs = {}

    # NOTE: kwargs are explicitly made bytestrings to work on Python
    # 2.6.0/2.6.1. See https://github.com/mopidy/mopidy/issues/302 for
    # details.

    def _retrieve(source_key, target_key, target):
        if source_key in data:
            target[str(target_key)] = data[source_key]

    _retrieve(gst.TAG_ALBUM, 'name', album_kwargs)
    _retrieve(gst.TAG_TRACK_COUNT, 'num_tracks', album_kwargs)
    _retrieve(gst.TAG_ARTIST, 'name', artist_kwargs)

    if gst.TAG_DATE in data and data[gst.TAG_DATE]:
        date = data[gst.TAG_DATE]
        try:
            date = datetime.date(date.year, date.month, date.day)
        except ValueError:
            pass  # Ignore invalid dates
        else:
            track_kwargs[b'date'] = date.isoformat()

    _retrieve(gst.TAG_TITLE, 'name', track_kwargs)
    _retrieve(gst.TAG_TRACK_NUMBER, 'track_no', track_kwargs)

    # Following keys don't seem to have TAG_* constant.
    _retrieve('album-artist', 'name', albumartist_kwargs)
    _retrieve('musicbrainz-trackid', 'musicbrainz_id', track_kwargs)
    _retrieve('musicbrainz-artistid', 'musicbrainz_id', artist_kwargs)
    _retrieve('musicbrainz-albumid', 'musicbrainz_id', album_kwargs)
    _retrieve(
        'musicbrainz-albumartistid', 'musicbrainz_id', albumartist_kwargs)

    if albumartist_kwargs:
        album_kwargs[b'artists'] = [Artist(**albumartist_kwargs)]

    track_kwargs['uri'] = data['uri']
    track_kwargs['length'] = data[gst.TAG_DURATION]
    track_kwargs['album'] = Album(**album_kwargs)
    track_kwargs['artists'] = [Artist(**artist_kwargs)]

    return Track(**track_kwargs)",0.61594184242644,0.6414722436169737,0.7459809273740693,0.6850560091437755
112,"def parse_options():
    parser = optparse.OptionParser(
        version='Mopidy %s' % versioning.get_version())
    parser.add_option(
        '-q', '--quiet',
        action='store_const', const=0, dest='verbosity_level',
        help='less output (warning level)')
    parser.add_option(
        '-v', '--verbose',
        action='count', default=1, dest='verbosity_level',
        help='more output (debug level)')
    return parser.parse_args(args=mopidy_args)[0]",0.9851318677611088,0.9191483451004732,0.7064487122682759,0.7476608387522807
113,"    def __init__(self, core):
        super(MpdFrontend, self).__init__()
        hostname = network.format_hostname(settings.MPD_SERVER_HOSTNAME)
        port = settings.MPD_SERVER_PORT

        try:
            network.Server(
                hostname, port,
                protocol=session.MpdSession, protocol_kwargs={'core': core},
                max_connections=settings.MPD_SERVER_MAX_CONNECTIONS,
                timeout=settings.MPD_SERVER_CONNECTION_TIMEOUT)
        except IOError as error:
            logger.error(
                'MPD server startup failed: %s',
                encoding.locale_decode(error))
            sys.exit(1)

        logger.info('MPD server running at [%s]:%s', hostname, port)",0.7830907751411168,0.629255346907383,0.9224607676115036,0.9224607676115036
114,"def handle_request(pattern, auth_required=True):
    """"""
    Decorator for connecting command handlers to command requests.

    If you use named groups in the pattern, the decorated method will get the
    groups as keyword arguments. If the group is optional, remember to give the
    argument a default value.

    For example, if the command is ``do that thing`` the ``what`` argument will
    be ``this thing``::

        @handle_request('^do (?P<what>.+)$')
        def do(what):
            ...

    :param pattern: regexp pattern for matching commands
    :type pattern: string
    """"""
    def decorator(func):
        match = re.search('([a-z_]+)', pattern)
        if match is not None:
            mpd_commands.add(
                MpdCommand(name=match.group(), auth_required=auth_required))
        compiled_pattern = re.compile(pattern, flags=re.UNICODE)
        if compiled_pattern in request_handlers:
            raise ValueError('Tried to redefine handler for %s with %s' % (
                pattern, func))
        request_handlers[compiled_pattern] = func
        func.__doc__ = '    - *Pattern:* ``%s``\\n\\n%s' % (
            pattern, func.__doc__ or '')
        return func
    return decorator",0.9916758214067712,0.9916758214067712,0.8618654623623678,0.9750128414241882
115,"    def decorator(func):
        match = re.search('([a-z_]+)', pattern)
        if match is not None:
            mpd_commands.add(
                MpdCommand(name=match.group(), auth_required=auth_required))
        compiled_pattern = re.compile(pattern, flags=re.UNICODE)
        if compiled_pattern in request_handlers:
            raise ValueError('Tried to redefine handler for %s with %s' % (
                pattern, func))
        request_handlers[compiled_pattern] = func
        func.__doc__ = '    - *Pattern:* ``%s``\\n\\n%s' % (
            pattern, func.__doc__ or '')
        return func",0.7934573055217785,0.7751540969645041,0.7447578512193366,0.7577757613039143
116,"def parse_options():
    parser = optparse.OptionParser(
        version='Mopidy %s' % versioning.get_version())
    # NOTE Python 2.6: To support Python versions < 2.6.2rc1 we must use
    # bytestrings for the first argument to ``add_option``
    # See https://github.com/mopidy/mopidy/issues/302 for details
    parser.add_option(
        b'--help-gst',
        action='store_true', dest='help_gst',
        help='show GStreamer help options')
    parser.add_option(
        b'-i', '--interactive',
        action='store_true', dest='interactive',
        help='ask interactively for required settings which are missing')
    parser.add_option(
        b'-q', '--quiet',
        action='store_const', const=0, dest='verbosity_level',
        help='less output (warning level)')
    parser.add_option(
        b'-v', '--verbose',
        action='count', default=1, dest='verbosity_level',
        help='more output (debug level)')
    parser.add_option(
        b'--save-debug-log',
        action='store_true', dest='save_debug_log',
        help='save debug log to ""./mopidy.log""')
    parser.add_option(
        b'--list-settings',
        action='callback',
        callback=settings_utils.list_settings_optparse_callback,
        help='list current settings')
    parser.add_option(
        b'--list-deps',
        action='callback', callback=deps.list_deps_optparse_callback,
        help='list dependencies and their versions')
    parser.add_option(
        b'--debug-thread',
        action='store_true', dest='debug_thread',
        help='run background thread that dumps tracebacks on SIGUSR1')
    return parser.parse_args(args=mopidy_args)[0]",0.9110446812061668,0.8938269378981012,0.7624113438389865,0.8801908895442472
117,"def _convert_mpd_data(data, tracks, music_dir):
    if not data:
        return

    # NOTE: kwargs are explicitly made bytestrings to work on Python
    # 2.6.0/2.6.1. See https://github.com/mopidy/mopidy/issues/302 for details.

    track_kwargs = {}
    album_kwargs = {}
    artist_kwargs = {}
    albumartist_kwargs = {}

    if 'track' in data:
        if '/' in data['track']:
            album_kwargs[b'num_tracks'] = int(data['track'].split('/')[1])
            track_kwargs[b'track_no'] = int(data['track'].split('/')[0])
        else:
            track_kwargs[b'track_no'] = int(data['track'])

    if 'artist' in data:
        artist_kwargs[b'name'] = data['artist']
        albumartist_kwargs[b'name'] = data['artist']

    if 'albumartist' in data:
        albumartist_kwargs[b'name'] = data['albumartist']

    if 'album' in data:
        album_kwargs[b'name'] = data['album']

    if 'title' in data:
        track_kwargs[b'name'] = data['title']

    if 'date' in data:
        track_kwargs[b'date'] = data['date']

    if 'musicbrainz_trackid' in data:
        track_kwargs[b'musicbrainz_id'] = data['musicbrainz_trackid']

    if 'musicbrainz_albumid' in data:
        album_kwargs[b'musicbrainz_id'] = data['musicbrainz_albumid']

    if 'musicbrainz_artistid' in data:
        artist_kwargs[b'musicbrainz_id'] = data['musicbrainz_artistid']

    if 'musicbrainz_albumartistid' in data:
        albumartist_kwargs[b'musicbrainz_id'] = (
            data['musicbrainz_albumartistid'])

    if data['file'][0] == '/':
        path = data['file'][1:]
    else:
        path = data['file']
    path = urllib.unquote(path)

    if artist_kwargs:
        artist = Artist(**artist_kwargs)
        track_kwargs[b'artists'] = [artist]

    if albumartist_kwargs:
        albumartist = Artist(**albumartist_kwargs)
        album_kwargs[b'artists'] = [albumartist]

    if album_kwargs:
        album = Album(**album_kwargs)
        track_kwargs[b'album'] = album

    track_kwargs[b'uri'] = path_to_uri(music_dir, path)
    track_kwargs[b'length'] = int(data.get('time', 0)) * 1000

    track = Track(**track_kwargs)
    tracks.add(track)",0.5587025370353393,0.5711595670308252,0.6939307782610235,0.6764405704862595
118,"    def __init__(self, core):
        super(MpdFrontend, self).__init__()
        hostname = network.format_hostname(settings.MPD_SERVER_HOSTNAME)
        port = settings.MPD_SERVER_PORT

        # NOTE: dict key must be bytestring to work on Python < 2.6.5
        # See https://github.com/mopidy/mopidy/issues/302 for details
        try:
            network.Server(
                hostname, port,
                protocol=session.MpdSession, protocol_kwargs={b'core': core},
                max_connections=settings.MPD_SERVER_MAX_CONNECTIONS,
                timeout=settings.MPD_SERVER_CONNECTION_TIMEOUT)
        except IOError as error:
            logger.error(
                'MPD server startup failed: %s',
                encoding.locale_decode(error))
            sys.exit(1)

        logger.info('MPD server running at [%s]:%s', hostname, port)",0.8033007135603498,0.5959502167997848,0.9804893787178334,0.972707384539717
119,"def handle_request(pattern, auth_required=True):
    """"""
    Decorator for connecting command handlers to command requests.

    If you use named groups in the pattern, the decorated method will get the
    groups as keyword arguments. If the group is optional, remember to give the
    argument a default value.

    For example, if the command is ``do that thing`` the ``what`` argument will
    be ``this thing``::

        @handle_request('^do (?P<what>.+)$')
        def do(what):
            ...

    :param pattern: regexp pattern for matching commands
    :type pattern: string
    """"""
    def decorator(func):
        match = re.search('([a-z_]+)', pattern)
        if match is not None:
            mpd_commands.add(
                MpdCommand(name=match.group(), auth_required=auth_required))
        # NOTE: Make pattern a bytestring to get bytestring keys in the dict
        # returned from matches.groupdict(), which is again used as a **kwargs
        # dict. This is needed to work on Python < 2.6.5. See
        # https://github.com/mopidy/mopidy/issues/302 for details.
        bytestring_pattern = pattern.encode('utf-8')
        compiled_pattern = re.compile(bytestring_pattern, flags=re.UNICODE)
        if compiled_pattern in request_handlers:
            raise ValueError('Tried to redefine handler for %s with %s' % (
                pattern, func))
        request_handlers[compiled_pattern] = func
        func.__doc__ = '    - *Pattern:* ``%s``\\n\\n%s' % (
            pattern, func.__doc__ or '')
        return func
    return decorator",0.628998355730054,0.9430601254792932,0.9926866845623304,0.9926866845623304
120,"    def decorator(func):
        match = re.search('([a-z_]+)', pattern)
        if match is not None:
            mpd_commands.add(
                MpdCommand(name=match.group(), auth_required=auth_required))
        # NOTE: Make pattern a bytestring to get bytestring keys in the dict
        # returned from matches.groupdict(), which is again used as a **kwargs
        # dict. This is needed to work on Python < 2.6.5. See
        # https://github.com/mopidy/mopidy/issues/302 for details.
        bytestring_pattern = pattern.encode('utf-8')
        compiled_pattern = re.compile(bytestring_pattern, flags=re.UNICODE)
        if compiled_pattern in request_handlers:
            raise ValueError('Tried to redefine handler for %s with %s' % (
                pattern, func))
        request_handlers[compiled_pattern] = func
        func.__doc__ = '    - *Pattern:* ``%s``\\n\\n%s' % (
            pattern, func.__doc__ or '')
        return func",0.7732904539771701,0.6873459299130303,0.991238802327855,0.6654763862750914
121,"def query_from_mpd_list_format(field, mpd_query):
    """"""
    Converts an MPD ``list`` query to a Mopidy query.
    """"""
    if mpd_query is None:
        return {}
    try:
        # shlex does not seem to be friends with unicode objects
        tokens = shlex.split(mpd_query.encode('utf-8'))
    except ValueError as error:
        if str(error) == 'No closing quotation':
            raise MpdArgError('Invalid unquoted character', command='list')
        else:
            raise
    tokens = [t.decode('utf-8') for t in tokens]
    if len(tokens) == 1:
        if field == 'album':
            if not tokens[0]:
                raise ValueError
            return {'artist': [tokens[0]]}
        else:
            raise MpdArgError(
                'should be ""Album"" for 3 arguments', command='list')
    elif len(tokens) % 2 == 0:
        query = {}
        while tokens:
            key = tokens[0].lower()
            key = str(key)  # Needed for kwargs keys on OS X and Windows
            value = tokens[1]
            tokens = tokens[2:]
            if key not in ('artist', 'album', 'date', 'genre'):
                raise MpdArgError('not able to parse args', command='list')
            if not value:
                raise ValueError
            if key in query:
                query[key].append(value)
            else:
                query[key] = [value]
        return query
    else:
        raise MpdArgError('not able to parse args', command='list')",0.5197161399165209,0.5262735169657011,0.9959523165475422,0.621340171130473
122,"    def copy(self, **values):
        """"""
        Copy the model with ``field`` updated to new value.

        Examples::

            # Returns a track with a new name
            Track(name='foo').copy(name='bar')
            # Return an album with a new number of tracks
            Album(num_tracks=2).copy(num_tracks=5)

        :param values: the model fields to modify
        :type values: dict
        :rtype: new instance of the model being copied
        """"""
        data = {}
        for key in self.__dict__.keys():
            public_key = key.lstrip('_')
            data[public_key] = values.pop(public_key, self.__dict__[key])
        for key in values.keys():
            if hasattr(self, key):
                data[key] = values.pop(key)
        if values:
            raise TypeError(
                'copy() got an unexpected keyword argument ""%s""' % key)
        return self.__class__(**data)",0.7420900591394224,0.5661941056382658,0.8053358020356702,0.801148736094414
123,"    def __init__(self, *args, **kwargs):
        self.__dict__['artists'] = frozenset(kwargs.pop('artists', []))
        super(Album, self).__init__(*args, **kwargs)",0.626164594290659,0.7787887100964324,0.7928446111855567,0.8168740961613808
124,"    def __init__(self, *args, **kwargs):
        self.__dict__['artists'] = frozenset(kwargs.pop('artists', []))
        super(Track, self).__init__(*args, **kwargs)",0.52925583457947,0.7163113688720988,0.7253370736064008,0.8002967607812621
125,"    def __init__(self, *args, **kwargs):
        if len(args) == 2 and len(kwargs) == 0:
            kwargs['tlid'] = args[0]
            kwargs['track'] = args[1]
            args = []
        super(TlTrack, self).__init__(*args, **kwargs)",0.6604509280751848,0.9651205738001704,0.789869231547085,0.2658931526873198
126,"    def __init__(self, *args, **kwargs):
        self.__dict__['tracks'] = tuple(kwargs.pop('tracks', []))
        super(Playlist, self).__init__(*args, **kwargs)",0.5290846884577977,0.5484438716853239,0.730798415111924,0.5989141945945682
127,"    def __init__(self, *args, **kwargs):
        self.__dict__['tracks'] = tuple(kwargs.pop('tracks', []))
        self.__dict__['artists'] = tuple(kwargs.pop('artists', []))
        self.__dict__['albums'] = tuple(kwargs.pop('albums', []))
        super(SearchResult, self).__init__(*args, **kwargs)",0.3811794575181745,0.3100300646370515,0.6572069610116551,0.5270535869303017
128,"def parse_options():
    parser = optparse.OptionParser(
        version='Mopidy %s' % versioning.get_version())
    # NOTE Python 2.6: To support Python versions < 2.6.2rc1 we must use
    # bytestrings for the first argument to ``add_option``
    # See https://github.com/mopidy/mopidy/issues/302 for details
    parser.add_option(
        b'-q', '--quiet',
        action='store_const', const=0, dest='verbosity_level',
        help='less output (warning level)')
    parser.add_option(
        b'-v', '--verbose',
        action='count', default=1, dest='verbosity_level',
        help='more output (debug level)')
    return parser.parse_args(args=mopidy_args)[0]",0.8697487871785701,0.8324059108521528,0.7594224914614496,0.7548505438688364
129,"def translator(data):
    albumartist_kwargs = {}
    album_kwargs = {}
    artist_kwargs = {}
    track_kwargs = {}

    # NOTE: kwargs are explicitly made bytestrings to work on Python
    # 2.6.0/2.6.1. See https://github.com/mopidy/mopidy/issues/302 for
    # details.

    def _retrieve(source_key, target_key, target):
        if source_key in data:
            target[str(target_key)] = data[source_key]

    _retrieve(gst.TAG_ALBUM, 'name', album_kwargs)
    _retrieve(gst.TAG_TRACK_COUNT, 'num_tracks', album_kwargs)
    _retrieve(gst.TAG_ARTIST, 'name', artist_kwargs)

    if gst.TAG_DATE in data and data[gst.TAG_DATE]:
        date = data[gst.TAG_DATE]
        try:
            date = datetime.date(date.year, date.month, date.day)
        except ValueError:
            pass  # Ignore invalid dates
        else:
            track_kwargs[b'date'] = date.isoformat()

    _retrieve(gst.TAG_TITLE, 'name', track_kwargs)
    _retrieve(gst.TAG_TRACK_NUMBER, 'track_no', track_kwargs)

    # Following keys don't seem to have TAG_* constant.
    _retrieve('album-artist', 'name', albumartist_kwargs)
    _retrieve('musicbrainz-trackid', 'musicbrainz_id', track_kwargs)
    _retrieve('musicbrainz-artistid', 'musicbrainz_id', artist_kwargs)
    _retrieve('musicbrainz-albumid', 'musicbrainz_id', album_kwargs)
    _retrieve(
        'musicbrainz-albumartistid', 'musicbrainz_id', albumartist_kwargs)

    if albumartist_kwargs:
        album_kwargs[b'artists'] = [Artist(**albumartist_kwargs)]

    track_kwargs[b'uri'] = data['uri']
    track_kwargs[b'length'] = data[gst.TAG_DURATION]
    track_kwargs[b'album'] = Album(**album_kwargs)
    track_kwargs[b'artists'] = [Artist(**artist_kwargs)]

    return Track(**track_kwargs)",0.624504132715804,0.6852250690829441,0.6618786588080844,0.8215369068104709
130,"def model_json_decoder(dct):
    """"""
    Automatically deserialize Mopidy models from JSON.

    Usage::

        >>> import json
        >>> json.loads(
        ...     '{""a_track"": {""__model__"": ""Track"", ""name"": ""name""}}',
        ...     object_hook=model_json_decoder)
        {u'a_track': Track(artists=[], name=u'name')}

    """"""
    if '__model__' in dct:
        model_name = dct.pop('__model__')
        cls = globals().get(model_name, None)
        if issubclass(cls, ImmutableObject):
            return cls(**dct)
    return dct",0.498160890398106,0.6811237345204881,0.4,0.7086583569050149
131,"def _convert_mpd_data(data, tracks, music_dir):
    if not data:
        return

    # NOTE kwargs dict keys must be bytestrings to work on Python < 2.6.5
    # See https://github.com/mopidy/mopidy/issues/302 for details.

    track_kwargs = {}
    album_kwargs = {}
    artist_kwargs = {}
    albumartist_kwargs = {}

    if 'track' in data:
        if '/' in data['track']:
            album_kwargs[b'num_tracks'] = int(data['track'].split('/')[1])
            track_kwargs[b'track_no'] = int(data['track'].split('/')[0])
        else:
            track_kwargs[b'track_no'] = int(data['track'])

    if 'artist' in data:
        artist_kwargs[b'name'] = data['artist']
        albumartist_kwargs[b'name'] = data['artist']

    if 'albumartist' in data:
        albumartist_kwargs[b'name'] = data['albumartist']

    if 'album' in data:
        album_kwargs[b'name'] = data['album']

    if 'title' in data:
        track_kwargs[b'name'] = data['title']

    if 'date' in data:
        track_kwargs[b'date'] = data['date']

    if 'musicbrainz_trackid' in data:
        track_kwargs[b'musicbrainz_id'] = data['musicbrainz_trackid']

    if 'musicbrainz_albumid' in data:
        album_kwargs[b'musicbrainz_id'] = data['musicbrainz_albumid']

    if 'musicbrainz_artistid' in data:
        artist_kwargs[b'musicbrainz_id'] = data['musicbrainz_artistid']

    if 'musicbrainz_albumartistid' in data:
        albumartist_kwargs[b'musicbrainz_id'] = (
            data['musicbrainz_albumartistid'])

    if artist_kwargs:
        artist = Artist(**artist_kwargs)
        track_kwargs[b'artists'] = [artist]

    if albumartist_kwargs:
        albumartist = Artist(**albumartist_kwargs)
        album_kwargs[b'artists'] = [albumartist]

    if album_kwargs:
        album = Album(**album_kwargs)
        track_kwargs[b'album'] = album

    if data['file'][0] == '/':
        path = data['file'][1:]
    else:
        path = data['file']
    path = urllib.unquote(path.encode('utf-8'))

    if isinstance(music_dir, unicode):
        music_dir = music_dir.encode('utf-8')

    # Make sure we only pass bytestrings to path_to_uri to avoid implicit
    # decoding of bytestrings to unicode strings
    track_kwargs[b'uri'] = path_to_uri(music_dir, path)

    track_kwargs[b'length'] = int(data.get('time', 0)) * 1000

    track = Track(**track_kwargs)
    tracks.add(track)",0.9511533565989988,0.2478866641619043,0.6512963609145503,0.5959858769823849
132,"    def __init__(self, config, core):
        super(MpdFrontend, self).__init__()
        hostname = network.format_hostname(config['mpd']['hostname'])
        port = config['mpd']['port']

        # NOTE kwargs dict keys must be bytestrings to work on Python < 2.6.5
        # See https://github.com/mopidy/mopidy/issues/302 for details.
        try:
            network.Server(
                hostname, port,
                protocol=session.MpdSession,
                protocol_kwargs={
                    b'config': config,
                    b'core': core,
                },
                max_connections=config['mpd']['max_connections'],
                timeout=config['mpd']['connection_timeout'])
        except IOError as error:
            logger.error(
                'MPD server startup failed: %s',
                encoding.locale_decode(error))
            sys.exit(1)

        logger.info('MPD server running at [%s]:%s', hostname, port)",0.6727386980367273,0.8688895117195916,0.97753017821342,0.9639346607210212
133,"def handle_request(pattern, auth_required=True):
    """"""
    Decorator for connecting command handlers to command requests.

    If you use named groups in the pattern, the decorated method will get the
    groups as keyword arguments. If the group is optional, remember to give the
    argument a default value.

    For example, if the command is ``do that thing`` the ``what`` argument will
    be ``this thing``::

        @handle_request('^do (?P<what>.+)$')
        def do(what):
            ...

    :param pattern: regexp pattern for matching commands
    :type pattern: string
    """"""
    def decorator(func):
        match = re.search('([a-z_]+)', pattern)
        if match is not None:
            mpd_commands.add(
                MpdCommand(name=match.group(), auth_required=auth_required))
        # NOTE Make pattern a bytestring to get bytestring keys in the dict
        # returned from matches.groupdict(), which is again used as a **kwargs
        # dict. This is needed to work on Python < 2.6.5.
        # See https://github.com/mopidy/mopidy/issues/302 for details.
        bytestring_pattern = pattern.encode('utf-8')
        compiled_pattern = re.compile(bytestring_pattern, flags=re.UNICODE)
        if compiled_pattern in request_handlers:
            raise ValueError('Tried to redefine handler for %s with %s' % (
                pattern, func))
        request_handlers[compiled_pattern] = func
        func.__doc__ = '    - *Pattern:* ``%s``\\n\\n%s' % (
            pattern, func.__doc__ or '')
        return func
    return decorator",0.9926866845623304,0.9137938649437848,0.9926866845623304,0.9926866845623304
134,"    def decorator(func):
        match = re.search('([a-z_]+)', pattern)
        if match is not None:
            mpd_commands.add(
                MpdCommand(name=match.group(), auth_required=auth_required))
        # NOTE Make pattern a bytestring to get bytestring keys in the dict
        # returned from matches.groupdict(), which is again used as a **kwargs
        # dict. This is needed to work on Python < 2.6.5.
        # See https://github.com/mopidy/mopidy/issues/302 for details.
        bytestring_pattern = pattern.encode('utf-8')
        compiled_pattern = re.compile(bytestring_pattern, flags=re.UNICODE)
        if compiled_pattern in request_handlers:
            raise ValueError('Tried to redefine handler for %s with %s' % (
                pattern, func))
        request_handlers[compiled_pattern] = func
        func.__doc__ = '    - *Pattern:* ``%s``\\n\\n%s' % (
            pattern, func.__doc__ or '')
        return func",0.7732904539771701,0.6873459299130303,0.991238802327855,0.6026657514705963
135,"def query_from_mpd_list_format(field, mpd_query):
    """"""
    Converts an MPD ``list`` query to a Mopidy query.
    """"""
    # NOTE kwargs dict keys must be bytestrings to work on Python < 2.6.5
    # See https://github.com/mopidy/mopidy/issues/302 for details
    if mpd_query is None:
        return {}
    try:
        # shlex does not seem to be friends with unicode objects
        tokens = shlex.split(mpd_query.encode('utf-8'))
    except ValueError as error:
        if str(error) == 'No closing quotation':
            raise MpdArgError('Invalid unquoted character', command='list')
        else:
            raise
    tokens = [t.decode('utf-8') for t in tokens]
    if len(tokens) == 1:
        if field == 'album':
            if not tokens[0]:
                raise ValueError
            return {b'artist': [tokens[0]]}  # See above NOTE
        else:
            raise MpdArgError(
                'should be ""Album"" for 3 arguments', command='list')
    elif len(tokens) % 2 == 0:
        query = {}
        while tokens:
            key = str(tokens[0].lower())  # See above NOTE
            value = tokens[1]
            tokens = tokens[2:]
            if key not in ('artist', 'album', 'date', 'genre'):
                raise MpdArgError('not able to parse args', command='list')
            if not value:
                raise ValueError
            if key in query:
                query[key].append(value)
            else:
                query[key] = [value]
        return query
    else:
        raise MpdArgError('not able to parse args', command='list')",0.5281482363677312,0.6269872788280179,0.7741336532191312,0.6545610439750088
136,"    def copy(self, **values):
        """"""
        Copy the model with ``field`` updated to new value.

        Examples::

            # Returns a track with a new name
            Track(name='foo').copy(name='bar')
            # Return an album with a new number of tracks
            Album(num_tracks=2).copy(num_tracks=5)

        :param values: the model fields to modify
        :type values: dict
        :rtype: new instance of the model being copied
        """"""
        # NOTE kwargs dict keys must be bytestrings to work on Python < 2.6.5
        # See https://github.com/mopidy/mopidy/issues/302 for details
        data = {}
        for key in self.__dict__.keys():
            public_key = key.lstrip('_')
            data[str(public_key)] = values.pop(public_key, self.__dict__[key])
        for key in values.keys():
            if hasattr(self, key):
                data[str(key)] = values.pop(key)
        if values:
            raise TypeError(
                'copy() got an unexpected keyword argument ""%s""' % key)
        return self.__class__(**data)",0.6492074071263556,0.5043930471177153,0.9914264349828348,0.9914264349828348
137,"def model_json_decoder(dct):
    """"""
    Automatically deserialize Mopidy models from JSON.

    Usage::

        >>> import json
        >>> json.loads(
        ...     '{""a_track"": {""__model__"": ""Track"", ""name"": ""name""}}',
        ...     object_hook=model_json_decoder)
        {u'a_track': Track(artists=[], name=u'name')}

    """"""
    # NOTE kwargs dict keys must be bytestrings to work on Python < 2.6.5
    # See https://github.com/mopidy/mopidy/issues/302 for details.
    if '__model__' in dct:
        model_name = dct.pop('__model__')
        cls = globals().get(model_name, None)
        if issubclass(cls, ImmutableObject):
            kwargs = {}
            for key, value in dct.items():
                kwargs[str(key)] = value
            return cls(**kwargs)
    return dct",0.5451684461707136,0.5903371458845336,0.7643137764942279,0.6598313515971042
138,"    def __init__(self, *args, **kwargs):
        # NOTE kwargs dict keys must be bytestrings to work on Python < 2.6.5
        # See https://github.com/mopidy/mopidy/issues/302 for details
        self.__dict__[b'artists'] = frozenset(kwargs.pop('artists', []))
        self.__dict__[b'images'] = frozenset(kwargs.pop('images', []))
        super(Album, self).__init__(*args, **kwargs)",0.5418483969131471,0.6168733222320596,0.958889151903783,0.7963404202183495
139,"    def __init__(self, *args, **kwargs):
        # NOTE kwargs dict keys must be bytestrings to work on Python < 2.6.5
        # See https://github.com/mopidy/mopidy/issues/302 for details
        self.__dict__[b'artists'] = frozenset(kwargs.pop('artists', []))
        super(Track, self).__init__(*args, **kwargs)",0.6757238791166307,0.7198518322462001,0.826735677854769,0.9341058415394417
140,"    def __init__(self, *args, **kwargs):
        # NOTE kwargs dict keys must be bytestrings to work on Python < 2.6.5
        # See https://github.com/mopidy/mopidy/issues/302 for details
        if len(args) == 2 and len(kwargs) == 0:
            kwargs[b'tlid'] = args[0]
            kwargs[b'track'] = args[1]
            args = []
        super(TlTrack, self).__init__(*args, **kwargs)",0.7788368479186035,0.9167747540351976,0.9842880738726422,0.406777404463383
141,"    def __init__(self, *args, **kwargs):
        # NOTE kwargs dict keys must be bytestrings to work on Python < 2.6.5
        # See https://github.com/mopidy/mopidy/issues/302 for details
        self.__dict__[b'tracks'] = tuple(kwargs.pop('tracks', []))
        super(Playlist, self).__init__(*args, **kwargs)",0.7387767402279043,0.5918796610498712,0.7811998531687332,0.9780488659200576
142,"    def __init__(self, *args, **kwargs):
        # NOTE kwargs dict keys must be bytestrings to work on Python < 2.6.5
        # See https://github.com/mopidy/mopidy/issues/302 for details
        self.__dict__[b'tracks'] = tuple(kwargs.pop('tracks', []))
        self.__dict__[b'artists'] = tuple(kwargs.pop('artists', []))
        self.__dict__[b'albums'] = tuple(kwargs.pop('albums', []))
        super(SearchResult, self).__init__(*args, **kwargs)",0.5698364913673517,0.528014479089574,0.7112094709587027,0.7260042804800755
143,"def parse_options():
    parser = optparse.OptionParser(
        version='Mopidy %s' % versioning.get_version())
    # NOTE First argument to add_option must be bytestrings on Python < 2.6.2
    # See https://github.com/mopidy/mopidy/issues/302 for details
    parser.add_option(
        b'-q', '--quiet',
        action='store_const', const=0, dest='verbosity_level',
        help='less output (warning level)')
    parser.add_option(
        b'-v', '--verbose',
        action='count', default=1, dest='verbosity_level',
        help='more output (debug level)')
    return parser.parse_args(args=mopidy_args)[0]",0.8490206459917577,0.8870986866012232,0.5973405757296351,0.6667898127350215
144,"def translator(data):
    albumartist_kwargs = {}
    album_kwargs = {}
    artist_kwargs = {}
    track_kwargs = {}

    # NOTE kwargs dict keys must be bytestrings to work on Python < 2.6.5
    # See https://github.com/mopidy/mopidy/issues/302 for details.

    def _retrieve(source_key, target_key, target):
        if source_key in data:
            target[str(target_key)] = data[source_key]

    _retrieve(gst.TAG_ALBUM, 'name', album_kwargs)
    _retrieve(gst.TAG_TRACK_COUNT, 'num_tracks', album_kwargs)
    _retrieve(gst.TAG_ARTIST, 'name', artist_kwargs)

    if gst.TAG_DATE in data and data[gst.TAG_DATE]:
        date = data[gst.TAG_DATE]
        try:
            date = datetime.date(date.year, date.month, date.day)
        except ValueError:
            pass  # Ignore invalid dates
        else:
            track_kwargs[b'date'] = date.isoformat()

    _retrieve(gst.TAG_TITLE, 'name', track_kwargs)
    _retrieve(gst.TAG_TRACK_NUMBER, 'track_no', track_kwargs)

    # Following keys don't seem to have TAG_* constant.
    _retrieve('album-artist', 'name', albumartist_kwargs)
    _retrieve('musicbrainz-trackid', 'musicbrainz_id', track_kwargs)
    _retrieve('musicbrainz-artistid', 'musicbrainz_id', artist_kwargs)
    _retrieve('musicbrainz-albumid', 'musicbrainz_id', album_kwargs)
    _retrieve(
        'musicbrainz-albumartistid', 'musicbrainz_id', albumartist_kwargs)

    if albumartist_kwargs:
        album_kwargs[b'artists'] = [Artist(**albumartist_kwargs)]

    track_kwargs[b'uri'] = data['uri']
    track_kwargs[b'length'] = data[gst.TAG_DURATION]
    track_kwargs[b'album'] = Album(**album_kwargs)
    track_kwargs[b'artists'] = [Artist(**artist_kwargs)]

    return Track(**track_kwargs)",0.5993182275071548,0.6864188161535673,0.8225944654673321,0.6035764721658301
145,"    def _retrieve(source_key, target_key, target):
        if source_key in data:
            target[str(target_key)] = data[source_key]",0.4,0.3277072644409081,0.4,0.3073276196714815
146,"    def _select_mixer_track(self, mixer, track_label):
        # Look for track with label == MIXER_TRACK, otherwise fallback to
        # master track which is also an output.
        for track in mixer.list_tracks():
            if track_label:
                if track.label == track_label:
                    return track
            elif track.flags & (gst.interfaces.MIXER_TRACK_MASTER |
                                gst.interfaces.MIXER_TRACK_OUTPUT):
                return track",0.6284502750907429,0.4443767805464103,0.8360424654891663,0.8854613549871044
147,"def parse_m3u(file_path):
    """"""
    Convert M3U file list of uris

    Example M3U data::

        # This is a comment
        Alternative\\Band - Song.mp3
        Classical\\Other Band - New Song.mp3
        Stuff.mp3
        D:\\More Music\\Foo.mp3
        http://www.example.com:8000/Listen.pls
        http://www.example.com/~user/Mine.mp3

    - Relative paths of songs should be with respect to location of M3U.
    - Paths are normaly platform specific.
    - Lines starting with # should be ignored.
    - m3u files are latin-1.
    - This function does not bother with Extended M3U directives.
    """"""

    uris = []
    folder = os.path.dirname(file_path)

    try:
        with open(file_path) as m3u:
            contents = m3u.readlines()
    except IOError as error:
        logger.error('Couldn\\'t open m3u: %s', locale_decode(error))
        return uris

    for line in contents:
        line = line.strip().decode('latin1')

        if line.startswith('#'):
            continue

        # FIXME what about other URI types?
        if line.startswith('file://'):
            uris.append(line)
        else:
            path = path_to_uri(folder, line)
            uris.append(path)

    return uris",0.6806506173173992,0.6272334891007953,0.6909440600190995,0.7630973572131199
148,"    def __init__(self, folder, data_callback, error_callback=None):
        self.uris = [path_to_uri(f) for f in find_files(folder)]
        self.data_callback = data_callback
        self.error_callback = error_callback
        self.loop = gobject.MainLoop()

        fakesink = gst.element_factory_make('fakesink')

        self.uribin = gst.element_factory_make('uridecodebin')
        self.uribin.set_property('caps', gst.Caps('audio/x-raw-int'))
        self.uribin.connect('pad-added', self.process_new_pad,
            fakesink.get_pad('sink'))

        self.pipe = gst.element_factory_make('pipeline')
        self.pipe.add(self.uribin)
        self.pipe.add(fakesink)

        bus = self.pipe.get_bus()
        bus.add_signal_watch()
        bus.connect('message::tag', self.process_tags)
        bus.connect('message::error', self.process_error)",0.6530084632426043,0.641393659795421,0.6614953390964344,0.7082161946738659
149,"    def next_uri(self):
        if not self.uris:
            return self.stop()

        self.pipe.set_state(gst.STATE_NULL)
        self.uribin.set_property('uri', self.uris.pop())
        self.pipe.set_state(gst.STATE_PAUSED)",0.7068753388646793,0.706698119520859,0.6564358744858098,0.8897897140388837
150,"    def start(self):
        if not self.uris:
            return
        self.next_uri()
        self.loop.run()",0.7600883950627211,0.6470065085972411,0.7509699001663637,0.4928177270512513
151,"def import_targets(request):
    context = {}
    context['import_target_li'] = 'active'
    context['target_data_active'] = 'true'
    if request.method == 'POST':
        if 'txtFile' in request.FILES:
            txt_file = request.FILES['txtFile']
            if txt_file.content_type == 'text/plain':
                target_count = 0
                txt_content = txt_file.read().decode('UTF-8')
                io_string = io.StringIO(txt_content)
                for target in io_string:
                    if validators.domain(target):
                        Domain.objects.create(
                            domain_name=target, insert_date=timezone.now())
                        target_count += 1
                if target_count:
                    messages.add_message(request, messages.SUCCESS, str(
                        target_count) + ' targets added successfully!')
                    return http.HttpResponseRedirect(reverse('list_target'))
                else:
                    messages.add_message(
                        request,
                        messages.ERROR,
                        'Oops! File format was invalid, could not import any targets.')
            else:
                messages.add_message(
                    request, messages.ERROR, 'Invalid File type!')
        elif 'csvFile' in request.FILES:
            csv_file = request.FILES['csvFile']
            if csv_file.content_type == 'text/csv':
                target_count = 0
                csv_content = csv_file.read().decode('UTF-8')
                io_string = io.StringIO(csv_content)
                for column in csv.reader(io_string, delimiter=','):
                    if validators.domain(column[0]):
                        Domain.objects.create(
                            domain_name=column[0],
                            domain_description=column[1],
                            insert_date=timezone.now())
                        target_count += 1
                if target_count:
                    messages.add_message(request, messages.SUCCESS, str(
                        target_count) + ' targets added successfully!')
                    return http.HttpResponseRedirect(reverse('list_target'))
                else:
                    messages.add_message(
                        request,
                        messages.ERROR,
                        'Oops! File format was invalid, could not import any targets.')
            else:
                messages.add_message(
                    request, messages.ERROR, 'Invalid File type!')
    return render(request, 'target/import.html', context)",0.8573504931042166,0.7587151999995239,0.6714921863846507,0.9960877063793252
152,"    def __init__(self, key_prefixes):
        key_prefixes = map(self._sanitize, key_prefixes)
        # compute read and write dirs from base runtime dirs: the first base
        # dir is selected for writes and prefered for reads
        self._read_dirs = [os.path.join(x, *key_prefixes) for x in get_runtime_dirs()]
        self._write_dir = self._read_dirs[0]
        os.makedirs(self._write_dir, exist_ok=True)
        if sys.platform == 'linux':
            # set the sticky bit to prevent removal during cleanup
            os.chmod(self._write_dir, 0o1700)
        _LOGGER.debug('data in %s', self._write_dir)",0.4,0.955728683205205,0.6863332465251291,0.9415346450288272
153,"    def load(self, key):
        for base in self._read_dirs:
            path = os.path.join(base, key)
            if not os.path.isfile(path):
                continue
            try:
                with open(path, mode='r') as f:
                    data = f.read().strip()
                    if len(data) == 0:
                        value = None
                    else:
                        value = literal_eval(data)
                    _LOGGER.debug('loaded %s=%r (from %s)', key, value, path)
            except OSError as err:
                _LOGGER.warning('%s exists but cannot be read: %s', path, err)
                continue
            return value
        _LOGGER.debug('no data (file) found for %s', key)
        return None",0.9754547428377576,0.9199619844214916,0.8092754148561047,0.709751761412079
154,"    def _write(self, data):
        padding = [0x0]*(_WRITE_LENGTH - len(data))
        self.device.write(data + padding)",0.7557724115614449,0.5657233509670347,0.7689993364025332,0.4809125208055899
155,"    def _read(self):
        return self.device.read(_READ_LENGTH)",0.598272085854756,0.7370747521518928,0.4,0.8594737809859948
156,"    def connect(self, **kwargs):
        """"""Connect to the device.""""""
        super().connect(**kwargs)
        ids = f'vid{self.vendor_id:04x}_pid{self.product_id:04x}'
        # must use the HID path because there is no serial number; however,
        # these can be quite long on Windows and macOS, so only take the
        # numbers, since they are likely the only parts that vary between two
        # devices of the same model
        loc = 'loc' + '_'.join((num.decode() for num in re.findall(b'\\\\d+', self.address)))
        self._data = RuntimeStorage(key_prefixes=[ids, loc])
        self._sequence = _sequence(self._data)",0.7327607924215951,0.5563842110693735,0.9674185789552656,0.5880339250595823
157,"    def connect(self, **kwargs):
        """"""Connect to the device.""""""
        super().connect()
        try:
            self._open()
        except usb.core.USBError as err:
            LOGGER.warning('report: failed to open right away, will close first')
            LOGGER.debug(err, exc_info=True)
            self._close()
            self._open()
        finally:
            self.device.release()",0.7417696981523272,0.5668052273520908,0.824503791078373,0.646786258004825
158,"    def disconnect(self, **kwargs):
        """"""Disconnect from the device.""""""
        self._close()
        super().disconnect()
        self.device.release()",0.8968806993720946,0.636457765032224,0.9003103419616046,0.6996214870774635
159,"    def dump(self):
        """"""
        Returns the string that represents the nyan file.
        """"""
        output_str = f""# NYAN FILE\\nversion {FILE_VERSION}\\n\\n""

        import_aliases = self.import_tree.establish_import_dict(self,
                                                                ignore_names=[""type"", ""types""])

        for alias, fqon in import_aliases.items():
            output_str += ""import ""

            output_str += ""."".join(fqon)

            output_str += f"" as {alias}\\n""

        output_str += ""\\n""

        for nyan_object in self.nyan_objects:
            output_str += nyan_object.dump(import_tree=self.import_tree)

        self.import_tree.clear_marks()

        # Removes one empty line at the end of the file
        output_str = output_str[:-1]

        return output_str",0.7144813747789743,0.7228684192583037,0.6889182629652262,0.9908823652090128
160,"def convert_assets(assets, args, srcdir=None, prev_source_dir_path=None):
    """"""
    Perform asset conversion.

    Requires original assets and stores them in usable and free formats.

    assets must be a filesystem-like object pointing at the game's asset dir.
    srcdir must be None, or point at some source directory.

    If gen_extra_files is True, some more files, mostly for debugging purposes,
    are created.

    This method prepares srcdir and targetdir to allow a pleasant, unified
    conversion experience, then passes them to .driver.convert().
    """"""
    # acquire conversion source directory
    if srcdir is None:
        srcdir = acquire_conversion_source_dir(prev_source_dir_path)

    converted_path = assets / ""converted""
    converted_path.mkdirs()
    targetdir = DirectoryCreator(converted_path).root

    # Set compression level for media output if it was not set
    if ""compression_level"" not in vars(args):
        args.compression_level = 1

    # Set verbosity for debug output
    if ""debug_info"" not in vars(args):
        if args.devmode:
            args.debug_info = 3

        else:
            args.debug_info = 0

    # add a dir for debug info
    debug_log_path = converted_path / ""debug"" / datetime.now().strftime(""%Y-%m-%d-%H-%M-%S"")
    debugdir = DirectoryCreator(debug_log_path).root
    args.debugdir = AccessSynchronizer(debugdir).root

    # Create CLI args info
    debug_cli_args(args.debugdir, args.debug_info, args)

    # Initialize game versions data
    auxiliary_files_dir = args.cfg_dir / ""converter"" / ""games""
    args.avail_game_eds, args.avail_game_exps = create_version_objects(auxiliary_files_dir)

    # Acquire game version info
    args.game_version = get_game_version(srcdir, args.avail_game_eds, args.avail_game_exps)
    debug_game_version(args.debugdir, args.debug_info, args)

    # Mount assets into conversion folder
    data_dir = mount_asset_dirs(srcdir, args.game_version)
    if not data_dir:
        return None

    # make srcdir and targetdir safe for threaded conversion
    args.srcdir = AccessSynchronizer(data_dir).root
    args.targetdir = AccessSynchronizer(targetdir).root

    # Create mountpoint info
    debug_mounts(args.debugdir, args.debug_info, args)

    def flag(name):
        """"""
        Convenience function for accessing boolean flags in args.
        Flags default to False if they don't exist.
        """"""
        return getattr(args, name, False)

    args.flag = flag

    # import here so codegen.py doesn't depend on it.
    from .tool.driver import convert

    converted_count = 0
    total_count = None
    for current_item in convert(args):
        if isinstance(current_item, int):
            # convert is informing us about the estimated number of remaining
            # items.
            total_count = current_item + converted_count
            continue

        # TODO a GUI would be nice here.

        if total_count is None:
            info(""[%s] %s"", converted_count, current_item)
        else:
            info(""[%s] %s"", format_progress(converted_count, total_count), current_item)

        converted_count += 1

    # clean args
    del args.srcdir
    del args.targetdir

    return data_dir.resolve_native_path()",0.907237549414136,0.952894503144696,0.6632005780455372,0.7180307489347686
161,"    def _get_aoe2_base(cls, gamedata):
        """"""
        Create the aoe2-base modpack.
        """"""
        modpack = Modpack(""aoe2-base"")

        mod_def = modpack.get_info()

        mod_def.set_version(""1.0c"")
        mod_def.set_uid(2000)

        mod_def.add_assets_to_load(""data/*"")

        cls.organize_nyan_objects(modpack, gamedata)
        cls.organize_media_objects(modpack, gamedata)

        return modpack",0.7284068702080915,0.8726898654934412,0.7715163890456618,0.8757649549127864
162,"    def organize_nyan_objects(modpack, full_data_set):
        """"""
        Put available nyan objects into a given modpack.
        """"""
        created_nyan_files = {}

        # Access all raw API objects
        raw_api_objects = []
        raw_api_objects.extend(full_data_set.pregen_nyan_objects.values())

        for unit_line in full_data_set.unit_lines.values():
            raw_api_objects.extend(unit_line.get_raw_api_objects().values())

        for building_line in full_data_set.building_lines.values():
            raw_api_objects.extend(building_line.get_raw_api_objects().values())

        for ambient_group in full_data_set.ambient_groups.values():
            raw_api_objects.extend(ambient_group.get_raw_api_objects().values())

        for variant_group in full_data_set.variant_groups.values():
            raw_api_objects.extend(variant_group.get_raw_api_objects().values())

        for tech_group in full_data_set.tech_groups.values():
            raw_api_objects.extend(tech_group.get_raw_api_objects().values())

        for terrain_group in full_data_set.terrain_groups.values():
            raw_api_objects.extend(terrain_group.get_raw_api_objects().values())

        for civ_group in full_data_set.civ_groups.values():
            raw_api_objects.extend(civ_group.get_raw_api_objects().values())

        # Create the files
        for raw_api_object in raw_api_objects:
            obj_location = raw_api_object.get_location()

            if isinstance(obj_location, ForwardRef):
                # Resolve location and add nested object
                nyan_object = obj_location.resolve()
                nyan_object.add_nested_object(raw_api_object.get_nyan_object())
                continue

            obj_filename = raw_api_object.get_filename()
            nyan_file_path = f""{modpack.info.name}/{obj_location}{obj_filename}""

            if nyan_file_path in created_nyan_files.keys():
                nyan_file = created_nyan_files[nyan_file_path]

            else:
                nyan_file = NyanFile(obj_location, obj_filename,
                                     modpack.info.name)
                created_nyan_files.update({nyan_file.get_relative_file_path(): nyan_file})
                modpack.add_data_export(nyan_file)

            nyan_file.add_nyan_object(raw_api_object.get_nyan_object())

        # Create an import tree from the files
        import_tree = ImportTree()

        for nyan_file in created_nyan_files.values():
            import_tree.expand_from_file(nyan_file)

        for nyan_object in full_data_set.nyan_api_objects.values():
            import_tree.expand_from_object(nyan_object)

        for nyan_file in created_nyan_files.values():
            nyan_file.set_import_tree(import_tree)",0.5682643308196937,0.5040048190915383,0.7448765074977668,0.6344133240788752
163,"    def generate_modifiers(full_data_set, pregen_converter_group):
        """"""
        Generate standard modifiers.

        :param full_data_set: GenieObjectContainer instance that
                              contains all relevant data for the conversion
                              process.
        :type full_data_set: ...dataformat.aoc.genie_object_container.GenieObjectContainer
        :param pregen_converter_group: GenieObjectGroup instance that stores
                                       pregenerated API objects for referencing with
                                       ForwardRef
        :type pregen_converter_group: ...dataformat.aoc.genie_object_container.GenieObjectGroup
        """"""
        pregen_nyan_objects = full_data_set.pregen_nyan_objects
        api_objects = full_data_set.nyan_api_objects

        modifier_parent = ""engine.modifier.multiplier.MultiplierModifier""
        type_parent = ""engine.modifier.multiplier.effect.flat_attribute_change.type.Flyover""
        types_location = ""data/aux/modifier/flyover_cliff""

        # =======================================================================
        # Flyover effect multiplier
        # =======================================================================
        modifier_ref_in_modpack = ""aux.modifier.flyover_cliff.AttackMultiplierFlyover""
        modifier_raw_api_object = RawAPIObject(modifier_ref_in_modpack,
                                               ""AttackMultiplierFlyover"", api_objects,
                                               types_location)
        modifier_raw_api_object.set_filename(""flyover_cliff"")
        modifier_raw_api_object.add_raw_parent(type_parent)

        pregen_converter_group.add_raw_api_object(modifier_raw_api_object)
        pregen_nyan_objects.update({modifier_ref_in_modpack: modifier_raw_api_object})

        # Increases effect value by 25%
        modifier_raw_api_object.add_raw_member(""multiplier"",
                                               1.25,
                                               modifier_parent)

        # Relative angle to cliff must not be larger than 90°
        modifier_raw_api_object.add_raw_member(""relative_angle"",
                                               90,
                                               type_parent)

        # Affects all cliffs
        types = [ForwardRef(pregen_converter_group, ""aux.game_entity_type.types.Cliff"")]
        modifier_raw_api_object.add_raw_member(""flyover_types"",
                                               types,
                                               type_parent)
        modifier_raw_api_object.add_raw_member(""blacklisted_entities"",
                                               [],
                                               type_parent)

        # =======================================================================
        # Elevation difference effect multiplier (higher unit)
        # =======================================================================
        modifier_parent = ""engine.modifier.multiplier.MultiplierModifier""
        type_parent = ""engine.modifier.multiplier.effect.flat_attribute_change.type.ElevationDifferenceHigh""
        types_location = ""data/aux/modifier/elevation_difference""

        modifier_ref_in_modpack = ""aux.modifier.elevation_difference.AttackMultiplierHigh""
        modifier_raw_api_object = RawAPIObject(modifier_ref_in_modpack,
                                               ""AttackMultiplierHigh"", api_objects,
                                               types_location)
        modifier_raw_api_object.set_filename(""elevation_difference"")
        modifier_raw_api_object.add_raw_parent(type_parent)

        pregen_converter_group.add_raw_api_object(modifier_raw_api_object)
        pregen_nyan_objects.update({modifier_ref_in_modpack: modifier_raw_api_object})

        # Increases effect value to 125%
        modifier_raw_api_object.add_raw_member(""multiplier"",
                                               1.25,
                                               modifier_parent)

        # Min elevation difference is not set

        # =======================================================================
        # Elevation difference effect multiplier (lower unit)
        # =======================================================================
        modifier_parent = ""engine.modifier.multiplier.MultiplierModifier""
        type_parent = ""engine.modifier.multiplier.effect.flat_attribute_change.type.ElevationDifferenceLow""
        types_location = ""data/aux/modifier/elevation_difference""

        modifier_ref_in_modpack = ""aux.modifier.elevation_difference.AttackMultiplierLow""
        modifier_raw_api_object = RawAPIObject(modifier_ref_in_modpack,
                                               ""AttackMultiplierLow"", api_objects,
                                               types_location)
        modifier_raw_api_object.set_filename(""elevation_difference"")
        modifier_raw_api_object.add_raw_parent(type_parent)

        pregen_converter_group.add_raw_api_object(modifier_raw_api_object)
        pregen_nyan_objects.update({modifier_ref_in_modpack: modifier_raw_api_object})

        # Decreases effect value to 75%
        modifier_raw_api_object.add_raw_member(""multiplier"",
                                               0.75,
                                               modifier_parent)",0.8541673391979606,0.673888206061789,0.604759172596837,0.6499490558226817
164,"    def live_ability(converter_group, line, container_obj_ref, diff=None):
        """"""
        Creates a patch for the Live ability of a line.

        :param converter_group: Group that gets the patch.
        :type converter_group: ...dataformat.converter_object.ConverterObjectGroup
        :param line: Unit/Building line that has the ability.
        :type line: ...dataformat.converter_object.ConverterObjectGroup
        :param container_obj_ref: Reference of the raw API object the patch is nested in.
        :type container_obj_ref: str
        :param diff: A diff between two ConvertObject instances.
        :type diff: ...dataformat.converter_object.ConverterObject
        :returns: The forward references for the generated patches.
        :rtype: list
        """"""
        head_unit_id = line.get_head_unit_id()
        tech_id = converter_group.get_id()
        dataset = line.data

        patches = []

        name_lookup_dict = internal_name_lookups.get_entity_lookups(dataset.game_version)
        tech_lookup_dict = internal_name_lookups.get_tech_lookups(dataset.game_version)

        game_entity_name = name_lookup_dict[head_unit_id][0]

        if diff:
            diff_hp = diff[""hit_points""]
            if isinstance(diff_hp, NoDiffMember):
                return patches

            diff_hp_value = diff_hp.get_value()

        else:
            return patches

        patch_target_ref = f""{game_entity_name}.Live.Health""
        patch_target_forward_ref = ForwardRef(line, patch_target_ref)

        # Wrapper
        wrapper_name = f""Change{game_entity_name}HealthWrapper""
        wrapper_ref = f""{container_obj_ref}.{wrapper_name}""
        wrapper_raw_api_object = RawAPIObject(wrapper_ref,
                                              wrapper_name,
                                              dataset.nyan_api_objects)
        wrapper_raw_api_object.add_raw_parent(""engine.aux.patch.Patch"")

        if isinstance(line, GenieBuildingLineGroup):
            # Store building upgrades next to their game entity definition,
            # not in the Age up techs.
            wrapper_raw_api_object.set_location(""data/game_entity/generic/%s/""
                                                % (name_lookup_dict[head_unit_id][1]))
            wrapper_raw_api_object.set_filename(f""{tech_lookup_dict[tech_id][1]}_upgrade"")

        else:
            wrapper_raw_api_object.set_location(ForwardRef(converter_group, container_obj_ref))

        # Nyan patch
        nyan_patch_name = f""Change{game_entity_name}Health""
        nyan_patch_ref = f""{container_obj_ref}.{wrapper_name}.{nyan_patch_name}""
        nyan_patch_location = ForwardRef(converter_group, wrapper_ref)
        nyan_patch_raw_api_object = RawAPIObject(nyan_patch_ref,
                                                 nyan_patch_name,
                                                 dataset.nyan_api_objects,
                                                 nyan_patch_location)
        nyan_patch_raw_api_object.add_raw_parent(""engine.aux.patch.NyanPatch"")
        nyan_patch_raw_api_object.set_patch_target(patch_target_forward_ref)

        # HP max value
        nyan_patch_raw_api_object.add_raw_patch_member(""max_value"",
                                                       diff_hp_value,
                                                       ""engine.aux.attribute.AttributeSetting"",
                                                       MemberOperator.ADD)

        patch_forward_ref = ForwardRef(converter_group, nyan_patch_ref)
        wrapper_raw_api_object.add_raw_member(""patch"",
                                              patch_forward_ref,
                                              ""engine.aux.patch.Patch"")

        converter_group.add_raw_api_object(wrapper_raw_api_object)
        converter_group.add_raw_api_object(nyan_patch_raw_api_object)

        wrapper_forward_ref = ForwardRef(converter_group, wrapper_ref)
        patches.append(wrapper_forward_ref)

        return patches",0.2957121658842374,0.7275094852022104,0.6945669745540968,0.6486670213751019
165,"    def hp_upgrade(converter_group, line, value, operator, team=False):
        """"""
        Creates a patch for the HP modify effect (ID: 0).

        :param converter_group: Tech/Civ that gets the patch.
        :type converter_group: ...dataformat.converter_object.ConverterObjectGroup
        :param line: Unit/Building line that has the ability.
        :type line: ...dataformat.converter_object.ConverterObjectGroup
        :param value: Value used for patching the member.
        :type value: MemberOperator
        :param operator: Operator used for patching the member.
        :type operator: MemberOperator
        :returns: The forward references for the generated patches.
        :rtype: list
        """"""
        head_unit_id = line.get_head_unit_id()
        dataset = line.data

        patches = []

        obj_id = converter_group.get_id()
        if isinstance(converter_group, GenieTechEffectBundleGroup):
            tech_lookup_dict = internal_name_lookups.get_tech_lookups(dataset.game_version)
            obj_name = tech_lookup_dict[obj_id][0]

        else:
            civ_lookup_dict = internal_name_lookups.get_civ_lookups(dataset.game_version)
            obj_name = civ_lookup_dict[obj_id][0]

        name_lookup_dict = internal_name_lookups.get_entity_lookups(dataset.game_version)

        game_entity_name = name_lookup_dict[head_unit_id][0]

        patch_target_ref = f""{game_entity_name}.Live.Health""
        patch_target_forward_ref = ForwardRef(line, patch_target_ref)

        # Wrapper
        wrapper_name = f""Change{game_entity_name}MaxHealthWrapper""
        wrapper_ref = f""{obj_name}.{wrapper_name}""
        wrapper_location = ForwardRef(converter_group, obj_name)
        wrapper_raw_api_object = RawAPIObject(wrapper_ref,
                                              wrapper_name,
                                              dataset.nyan_api_objects,
                                              wrapper_location)
        wrapper_raw_api_object.add_raw_parent(""engine.aux.patch.Patch"")

        # Nyan patch
        nyan_patch_name = f""Change{game_entity_name}MaxHealth""
        nyan_patch_ref = f""{obj_name}.{wrapper_name}.{nyan_patch_name}""
        nyan_patch_location = ForwardRef(converter_group, wrapper_ref)
        nyan_patch_raw_api_object = RawAPIObject(nyan_patch_ref,
                                                 nyan_patch_name,
                                                 dataset.nyan_api_objects,
                                                 nyan_patch_location)
        nyan_patch_raw_api_object.add_raw_parent(""engine.aux.patch.NyanPatch"")
        nyan_patch_raw_api_object.set_patch_target(patch_target_forward_ref)

        nyan_patch_raw_api_object.add_raw_patch_member(""max_value"",
                                                       value,
                                                       ""engine.aux.attribute.AttributeSetting"",
                                                       operator)

        patch_forward_ref = ForwardRef(converter_group, nyan_patch_ref)
        wrapper_raw_api_object.add_raw_member(""patch"",
                                              patch_forward_ref,
                                              ""engine.aux.patch.Patch"")

        if team:
            wrapper_raw_api_object.add_raw_parent(""engine.aux.patch.type.DiplomaticPatch"")
            stances = [
                dataset.nyan_api_objects[""engine.aux.diplomatic_stance.type.Self""],
                dataset.pregen_nyan_objects[""aux.diplomatic_stance.types.Friendly""].get_nyan_object()
            ]
            wrapper_raw_api_object.add_raw_member(""stances"",
                                                  stances,
                                                  ""engine.aux.patch.type.DiplomaticPatch"")

        converter_group.add_raw_api_object(wrapper_raw_api_object)
        converter_group.add_raw_api_object(nyan_patch_raw_api_object)

        wrapper_forward_ref = ForwardRef(converter_group, wrapper_ref)
        patches.append(wrapper_forward_ref)

        return patches",0.642067423240084,0.599342808069679,0.6742834128973418,0.5822812879181983
166,"    def _get_aoe2_base(cls, gamedata):
        """"""
        Create the aoe2-base modpack.
        """"""
        modpack = Modpack(""de2-base"")

        mod_def = modpack.get_info()

        mod_def.set_version(""TODO"")
        mod_def.set_uid(2000)

        mod_def.add_assets_to_load(""data/*"")

        AoCModpackSubprocessor.organize_nyan_objects(modpack, gamedata)
        AoCModpackSubprocessor.organize_media_objects(modpack, gamedata)

        return modpack",0.6460628668874863,0.8732856602375385,0.8087376064512697,0.7218862484830076
167,"    def _get_aoe1_base(cls, gamedata):
        """"""
        Create the aoe1-base modpack.
        """"""
        modpack = Modpack(""aoe1-base"")

        mod_def = modpack.get_info()

        mod_def.set_version(""1.0B"")
        mod_def.set_uid(1000)

        mod_def.add_assets_to_load(""data/*"")

        AoCModpackSubprocessor.organize_nyan_objects(modpack, gamedata)
        AoCModpackSubprocessor.organize_media_objects(modpack, gamedata)

        return modpack",0.6425605059395048,0.7690017384275228,0.7578474897309564,0.8198833833708021
168,"    def _get_swgb_base(cls, gamedata):
        """"""
        Create the swgb-base modpack.
        """"""
        modpack = Modpack(""swgb-base"")

        mod_def = modpack.get_info()

        mod_def.set_version(""GOG"")
        mod_def.set_uid(5000)

        mod_def.add_assets_to_load(""data/*"")

        AoCModpackSubprocessor.organize_nyan_objects(modpack, gamedata)
        AoCModpackSubprocessor.organize_media_objects(modpack, gamedata)

        return modpack",0.6684396771030151,0.7356659452118406,0.6627836604751162,0.7029139499554599
169,"    def expand_from_file(self, nyan_file):
        """"""
        Expands the tree from a nyan file.

        :param nyan_file: File with nyan objects.
        :type nyan_file: .convert.export.formats.nyan_file.NyanFile
        """"""
        # Process fqon of the file
        current_node = self.root
        fqon = nyan_file.get_fqon()
        node_type = NodeType.FILESYS

        for node_str in fqon:
            if current_node.has_child(node_str):
                # Choose the already created node
                current_node = current_node.get_child(node_str)

            else:
                # Add a new node
                new_node = Node(node_str, node_type, current_node)
                current_node.add_child(new_node)
                current_node = new_node

        # Process fqons of the contained objects
        for nyan_object in nyan_file.nyan_objects:
            self.expand_from_object(nyan_object)",0.6700140542564248,0.7527895555274855,0.8630716859275425,0.98875605338216
170,"    def get_alias_fqon(self, fqon):
        """"""
        Find the (shortened) fqon by traversing the tree to the fqon node and
        then going upwards until a marked node is found.
        """"""
        # Traverse the tree downwards
        current_node = self.root
        for part in fqon:
            current_node = current_node.get_child(part)

        # Traverse the tree upwards
        sfqon = []
        while current_node.depth > 0:
            sfqon.insert(0, current_node.name)

            if current_node.alias:
                break

            current_node = current_node.parent

        return tuple(sfqon)",0.6347111540195717,0.985787166643852,0.8220170254958881,0.7947495587611263
171,"    def __init__(self, name, node_type, parent):
        """"""
        Create a node for an import tree.

        :param name: Name of the node.
        :type name: str
        :param node_type: Type of the node.
        :type node_type: NodeType
        :param parent: Parent node of this node.
        :type parent: Node
        """"""

        self.name = name
        self.node_type = node_type
        self.parent = parent

        if not self.parent and self.node_type is not NodeType.ROOT:
            raise Exception(""Only node with type ROOT are allowed to have no parent"")

        self.depth = 0
        if self.node_type is NodeType.ROOT:
            self.depth = 0

        else:
            self.depth = self.parent.depth + 1

        self.children = {}

        self.alias = False",0.7290104369764817,0.6801084969820577,0.7011783369339053,0.7191719374734069
172,"    def mark(self):
        """"""
        Mark this node as an alias node.
        """"""
        self.alias = True",0.5376907578520086,0.4650850315104294,0.4674498971936259,0.3624173752965741
173,"    def unmark(self):
        """"""
        Unmark this node as an alias node.
        """"""
        self.alias = False",0.4,0.3329839134971517,0.5920516647560696,0.6145201565452725
174,"    def _prepare_object_content(self, indent_depth, import_tree=None):
        """"""
        Returns a string containing the nyan object's content
        (members, nested objects).

        Subroutine of dump().
        """"""
        output_str = """"
        empty = True

        if len(self._inherited_members) > 0:
            for inherited_member in self._inherited_members:
                if inherited_member.has_value():
                    empty = False
                    output_str += ""%s%s\\n"" % (
                        (indent_depth + 1) * INDENT,
                        inherited_member.dump(import_tree=import_tree)
                    )
            if not empty:
                output_str += ""\\n""

        if len(self._members) > 0:
            empty = False
            for member in self._members:
                if self.is_patch():
                    # Patches do not need the type definition
                    output_str += ""%s%s\\n"" % (
                        (indent_depth + 1) * INDENT,
                        member.dump_short(import_tree=import_tree)
                    )
                else:
                    output_str += ""%s%s\\n"" % (
                        (indent_depth + 1) * INDENT,
                        member.dump(import_tree=import_tree)
                    )

            output_str += ""\\n""

        # Nested objects
        if len(self._nested_objects) > 0:
            empty = False
            for nested_object in self._nested_objects:
                output_str += ""%s%s"" % (
                    (indent_depth + 1) * INDENT,
                    nested_object.dump(
                        indent_depth + 1,
                        import_tree
                    )
                )

            output_str += """"

        # Empty objects need a 'pass' line
        if empty:
            output_str += f""{(indent_depth + 1) * INDENT}pass\\n\\n""

        return output_str",0.9956846720416516,0.2806560882251662,0.6649534684314489,0.6737577432355661
175,"    def _prepare_inheritance_content(self, import_tree=None):
        """"""
        Returns a string containing the nyan object's inheritance set
        in the header.

        Subroutine of dump().
        """"""
        output_str = ""(""

        if len(self._parents) > 0:
            for parent in self._parents:
                if import_tree:
                    sfqon = ""."".join(import_tree.get_alias_fqon(parent.get_fqon()))

                else:
                    sfqon = ""."".join(parent.get_fqon())

                output_str += f""{sfqon}, ""

            output_str = output_str[:-2]

        output_str += ""):\\n""

        return output_str",0.7074062675213142,0.6459151030682473,0.5346311994771271,0.5366149715146763
176,"    def dump(self, import_tree=None):
        """"""
        Returns the nyan string representation of the member.
        """"""
        output_str = f""{self.name}""

        type_str = """"

        if isinstance(self._member_type, NyanObject):
            if import_tree:
                sfqon = ""."".join(import_tree.get_alias_fqon(self._member_type.get_fqon()))

            else:
                sfqon = ""."".join(self._member_type.get_fqon())

            type_str = sfqon

        else:
            type_str = self._member_type.value

        if self._optional:
            output_str += f"" : optional({type_str})""

        else:
            output_str += f"" : {type_str}""

        if self.is_complex():
            if isinstance(self._set_type, NyanObject):
                if import_tree:
                    sfqon = ""."".join(import_tree.get_alias_fqon(self._set_type.get_fqon()))

                else:
                    sfqon = ""."".join(self._set_type.get_fqon())

                output_str += f""({sfqon})""

            else:
                output_str += f""({self._set_type.value})""

        if self.is_initialized():
            output_str += "" %s%s %s"" % (""@"" * self._override_depth,
                                        self._operator.value,
                                        self._get_str_representation(import_tree=import_tree))

        return output_str",0.9605590814805722,0.9586969045606094,0.6021582368826363,0.6548360502089863
177,"    def dump_short(self, import_tree=None):
        """"""
        Returns the nyan string representation of the member, but
        without the type definition.
        """"""
        return ""%s %s%s %s"" % (self.get_name(),
                               ""@"" * self._override_depth,
                               self._operator.value,
                               self._get_str_representation(import_tree=import_tree))",0.8829752629236378,0.8751856666456893,0.6188819965564609,0.558255368718984
178,"    def _get_primitive_value_str(self, member_type, value, import_tree=None):
        """"""
        Returns the nyan string representation of primitive values.

        Subroutine of _get_str_representation()
        """"""
        if member_type is MemberType.FLOAT:
            return f""{value}f""

        elif member_type in (MemberType.TEXT, MemberType.FILE):
            return f""\\""{value}\\""""

        elif isinstance(member_type, NyanObject):
            if import_tree:
                sfqon = ""."".join(import_tree.get_alias_fqon(value.get_fqon()))

            else:
                sfqon = ""."".join(value.get_fqon())

            return sfqon

        return f""{value}""",0.4,0.7643457356922292,0.74780348281896,0.880726549889951
179,"    def _get_str_representation(self, import_tree=None):
        """"""
        Returns the nyan string representation of the value.
        """"""
        if not self.is_initialized():
            return f""UNINITIALIZED VALUE {self.__repr__()}""

        if self._optional and self.value is MemberSpecialValue.NYAN_NONE:
            return MemberSpecialValue.NYAN_NONE.value

        if self.value is MemberSpecialValue.NYAN_INF:
            return MemberSpecialValue.NYAN_INF.value

        if self._member_type in (MemberType.INT, MemberType.FLOAT,
                                 MemberType.TEXT, MemberType.FILE,
                                 MemberType.BOOLEAN):
            return self._get_primitive_value_str(self._member_type,
                                                 self.value,
                                                 import_tree=import_tree)

        elif self._member_type in (MemberType.SET, MemberType.ORDEREDSET):
            output_str = """"

            if self._member_type is MemberType.ORDEREDSET:
                output_str += ""o""

            output_str += ""{""

            if len(self.value) > 0:
                for val in self.value:
                    output_str += ""%s, "" % self._get_primitive_value_str(
                        self._set_type,
                        val,
                        import_tree=import_tree
                    )

                return output_str[:-2] + ""}""

            return output_str + ""}""

        elif isinstance(self._member_type, NyanObject):
            if import_tree:
                sfqon = ""."".join(import_tree.get_alias_fqon(self.value.get_fqon()))

            else:
                sfqon = ""."".join(self.value.get_fqon())

            return sfqon

        else:
            raise Exception(f""{self.__repr__()} has no valid type"")",0.8796560274913529,0.858052648307558,0.8861551740108458,0.994535863509688
180,"    def dump(self, import_tree=None):
        """"""
        Returns the string representation of the member.
        """"""
        return self.dump_short(import_tree=import_tree)",0.5671549285603326,0.5531203452733935,0.7390546109802102,0.5073284495628575
181,"    def dump_short(self, import_tree=None):
        """"""
        Returns the nyan string representation of the member, but
        without the type definition.
        """"""
        return ""%s %s%s %s"" % (self.get_name_with_origin(),
                               ""@"" * self._override_depth,
                               self._operator.value,
                               self._get_str_representation(import_tree=import_tree))",0.7771744069405557,0.6747082303320052,0.4,0.7432534665599017
182,"def convert_assets(assets, args, srcdir=None, prev_source_dir_path=None):
    """"""
    Perform asset conversion.

    Requires original assets and stores them in usable and free formats.

    assets must be a filesystem-like object pointing at the game's asset dir.
    srcdir must be None, or point at some source directory.

    If gen_extra_files is True, some more files, mostly for debugging purposes,
    are created.

    This method prepares srcdir and targetdir to allow a pleasant, unified
    conversion experience, then passes them to .driver.convert().
    """"""
    # acquire conversion source directory
    if srcdir is None:
        srcdir = acquire_conversion_source_dir(prev_source_dir_path)

    converted_path = assets / ""converted""
    converted_path.mkdirs()
    targetdir = DirectoryCreator(converted_path).root

    # add a dir for debug info
    debug_log_path = converted_path / ""debug"" / datetime.now().strftime(""%Y-%m-%d-%H-%M-%S"")
    debugdir = DirectoryCreator(debug_log_path).root
    args.debugdir = AccessSynchronizer(debugdir).root

    # Create CLI args info
    debug_cli_args(args.debugdir, args.debug_info, args)

    # Initialize game versions data
    auxiliary_files_dir = args.cfg_dir / ""converter"" / ""games""
    args.avail_game_eds, args.avail_game_exps = create_version_objects(auxiliary_files_dir)

    # Acquire game version info
    args.game_version = get_game_version(srcdir, args.avail_game_eds, args.avail_game_exps)
    debug_game_version(args.debugdir, args.debug_info, args)

    # Mount assets into conversion folder
    data_dir = mount_asset_dirs(srcdir, args.game_version)
    if not data_dir:
        return None

    # make srcdir and targetdir safe for threaded conversion
    args.srcdir = AccessSynchronizer(data_dir).root
    args.targetdir = AccessSynchronizer(targetdir).root

    # Create mountpoint info
    debug_mounts(args.debugdir, args.debug_info, args)

    def flag(name):
        """"""
        Convenience function for accessing boolean flags in args.
        Flags default to False if they don't exist.
        """"""
        return getattr(args, name, False)

    args.flag = flag

    # import here so codegen.py doesn't depend on it.
    from .tool.driver import convert

    converted_count = 0
    total_count = None
    for current_item in convert(args):
        if isinstance(current_item, int):
            # convert is informing us about the estimated number of remaining
            # items.
            total_count = current_item + converted_count
            continue

        # TODO a GUI would be nice here.

        if total_count is None:
            info(""[%s] %s"", converted_count, current_item)
        else:
            info(""[%s] %s"", format_progress(converted_count, total_count), current_item)

        converted_count += 1

    # clean args
    del args.srcdir
    del args.targetdir

    return data_dir.resolve_native_path()",0.878560085336914,0.88891766296936,0.6315485504110142,0.7223458302167689
183,"def main(args, error):
    """""" CLI entry point """"""
    del error  # unused

    # initialize libopenage
    from ..cppinterface.setup import setup
    setup(args)

    # conversion source
    if args.source_dir is not None:
        srcdir = CaseIgnoringDirectory(args.source_dir).root
    else:
        srcdir = None

    # Set verbosity for debug output
    if not args.debug_info:
        if args.devmode:
            args.debug_info = 3

        else:
            args.debug_info = 0

    # mount the config folder at ""cfg/""
    from ..cvar.location import get_config_path
    from ..util.fslike.union import Union
    root = Union().root
    root[""cfg""].mount(get_config_path())
    args.cfg_dir = root[""cfg""]

    if args.interactive:
        interactive_browser(root[""cfg""], srcdir)
        return 0

    # conversion target
    from ..assets import get_asset_path
    outdir = get_asset_path(args.output_dir)

    if args.force or wanna_convert() or conversion_required(outdir, args):
        if not convert_assets(outdir, args, srcdir):
            return 1
    else:
        print(""assets are up to date; no conversion is required."")
        print(""override with --force."")

    return 0",0.9035884878752426,0.9617941652440612,0.975840270059708,0.925121009155418
184,"def main(args, error):
    """"""
    Makes sure that the assets have been converted,
    and jumps into the C++ main method.
    """"""
    del error  # unused

    # we have to import stuff inside the function
    # as it depends on generated/compiled code
    from .main_cpp import run_game
    from .. import config
    from ..assets import get_asset_path
    from ..convert.main import conversion_required, convert_assets
    from ..cppinterface.setup import setup as cpp_interface_setup
    from ..cvar.location import get_config_path
    from ..util.fslike.union import Union

    # initialize libopenage
    cpp_interface_setup(args)

    info(""launching openage %s"", config.VERSION)
    info(""compiled by %s"", config.COMPILER)

    if config.DEVMODE:
        info(""running in DEVMODE"")

    # create virtual file system for data paths
    root = Union().root

    # mount the assets folder union at ""assets/""
    root[""assets""].mount(get_asset_path(args.asset_dir))

    # mount the config folder at ""cfg/""
    root[""cfg""].mount(get_config_path(args.cfg_dir))

    # ensure that the assets have been converted
    if wanna_convert() or conversion_required(root[""assets""], args):
        # try to get previously used source dir
        asset_location_path = root[""cfg""] / ""asset_location""
        try:
            with asset_location_path.open(""r"") as file_obj:
                prev_source_dir_path = file_obj.read().strip()
        except FileNotFoundError:
            prev_source_dir_path = None
        used_asset_path = convert_assets(root[""assets""], root[""cfg""], args,
                                         prev_source_dir_path=prev_source_dir_path)
        if used_asset_path:
            # Remember the asset location
            with asset_location_path.open(""wb"") as file_obj:
                file_obj.write(used_asset_path)
        else:
            err(""game asset conversion failed"")
            return 1

    # start the game, continue in main_cpp.pyx!
    return run_game(args, root)",0.8196443346510864,0.8348199159697696,0.9904934392799396,0.844692488827842
185,"def main():
    """""" CLI entry point """"""
    args = parse_args()
    cppname = ""libopenage""
    cppdir = Path(cppname).absolute()
    out_cppdir = Path(args.output_dir) / cppname

    if args.verbose:
        hdr_count = len(args.all_files)
        plural = ""s"" if hdr_count > 1 else """"

        print(""extracting pxd information ""
              ""from {} header{}..."".format(hdr_count, plural))

    for filename in args.all_files:
        filename = Path(filename).resolve()
        if cppdir not in filename.parents:
            print(""pxdgen source file is not in "" + cppdir + "": "" + filename)
            sys.exit(1)

        # join out_cppdir with relative path from cppdir
        pxdfile_relpath = filename.with_suffix('.pxd').relative_to(cppdir)
        pxdfile = out_cppdir / pxdfile_relpath

        if args.verbose:
            print(""creating '{}' for '{}':"".format(pxdfile, filename))

        generator = PXDGenerator(filename)

        result = generator.generate(
            pxdfile,
            ignore_timestamps=args.ignore_timestamps,
            print_warnings=True
        )

        if args.verbose and not result:
            print(""nothing done."")

        # create empty __init__.py in all parent directories.
        # Cython requires this; else it won't find the .pxd files.
        for dirname in pxdfile_relpath.parents:
            template = out_cppdir / dirname / ""__init__""
            for extension in (""py"", ""pxd""):
                initfile = template.with_suffix(""."" + extension)
                if not initfile.exists():
                    print(""\\x1b[36mpxdgen: create package index %s\\x1b[0m"" % (
                        initfile.relative_to(CWD)))

                    initfile.touch()",0.8798393946835452,0.8492128455061809,0.9243594726209434,0.6909845376985643
186,"def main(argv=None):
    """""" Top-level argparsing; invokes subparser for all submodules. """"""
    cli = argparse.ArgumentParser(
        ""openage"",
        description=(""free age of empires II engine clone"")
    )

    cli.add_argument(""--version"", ""-V"", nargs=0, action=PrintVersion,
                     help=""print version info and exit"")

    # shared arguments for all subcommands
    global_cli = argparse.ArgumentParser(add_help=False)
    global_cli.add_argument(""--verbose"", ""-v"", action='count',
                            default=ENV_VERBOSITY,
                            help=""increase verbosity"")
    global_cli.add_argument(""--quiet"", ""-q"", action='count', default=0,
                            help=""decrease verbosity"")
    global_cli.add_argument(""--devmode"", action=""store_true"",
                            help=""force-enable development mode"")
    global_cli.add_argument(""--no-devmode"", action=""store_true"",
                            help=""force-disable devlopment mode"")
    global_cli.add_argument(""--trap-exceptions"", action=""store_true"",
                            help=(""upon throwing an exception a debug break is ""
                                  ""triggered. this will crash openage if no ""
                                  ""debugger is present""))

    # shared directory arguments for most subcommands
    cfg_cli = argparse.ArgumentParser(add_help=False)

    cfg_cli.add_argument(""--asset-dir"",
                         help=""Use this as an additional asset directory."")
    cfg_cli.add_argument(""--cfg-dir"",
                         help=""Use this as an additional config directory."")

    subparsers = cli.add_subparsers(dest=""subcommand"")

    # enable reimports for ""init_subparser""
    # pylint: disable=reimported

    from .game.main import init_subparser
    game_cli = subparsers.add_parser(
        ""game"",
        parents=[global_cli, cfg_cli])
    init_subparser(game_cli)

    from .testing.main import init_subparser
    init_subparser(subparsers.add_parser(
        ""test"",
        parents=[global_cli, cfg_cli]))

    from .convert.main import init_subparser
    init_subparser(subparsers.add_parser(
        ""convert"",
        parents=[global_cli, cfg_cli]))

    from .convert.singlefile import init_subparser
    init_subparser(subparsers.add_parser(
        ""convert-file"",
        parents=[global_cli, cfg_cli]))

    from .codegen.main import init_subparser
    init_subparser(subparsers.add_parser(
        ""codegen"",
        parents=[global_cli]))

    args = cli.parse_args(argv)

    if not args.subcommand:
        # the user didn't specify a subcommand. default to 'game'.
        args = game_cli.parse_args(argv)

    # process the shared args
    set_loglevel(verbosity_to_level(args.verbose - args.quiet))

    if args.no_devmode and args.devmode:
        cli.error(""can't force enable and disable devmode at the same time"")

    try:
        from . import config

        if args.no_devmode:
            config.DEVMODE = False
        if args.devmode:
            config.DEVMODE = True
    except ImportError:
        if args.no_devmode or args.devmode:
            print(""code was not yet generated, ignoring devmode activation request"")

    if ""asset_dir"" in args and args.asset_dir:
        if not os.path.exists(args.asset_dir):
            cli.error(""directory does not exist: "" + args.asset_dir)

    # call the entry point for the subcommand.
    return args.entrypoint(args, cli.error)",0.9242871394821202,0.9049907571835816,0.7269947620023205,0.9767487792225534
187,"def get_asset_path(args):
    """"""
    Returns a Path object for the game assets.

    args are the arguments, as provided by the CLI's ArgumentParser.
    """"""

    # if we're in devmode, use only the build source asset folder
    if not args.asset_dir and config.DEVMODE:
        return Directory(os.path.join(config.BUILD_SRC_DIR, ""assets"")).root

    # else, mount the possible locations in an union:
    # overlay the global dir and the user dir.
    result = Union().root

    # the cmake-determined folder for storing assets
    global_data = Path(config.GLOBAL_ASSET_DIR)
    if global_data.is_dir():
        result.mount(WriteBlocker(Directory(global_data).root).root)

    # user-data directory as provided by environment variables
    # and platform standards
    # we always create this!
    home_data = default_dirs.get_dir(""data_home"") / ""openage""
    result.mount(
        Directory(
            home_data,
            create_if_missing=True
        ).root / ""assets""
    )

    # the program argument overrides it all
    if args.asset_dir:
        result.mount(Directory(args.asset_dir).root)

    return result",0.9069883016738624,0.844344350342882,0.7664873109898669,0.972073875701676
188,"def interactive_browser(srcdir=None):
    """"""
    launch an interactive view for browsing the original
    archives.
    """"""

    info(""launching interactive data browser..."")

    # the variable is actually used, in the interactive prompt.
    # pylint: disable=unused-variable
    data, game_versions = mount_input(srcdir)

    if not data:
        warn(""cannot launch browser as no valid input assets were found."")
        return

    def save(path, target):
        """"""
        save a path to a custom target
        """"""
        with path.open(""rb"") as infile:
            with open(target, ""rb"") as outfile:
                outfile.write(infile.read())

    def save_slp(path, target, palette=None):
        """"""
        save a slp as png.
        """"""
        from .texture import Texture
        from .slp import SLP
        from .driver import get_palette

        if not palette:
            palette = get_palette(data, game_versions)

        with path.open(""rb"") as slpfile:
            tex = Texture(SLP(slpfile.read()), palette)

            out_path, filename = os.path.split(target)
            tex.save(Directory(out_path).root, filename)

    import code
    from pprint import pprint

    import rlcompleter

    completer = rlcompleter.Completer(locals())
    readline.parse_and_bind(""tab: complete"")
    readline.parse_and_bind(""set show-all-if-ambiguous on"")
    readline.set_completer(completer.complete)

    code.interact(
        banner=(""\\nuse `pprint` for beautiful output!\\n""
                ""you can access stuff by the `data` variable!\\n""
                ""`data` is an openage.util.fslike.path.Path!\\n""
                ""* list contents:      `pprint(list(data['graphics'].list()))`\\n""
                ""* dump data:          `save(data['file/path'], '/tmp/outputfile')`.\\n""
                ""* save a slp as png:  `save_slp(data['dir/123.slp'], '/tmp/pic.png')`.\\n""),
        local=locals()
    )",0.9457872927753138,0.8453656193412117,0.8187974844576256,0.9780246273789488
189,"    def save_slp(path, target, palette=None):
        """"""
        save a slp as png.
        """"""
        from .texture import Texture
        from .slp import SLP
        from .driver import get_palette

        if not palette:
            palette = get_palette(data, game_versions)

        with path.open(""rb"") as slpfile:
            tex = Texture(SLP(slpfile.read()), palette)

            out_path, filename = os.path.split(target)
            tex.save(Directory(out_path).root, filename)",0.719330049468474,0.7779339131317553,0.96216937777667,0.96216937777667
190,"def init_subparser(cli):
    """""" Initializes the parser for convert-specific args. """"""
    cli.set_defaults(entrypoint=main)

    cli.add_argument(
        ""--source-dir"", default=None,
        help=""source data directory"")

    cli.add_argument(
        ""--force"", action='store_true',
        help=""force conversion, even if up-to-date assets already exist."")

    cli.add_argument(
        ""--gen-extra-files"", action='store_true',
        help=""generate some extra files, useful for debugging the converter."")

    cli.add_argument(
        ""--no-media"", action='store_true',
        help=""do not convert any media files (slp, wav, ...)"")

    cli.add_argument(
        ""--no-metadata"", action='store_true',
        help=(""do not store any metadata ""
              ""(except for those associated with media files)""))

    cli.add_argument(
        ""--no-sounds"", action='store_true',
        help=""do not convert any sound files"")

    cli.add_argument(
        ""--no-graphics"", action='store_true',
        help=""do not convert game graphics"")

    cli.add_argument(
        ""--no-interface"", action='store_true',
        help=""do not convert interface graphics"")

    cli.add_argument(
        ""--no-scripts"", action='store_true',
        help=""do not convert scripts (AI and Random Maps)"")

    cli.add_argument(
        ""--no-pickle-cache"", action='store_true',
        help=""don't use a pickle file to skip the dat file reading."")

    cli.add_argument(
        ""--jobs"", ""-j"", type=int, default=None)

    cli.add_argument(
        ""--interactive"", ""-i"", action='store_true',
        help=""browse the files interactively"")

    cli.add_argument(
        ""--id"", type=int, default=None,
        help=""only convert files with this id (used for debugging..)"")",0.6142445352475654,0.6860596892926505,0.6287702179694866,0.6476702840915365
191,"def main(args, error):
    """""" CLI entry point """"""
    del error  # unused

    # initialize libopenage
    from ..cppinterface.setup import setup
    setup(args)

    # conversion source
    if args.source_dir is not None:
        srcdir = CaseIgnoringDirectory(args.source_dir).root
    else:
        srcdir = None

    if args.interactive:
        interactive_browser(srcdir)
        return 0

    # conversion target
    from ..assets import get_asset_path
    assets = get_asset_path(args)

    if args.force or conversion_required(assets, args):
        if not convert_assets(assets, args, srcdir):
            return 1
    else:
        print(""assets are up to date; no conversion is required."")
        print(""override with --force."")",0.915269932101267,0.8653519291004148,0.6819687720301342,0.682273614731391
192,"def init_subparser(cli):
    """""" Initializes the parser for convert-specific args. """"""
    cli.set_defaults(entrypoint=main)

    cli.add_argument(""--palette"", default=""50500"", help=""palette number"")
    cli.add_argument(""--interfac"", help=(""drs archive where palette ""
                                         ""is contained (interfac.drs)""))
    cli.add_argument(""drs"", help=(""drs archive filename that contains the slp ""
                                  ""e.g. path ~/games/aoe/graphics.drs""))
    cli.add_argument(""slp"", help=(""slp filename inside the drs archive ""
                                  ""e.g. 326.slp""))
    cli.add_argument(""output"", help=""image output path name"")",0.684471587722254,0.7154954794727812,0.6524611342591359,0.6953846674800415
193,"def main(args, error):
    """""" CLI entry point for single file conversions """"""
    del error  # unused

    if args.interfac:
        interfacfile = args.interfac
    else:
        # if no interfac was given, assume
        # the same path of the drs archive.
        drspath = os.path.split(args.drs)[0]
        interfacfile = os.path.join(drspath, ""interfac.drs"")

    # if .png was passed, strip it away.
    if args.output.endswith("".png""):
        args.output = args.output[:-4]

    # here, try opening slps from interfac or whereever
    info(""opening slp in drs '%s:%s'..."" % (args.drs, args.slp))
    slpfile = DRS(open(args.drs, ""rb"")).root[args.slp].open(""rb"")

    # import here to prevent that the __main__ depends on SLP
    # just by importing this singlefile.py.
    from .slp import SLP

    # parse the slp image
    info(""parsing slp image..."")
    slpimage = SLP(slpfile.read())

    # open color palette
    info(""opening palette in drs '%s:%s.bina'..."" % (interfacfile,
                                                     args.palette))
    palettefile = DRS(open(interfacfile, ""rb"")).\\
        root[""%s.bina"" % args.palette].open(""rb"")
    info(""parsing palette data..."")
    palette = ColorTable(palettefile.read())

    # create texture
    info(""packing texture..."")
    tex = Texture(slpimage, palette)

    # to save as png:
    path, filename = os.path.split(args.output)
    tex.save(Directory(path).root, filename)",0.96496365711924,0.9766523331038384,0.7374477984640159,0.9008505467456052
194,"    def save(self, targetdir, filename, meta_formats=None):
        """"""
        Store the image data into the target directory path,
        with given filename=""dir/out.png""
        If metaformats are requested, export e.g. as ""dir/out.docx"".
        """"""
        if not isinstance(targetdir, Path):
            raise ValueError(""util.fslike Path expected as targetdir"")
        if not isinstance(filename, str):
            raise ValueError(""str expected as filename, not %s"" % type(filename))

        basename, ext = os.path.splitext(filename)

        if ext != "".png"":
            raise ValueError(""Texture must be saved as name.png. got: %s"" % filename)

        # without the dot
        ext = ext[1:]

        # generate PNG file
        with targetdir[filename].open(""wb"") as imagefile:
            self.image_data.get_pil_image().save(imagefile, ext)

        if meta_formats:
            # generate formatted texture metadata
            formatter = data_formatter.DataFormatter()
            formatter.add_data(self.dump(basename))
            formatter.export(targetdir, meta_formats)",0.9206288141426728,0.9105836390640264,0.7210520853340727,0.982623315747744
195,"def get_config_path(args):
    """"""
    Locates the main configuration file by name in some searchpaths.
    """"""

    # if we're in devmode, use only the build source config folder
    if config.DEVMODE:
        return Directory(os.path.join(config.BUILD_SRC_DIR, ""cfg"")).root

    # else, mount the possible locations in an union
    # to overlay the global dir and the user dir.
    result = Union().root

    # mount the global config dir
    # we don't use xdg config_dirs because that would be /etc/xdg/openage
    # and nobody would ever look in there.
    global_configs = pathlib.Path(config.GLOBAL_CONFIG_DIR)
    if global_configs.is_dir():
        result.mount(WriteBlocker(Directory(global_configs).root).root)

    # then the per-user config dir (probably ~/.config/openage)
    home_cfg = default_dirs.get_dir(""config_home"") / ""openage""
    result.mount(
        Directory(
            home_cfg,
            create_if_missing=True
        ).root
    )

    # the optional command line argument overrides it all
    if args.cfg_dir:
        result.mount(Directory(args.cfg_dir).root)

    return result",0.820783249228359,0.6895909190056064,0.7930041002122352,0.9868047954193035
196,"def main(args, error):
    """"""
    Makes sure that the assets have been converted,
    and jumps into the C++ main method.
    """"""
    del error  # unused

    # we have to import stuff inside the function
    # as it depends on generated/compiled code
    from .main_cpp import run_game
    from .. import config
    from ..assets import get_asset_path
    from ..convert.main import conversion_required, convert_assets
    from ..cppinterface.setup import setup as cpp_interface_setup
    from ..cvar.location import get_config_path
    from ..util.fslike.union import Union

    # initialize libopenage
    cpp_interface_setup(args)

    info(""launching openage {}"".format(config.VERSION))
    info(""compiled by {}"".format(config.COMPILER))

    if config.DEVMODE:
        info(""running in DEVMODE"")

    # create virtual file system for data paths
    root = Union().root

    # mount the assets folder union at ""assets/""
    root[""assets""].mount(get_asset_path(args))

    # mount the config folder at ""cfg/""
    root[""cfg""].mount(get_config_path(args))

    # ensure that the assets have been converted
    if conversion_required(root[""assets""], args):
        if not convert_assets(root[""assets""], args):
            err(""game asset conversion failed"")
            return 1

    # start the game, continue in main_cpp.pyx!
    return run_game(args, root)",0.9163472637715784,0.9082144416508208,0.941001299923934,0.833604101057005
197,"def get_string_resources(args):
    """""" reads the (language) string resources """"""
    from .stringresource import StringResource
    stringres = StringResource()

    srcdir = args.srcdir
    count = 0

    # AoK:TC uses .DLL PE files for its string resources,
    # HD uses plaintext files
    if srcdir[""language.dll""].is_file():
        from .pefile import PEFile
        for name in [""language.dll"", ""language_x1.dll"", ""language_x1_p1.dll""]:
            pefile = PEFile(srcdir[name].open('rb'))
            stringres.fill_from(pefile.resources().strings)
            count += 1
    elif GameVersion.age2_fe in args.game_versions:
        from .hdlanguagefile import read_hd_language_file

        for lang in srcdir[""resources""].list():
            try:
                if lang == b'_common':
                    continue
                langfilename = [""resources"", lang.decode(), ""strings"", ""key-value"",
                                ""key-value-strings-utf8.txt""]

                with srcdir[langfilename].open('rb') as langfile:
                    stringres.fill_from(read_hd_language_file(langfile, lang))

                count += 1
            except FileNotFoundError:
                # that's fine, there are no language files for every language.
                pass
    elif GameVersion.age2_hd_3x in args.game_versions:
        from .hdlanguagefile import read_hd_language_file

        # HD Edition 3.x and below store language .txt files in the Bin/ folder.
        # Specific language strings are in Bin/$LANG/*.txt.
        for lang in srcdir[""bin""].list():
            dirname = [""bin"", lang.decode()]

            # There are some .txt files immediately in bin/, but they don't seem
            # to contain anything useful. (Everything is overridden by files in
            # Bin/$LANG/.)
            if not srcdir[dirname].is_dir():
                continue

            for basename in srcdir[dirname].list():
                langfilename = [""bin"", lang.decode(), basename]
                with srcdir[langfilename].open('rb') as langfile:
                    # No utf-8 :(
                    stringres.fill_from(read_hd_language_file(langfile, lang, enc='iso-8859-1'))
                count += 1

    if not count:
        raise FileNotFoundError(""could not find any language files"")

    # TODO transform and cleanup the read strings:
    #      convert formatting indicators from HTML to something sensible, etc

    return stringres",0.685002438677672,0.5875269605860589,0.7626725609811894,0.7243567088433448
198,"def get_domain_mx_list(domain):
    """"""Return a list of MX IP address for domain.""""""
    result = []
    logger = logging.getLogger(""modoboa.admin"")
    dns_server = param_tools.get_global_parameter(""custom_dns_server"")
    if dns_server:
        resolver = dns.resolver.Resolver()
        resolver.nameservers = [dns_server]
    else:
        resolver = dns.resolver

    try:
        dns_answers = resolver.query(domain, ""MX"")
    except dns.resolver.NXDOMAIN as e:
        logger.error(_(""No DNS records found for %s"") % domain, exc_info=e)
    except dns.resolver.NoAnswer as e:
        logger.error(_(""No MX record for %s"") % domain, exc_info=e)
    except dns.resolver.NoNameservers as e:
        logger.error(_(""No working name servers found""), exc_info=e)
    except dns.resolver.Timeout as e:
        logger.warning(
            _(""DNS resolution timeout, unable to query %s at the moment"") %
            domain, exc_info=e)
    else:
        for dns_answer in dns_answers:
            for rtype in [""A"", ""AAAA""]:
                try:
                    mx_domain = dns_answer.exchange.to_unicode(
                        omit_final_dot=True, idna_codec=IDNA_2008_UTS_46)
                    ip_answers = resolver.query(mx_domain, rtype)
                except dns.resolver.NXDOMAIN as e:
                    logger.error(
                        _(""No {type} record found for MX {mx}"").format(
                            type=rtype, mx=domain), exc_info=e)
                else:
                    for ip_answer in ip_answers:
                        try:
                            address_smart = smart_text(ip_answer.address)
                            mx_ip = ipaddress.ip_address(address_smart)
                        except ValueError as e:
                            logger.warning(
                                _(""Invalid IP address format for ""
                                  ""{domain}; {addr}"").format(
                                      domain=mx_domain,
                                      addr=smart_text(ip_answer.address)
                                  ), exc_info=e)
                        else:
                            result.append((mx_domain, mx_ip))
    return result",0.7369743437912764,0.6366987104312576,0.8081753213196512,0.751716369814067
199,"    def authenticate(self, username=None, password=None):
        """"""Check the username/password and return a User.""""""
        host = getattr(settings, ""AUTH_SMTP_SERVER_ADDRESS"", ""localhost"")
        port = getattr(settings, ""AUTH_SMTP_SERVER_PORT"", 25)
        secured_mode = getattr(settings, ""AUTH_SMTP_SECURED_MODE"", None)
        if secured_mode == ""ssl"":
            smtp = smtplib.SMTP_SSL(host, port)
        else:
            smtp = smtplib.SMTP(host, port)
            if secured_mode == ""starttls"":
                smtp.starttls()
        try:
            smtp.login(username, password)
        except smtplib.SMTPException:
            return None
        return self.get_or_build_user(username)",0.4,0.7080323554387599,0.6556060101286578,0.920811436454587
200,"        def authenticate(self, username, password):
            if self.global_params[""authentication_type""] == ""ldap"":
                return super(LDAPBackend, self).authenticate(
                    username=username, password=password)
            return None",0.9157775889281996,0.8580860020881889,0.905631568661627,0.8514123119136567
